,title,abstract
0,"Supporting Knowledge and Expertise Finding within Australia's Defence
  Science and Technology Organisation","  This paper reports on work aimed at supporting knowledge and expertise
finding within a large Research and Development (R&D) organisation. The paper
first discusses the nature of knowledge important to R&D organisations and
presents a prototype information system developed to support knowledge and
expertise finding. The paper then discusses a trial of the system within an R&D
organisation, the implications and limitations of the trial, and discusses
future research questions.
"
1,"Performance Analysis of the IEEE 802.11e Enhanced Distributed
  Coordination Function using Cycle Time Approach","  The recently ratified IEEE 802.11e standard defines the Enhanced Distributed
Channel Access (EDCA) function for Quality-of-Service (QoS) provisioning in the
Wireless Local Area Networks (WLANs). The EDCA uses Carrier Sense Multiple
Access with Collision Avoidance (CSMA/CA) and slotted Binary Exponential
Backoff (BEB) mechanism. We present a simple mathematical analysis framework
for the EDCA function. Our analysis considers the fact that the distributed
random access systems exhibit cyclic behavior where each station successfully
transmits a packet in a cycle. Our analysis shows that an AC-specific cycle
time exists for the EDCA function. Validating the theoretical results via
simulations, we show that the proposed analysis accurately captures EDCA
saturation performance in terms of average throughput, medium access delay, and
packet loss ratio. The cycle time analysis is a simple and insightful
substitute for previously proposed more complex EDCA models.
"
2,Fairness Provision in the IEEE 802.11e Infrastructure Basic Service Set,"  Most of the deployed IEEE 802.11e Wireless Local Area Networks (WLANs) use
infrastructure Basic Service Set (BSS) in which an Access Point (AP) serves as
a gateway between wired and wireless domains. We present the unfairness problem
between the uplink and the downlink flows of any Access Category (AC) in the
802.11e Enhanced Distributed Channel Access (EDCA) when the default settings of
the EDCA parameters are used. We propose a simple analytical model to calculate
the EDCA parameter settings that achieve weighted fair resource allocation for
all uplink and downlink flows. We also propose a simple model-assisted
measurement-based dynamic EDCA parameter adaptation algorithm. Moreover, our
dynamic solution addresses the differences in the transport layer and the
Medium Access Control (MAC) layer interactions of User Datagram Protocol (UDP)
and Transmission Control Protocol (TCP). We show that proposed Contention
Window (CW) and Transmit Opportunity (TXOP) limit adaptation at the AP provides
fair UDP and TCP access between uplink and downlink flows of the same AC while
preserving prioritization among ACs.
"
3,Kekul\'e Cells for Molecular Computation,"  The configurations of single and double bonds in polycyclic hydrocarbons are
abstracted as Kekul\'e states of graphs. Sending a so-called soliton over an
open channel between ports (external nodes) of the graph changes the Kekul\'e
state and therewith the set of open channels in the graph. This switching
behaviour is proposed as a basis for molecular computation. The proposal is
highly speculative but may have tremendous impact.
  Kekul\'e states with the same boundary behaviour (port assignment) can be
regarded as equivalent. This gives rise to the abstraction of Kekul\'e cells.
The basic theory of Kekul\'e states and Kekul\'e cells is developed here, up to
the classification of Kekul\'e cells with $\leq 4$ ports. To put the theory in
context, we generalize Kekul\'e states to semi-Kekul\'e states, which form the
solutions of a linear system of equations over the field of the bits 0 and 1.
We briefly study so-called omniconjugated graphs, in which every port
assignment of the right signature has a Kekul\'e state. Omniconjugated graphs
may be useful as connectors between computational elements. We finally
investigate some examples with potentially useful switching behaviour.
"
4,"The Use of ITIL for Process Optimisation in the IT Service Centre of
  Harz University, exemplified in the Release Management Process","  This paper details the use of the IT Infrastructure Library Framework (ITIL)
for optimising process workflows in the IT Service Centre of Harz University in
Wernigerode, Germany, exemplified by the Release Management Process. It is
described, how, during the course of a special ITIL project, the As-Is-Status
of the various original processes was documented as part of the process life
cycle and then transformed in the To-Be-Status, according to the ITIL Best
Practice Framework. It is also shown, how the ITIL framework fits into the
four-layered-process model, that could be derived from interviews with the
universities IT support staff, and how the various modified processes
interconnect with each other to form a value chain. The paper highlights the
final results of the project and gives an outlook on the future use of ITIL as
a business modelling tool in the IT Service Centre of Harz University. It is
currently being considered, whether the process model developed during the
project could be used as a reference model for other university IT centres.
"
5,Collaborative product and process model: Multiple Viewpoints approach,"  The design and development of complex products invariably involves many
actors who have different points of view on the problem they are addressing,
the product being developed, and the process by which it is being developed.
The actors' viewpoints approach was designed to provide an organisational
framework in which these different perspectives or points of views, and their
relationships, could be explicitly gathered and formatted (by actor activity's
focus). The approach acknowledges the inevitability of multiple interpretation
of product information as different views, promotes gathering of actors'
interests, and encourages retrieved adequate information while providing
support for integration through PLM and/or SCM collaboration. In this paper, we
present our multiple viewpoints approach, and we illustrate it by an industrial
example on cyclone vessel product.
"
6,Remote laboratories: new technology and standard based architecture,"  E-Laboratories are important components of e- learning environments,
especially in scientific and technical disciplines. First widespread E-Labs
consisted in proposing simulations of real systems (virtual labs), as building
remote labs (remote control of real systems) was difficult by lack of
industrial standards and common protocols. Nowadays, robotics and automation
technologies make easier the interfacing of systems with computers. In this
frame, many researchers (such as those mentioned in [1]) focus on how to set up
such a remote control. But, only a few of them deal with the educational point
of view of the problem. This paper outlines our current research and reflection
about remote laboratory modelling.
"
7,Hypocomputation,"  Hypercomputational formal theories will, clearly, be both structurally and
foundationally different from the formal theories underpinning computational
theories. However, many of the maps that might guide us into this strange realm
have been lost. So little work has been done recently in the area of
metamathematics, and so many of the previous results have been folded into
other theories, that we are in danger of loosing an appreciation of the broader
structure of formal theories. As an aid to those looking to develop
hypercomputational theories, we will briefly survey the known landmarks both
inside and outside the borders of computational theory. We will not focus in
this paper on why the structure of formal theory looks the way it does. Instead
we will focus on what this structure looks like, moving from hypocomputational,
through traditional computational theories, and then beyond to
hypercomputational theories.
"
8,Hilbert++ Manual,"  We present here an installation guide, a hand-on mini-tutorial through
examples, and the theoretical foundations of the Hilbert++ code.
"
9,2-State 3-Symbol Universal Turing Machines Do Not Exist,"  In this brief note, we give a simple information-theoretic proof that 2-state
3-symbol universal Turing machines cannot possibly exist, unless one loosens
the definition of ""universal"".
"
10,"Performance of Linear Field Reconstruction Techniques with Noise and
  Uncertain Sensor Locations","  We consider a wireless sensor network, sampling a bandlimited field,
described by a limited number of harmonics. Sensor nodes are irregularly
deployed over the area of interest or subject to random motion; in addition
sensors measurements are affected by noise. Our goal is to obtain a high
quality reconstruction of the field, with the mean square error (MSE) of the
estimate as performance metric. In particular, we analytically derive the
performance of several reconstruction/estimation techniques based on linear
filtering. For each technique, we obtain the MSE, as well as its asymptotic
expression in the case where the field number of harmonics and the number of
sensors grow to infinity, while their ratio is kept constant. Through numerical
simulations, we show the validity of the asymptotic analysis, even for a small
number of sensors. We provide some novel guidelines for the design of sensor
networks when many parameters, such as field bandwidth, number of sensors,
reconstruction quality, sensor motion characteristics, and noise level of the
measures, have to be traded off.
"
11,Bandlimited Field Reconstruction for Wireless Sensor Networks,"  Wireless sensor networks are often used for environmental monitoring
applications. In this context sampling and reconstruction of a physical field
is one of the most important problems to solve. We focus on a bandlimited field
and find under which conditions on the network topology the reconstruction of
the field is successful, with a given probability. We review irregular sampling
theory, and analyze the problem using random matrix theory. We show that even a
very irregular spatial distribution of sensors may lead to a successful signal
reconstruction, provided that the number of collected samples is large enough
with respect to the field bandwidth. Furthermore, we give the basis to
analytically determine the probability of successful field reconstruction.
"
12,"Design of Multistage Decimation Filters Using Cyclotomic Polynomials:
  Optimization and Design Issues","  This paper focuses on the design of multiplier-less decimation filters
suitable for oversampled digital signals. The aim is twofold. On one hand, it
proposes an optimization framework for the design of constituent decimation
filters in a general multistage decimation architecture. The basic building
blocks embedded in the proposed filters belong, for a simple reason, to the
class of cyclotomic polynomials (CPs): the first 104 CPs have a z-transfer
function whose coefficients are simply {-1,0,+1}. On the other hand, the paper
provides a bunch of useful techniques, most of which stemming from some key
properties of CPs, for designing the proposed filters in a variety of
architectures. Both recursive and non-recursive architectures are discussed by
focusing on a specific decimation filter obtained as a result of the
optimization algorithm.
  Design guidelines are provided with the aim to simplify the design of the
constituent decimation filters in the multistage chain.
"
13,"On the Polyphase Decomposition for Design of Generalized Comb Decimation
  Filters","  Generalized comb filters (GCFs) are efficient anti-aliasing decimation
filters with improved selectivity and quantization noise (QN) rejection
performance around the so called folding bands with respect to classical comb
filters.
  In this paper, we address the design of GCF filters by proposing an efficient
partial polyphase architecture with the aim to reduce the data rate as much as
possible after the Sigma-Delta A/D conversion. We propose a mathematical
framework in order to completely characterize the dependence of the frequency
response of GCFs on the quantization of the multipliers embedded in the
proposed filter architecture. This analysis paves the way to the design of
multiplier-less decimation architectures.
  We also derive the impulse response of a sample 3rd order GCF filter used as
a reference scheme throughout the paper.
"
14,RS-232 Led Board,"  This article demonstrates how to develop a Microchip PIC16F84 based device
that supports RS-232 interface with PC. Circuit (LED Board) design and software
development will be discussed. PicBasic Pro Compiler from microEngineering
Labs, Inc. is used for PIC programming. Development of LED Board Control
Console using C/C++ is also briefly discussed. The project requires basic work
experience with Microchip PICs, serial communication and programming.
"
15,"Products of irreducible random matrices in the (Max,+) Algebra","  We consider the recursive equation ``x(n+1)=A(n)x(n)'' where x(n+1) and x(n)
are column vectors of size k and where A(n) is an irreducible random matrix of
size k x k. The matrix-vector multiplication in the (max,+) algebra is defined
by (A(n)x(n))_i= max_j [ A(n)_{ij} +x(n)_j ]. This type of equation can be used
to represent the evolution of Stochastic Event Graphs which include cyclic
Jackson Networks, some manufacturing models and models with general blocking
(such as Kanban). Let us assume that the sequence (A(n))_n is i.i.d or more
generally stationary and ergodic. The main result of the paper states that the
system couples in finite time with a unique stationary regime if and only if
there exists a set of matrices C such that P {A(0) in C} > 0, and the matrices
in C have a unique periodic regime.
"
16,"Further Comments on ""Residue-to-Binary Converters Based on New Chinese
  Remainder Theorems""","  Ananda Mohan suggested that the first New Chinese Remainder Theorem
introduced by Wang can be derived from the constructive proof of the well-known
Chinese Remainder Theorem (CRT) and claimed that Wang's approach is the same as
the one proposed earlier by Huang. Ananda Mohan's proof is however erroneous
and we show here that Wang's New CRT I is a rewriting of an algorithm
previously sketched by Hitz and Kaltofen.
"
17,"Reductionism, emergence, and levels of abstractions","  Can there be independent higher level laws of nature if everything is
reducible to the fundamental laws of physics? The computer science notion of
level of abstraction explains why there can -- illustrating how computational
thinking can solve one of philosophy's most vexing problems.
"
18,"Blocking a transition in a Free Choice net and what it tells about its
  throughput","  In a live and bounded Free Choice Petri net, pick a non-conflicting
transition. Then there exists a unique reachable marking in which no transition
is enabled except the selected one. For a routed live and bounded Free Choice
net, this property is true for any transition of the net. Consider now a live
and bounded stochastic routed Free Choice net, and assume that the routings and
the firing times are independent and identically distributed. Using the above
results, we prove the existence of asymptotic firing throughputs for all
transitions in the net. Furthermore the vector of the throughputs at the
different transitions is explicitly computable up to a multiplicative constant.
"
19,"Modeling Context, Collaboration, and Civilization in End-User
  Informatics","  End-user informatics applications are Internet data web management automation
solutions. These are mass modeling and mass management collaborative communal
consensus solutions. They are made and maintained by managerial, professional,
technical and specialist end-users. In end-user informatics the end-users are
always right. So it becomes necessary for information technology professionals
to understand information and informatics from the end-user perspective.
End-user informatics starts with the observation that practical prose is a mass
consensus communal modeling technology. This high technology is the mechanistic
modeling medium we all use every day in all of our practical pursuits.
Practical information flows are the lifeblood of modern capitalist communities.
But what exactly is practical information? It's ultimately physical
information, but the physics is highly emergent rather than elementary. So
practical reality is just physical reality in deep disguise. Practical prose is
the medium that we all use to model the everyday and elite mechanics of
practical reality. So this is the medium that end-user informatics must
automate and animate.
"
20,"The Accidental Detection Index as a Fault Ordering Heuristic for
  Full-Scan Circuits","  We investigate a new fault ordering heuristic for test generation in
full-scan circuits. The heuristic is referred to as the accidental detection
index. It associates a value ADI (f) with every circuit fault f. The heuristic
estimates the number of faults that will be detected by a test generated for f.
Fault ordering is done such that a fault with a higher accidental detection
index appears earlier in the ordered fault set and targeted earlier during test
generation. This order is effective for generating compact test sets, and for
obtaining a test set with a steep fault coverage curve. Such a test set has
several applications. We present experimental results to demonstrate the
effectiveness of the heuristic.
"
21,Modeling and Propagation of Noisy Waveforms in Static Timing Analysis,"  A technique based on the sensitivity of the output to input waveform is
presented for accurate propagation of delay information through a gate for the
purpose of static timing analysis (STA) in the presence of noise. Conventional
STA tools represent a waveform by its arrival time and slope. However, this is
not an accurate way of modeling the waveform for the purpose of noise analysis.
The key contribution of our work is the development of a method that allows
efficient propagation of equivalent waveforms throughout the circuit.
Experimental results demonstrate higher accuracy of the proposed
sensitivity-based gate delay propagation technique, SGDP, compared to the best
of existing approaches. SGDP is compatible with the current level of gate
characterization in conventional ASIC cell libraries, and as a result, it can
be easily incorporated into commercial STA tools to improve their accuracy.
"
22,RIP: An Efficient Hybrid Repeater Insertion Scheme for Low Power,"  This paper presents a novel repeater insertion algorithm for interconnect
power minimization. The novelty of our approach is in the judicious integration
of an analytical solver and a dynamic programming based method. Specifically,
the analytical solver chooses a concise repeater library and a small set of
repeater location candidates such that the dynamic programming algorithm can be
performed fast with little degradation of the solution quality. In comparison
with previously reported repeater insertion schemes, within comparable
runtimes, our approach achieves up to 37% higher power savings. Moreover, for
the same design quality, our scheme attains a speedup of two orders of
magnitude.
"
23,HEBS: Histogram Equalization for Backlight Scaling,"  In this paper, a method is proposed for finding a pixel transformation
function that maximizes backlight dimming while maintaining a pre-specified
image distortion level for a liquid crystal display. This is achieved by
finding a pixel transformation function, which maps the original image
histogram to a new histogram with lower dynamic range. Next the contrast of the
transformed image is enhanced so as to compensate for brightness loss that
would arise from backlight dimming. The proposed approach relies on an accurate
definition of the image distortion which takes into account both the pixel
value differences and a model of the human visual system and is amenable to
highly efficient hardware realization. Experimental results show that the
histogram equalization for backlight scaling method results in about 45% power
saving with an effective distortion rate of 5% and 65% power saving for a 20%
distortion rate. This is significantly higher power savings compared to
previously reported backlight dimming approaches.
"
24,Noise Figure Evaluation Using Low Cost BIST,"  A technique for evaluating noise figure suitable for BIST implementation is
described. It is based on a low cost single-bit digitizer, which allows the
simultaneous evaluation of noise figure in several test points of the analog
circuit. The method is also able to benefit from SoC resources, like memory and
processing power. Theoretical background and experimental results are presented
in order to demonstrate the feasibility of the approach.
"
25,Efficient Feasibility Analysis for Real-Time Systems with EDF Scheduling,"  This paper presents new fast exact feasibility tests for uniprocessor
real-time systems using preemptive EDF scheduling. Task sets which are accepted
by previously described sufficient tests will be evaluated in nearly the same
time as with the old tests by the new algorithms. Many task sets are not
accepted by the earlier tests despite them beeing feasible. These task sets
will be evaluated by the new algorithms a lot faster than with known exact
feasibility tests. Therefore it is possible to use them for many applications
for which only sufficient test are suitable. Additionally this paper shows that
the best previous known sufficient test, the best known feasibility bound and
the best known approximation algorithm can be derived from these new tests. In
result this leads to an integrated schedulability theory for EDF.
"
26,Q-DPM: An Efficient Model-Free Dynamic Power Management Technique,"  When applying Dynamic Power Management (DPM) technique to pervasively
deployed embedded systems, the technique needs to be very efficient so that it
is feasible to implement the technique on low end processor and tight-budget
memory. Furthermore, it should have the capability to track time varying
behavior rapidly because the time varying is an inherent characteristic of real
world system. Existing methods, which are usually model-based, may not satisfy
the aforementioned requirements. In this paper, we propose a model-free DPM
technique based on Q-Learning. Q-DPM is much more efficient because it removes
the overhead of parameter estimator and mode-switch controller. Furthermore,
its policy optimization is performed via consecutive online trialing, which
also leads to very rapid response to time varying behavior.
"
27,A New Approach to Component Testing,"  Carefully tested electric/electronic components are a requirement for
effective hardware-in-the-loop tests and vehicle tests in automotive industry.
A new method for definition and execution of component tests is described. The
most important advantage of this method is independance from the test stand. It
therefore offers the oppportunity to build up knowledge over a long period of
time and the ability to share this knowledge with different partners.
"
28,Embedded Automotive System Development Process,"  Model based design enables the automatic generation of final-build software
from models for high-volume automotive embedded systems. This paper presents a
framework of processes, methods and tools for the design of automotive embedded
systems. A steer-by-wire system serves as an example.
"
29,"An Iterative Algorithm for Battery-Aware Task Scheduling on Portable
  Computing Platforms","  In this work we consider battery powered portable systems which either have
Field Programmable Gate Arrays (FPGA) or voltage and frequency scalable
processors as their main processing element. An application is modeled in the
form of a precedence task graph at a coarse level of granularity. We assume
that for each task in the task graph several unique design-points are available
which correspond to different hardware implementations for FPGAs and different
voltage-frequency combinations for processors. It is assumed that performance
and total power consumption estimates for each design-point are available for
any given portable platfrom, including the peripheral components such as memory
and display power usage. We present an iterative heuristic algorithm which
finds a sequence of tasks along with an appropriate design-point for each task,
such that a deadline is met and the amount of battery energy used is as small
as possible. A detailed illustrative example along with a case study of a
real-world application of a robotic arm controller which demonstrates the
usefulness of our algorithm is also presented.
"
30,"Exploiting Dynamic Workload Variation in Low Energy Preemptive Task
  Scheduling","  A novel energy reduction strategy to maximally exploit the dynamic workload
variation is proposed for the offline voltage scheduling of preemptive systems.
The idea is to construct a fully-preemptive schedule that leads to minimum
energy consumption when the tasks take on approximately the average execution
cycles yet still guarantees no deadline violation during the worst-case
scenario. End-time for each sub-instance of the tasks obtained from the
schedule is used for the on-line dynamic voltage scaling (DVS) of the tasks.
For the tasks that normally require a small number of cycles but occasionally a
large number of cycles to complete, such a schedule provides more opportunities
for slack utilization and hence results in larger energy saving. The concept is
realized by formulating the problem as a Non-Linear Programming (NLP)
optimization problem. Experimental results show that, by using the proposed
scheme, the total energy consumption at runtime is reduced by as high as 60%
for randomly generated task sets when comparing with the static scheduling
approach only using worst case workload.
"
31,Rapid Generation of Thermal-Safe Test Schedules,"  Overheating has been acknowledged as a major issue in testing complex SOCs.
Several power constrained system-level DFT solutions (power constrained test
scheduling) have recently been proposed to tackle this problem. However, as it
will be shown in this paper, imposing a chip-level maximum power constraint
doesn't necessarily avoid local overheating due to the non-uniform distribution
of power across the chip. This paper proposes a new approach for dealing with
overheating during test, by embedding thermal awareness into test scheduling.
The proposed approach facilitates rapid generation of thermal-safer test
schedules without requiring time-consuming thermal simulations. This is
achieved by employing a low-complexity test session thermal model used to guide
the test schedule generation algorithm. This approach reduces the chances of a
design re-spin due to potential overheating during test.
"
32,System Synthesis for Networks of Programmable Blocks,"  The advent of sensor networks presents untapped opportunities for synthesis.
We examine the problem of synthesis of behavioral specifications into networks
of programmable sensor blocks. The particular behavioral specification we
consider is an intuitive user-created network diagram of sensor blocks, each
block having a pre-defined combinational or sequential behavior. We synthesize
this specification to a new network that utilizes a minimum number of
programmable blocks in place of the pre-defined blocks, thus reducing network
size and hence network cost and power. We focus on the main task of this
synthesis problem, namely partitioning pre-defined blocks onto a minimum number
of programmable blocks, introducing the efficient but effective PareDown
decomposition algorithm for the task. We describe the synthesis and simulation
tools we developed. We provide results showing excellent network size
reductions through such synthesis, and significant speedups of our algorithm
over exhaustive search while obtaining near-optimal results for 15 real network
designs as well as nearly 10,000 randomly generated designs.
"
33,"Access Pattern-Based Code Compression for Memory-Constrained Embedded
  Systems","  As compared to a large spectrum of performance optimizations, relatively
little effort has been dedicated to optimize other aspects of embedded
applications such as memory space requirements, power, real-time
predictability, and reliability. In particular, many modern embedded systems
operate under tight memory space constraints. One way of satisfying these
constraints is to compress executable code and data as much as possible. While
research on code compression have studied efficient hardware and software based
code strategies, many of these techniques do not take application behavior into
account, that is, the same compression/decompression strategy is used
irrespective of the application being optimized. This paper presents a code
compression strategy based on control flow graph (CFG) representation of the
embedded program. The idea is to start with a memory image wherein all basic
blocks are compressed, and decompress only the blocks that are predicted to be
needed in the near future. When the current access to a basic block is over,
our approach also decides the point at which the block could be compressed. We
propose several compression and decompression strategies that try to reduce
memory requirements without excessively increasing the original instruction
cycle counts.
"
34,Mutation Sampling Technique for the Generation of Structural Test Data,"  Our goal is to produce validation data that can be used as an efficient (pre)
test set for structural stuck-at faults. In this paper, we detail an original
test-oriented mutation sampling technique used for generating such data and we
present a first evaluation on these validation data with regard to a structural
test.
"
35,System Level Analysis of the Bluetooth Standard,"  The SystemC modules of the Link Manager Layer and Baseband Layer have been
designed in this work at behavioral level to analyze the performances of the
Bluetooth standard. In particular the probability of the creation of a piconet
in presence of noise in the channel and the power reduction using the sniff and
hold mode have been investigated.
"
36,LC Oscillator Driver for Safety Critical Applications,"  A CMOS harmonic signal LC oscillator driver for automotive applications
working in a harsh environment with high safety critical requirements is
described. The driver can be used with a wide range of external components
parameters (LC resonance network of a sensor). Quality factor of the external
LC network can vary two decades. Amplitude regulation of the driver is
digitally controlled and the DAC is constructed as exponential with
piece-wise-linear (PWL) approximation. Low current consumption for high quality
resonance networks is achieved. Realized oscillator is robust, used in safety
critical application and has low EMC emissions.
"
37,A CMOS-Based Tactile Sensor for Continuous Blood Pressure Monitoring,"  A monolithic integrated tactile sensor array is presented, which is used to
perform non-invasive blood pressure monitoring of a patient. The advantage of
this device compared to a hand cuff based approach is the capability of
recording continuous blood pressure data. The capacitive, membrane-based sensor
device is fabricated in an industrial CMOS-technology combined with post-CMOS
micromachining. The capacitance change is detected by a S?-modulator. The
modulator is operated at a sampling rate of 128kS/s and achieves a resolution
of 12bit with an external decimation filter and an OSR of 128.
"
38,"A Tool and Methodology for AC-Stability Analysis of Continuous-Time
  Closed-Loop Systems","  Presented are a methodology and a DFII-based tool for AC-stability analysis
of a wide variety of closed-loop continuous-time (operational amplifiers and
other linear circuits). The methodology used allows for easy identification and
diagnostics of ac-stability problems including not only main-loop effects but
also local-instability loops in current mirrors, bias circuits and emitter or
source followers without breaking the loop. The results of the analysis are
easy to interpret. Estimated phase margin is readily available. Instability
nodes and loops along with their respective oscillation frequencies are
immediately identified and mapped to the existing circuit nodes thus offering
significant advantages compared to traditional ""black-box"" methods of stability
analysis (Transient Overshoot, Bode and Phase margin plots etc.). The tool for
AC-Stability analysis is written in SKILL? and is fully integrated in DFII?
environment. Its ""push-button"" graphical user interface (GUI) is easy to use
and understand. The tool can be invoked directly from Composer? schematic and
does not require active Analog Artist? session. The tool is not dependent on
the use of a specific fabrication technology or Process Design Kit
customization. It requires OCEAN?, Spectre? and Waveform calculator
capabilities to run.
"
39,An Assembler Driven Verification Methodology (ADVM),"  This paper presents an overview of an assembler driven verification
methodology (ADVM) that was created and implemented for a chip card project at
Infineon Technologies AG. The primary advantage of this methodology is that it
enables rapid porting of directed tests to new targets and derivatives, with
only a minimum amount of code refactoring. As a consequence, considerable
verification development time and effort was saved.
"
40,"Conception individuelle et collective. Approche de l'ergonomie cognitive
  [Individual and Collective Design. The Cognitive-Ergonomics Approach]","  This text presents the cognitive-ergonomics approach to design, in both its
individual and collective form. It focuses on collective design with respect to
individual design. The theoretical framework adopted is that of information
processing, specified for design problems. The cognitive characteristics of
design problems are presented: the effects of their ill-defined character and
of the different types of representation implemented in solving these problems,
amongst others the more or less ""satisficing"" character of the different
possible solutions. The text first describes the cognitive activities
implemented in both individual and collective design: different types of
control activities and of the executive activities of solution development and
evaluation. Specific collective-design characteristics are then presented:
co-design and distributed-design activities, temporo-operative and cognitive
synchronisation, and different types of argumentation, of co-designers'
intervention modes in the design process, of solution-proposals evaluation. The
paper concludes by a confrontation between the two types of design, individual
and collective.
"
41,"Applying Software Defect Estimations: Using a Risk Matrix for Tuning
  Test Effort","  Applying software defect esimation techniques and presenting this information
in a compact and impactful decision table can clearly illustrate to
collaborative groups how critical this position is in the overall development
cycle. The Test Risk Matrix described here has proven to be a valuable addition
to the management tools and approaches used in developing large scale software
on several releases. Use of this matrix in development planning meetings can
clarify the attendant risks and possible consequences of carrying out or
bypassing specific test activities.
"
42,Microsystem Product Development,"  Over the last decade the successful design and fabrication of complex MEMS
(MicroElectroMechanical Systems), optical circuits and ASICs have been
demonstrated. Packaging and integration processes have lagged behind MEMS
research but are rapidly maturing. As packaging processes evolve, a new
challenge presents itself, microsystem product development. Product development
entails the maturation of the design and all the processes needed to
successfully produce a product. Elements such as tooling design, fixtures,
gages, testers, inspection, work instructions, process planning, etc., are
often overlooked as MEMS engineers concentrate on design, fabrication and
packaging processes. Thorough, up-front planning of product development efforts
is crucial to the success of any project.
"
43,Parasitic Effects Reduction for Wafer-Level Packaging of RF-Mems,"  In RF-MEMS packaging, next to the protection of movable structures,
optimization of package electrical performance plays a very important role. In
this work, a wafer-level packaging process has been investigated and optimized
in order to minimize electrical parasitic effects. The RF-MEMS package concept
used is based on a wafer-level bonding of a capping silicon substrate to an
RF-MEMS wafer. The capping silicon substrate resistivity, substrate thickness
and the geometry of through-substrate electrical interconnect vias have been
optimized using finite-element electromagnetic simulations (Ansoft HFSS). Test
structures for electrical characterization have been designed and after their
fabrication, measurement results will be compared with simulations.
"
44,"Surface Conditioning Effect on Vacuum Microelectronics Components
  Fabricated by Deep Reactive Ion Etching","  Advances in material processing such as silicon micromachining are opening
the way to vacuum microelectronics. Two-dimensional vacuum components can be
fabricated using the microsystems processes. We developed such devices using a
single metal layer and silicon micromachining by DRIE. The latter technological
step has significant impact on the characteristics of the vacuum components.
This paper presents a brief summary of electron emission possibilities and the
design leading to the fabrication of a lateral field emission diode. First
measurement results and the aging of the devices are also discussed.
"
45,3-D Self-Assembled Soi Mems: An Example of Multiphysics Simulation,"  MEMS devices are typical systems where multiphysics simulations are
unavoidable. In this work, we present possible applications of 3-D
self-assembled SOI (Silicon-on-Insulator) MEMS such as, for instance, thermal
actuators and flow sensors. The numerical simulations of these microsystems are
presented. Structural and thermal parts have to be strongly coupled for
correctly describing the fabrication process and for simulating the behavior of
these 3-D SOI MEMS.
"
46,"Influence of the Feedback Filter on the Response of the Pulsed Digital
  Oscillator","  This paper introduces a new feedback topology for the Pulsed Digital
Oscillator (PDO) and compares it to the classical topology. The `classic' or
single feedback topology, introduced in previous works, shows a strong behavior
dependence on the damping losses in the MEMS resonator. A new double feedback
topology is introduced here in order to help solving this problem. Comparative
discrete-time simulations and preliminary experimental measurements have been
carried out for both topologies, showing how the new double feedback topology
may increase PDO performance for some frequency ranges.
"
47,"Concave Microlens Array Mold Fabrication in Photoresist Using UV
  Proximity Printing","  This paper presents a simple and effective method to fabricate a
polydimethyl-siloxane (PDMS) microlens array with a high fill factor, which
utilizes the UV proximity printing and photoresist replication methods. The
concave microlens array mold was made using a printing gap in lithography
process, which utilizes optical diffraction of UV light to deflect away from
the aperture edges and produces a certain exposure in the photoresist material
outside the aperture edges. This method can precisely control the geometric
profile of concave microlens array. The experimental results showed that the
concave micro-lens array in photoresist could be formed automatically when the
printing gap ranged from 240 micron to 720 micron. High fill factor microlens
array can be produced, when the control pitch distance between the adjacent
apertures of the concave microlens array was decreased to the aperture size.
"
48,"The annealing induced extraordinary properties of SI based ZNO film
  grown by RF sputtering","  Pb(Zr0.52Ti0.48)O3 (PZT) thin films were in situ deposited by pulsed laser
deposition (PLD) on Pt/Ti/SiO2/Si substrates using a template layer derived by
sol-gel method. A 0.1-$\mu$m-thick PZT layer with (111) or (100)-preferred
orientation was first deposited onto Pt/Ti/SiO2/Si substrates using the sol-gel
method, and than a PZT layer with thickness of 1$\mu$m was in situ deposited by
PLD on the above-mentioned PZT layer. The crystalline phases and the preferred
orientations of the PZT films were investigated by X-ray diffraction analysis.
Surface and cross-sectional morphologies were observed by scanning electron
microscopy and transmission electron microscopy. The electrical properties of
the films were evaluated by measuring their P-E hysteresis loops and dielectric
constants. The preferred orientation of the films can be controlled using the
template layer derived by the sol-gel method. The deposition temperature
required to obtain the perovskite phase in this process is approximately 460
degrees C, and is significantly lower than that in the case of direct film
deposition by PLD on the Pt/Ti/SiO2/Si substrates.
  Keywords: lead zirconate titanate (PZT), thin film, sol-gel method, laser
ablation, electrical properties
"
49,Parametric Yield Analysis of Mems via Statistical Methods,"  This paper considers a developing theory on the effects of inevitable process
variations during the fabrication of MEMS and other microsystems. The effects
on the performance and design yield of the microsystems devices are analyzed
and presented. A novel methodology in the design cycle of MEMS and other
microsystems is briefly introduced. This paper describes the initial steps of
this methodology that is aimed at counteracting the parametric variations in
the product cycle of microsystems. It is based on a concept of worst-case
analysis that has proven successful in the parent IC technology. Issues ranging
from the level of abstraction of the microsystem models to the availability of
such models are addressed
"
50,"Electrostatically-Driven Resonator on Soi with Improved Temperature
  Stability","  This paper deals with a single-crystal-silicon (SCS) MEMS resonator with
improved temperature stability. While simulations have shown that the
temperature coefficient of resonant frequency can be down to 1 ppm/degrees C,
preliminary measurements on non-optimised structures gave evidence of a
temperature coefficient of 29 ppm/degrees C. Design, optimisation, experimental
results with post process simulation and prospective work are presented.
"
51,"Electromechanical Reliability Testing of Three-Axial Silicon Force
  Sensors","  This paper reports on the systematic electromechanical characterization of a
new three-axial force sensor used in dimensional metrology of micro components.
The siliconbased sensor system consists of piezoresistive mechanicalstress
transducers integrated in thin membrane hinges supporting a suspended flexible
cross structure. The mechanical behavior of the fragile micromechanical
structure isanalyzed for both static and dynamic load cases. This work
demonstrates that the silicon microstructure withstands static forces of 1.16N
applied orthogonally to the front-side of the structure. A statistical Weibull
analysis of the measured data shows that these values are significantly reduced
if the normal force is applied to the back of the sensor. Improvements of the
sensor system design for future development cycles are derived from the
measurement results.
"
52,"An Active Chaotic Micromixer Integrating Thermal Actuation Associating
  PDMS and Silicon Microtechnology","  Due to scaling laws, in microfluidic, flows are laminar. Consequently, mixing
between two liquids is mainly obtained by natural diffusion which may take a
long time or equivalently requires centimetre length channels. To reduce time
and length for mixing, it is possible to generate chaotic-like flows either by
modifying the channel geometry or by creating an external perturbation of the
flow. In this paper, an active micromixer is presented consisting on thermal
actuation with heating resistors. In order to disturb the liquid flow, an
oscillating transverse flow is generated by heating the liquid. Depending on
the value of boiling point, either bubble expansion or volumetric dilation
controlled the transverse flow amplitude. A chaotic like mixing is then induced
under particular conditions depending on volume expansion, liquid velocity,
frequency of actuation... This solution presents the advantage to achieve
mixing in a very short time (1s) and along a short channel distance (channel
width). It can also be integrated in a more complex device due to actuator
integration with microfluidics.
"
53,"Resolution Limits for Resonant Mems Sensors Based on Discrete Relay
  Feedback Techniques","  This paper is devoted to the analysis of resonant MEMS sensors based on
discrete relay feedback techniques. One drawback of such techniques is that
some synchronization usually occurs between the discrete part and the
continuous part of the system: this results in sensor responses that are very
similar to the curves known as devil's staircases, i.e. the frequency does not
vary smoothly with the sensor's input. The main contribution of this paper is a
theoretical calculation of the resolution of such systems. The resolutions of
two existing resonant MEMS architectures are then calculated and these results
are discussed.
"
54,A Silicon-Based Micro Gas Turbine Engine for Power Generation,"  This paper reports on our research in developing a micro power generation
system based on gas turbine engine and piezoelectric converter. The micro gas
turbine engine consists of a micro combustor, a turbine and a centrifugal
compressor. Comprehensive simulation has been implemented to optimal the
component design. We have successfully demonstrated a silicon-based micro
combustor, which consists of seven layers of silicon structures. A
hairpin-shaped design is applied to the fuel/air recirculation channel. The
micro combustor can sustain a stable combustion with an exit temperature as
high as 1600 K. We have also successfully developed a micro turbine device,
which is equipped with enhanced micro air-bearings and driven by compressed
air. A rotation speed of 15,000 rpm has been demonstrated during lab test. In
this paper, we will introduce our research results major in the development of
micro combustor and micro turbine test device.
"
55,Energy Conversion Using New Thermoelectric Generator,"  During recent years, microelectronics helped to develop complex and varied
technologies. It appears that many of these technologies can be applied
successfully to realize Seebeck micro generators: photolithography and
deposition methods allow to elaborate thin thermoelectric structures at the
micro-scale level. Our goal is to scavenge energy by developing a miniature
power source for operating electronic components. First Bi and Sb micro-devices
on silicon glass substrate have been manufactured with an area of 1cm2
including more than one hundred junctions. Each step of process fabrication has
been optimized: photolithography, deposition process, anneals conditions and
metallic connections. Different device structures have been realized with
different micro-line dimensions. Each devices performance will be reviewed and
discussed in function of their design structure.
"
56,Above Ic Micro-Power Generators for RF-Mems,"  This work presents recent advances in the development and the integration of
an electrochemical (chemicalelectrical energy conversion) micro power generator
used as a high voltage energy source for RF-MEMS powering. Autonomous MEMS
require similarly miniaturized power sources. Up to day, solid state thin film
batteries are realized with mechanical masks. This method doesn't allow
dimensions below a few mm^2 active area, and besides the whole process flow is
done under controlled atmosphere so as to ensure materials chemical stability
(mainly lithiated materials). Within this context, Microelectronics
micro-fabrication procedures (photolithography, Reactive Ion Etching...) are
used to reach both miniaturisation (100x100 $\mu$m^2 targeted unit cell active
area) and Above IC technological compatibility. All process steps developed
here are realized in clean room environment.
"
57,Packaging of RF Mems Switching Functions on Alumina Substrate,"  Recently the strong demands in wireless communication requires expanding
development for the application of RF MEMS (Radio Frequency micro electro
mechanical systems) sensing devices such as micro-switches, tunable capacitors
because it offers lower power consumption, lower losses, higher linearity and
higher Q factors compared with conventional communications components. To
accelerate commercialisation of RF MEMS products, development for packaging
technologies is one of the most critical issues should be solved beforehand.
"
58,Recent Developments in Mems-Based Micro Fuel Cells,"  Micro fuel cells ($\mu$-FC) represent promising power sources for portable
applications. Today, one of the technological ways to make $\mu$-FC is to have
recourse to standard microfabrication techniques used in the fabrication of
micro electromechanical systems (MEMS). This paper shows an overview on the
applications of MEMS techniques on miniature FC by presenting several solutions
developed throughout the world. It also describes the latest developments of a
new porous silicon-based miniature fuel cell. Using a silane grafted on an
inorganic porous media as the proton-exchange membrane instead of a common
ionomer such as Nafion, the fuel cell achieved a maximum power density of 58 mW
cm-2 at room temperature with hydrogen as fuel.
"
59,Influence of Micro-Cantilever Geometry and Gap on Pull-in Voltage,"  In this paper, we study the behaviour of a microcantilever beam under
electrostatic actuation using finite difference method. This problem has a lot
of applications in MEMS based devices like accelerometers, switches and others.
In this paper, we formulated the problem of a cantilever beam with proof mass
at its end and carried out the finite difference solution. we studied the
effects of length, width, and the gap size on the pull-in voltage using data
that are available in the literature. Also, the stability limit is compared
with the single degree of freedom commonly used in the earlier literature as an
approximation to calculate the pull-in voltage.
"
60,"Design and Development of Novel Electroplating Spring Frame Mems
  Structure Specimens for the Microtensile Testing of Thin Film Materials","  Microelectromechanical systems (MEMS) technologies are developing rapidly
with increasing study of the design, fabrication and commercialization of
microscale systems and devices. Accurate mechanical properties are important
for successful design and development of MEMS. We have demonstrated here a
novel electroplating spring frame MEMS Structure Specimen integrates pin-pin
align holes, misalignment compensate spring structure frame, load sensor beam
and freestanding thin film. The specimen can be fit into a specially designed
microtensile apparatus which is capable of carrying out a series of tests on
sub-micro scale freestanding thin films.
"
61,"Characterisation of the Etching Quality in Micro-Electro-Mechanical
  Systems by Thermal Transient Methodology","  Our paper presents a non-destructive thermal transient measurement method
that is able to reveal differences even in the micron size range of MEMS
structures. Devices of the same design can have differences in their
sacrificial layers as consequence of the differences in their manufacturing
processes e.g. different etching times. We have made simulations examining how
the etching quality reflects in the thermal behaviour of devices. These
simulations predicted change in the thermal behaviour of MEMS structures having
differences in their sacrificial layers. The theory was tested with
measurements of similar MEMS devices prepared with different etching times. In
the measurements we used the T3Ster thermal transient tester equipment. The
results show that deviations in the devices, as consequence of the different
etching times, result in different temperature elevations and manifest also as
shift in time in the relevant temperature transient curves.
"
62,"The Effects of Additives on the Physical Properties of Electroformed
  Nickel and on the Stretch of Photoelectroformed Nickel Components","  The process of nickel electroforming is becoming increasingly important in
the manufacture of MST products, as it has the potential to replicate complex
geometries with extremely high fidelity. Electroforming of nickel uses
multi-component electrolyte formulations in order to maximise desirable product
properties. In addition to nickel sulphamate (the major electrolyte component),
formulation additives can also comprise nickel chloride (to increase nickel
anode dissolution), sulphamic acid (to control pH), boric acid (to act as a pH
buffer), hardening/levelling agents (to increase deposit hardness and lustre)
and wetting agents (to aid surface wetting and thus prevent gas bubbles and
void formation). This paper investigates the effects of some of these variables
on internal stress and stretch as a function of applied current density.
"
63,"A Novel Contact Resistance Model of Anisotropic Conductive Film for FPD
  Packaging","  In this research, a novel contact resistance model for the flat panel display
(FPD) packaging based on the within layer parallel and between layers series
resistance concepts was proposed. The FJ2530 anisotropic conductive films (ACF)
by Sony Inc. containing the currently smallest 3micron conductive particles was
used to conduct the experiments to verify the accuracy of the proposed model.
Calculated resistance of the chip-on-glass (COG) packaging by the proposed
model is 0.163\Omega. It is found that the gold bump with 0.162\Omega
resistance play the major role of the overall resistance. Although the
predicted resistance by the proposed model is only one third of the
experimentally measured value, it has been three-fold improvement compared to
the existing models.
"
64,"Measurement Technique for Elastic and Mechanical Properties of
  Polycrystalline Silicon-Germanium Films Using Surface Acoustic Waves and
  Projection Masks","  Using Rayleigh surface acoustic waves (SAW), the Young's modulus, the density
and the thickness of polycrystalline Silicon-Germanium (SiGe) films deposited
on silicon and SiO2 were measured, in excellent agreement with theory. The
dispersion curve of the propagating SAW is calculated with a Boundary Element
Method (BEM)-Model based on Green's functions. The propagating SAW is generated
with a nanosecond laser in a narrowband scheme projecting stripes from a mask
on the surface of the sample. For this purpose a glass mask and a liquid
crystal display (LCD) mask are used. The slope of the SAW is then measured
using a probe beam setup. From the wavelength of the mask and the frequency of
the measured SAW, the dispersion curve is determined point by point. Fitting
the BEM-Model to the measured nonlinear dispersion curve provides several
physical parameters simultaneously. In the present work this is demonstrated
for the Young's modulus, the density and the thickness of SiGe films. The
results from the narrowband scheme measurement are in excellent agreement with
separated measurements of the thickness (profilometer), the density (balance)
and the Young's modulus (nanoindenter).
"
65,"Effect of Surface Finish of Substrate on Mechanical Reliability of
  in-48SN Solder Joints in Moems Package","  Interfacial reactions and shear properties of the In-48Sn (in wt.%) ball grid
array (BGA) solder joints after bonding were investigated with four different
surface finishes of the substrate over an underlying Cu pad: electroplated
Ni/Au (hereafter E-NG), electroless Ni/immersion Au (hereafter ENIG), immersion
Ag (hereafter I-Ag) and organic solderability preservative (hereafter OSP).
During bonding, continuous AuIn2, Ni3(Sn,In)4 and Cu6(Sn,In)5 intermetallic
compound (IMC) layers were formed at the solder/E-NG, solder/ENIG and
solder/OSP interface, respectively. The interfacial reactions between the
solder and I-Ag substrate during bonding resulted in the formation of
Cu6(Sn,In)5 and Cu(Sn,In)2 IMCs with a minor Ag element. The In-48Sn/I-Ag
solder joint showed the best shear properties among the four solder joints
after bonding, whereas the solder/ENIG solder joint exhibited the weakest
mechanical integrity.
"
66,"Non Linear Techniques for Increasing Harvesting Energy from
  Piezoelectric and Electromagnetic Micro-Power-Generators","  Non-linear techniques are used to optimize the harvested energy from
piezoelectric and electromagnetic generators. This paper introduces an
analytical study for the voltage amplification obtained from these techniques.
The analytical study is experimentally validated using a macro model of
piezoelectric generator. Moreover, the integration influences on these
techniques is studied. Through all the obtained results, a suitable structure
for autonomous microsystems is proposed.
"
67,"Optimization of Piezoelectric Electrical Generators Powered by Random
  Vibrations","  This paper compares the performances of a vibrationpowered electrical
generators using PZT piezoelectric ceramic associated to two different power
conditioning circuits. A new approach of the piezoelectric power conversion
based on a nonlinear voltage processing is presented and implemented with a
particular power conditioning circuit topology. Theoretical predictions and
experimental results show that the nonlinear processing technique may increase
the power harvested by a factor up to 4 compared to the Standard optimization
technique. Properties of this new technique are analyzed in particular in the
case of broadband, random vibrations, and compared to those of the Standard
interface.
"
68,Power Processing Circuits for Mems Inertial Energy Scavengers,"  Inertial energy scavengers are self-contained devices which generate power
from ambient motion, by electrically damping the internal motion of a suspended
proof mass. There are significant challenges in converting the power generated
from such devices to useable form, particularly in micro-engineered variants.
This paper presents approaches to this power conversion requirement, with
emphasis on the cases of electromagnetic and electrostatic transduction.
"
69,Motion-Based Generators for Industrial Applications,"  Scaling down of electronic systems has generated a large interest in the
research on miniature energy sources. In this paper a closer look is given to
the use of vibration based scavengers in industrial environments, where waste
energy is abundantly available as engine related vibrations or large amplitude
motions. The modeling of mechanical generators resulted in the design and
realization of two prototypes, based on electromagnetic and electrostatic
conversion of energy. Although the prototypes are not yet optimized against
size and efficiency, a power of 0.3 mW has been generated in a 5 Hz motion with
a 0.5 meter amplitude.
"
70,"Design and Fabrication of a Micro Electrostatic Vibration-to-Electricity
  Energy Converter","  This paper presents a micro electrostatic vibration-toelectricity energy
converter. For the 3.3 V supply voltage and 1cm2 chip area constraints, optimal
design parameters were found from theoretical calculation and Simulink
simulation. In the current design, the output power is 200 $\mu$W/cm2 for the
optimal load of 8 M\Omega. The device was fabricated in a silicon-on-insulator
(SOI) wafer. Mechanical and electrical measurements were conducted. Residual
particles caused shortage of the variable capacitor and the output power could
not be measured. Device design and fabrication processes are being refined.
"
71,"Macro and Micro Scale Electromagnetic Kinetic Energy Harvesting
  Generators","  This paper is concerned with generators that harvest electrical energy from
the kinetic energy present in the sensor nodes environment. These generators
have the potential to replace or augment battery power which has a limited
lifetime and requires periodic replacement which limits the placement and
application of the sensor node.
"
72,Impact of Thermal Behavior on Offset in a High-Q Gyroscope,"  In this paper, CFD approach is used to simulate the thermal behavior in a
sensitive high-Q gyroscope. The electromagnetically driving wires, in which AC
current flows, are treated as Joule heat sources in the model. We found that
the differences of temperature, pressure and velocity along the driving
direction and transversely across the proof masses increased as the gap height
between the proof mass and top glass became smaller. Local pressure gradient is
expected to possibly enhance the impact of any imperfect led by MEMS processes
or designs on the offset of our tuning fork type gyroscope, which has been
experimentally verified. A device with 200um gap gives a two-third offset down
compared with that of its counterpart with 50um gap.
"
73,Scaling Effects for Electromagnetic Vibrational Power Generators,"  This paper investigates how the power generated by electromagnetic based
vibrational power generators scales with the dimension of the generator. The
effects of scaling on the magnetic fields, the coil parameters and the
electromagnetic damping are presented. An analysis is presented for both
wire-wound coil technology and micro-fabricated coils.
"
74,"A Generic Surface Micromachining Module for Mems Hermetic Packaging at
  Temperatures Below 200 degrees C","  This paper presents the different processing steps of a new generic surface
micromachining module for MEMS hermetic packaging at temperatures around 180
degrees C based on nickel plating and photoresist sacrificial layers. The
advantages of thin film caps are the reduced thickness and area consumption and
the promise of being a low-cost batch process. Moreover, sealing happens by a
reflow technique, giving the freedom of choosing the pressure and atmosphere
inside the cavity. Sacrificial etch holes are situated above the device
allowing shorter release times compared to the state-of-the-art. With the
so-called over-plating process, small etch holes can be created in the membrane
without the need of expensive lithography tools. The etch holes in the membrane
have been shown to be sufficiently small to block the sealing material to pass
through, but still large enough to enable an efficient release.
"
75,"A Ku-Band Novel Micromachined Bandpass Filter with Two Transmission
  Zeros","  This paper presents a micromachined bandpass filter with miniature size that
has relatively outstanding performance. A silicon-based eight-order microstrip
bandpass filter is fabricated and measured. A novel design method of the
interdigital filter that can create two transmission zeros is described. The
location of the transmission zeros can be shifted arbitrarily in the stopband.
By adjusting the zero location properly, the filter provides much better skirt
rejection and lower insertion loss than a conventional microstrip interdigital
filter. To reduce the chip size, through-silicon-substrate-via-hole is used.
Good experimental results are obtained.
"
76,The Design and Fabrication of Platform Device for Dna Amplification,"  Thermalcycler were extensively used machine for amplify DNA sample. One of
the major problems in the working time was that it spent most of time for
cooling and heating. In order to improve the efficient, this study presented a
novel method for amplify DNA sample. For this concept, the DNA sample in the
silicon chamber which was pushed by a beam through three temperature regions
around a center and then the DNA segments could be amplified rapidly after 30
cycles. The polymerase chain reaction platform was composed of thin-film
heaters, copper plates, DC powers, and temperature controllers. The
photolithography and bulk etching technologies were utilized to construct the
thin-film heater and DNA reaction chambers. Finally, 1 pound gL 100bp DNA
segment of E. coli K12 was amplified successfully within 36 minutes on this PCR
platform.
"
77,"Design and Modeling of a Mems-Based Valveless Pump Driven by an
  Electromagnetic Force","  A novel valveless micro impedance pump is proposed and analyzed in this
study.
"
78,"Electrostatic Actuators Operating in Liquid Environment : Suppression of
  Pull-in Instability and Dynamic Response","  This paper presents results about fabrication and operation of electrostatic
actuators in liquids with various permittivities. In the static mode, we
provide experimental and theoretical demonstration that the pull-in effect can
be shifted beyond one third of the initial gap and even be eliminated when
electrostatic actuators are operated in liquids. This should benefit to
applications in microfluidics requiring either binary state actuation (e.g.
pumps, valves) or continuous displacements over the whole gap (e.g.
microtweezers). In dynamic mode, actuators like micro-cantilevers present a
great interest for Atomic Force Microscopy (AFM) in liquids. As this
application requires a good understanding of the cantilever resonance frequency
and Q-factor, an analytical modeling in liquid environment has been
established. The theoretically derived curves are validated by experimental
results using a nitride encapsulated cantilever with integrated electrostatic
actuation. Electrode potential screening and undesirable electrochemistry in
dielectric liquids are counteracted using AC-voltages. Both experimental and
theoretical results should prove useful in micro-cantilever design for AFM in
liquids.
"
79,"Au-SN Flip-Chip Solder Bump for Microelectronic and Optoelectronic
  Applications","  As an alternative to the time-consuming solder pre-forms and pastes currently
used, a co-electroplating method of eutectic Au-Sn alloy was used in this
study. Using a co-electroplating process, it was possible to plate the Au-Sn
solder directly onto a wafer at or near the eutectic composition from a single
solution. Two distinct phases, Au5Sn and AuSn, were deposited at a composition
of 30at.%Sn. The Au-Sn flip-chip joints were formed at 300 and 400 degrees
without using any flux. In the case where the samples were reflowed at 300
degrees, only an (Au,Ni)3Sn2 IMC layer formed at the interface between the
Au-Sn solder and Ni UBM. On the other hand, two IMC layers, (Au,Ni)3Sn2 and
(Au,Ni)3Sn, were found at the interfaces of the samples reflowed at 400
degrees. As the reflow time increased, the thickness of the (Au,Ni)3Sn2 and
(Au,Ni)3Sn IMC layers formed at the interface increased and the eutectic
lamellae in the bulk solder coarsened.
"
80,"Contactless Thermal Characterization Method of PCB-s Using an IR Sensor
  Array","  In this paper the feasibility study of an IR sensor card is presented. The
methodology and the results of a quasi real-time thermal characterization tool
and method for the temperature mapping of circuits and boards based on sensing
the infrared radiation is introduced. With the proposed method the IR
radiation-distribution of boards from the close proximity of the sensor card is
monitored in quasi real-time. The proposed method is enabling in situ IR
measurement among operating cards of a system e.g. in a rack.
"
81,"Miniaturized Fluorescence Excitation Platform with Optical Fiber for
  Bio-Detection Chips","  This paper presents a new research study on the platform fabrication of
fluorescence bio-detection chip with an optical fiber transmission. Anisotropic
wet etching on (100) silicon wafers to fabrication V-groove for optical fiber
alignment and micro-mirror were included. Combing with anodic bonding technique
to adhere glass, silicon structure and optical fiber for a fluorescence
excitation platform was completed. In this study, the etching solution 40% KOH
was used to study the parameters effect. The results show that working
temperature is the main parameter to significantly effect the etch rate. The
anisotropic etching resulted 54.7 degrees reflective mirrors and its
reflectivity for optical beam were also examined. The surface roughness of the
micro-mirror is Ra 4.1 nm measured using AFM, it provides excellent optical
reflection. The incident light and beam profiles were also examined for further
study. This study can show this micro-platform adaptable for fluorescence
bio-detection.
"
82,Characterization of Flexible RF Microcoil Dedicated to Surface Mri,"  In Magnetic Resonance Imaging (MRI), to achieve sufficient Signal to Noise
Ratio (SNR), the electrical performance of the RF coil is critical. We
developed a device (microcoil) based on the original concept of monolithic
resonator. This paper presents the used fabrication process based on
micromoulding. The dielectric substrates are flexible thin films of polymer,
which allow the microcoil to be form fitted to none-plane surface. Electrical
characterizations of the RF coils are first performed and results are compared
to the attempted values. Proton MRI of a saline phantom using a flexible RF
coil of 15 mm in diameter is performed. When the coil is conformed to the
phantom surface, a SNR gain up to 2 is achieved as compared to identical but
planar RF coil. Finally, the flexible coil is used in vivo to perform MRI with
high spatial resolution on a mouse using a small animal dedicated scanner
operating at in a 2.35 T.
"
83,"Integration of Micro-Electro-Mechanical Deformable Mirrors in Doped
  Fiber Amplifiers","  We present a simple technique to produce active Q-switching in various types
of fiber amplifiers by active integration of an electrostatic actuated
deformable metallic micro-mirror. The optical MEMS (MOEMS) device acts as one
of the laser cavity reflectors and, at the same time, as switching/ modulator
element. We aim to obtain laser systems emitting short, high-power pulses and
having variable repetition rate. The electro-mechanical behavior of membrane
(bridge-type) was simulated by using electrostatic and modal 3D finite element
analysis (FEA). The results of the simulations fit well with the experimental
mechanical, electrical and thermal measurements of the components. In order to
decrease the sensitiveness to fiber-mirror alignment we are developing novel
optical devices based on stressed-metal cantilever-type geometry that allow
deflections up to 50 $\mu$m with increased reflectivity discrimination during
actuation.
"
84,"Micro-Ball Lens Array Fabrication in Photoresist Using Ptfe Hydrophobic
  Effect","  This paper presents a simple method to fabricate micro-ball lens and its
array. The key technology is to use the hydrophobic characteristics of
polyterafluoroethylene (PTFE) substrate. High contact angle between melted
photoresist pattern and PTFE can generate micro-ball lens and its array. PTFE
thin film was spun onto a silicon wafer and dried in oven. Photoresist AZ4620
was used to pattern micro-columns with different diameters 60, 70 and 80
$\mu$m. A thermal reflow process then was applied to melt these micro-column
patterns resulted in micro-ball lens array. The achieved micro-ball lens array
with diameter 98 $\mu$m was fabricated using 80 $\mu$m in diameter patterns.
This method provides a simple fabrication process and low material cost.
"
85,"Reduced-Order Modelling of the Bending of an Array of Torsional
  Micromirrors","  Reduced-Order Modelling of the Bending of an Array of An array of
micromirrors for beam steering optical switching has been designed in a thick
polysilicon technology. A novel semi-analytical method to calculate the static
characteristics of the micromirrors by taking into account the flexural
deformation of the structure is presented. The results are compared with 3D
coupled-field FEM simulation.
"
86,"Model of Electrostatic Actuated Deformable Mirror Using Strongly Coupled
  Electro-Mechanical Finite Element","  The aim of this paper is to deal with multi-physics simulation of
micro-electro-mechanical systems (MEMS) based on an advanced numerical
methodology. MEMS are very small devices in which electric as well as
mechanical and fluid phenomena appear and interact. Because of their
microscopic scale, strong coupling effects arise between the different physical
fields, and some forces, which were negligible at macroscopic scale, have to be
taken into account. In order to accurately design such micro-electro-mechanical
systems, it is of primary importance to be able to handle the strong coupling
between the electric and the mechanical fields. In this paper, the finite
element method (FEM) is used to model the strong coupled electro-mechanical
interactions and to perform static and transient analyses taking into account
large mesh displacements. These analyses will be used to study the behaviour of
electrostatically actuated micro-mirrors.
"
87,"New Internal Stress Driven on-Chip Micromachines for Extracting
  Mechanical Properties of Thin Films","  A new concept of micromachines has been developed for measuring the
mechanical properties of thin metallic films. The actuator is a beam undergoing
large internal stresses built up during the deposition process. Al thin films
are deposited partly on the actuator beam and on the substrate. By etching the
structure, the actuator contracts and pulls the Al film. Full stress strain
curves can be generated by designing a set of micromachines with various
actuator lengths. In the present study, the displacements have been measured by
scanning electronic microscopy. The stress is derived from simple continuum
mechanics relationships. The tensile properties of Al films of various
thicknesses have been tested. A marked increase of the strength with decreasing
film thickness is observed.
"
88,Process Issues for a Multi-Layer Microelectrofluidic Platform,"  We report on the development of some process capabilities for a
polymer-based, multi-layer microelectrofluidic platform, namely: the hot
embossing process, metallization on polymer and polymer bonding. Hot embossing
experiments were conducted to look at the effects of load applied, embossing
temperature and embossing time on the fidelity of line arrays representing
micro channels. The results revealed that the embossing temperature is a more
sensitive parameter than the others due to its large effect on the polymer
material's viscoelastic properties. Dynamic mechanical analysis (DMA) on
polymethyl methacrylate (PMMA) revealed a steep glass transition over a 20 oC
range, with the material losing more than 95 % of its storage modulus. The data
explained the hot embossing results which showed large change in the embossed
channel dimensions when the temperature is within the glass transition range.
It was demonstrated that the micro-printing of silver epoxy is a possible
low-cost technique in the mass production of disposable lab chips. An
interconnecting network of electrical traces was fabricated in the form of a
four-layer PMMA-based device. A four PMMA layer device with interconnecting
microfluidic channels was also fabricated and tested.
"
89,Fabrication of Switches on Polymer-Based by Hot Embossing,"  In MEMS technology, most of the devices are fabricated on glass or silicon
substrate. However, this research presents a novel manufacture method that is
derived from conventional hot embossing technology to fabricate the
electrostatic switches on polymer material. The procedures of fabrication
involve the metal deposition, photolithography, electroplating, hot embossing
and hot embed techniques. The fundamental concept of the hot embed technology
is that the temperature should be increased above Tg of polymer, and the
polymer becomes plastic and viscous and could be molded. According to the
fundamental concept, the metal layer on the silicon/glass substrate could be
embedded into polymer material during the hot embossing process. Afterward, the
metal layer is bonded together with the polymer after removing the substrate in
the de-embossing step. Finally, the electrostatic switch is fabricated on
polymethylmethacrylate(PMMA) material to demonstrate the novel method.
"
90,"A New Model of Fringing Capacitance and its Application to the Control
  of Parallel-Plate Electrostatic Micro Actuators","  Fringing field has to be taken into account in the formulation of
electrostatic parallel-plate actuators when the gap separating the electrodes
is comparable to the geometrical dimensions of the moving plate. Even in this
case, the existing formulations often result in complicated mathematical models
from which it is difficult to determine the deflection of the moving plate for
given voltages and therefore to predict the necessary applied voltages for
actuation control. This work presents a new method for the modeling of fringing
field, in which the effect of fringing field is modeled as a serial capacitor.
Numerical simulation demonstrates the suitability of this formulation. Based on
this model, a robust control scheme is constructed using the theory of
input-to-state stabilization (ISS) and back-stepping state feedback design. The
stability and the performance of the system using this control scheme are
demonstrated through both stability analysis and numerical simulation.
"
91,Fluoroscopy-based navigation system in spine surgery,"  The variability in width, height, and spatial orientation of a spinal pedicle
makes pedicle screw insertion a delicate operation. The aim of the current
paper is to describe a computer-assisted surgical navigation system based on
fluoroscopic X-ray image calibration and three-dimensional optical localizers
in order to reduce radiation exposure while increasing accuracy and reliability
of the surgical procedure for pedicle screw insertion. Instrumentation using
transpedicular screw fixation was performed: in a first group, a conventional
surgical procedure was carried out with 26 patients (138 screws); in a second
group, a navigated surgical procedure (virtual fluoroscopy) was performed with
26 patients (140 screws). Evaluation of screw placement in every case was done
by using plain X-rays and post-operative computer tomography scan. A 5 per cent
cortex penetration (7 of 140 pedicle screws) occurred for the computer-assisted
group. A 13 per cent penetration (18 of 138 pedicle screws) occurred for the
non computer-assisted group. The radiation running time for each vertebra level
(two screws) reached 3.5 s on average in the computer-assisted group and 11.5 s
on average in the non computer-assisted group. The operative time for two
screws on the same vertebra level reaches 10 min on average in the non
computer-assisted group and 11.9 min on average in the computer-assisted group.
The fluoroscopy-based (two-dimensional) navigation system for pedicle screw
insertion is a safe and reliable procedure for surgery in the lower thoracic
and lumbar spine.
"
92,"Robot-based tele-echography: clinical evaluation of the TER system in
  abdominal aortic exploration","  OBJECTIVE: The TER system is a robot-based tele-echography system allowing
remote ultrasound examination. The specialist moves a mock-up of the ultrasound
probe at the master site, and the robot reproduces the movements of the real
probe, which sends back ultrasound images and force feedback. This tool could
be used to perform ultrasound examinations in small health care centers or from
isolated sites. The objective of this study was to prove, under real
conditions, the feasibility and reliability of the TER system in detecting
abdominal aortic and iliac aneurysms. METHODS: Fifty-eight patients were
included in 2 centers in Brest and Grenoble, France. The remote examination was
compared with the reference standard, the bedside examination, for aorta and
iliac artery diameter measurement, detection and description of aneurysms,
detection of atheromatosis, the duration of the examination, and acceptability.
RESULTS: All aneurysms (8) were detected by both techniques as intramural
thrombosis and extension to the iliac arteries. The interobserver correlation
coefficient was 0.982 (P < .0001) for aortic diameters. The rate of concordance
between 2 operators in evaluating atheromatosis was 84% +/- 11% (95% confidence
interval). CONCLUSIONS: Our study on 58 patients suggests that the TER system
could be a reliable, acceptable, and effective robot-based system for
performing remote abdominal aortic ultrasound examinations. Research is
continuing to improve the equipment for general abdominal use.
"
93,"Development of miniaturized light endoscope-holder robot for
  laparoscopic surgery","  PURPOSE: We have conducted experiments with an innovatively designed robot
endoscope holder for laparoscopic surgery that is small and low cost. MATERIALS
AND METHODS: A compact light endoscope robot (LER) that is placed on the
patient's skin and can be used with the patient in the lateral or dorsal supine
position was tested on cadavers and laboratory pigs in order to allow
successive modifications. The current control system is based on voice
recognition. The range of vision is 360 degrees with an angle of 160 degrees .
Twenty-three procedures were performed. RESULTS: The tests made it possible to
advance the prototype on a variety of aspects, including reliability,
steadiness, ergonomics, and dimensions. The ease of installation of the robot,
which takes only 5 minutes, and the easy handling made it possible for 21 of
the 23 procedures to be performed without an assistant. CONCLUSION: The LER is
a camera holder guided by the surgeon's voice that can eliminate the need for
an assistant during laparoscopic surgery. The ease of installation and
manufacture should make it an effective and inexpensive system for use on
patients in the lateral and dorsal supine positions. Randomized clinical trials
will soon validate a new version of this robot prior to marketing.
"
94,"Prostate biopsies guided by three-dimensional real-time (4-D)
  transrectal ultrasonography on a phantom: comparative study versus
  two-dimensional transrectal ultrasound-guided biopsies","  OBJECTIVE: This study evaluated the accuracy in localisation and distribution
of real-time three-dimensional (4-D) ultrasound-guided biopsies on a prostate
phantom. METHODS: A prostate phantom was created. A three-dimensional real-time
ultrasound system with a 5.9MHz probe was used, making it possible to see
several reconstructed orthogonal viewing planes in real time. Fourteen
operators performed biopsies first under 2-D then 4-D transurethral ultrasound
(TRUS) guidance (336 biopsies). The biopsy path was modelled using segmentation
in a 3-D ultrasonographic volume. Special software was used to visualise the
biopsy paths in a reference prostate and assess the sampled area. A comparative
study was performed to examine the accuracy of the entry points and target of
the needle. Distribution was assessed by measuring the volume sampled and a
redundancy ratio of the sampled prostate. RESULTS: A significant increase in
accuracy in hitting the target zone was identified using 4-D ultrasonography as
compared to 2-D. There was no increase in the sampled volume or improvement in
the biopsy distribution with 4-D ultrasonography as compared to 2-D.
CONCLUSION: The 4-D TRUS guidance appears to show, on a synthetic model, an
improvement in location accuracy and in the ability to reproduce a protocol.
The biopsy distribution does not seem improved.
"
95,"Towards 3D ultrasound image based soft tissue tracking: a transrectal
  ultrasound prostate image alignment system","  The emergence of real-time 3D ultrasound (US) makes it possible to consider
image-based tracking of subcutaneous soft tissue targets for computer guided
diagnosis and therapy. We propose a 3D transrectal US based tracking system for
precise prostate biopsy sample localisation. The aim is to improve sample
distribution, to enable targeting of unsampled regions for repeated biopsies,
and to make post-interventional quality controls possible. Since the patient is
not immobilized, since the prostate is mobile and due to the fact that probe
movements are only constrained by the rectum during biopsy acquisition, the
tracking system must be able to estimate rigid transformations that are beyond
the capture range of common image similarity measures. We propose a fast and
robust multi-resolution attribute-vector registration approach that combines
global and local optimization methods to solve this problem. Global
optimization is performed on a probe movement model that reduces the
dimensionality of the search space and thus renders optimization efficient. The
method was tested on 237 prostate volumes acquired from 14 different patients
for 3D to 3D and 3D to orthogonal 2D slices registration. The 3D-3D version of
the algorithm converged correctly in 96.7% of all cases in 6.5s with an
accuracy of 1.41mm (r.m.s.) and 3.84mm (max). The 3D to slices method yielded a
success rate of 88.9% in 2.3s with an accuracy of 1.37mm (r.m.s.) and 4.3mm
(max).
"
96,The Fast Fibonacci Decompression Algorithm,"  Data compression has been widely applied in many data processing areas.
Compression methods use variable-size codes with the shorter codes assigned to
symbols or groups of symbols that appear in the data frequently. Fibonacci
coding, as a representative of these codes, is used for compressing small
numbers. Time consumption of a decompression algorithm is not usually as
important as the time of a compression algorithm. However, efficiency of the
decompression may be a critical issue in some cases. For example, a real-time
compression of tree data structures follows this issue. Tree's pages are
decompressed during every reading from a secondary storage into the main
memory. In this case, the efficiency of a decompression algorithm is extremely
important. We have developed a Fast Fibonacci decompression for this purpose.
Our approach is up to $3.5\times$ faster than the original implementation.
"
97,Knowledge Engineering Technique for Cluster Development,"  After the concept of industry cluster was tangibly applied in many countries,
SMEs trended to link to each other to maintain their competitiveness in the
market. The major key success factors of the cluster are knowledge sharing and
collaboration between partners. This knowledge is collected in form of tacit
and explicit knowledge from experts and institutions within the cluster. The
objective of this study is about enhancing the industry cluster with knowledge
management by using knowledge engineering which is one of the most important
method for managing knowledge. This work analyzed three well known knowledge
engineering methods, i.e. MOKA, SPEDE and CommonKADS, and compares the
capability to be implemented in the cluster context. Then, we selected one
method and proposed the adapted methodology. At the end of this paper, we
validated and demonstrated the proposed methodology with some primary result by
using case study of handicraft cluster in Thailand.
"
98,MRI/TRUS data fusion for brachytherapy,"  BACKGROUND: Prostate brachytherapy consists in placing radioactive seeds for
tumour destruction under transrectal ultrasound imaging (TRUS) control. It
requires prostate delineation from the images for dose planning. Because
ultrasound imaging is patient- and operator-dependent, we have proposed to fuse
MRI data to TRUS data to make image processing more reliable. The technical
accuracy of this approach has already been evaluated. METHODS: We present work
in progress concerning the evaluation of the approach from the dosimetry
viewpoint. The objective is to determine what impact this system may have on
the treatment of the patient. Dose planning is performed from initial TRUS
prostate contours and evaluated on contours modified by data fusion. RESULTS:
For the eight patients included, we demonstrate that TRUS prostate volume is
most often underestimated and that dose is overestimated in a correlated way.
However, dose constraints are still verified for those eight patients.
CONCLUSIONS: This confirms our initial hypothesis.
"
99,"Medical image computing and computer-aided medical interventions applied
  to soft tissues. Work in progress in urology","  Until recently, Computer-Aided Medical Interventions (CAMI) and Medical
Robotics have focused on rigid and non deformable anatomical structures.
Nowadays, special attention is paid to soft tissues, raising complex issues due
to their mobility and deformation. Mini-invasive digestive surgery was probably
one of the first fields where soft tissues were handled through the development
of simulators, tracking of anatomical structures and specific assistance
robots. However, other clinical domains, for instance urology, are concerned.
Indeed, laparoscopic surgery, new tumour destruction techniques (e.g. HIFU,
radiofrequency, or cryoablation), increasingly early detection of cancer, and
use of interventional and diagnostic imaging modalities, recently opened new
challenges to the urologist and scientists involved in CAMI. This resulted in
the last five years in a very significant increase of research and developments
of computer-aided urology systems. In this paper, we propose a description of
the main problems related to computer-aided diagnostic and therapy of soft
tissues and give a survey of the different types of assistance offered to the
urologist: robotization, image fusion, surgical navigation. Both research
projects and operational industrial systems are discussed.
"
100,"Cell mapping description for digital control system with quantization
  effect","  Quantization problem in digital control system have attracted more and more
attention in these years. Normally, a quantized variable is regarded as a
perturbed copy of the unquantized variable in the research of quantization
effect, but this model has shown many obvious disadvantages in control system
analysis and design process. In this paper, we give a new model for
quantization based 'cell mapping' concept. This cell model could clearly
describe the global dynamics of quantized digital system. Then some important
characteristics of control system like controllability are analyzed by this
model. The finite precision control design method based on cell concept is also
presented.
"
101,Computer- and robot-assisted urological surgery,"  The author reviews the computer and robotic tools available to urologists to
help in diagnosis and technical procedures. The first part concerns the
contribution of robotics and presents several systems at various stages of
development (laboratory prototypes, systems under validation or marketed
systems). The second part describes image fusion tools and navigation systems
currently under development or evaluation. Several studies on computerized
simulation of urological procedures are also presented.
"
102,"A Framework for Providing E-Services to the Rural Areas using Wireless
  Ad Hoc and Sensor Networks","  In recent years, the proliferation of mobile computing devices has driven a
revolutionary change in the computing world. The nature of ubiquitous devices
makes wireless networks the easiest solution for their interconnection. This
has led to the rapid growth of several wireless systems like wireless ad hoc
networks, wireless sensor networks etc. In this paper we have proposed a
framework for rural development by providing various e-services to the rural
areas with the help of wireless ad hoc and sensor networks. We have discussed
how timely and accurate information could be collected from the rural areas
using wireless technologies. In addition to this, we have also mentioned the
technical and operational challenges that could hinder the implementation of
such a framework in the rural areas in the developing countries.
"
103,Cyberspace security: How to develop a security strategy,"  Despite all visible dividers, the Internet is getting us closer and closer,
but with a great price. Our security is the price. The international community
is fully aware of the urgent need to secure the cyberspace as you see the
multiplication of security standards and national schemes interpreting them
beyond borders: ISO 15408, ISO 17799, and ISO 27001. Even though some
countries, including the Security Big Six (SB6), are equipped with their
security books and may feel relatively safe; this remains a wrong sense of
security as long as they share their networks with entities of less security.
The standards impose security best practices and system specifications for the
development of information security management systems. Partners beyond borders
have to be secure as this is only possible if all entities connected to the
partnership remain secure. Unfortunately, there is no way to verify the
continuous security of partners without periodic security auditing and
certification, and members who do not comply should be barred from the
partnership. This concept also applies to the cyber space or the electronic
society. In order to clean our society from cyber crimes and cyber terrorism we
need to impose strict security policies and enforce them in a cooperative
manner. The paper discusses a country's effort in the development of a national
security strategy given its security economic intelligence position, its
security readiness, and its adverse exposure.
"
104,The Physical World as a Virtual Reality,"  This paper explores the idea that the universe is a virtual reality created
by information processing, and relates this strange idea to the findings of
modern physics about the physical world. The virtual reality concept is
familiar to us from online worlds, but our world as a virtual reality is
usually a subject for science fiction rather than science. Yet logically the
world could be an information simulation running on a multi-dimensional
space-time screen. Indeed, if the essence of the universe is information,
matter, charge, energy and movement could be aspects of information, and the
many conservation laws could be a single law of information conservation. If
the universe were a virtual reality, its creation at the big bang would no
longer be paradoxical, as every virtual system must be booted up. It is
suggested that whether the world is an objective reality or a virtual reality
is a matter for science to resolve. Modern information science can suggest how
core physical properties like space, time, light, matter and movement could
derive from information processing. Such an approach could reconcile relativity
and quantum theories, with the former being how information processing creates
space-time, and the latter how it creates energy and matter.
"
105,"On Breaching Enterprise Data Privacy Through Adversarial Information
  Fusion","  Data privacy is one of the key challenges faced by enterprises today.
Anonymization techniques address this problem by sanitizing sensitive data such
that individual privacy is preserved while allowing enterprises to maintain and
share sensitive data. However, existing work on this problem make inherent
assumptions about the data that are impractical in day-to-day enterprise data
management scenarios. Further, application of existing anonymization schemes on
enterprise data could lead to adversarial attacks in which an intruder could
use information fusion techniques to inflict a privacy breach. In this paper,
we shed light on the shortcomings of current anonymization schemes in the
context of enterprise data. We define and experimentally demonstrate Web-based
Information- Fusion Attack on anonymized enterprise data. We formulate the
problem of Fusion Resilient Enterprise Data Anonymization and propose a
prototype solution to address this problem.
"
106,MRI/TRUS data fusion for prostate brachytherapy. Preliminary results,"  Prostate brachytherapy involves implanting radioactive seeds (I125 for
instance) permanently in the gland for the treatment of localized prostate
cancers, e.g., cT1c-T2a N0 M0 with good prognostic factors. Treatment planning
and seed implanting are most often based on the intensive use of transrectal
ultrasound (TRUS) imaging. This is not easy because prostate visualization is
difficult in this imaging modality particularly as regards the apex of the
gland and from an intra- and interobserver variability standpoint. Radioactive
seeds are implanted inside open interventional MR machines in some centers.
Since MRI was shown to be sensitive and specific for prostate imaging whilst
open MR is prohibitive for most centers and makes surgical procedures very
complex, this work suggests bringing the MR virtually in the operating room
with MRI/TRUS data fusion. This involves providing the physician with
bi-modality images (TRUS plus MRI) intended to improve treatment planning from
the data registration stage. The paper describes the method developed and
implemented in the PROCUR system. Results are reported for a phantom and first
series of patients. Phantom experiments helped characterize the accuracy of the
process. Patient experiments have shown that using MRI data linked with TRUS
data improves TRUS image segmentation especially regarding the apex and base of
the prostate. This may significantly modify prostate volume definition and have
an impact on treatment planning.
"
107,3D/4D ultrasound registration of bone,"  This paper presents a method to reduce the invasiveness of Computer Assisted
Orthopaedic Surgery (CAOS) using ultrasound. In this goal, we need to develop a
method for 3D/4D ultrasound registration. The premilinary results of this study
suggest that the development of a robust and ``realtime'' 3D/4D ultrasound
registration is feasible.
"
108,"3D-Ultrasound probe calibration for computer-guided diagnosis and
  therapy","  With the emergence of swept-volume ultrasound (US) probes, precise and almost
real-time US volume imaging has become available. This offers many new
opportunities for computer guided diagnosis and therapy, 3-D images containing
significantly more information than 2-D slices. However, computer guidance
often requires knowledge about the exact position of US voxels relative to a
tracking reference, which can only be achieved through probe calibration. In
this paper we present a 3-D US probe calibration system based on a membrane
phantom. The calibration matrix is retrieved by detection of a membrane plane
in a dozen of US acquisitions of the phantom. Plane detection is robustly
performed with the 2-D Hough transformation. The feature extraction process is
fully automated, calibration requires about 20 minutes and the calibration
system can be used in a clinical context. The precision of the system was
evaluated to a root mean square (RMS) distance error of 1.15mm and to an RMS
angular error of 0.61 degrees. The point reconstruction accuracy was evaluated
to 0.9mm and the angular reconstruction accuracy to 1.79 degrees.
"
109,Framework for 3D TransRectal Ultrasound,"  Prostate biopsies are mainly performed under 2D TransRectal UltraSound (TRUS)
control by sampling the prostate according to a predefined pattern. In case of
first biopsies, this pattern follows a random systematic plan. Sometimes,
repeat biopsies can be needed to target regions unsampled by previous biopsies
or resample critical regions (for example in case of cancer expectant
management or previous prostatic intraepithelial neoplasia findings). From a
clinical point of view, it could be useful to control the 3D spatial
distribution of theses biopsies inside the prostate. Modern 3D-TRUS probes
allow acquiring high-quality volumes of the prostate in few seconds. We
developed a framework to track the prostate in 3D TRUS images. It means that if
one acquires a reference volume at the beginning of the session and another
during each biopsy, it is possible to determine the relationship between the
prostate in the reference and the others volumes by aligning images. We used
this tool to evaluate the ability of a single operator (a young urologist
assistant professor) to perform a pattern of 12 biopsies under 2D TRUS
guidance.
"
110,TER: A Robot for Remote Ultrasonic Examination: Experimental Evaluations,"  This chapter:
  o Motivates the clinical use of robotic tele-echography
  o Introduces the TER system
  o Describes technical and clinical evaluations performed with TER
"
111,Abstractions for biomolecular computations,"  Deoxyribonucleic acid is increasingly being understood to be an informational
molecule, capable of information processing.It has found application in the
determination of non-deterministic algorithms and in the design of molecular
computing devices. This is a theoretical analysis of the mathematical
properties and relations of the molecules which constituting DNA, which
explains in part why DNA is a successful computing molecule.
"
112,"Biopsies prostatiques sous guidage \'echographique 3D et temps r\'eel
  (4D) sur fant\^ome. Etude comparative versus guidage 2D","  This paper analyzes the impact of using 2D or 3D ultrasound on the efficiency
of prostate biopsies. The evaluation is performed on home-made phantoms. The
study shows that the accuracy is significantly improved.
"
113,Graceful Degradation of Air Traffic Operations,"  The introduction of new technologies and concepts of operation in the air
transportation system is not possible, unless they can be proven not to
adversely affect the system operation under not only nominal, but also degraded
conditions. In extreme scenarios, degraded operations due to partial or
complete technological failures should never endanger system safety. Many past
system evolutions, whether ground-based or airborne, have been based on
trial-and-error, and system safety was addressed only after a specific event
yielded dramatic or near- dramatic consequences. Future system evolutions,
however, must leverage available computation, prior knowledge and abstract
reasoning to anticipate all possible system degradations and prove that such
degradations are graceful and safe. This paper is concerned with the graceful
degradation of high-density, structured arrival traffic against partial or
complete surveillance failures. It is shown that for equal performance
requirements, some traffic configurations might be easier to handle than
others, thereby offering a quantitative perspective on these traffic
configurations. ability to ""gracefully degrade"". To support our work, we also
introduce a new conflict resolution algorithm, aimed at solving conflicts
involving many aircraft when aircraft position information is in the process of
degrading.
"
114,Partitioning the Threads of a Mobile System,"  In this paper, we show how thread partitioning helps in proving properties of
mobile systems. Thread partitioning consists in gathering the threads of a
mobile system into several classes. The partitioning criterion is left as a
parameter of both the mobility model and the properties we are interested in.
Then, we design a polynomial time abstract interpretation-based static analysis
that counts the number of threads inside each partition class.
"
115,Generic and Typical Ranks of Three-Way Arrays,"  The concept of tensor rank, introduced in the twenties, has been popularized
at the beginning of the seventies. This has allowed to carry out Factor
Analysis on arrays with more than two indices. The generic rank may be seen as
an upper bound to the number of factors that can be extracted from a given
tensor. We explain in this short paper how to obtain numerically the generic
rank of tensors of arbitrary dimensions, and compare it with the rare algebraic
results already known at order three. In particular, we examine the cases of
symmetric tensors, tensors with symmetric matrix slices, or tensors with free
entries.
"
116,Structural aspects of tilings,"  In this paper, we study the structure of the set of tilings produced by any
given tile-set. For better understanding this structure, we address the set of
finite patterns that each tiling contains. This set of patterns can be analyzed
in two different contexts: the first one is combinatorial and the other
topological. These two approaches have independent merits and, once combined,
provide somehow surprising results. The particular case where the set of
produced tilings is countable is deeply investigated while we prove that the
uncountable case may have a completely different structure. We introduce a
pattern preorder and also make use of Cantor-Bendixson rank. Our first main
result is that a tile-set that produces only periodic tilings produces only a
finite number of them. Our second main result exhibits a tiling with exactly
one vector of periodicity in the countable case.
"
117,"Fabrication of Miniaturized Variable-focus Lens Using Liquid Filling
  Technique","  This paper describes a simple method for fabricating a variable-focus lens by
using PDMS (polydimethylsiloxane) and filling with liquid for the
variable-focus lens. The lens diameter of 2-mm was designed in this experiment
and expected to reach the focal length in the range of 3 ~ 12 mm. The
theoretical value between the liquid volume and the lens contact angle at
different focal lengths were simulated and measured. The pumped-in volumes
ranged from 200 to 1400 $\mu$l, the contact angles ranged from 14.25 degrees to
49.02 degrees. Changing the deformation of PDMS film using different
micro-fluidic volume produces the variable focal length from 4 10 mm in this
experiment. The proposed method successfully fabricated a variable-focus lens.
Bonding PDMS only once using no expensive instrument such as oxygen plasma was
accomplished. The final objective is to insert the variable focus lens into
portable optical imagery products.
"
118,"A Novel X-Axis Tuning Fork Gyroscope with ""8 Vertical Springs-Proofmass""
  Structure on (111)-Silicon","  A novel x-axis tuning fork MEMS gyroscope with ""8 vertical springs-proofmass""
structure for Coriolis effect detection is presented. Compared with the common
single-plane springs, the 8 vertical springs, symmetrically located at the top
and bottom sides, more stably suspend the large thick proofmass featuring large
capacitance variation and low mechanical noise. A bulk-micromachining
technology is applied to obtain the large proofmass and twins-like dual beams.
During the fabrication process, the dimensions of the 8 vertical springs are
precisely confined by thermal oxide protected limit trenches (LTs) sidewalls
and the extreme slowly etched (111)-planes; therefore a small mismatch of less
than 30 Hz is achieved before tuning. Initial test shows a sensitivity of
0.15mV/(deg/s) and rate resolution around 0.1deg/s under atmosphere pressure.
"
119,Fabrication of MEMS Resonators in Thin SOI,"  A simple and fast process for micro-electromechanical (MEM) resonators with
deep sub-micron transduction gaps in thin SOI is presented in this paper. Thin
SOI wafers are important for advanced CMOS technology and thus are evaluated as
resonator substrates for future co-integration with CMOS circuitry on a single
chip. As the transduction capacitance scales with the resonator thickness, it
is important to fabricate deep sub-micron trenches in order to achieve a good
capacitive coupling. Through the combination of conventional UV-lithography and
focused ion beam (FIB) milling the process needs only two lithography steps,
enabling therefore a way for fast prototyping of MEM-resonators. Different FIB
parameters and etching parameters are compared in this paper and their effect
on the process are reported.
"
120,Usage of Porous Al2O3 Layers for RH Sensing,"  At the Department of Electron Devices a cheap, more or less CMOS process
compatible capacitive type RH sensor has been developed. Capacitive sensors are
based on dielectric property changes of thin films upon water vapour uptake
which depends on the surrounding media's relative humidity content. Because of
the immense surface-to-volume ratio and the abundant void fraction, very high
sensitivities can be obtained with porous ceramics. One of the ceramics to be
used is porous Al2O3, obtained by electrochemical oxidation of aluminium under
anodic bias. The average pore sizes are between 6...9 nm. In our paper we
intend to demonstrate images representing the influence of the technological
parameters on the porous structure and the device sensitivity.
"
121,Modeling of large area hot embossing,"  Today, hot embossing and injection molding belong to the established plastic
molding processes in microengineering. Based on experimental findings, a
variety of microstructures have been replicated so far using the processes.
However, with increasing requirements regarding the embossing surface and the
simultaneous decrease of the structure size down into the nanorange, increasing
know-how is needed to adapt hot embossing to industrial standards. To reach
this objective, a German-Canadian cooperation project has been launched to
study hot embossing theoretically by a process simulation and experimentally.
The present publication shall report about the first results of the simulation
- the modeling and simulation of large area replication based on an eight inch
microstructured mold.
"
122,"Liquid Density Sensing Using Resonant Flexural Plate Wave Device with
  Sol-Gel PZT Thin Films","  This paper presents the design, fabrication and preliminary experimental
results of a flexure plate wave (FPW) resonator using sol-gel derived lead
zirconate titanates (PZT) thin films. The resonator adopts a two-port structure
with reflecting grates on the composite membrane of PZT and SiNx. The design of
the reflecting grate is derived from a SAW resonator model using COM theory to
produce a sharp resonant peak. The comparison between the mass and the
viscosity effects from the theoretical expression illustrates the applications
and the constraints of the proposed device in liquid sensing. Multiple coatings
of sol-gel derived PZT films are adopted because of the cost advantage and the
high electromechanical coupling effect over other piezoelectric films. The
fabrication issues of the proposed material structure are addressed.
Theoretical estimations of the mass and the viscosity effects are compared with
the experimental results. The resonant frequency has a good linear correlation
with the density of low viscosity liquids, which demonstrate the feasibility of
the proposed device.
"
123,"Design, Fabrication and Characterization of a Piezoelectric
  Microgenerator Including a Power Management Circuit","  We report in this paper the design, fabrication and experimental
characterization of a piezoelectric MEMS microgenerator. This device scavenges
the energy of ambient mechanical vibrations characterized by frequencies in the
range of 1 kHz. This component is made with Aluminum Nitride thin film
deposited with a CMOS compatible process. Moreover we analyze two possible
solutions for the signal rectification: a discrete doubler-rectifier and a full
custom power management circuit. The ASIC developed for this application takes
advantage of diodes with very low threshold voltage and therefore allows the
conversion of extremely low input voltages corresponding to very weak input
accelerations. The volume of the proposed generator is inferior to 1mm3 and the
generated powers are in the range of 1$\mu$W. This system is intended to supply
power to autonomous wireless sensor nodes.
"
124,"A complete study of electroactive polymers for energy scavenging:
  modelling and experiments","  Recent progresses in ultra low power microelectronics propelled the
development of several microsensors and particularly the self powered
microsystems (SPMS). One of their limitations is their size and their autonomy
due to short lifetime of the batteries available on the market. To ensure their
ecological energetic autonomy, a promising alternative is to scavenge the
ambient energy such as the mechanical one. Nowadays, few microgenerators
operate at low frequency. They are often rigid structures that can perturb the
application or the environment; none of them are perfectly flexible. Thus, our
objective is to create a flexible, non-intrusive scavenger using electroactive
polymers. The goal of this work is to design a generator which can provide
typically 100 ?W to supply a low consumption system. We report in this paper an
analytical model which predicts the energy produced by a simple electroactive
membrane, and some promising experimental results.
"
125,Step-up converter for electromagnetic vibrational energy scavenger,"  This paper introduces a voltage multiplier (VM) circuit which can step up a
minimum voltage of 150 mV (peak). The operation and characteristics of this
converter circuit are described. The voltage multiplier circuit is also tested
with micro and macro scale electromagnetic vibrational generators and the
effect of the VM on the optimum load conditions of the electromagnetic
generator is presented. The measured results show that 85% efficiency can be
achieved from this VM circuit at a power level of 18 ?W.
"
126,"Gas Damping Coefficient Research for MEMS Comb Linear Vibration
  Gyroscope","  Silicon-MEMS gyroscope is an important part of MEMS (Micro Electrical
Mechanical System). There are some disturb ignored in traditional gyroscope
that must be evaluated newly because of its smaller size (reach the level of
micron). In these disturb, the air pressure largely influences the performance
of MEMS gyroscope. Different air pressure causes different gas damping
coefficient for the MEMS comb linear vibration gyroscope and different gas
damping coefficient influences the quality factor of the gyroscope directive.
The quality factor influences the dynamic working bandwidth of the MEMS comb
linear vibration gyroscope, so it is influences the output characteristic of
the MEMS comb linear vibration gyroscope. The paper shows the relationship
between the air pressure and the output amplified and phase of the detecting
axis through analyzing the air pressure influence on the MEMS comb linear
vibration gyroscope. It discusses the influence on the frequency distribute and
quality factor of the MEMS comb linear vibration gyroscope for different air
pressure.
"
127,"Comparison of Two Low-Power Electronic Interfaces for Capacitive Mems
  Sensors","  The paper discusses the importance and the issues of interfacing capacitive
sensors. Two architectures applicable for interfacing capacitive sensors are
presented. The first solution was designed to interface a capacitive humidity
sensor designed and built for a humidity-dependent monolithic capacitor
developed at Budapest University of Technology and Economics. The second case
presents the possible read-out solutions for a SOI-MEMS accelerometer. Both of
the architectures were built and tested in a discrete implementation to qualify
the methods before the integrated realization. The paper presents a detailed
comparison of the two methods
"
128,High Efficiency 3-Phase Cmos Rectifier with Step Up and Regulated,"  This paper presents several design issues related to the monolithic
integration of a 3-phase AC to DC low voltage, low power rectifier for 3-phase
micro source electrical conditioning. Reduced input voltage operation (down to
1V), high efficiency, and output voltage regulations are implemented, based on
commercially available CMOS technology. Global design and system issues are
detailed. The management of start-up sequences under self supplied conditions
as well as output voltage regulations are specifically addressed. Simulation
results, practical implementation and validation are presented. They are based
on the association of three micro elements : a 3-phase micro-generator, a stand
alone 3-phase AC to DC integrated rectifier, and an output voltage conditioner
based on a commercially available IC.
"
129,Silicon on Nothing Mems Electromechanical Resonator,"  The very significant growth of the wireless communication industry has
spawned tremendous interest in the development of high performances radio
frequencies (RF) components. Micro Electro Mechanical Systems (MEMS) are good
candidates to allow reconfigurable RF functions such as filters, oscillators or
antennas. This paper will focus on the MEMS electromechanical resonators which
show interesting performances to replace SAW filters or quartz reference
oscillators, allowing smaller integrated functions with lower power
consumption. The resonant frequency depends on the material properties, such as
Young's modulus and density, and on the movable mechanical structure dimensions
(beam length defined by photolithography). Thus, it is possible to obtain multi
frequencies resonators on a wafer. The resonator performance (frequency,
quality factor) strongly depends on the environment, like moisture or pressure,
which imply the need for a vacuum package. This paper will present first
resonator mechanisms and mechanical behaviors followed by state of the art
descriptions with applications and specifications overview. Then MEMS resonator
developments at STMicroelectronics including FEM analysis, technological
developments and characterization are detailed.
"
130,Copper Planar Microcoils Applied to Magnetic Actuation,"  Recent advances in microtechnology allow realization of planar microcoils.
These components are integrated in MEMS as magnetic sensor or actuator. In the
latter case, it is necessary to maximize the effective magnetic field which is
proportional to the current passing through the copper track and depends on the
distance to the generation microcoil. The aim of this work was to determine the
optimal microcoil design configuration for magnetic field generation. The
results were applied to magnetic actuation, taking into account technological
constraints. In particular, we have considered different realistic
configurations that involve a magnetically actuated device coupled to a
microcoil. Calculations by a semi-analytical method using Matlab software were
validated by experimental measurements. The copper planar microcoils are
fabricated by U.V. micromoulding on different substrates: flexible polymer
(Kapton) and silicate on silicon. They are constituted by a spiral-like
continuous track. Their total surface is about 1 mm2.
"
131,Ni-MH battery modelling for ambient intelligence applications,"  Mobile devices, like sensor networks and MEMS actuators use mobile power
supplies to ensure energy for their operation. These are mostly batteries. The
lifetime of the devices depends on the power consumption and on the quality and
capacitance of the battery. Though the integrated circuits and their power
consumption improve continually, their clock frequency also increases with the
time, and the resultant power consumption seems not to vary, or slightly
increase. On the other hand, the properties of batteries are developing much
slower, necessitating the optimization of their usage on system level.
"
132,"Analysis of polysilicon micro beams buckling with temperature-dependent
  properties","  The suspended electrothermal polysilicon micro beams generate displacements
and forces by thermal buckling effects. In the previous electro-thermal and
thermo-elastic models of suspended polysilicon micro beams, the
thermo-mechanical properties of polysilicon have been considered constant over
a wide rang of temperature (20- 900 degrees C). In reality, the
thermo-mechanical properties of polysilicon depend on temperature and change
significantly at high temperatures. This paper describes the development and
validation of theoretical and Finite Element Model (FEM) including the
temperature dependencies of polysilicon properties such as thermal expansion
coefficient and Young's modulus. In the theoretical models, two parts of
elastic deflection model and thermal elastic model of micro beams buckling have
been established and simulated. Also, temperature dependent buckling of
polysilicon micro beam under high temperature has been modeled by Finite
Element Analysis (FEA). Analytical results and numerical results using FEA are
compared with experimental data available in literature. Their reasonable
agreement validates analytical model and FEM. This validation indicates the
importance of including temperature dependencies of polysilicon
thermo-mechanical properties such as Coefficient of Thermal Expansion (CTE) in
the previous models.
"
133,"Parameter Identification of Pressure Sensors by Static and Dynamic
  Measurements","  Fast identification methods of pressure sensors are investigated. With regard
to a complete accurate sensor parameter identification two different
measurement methods are combined. The approach consists on one hand in
performing static measurements - an applied pressure results in a membrane
deformation measured interferometrically and the corresponding output voltage.
On the other hand optical measurements of the modal responses of the sensor
membranes are performed. This information is used in an inverse identification
algorithm to identify geometrical and material parameters based on a FE model.
The number of parameters to be identified is thereby generally limited only by
the number of measurable modal frequencies. A quantitative evaluation of the
identification results permits furthermore the classification of processing
errors like etching errors. Algorithms and identification results for membrane
thickness, intrinsic stress and output voltage will be discussed in this
contribution on the basis of the parameter identification of relative pressure
sensors.
"
134,"New Horizontal Frustum Optical Waveguide Fabrication Using UV Proximity
  Printing","  This paper presents a novel method to fabricate the horizontal frustum
structure as a planar optical waveguide by using the proximity printing
technique. A horizontal frustum optical waveguide with a both lateral and
vertical taper structure was produced. The orthogonal and inclined masks with
the diffraction effect were employed in lithography process. This method can
precisely control each horizontal frustum optical waveguide geometric profile
in the fabrication process. The horizontal frustum optical waveguide and its
array with the same inclined angle were generated. The beam propagation
simulation software (BPM_CAD) was used to modeling the optical performance. The
simulation results reveal that the mode profile matched into horizontal frustum
optical waveguide and fiber from the laser diode. The optical loss of
horizontal hemi-frustum structure of optical waveguides was less than 0.2dB.
The horizontal hemifrustum waveguide will be used for fiber coupling on boards
for further optical communication systems.
"
135,"A Fully Parameterized Fem Model for Electromagnetic Optimization of an
  RF Mems Wafer Level Package","  In this work, we present a fully parameterized capped transmission line model
for electromagnetic optimization of a wafer level package (WLP) for RF MEMS
applications using the Ansoft HFSS-TM electromagnetic simulator. All the
degrees of freedom (DoF's) in the package fabrication can be modified within
the model in order to optimize for losses and mismatch (capacitive and
inductive couplings) introduced by the cap affecting the MEMS RF behaviour.
Ansoft HFSS-TM was also validated for the simulation of capped RF MEMS devices
by comparison against experimental data. A test run of capped 50 transmission
lines and shorts was fabricated and tested.
"
136,Development of a Nanostructual Microwave Probe Based on GaAs,"  With the development of nanotechnology, the measurement of electrical
properties in local area of materials and devices has become a great need.
Although a lot kind of scanning probe microscope have been developed for
satisfying the requirement of nanotechnology, a microscope technique which can
determine electrical properties in local area of materials and devices is not
yet developed. Recently, microwave microscope has been an interest to many
researchers, due to its potential in the evaluation of electrical properties of
materials and devices. The advance of microwave is that the response of
materials is directly relative to the electromagnetic properties of materials.
However, because of the problem of the structure of probes, nanometer-scale
resolution has not been successful. To achieve the goal, a new structure
microwave probe is required. In this paper, we report a nanostructural
microwave probe. To restrain the attenuation of microwave in the probe, GaAs
was used as the substrate of the probe. To obtain the desired structure, wet
etching was used to fabricate the probe. Different with the dry etching, a
side-etching will occur under the etching mask. Utilizing this property, a
micro tip can be fabricated by etching a wafer, of which a small mask was
introduced on the surface in advance.
"
137,Characterisation of an Electrostatic Vibration Harvester,"  Harvesting energy from ambient vibration is proposed as an alternative to
storage based power supplies for autonomous systems. The system presented
converts the mechanical energy of a vibration into electrical energy by means
of a variable capacitor, which is polarized by an electret. A lumped element
model is used to study the generator and design a prototype. The device has
been micromachined in silicon, based on a two-wafer process. The prototype was
successfully tested, both using an external polarization source and an
electret.
"
138,"Surface Generation Analysis in Micro End-Milling Considering the
  Influences of Grain","  Micro end-milling method is a universal micro manufacturing method, which can
be used to fabricating complex 3D structures and parts with many materials. But
compared with their micrometer order size, their surface roughness quality is
not satisfied. In this paper, the different metal phase grains influences are
researched, and the micro end-milling process is described while the material
is anisotropic. In this paper, the physical characteristics of different
grains, especially friction coefficient and elastic module, are very critical
to determine the chip formation process and surface generation. The chip is
often discontinues because of the grain boundary effect. Through the micro
end-milling experiment, the bottom surface results correlate very well with the
theory analysis.
"
139,"An Integrated Circuit Compatible Compact Package for Thermal Gas
  Flowmeters","  An original packaging method suitable for integrated thermal mass flow
sensors is presented. The method consists in the application of a plastic
transparent adapter to the chip surface. The adapter is sealed to the chip
surface by means of a thermal procedure. By this approach it is possible to
selectively convey the fluid flow to reduced chip areas, avoiding contact with
the pads. Fabrication and testing of a very compact flow sensor is described.
"
140,"A High Power Density Electrostatic Vibration-to-Electric Energy
  Converter Based On An In-Plane Overlap Plate (IPOP) Mechanism","  In this paper, design, fabrication and characterization issues of a bulk
silicon-based, vibration powered, electric energy generator are addressed. The
converter is based on an In-Plane Overlap Plate (IPOP) configuration [1].
Measurements have shown that with a theoretically lossless electronics and a
starting voltage of 5 V, power density of 58 $\mu$W/cm3 is achievable at the
resonance frequency of 290 Hz. It can be further improved by reducing the
parasitic capacitance, which can be achieved by silicon etching, but a
considerable mass is lost. In [2], it is shown that 19% of mass reduction
improves power density from 12.95 $\mu$W/cm3 to 59 $\mu$W/cm3. Hence an
enhancement in fabrication process is proposed, which is termed as Backside
DRIE. It helps in increasing power density without loosing an important
quantity of mass. Simulations have shown that 2.5% of mass removal improves
power density up to 76.71 $\mu$W/cm3. Initial simulation results and problems
of associated electronics are also discussed.
"
141,"Formation of Embedded Microstructures by Thermal Activated Solvent
  Bonding","  We present a thermal activated solvent bonding technique for the formation of
embedded microstrucutres in polymer. It is based on the temperature dependent
solubility of polymer in a liquid that is not a solvent at room temperature.
With thermal activation, the liquid is transformed into a solvent of the
polymer, creating a bonding capability through segmental or chain
interdiffusion at the bonding interface. The technique has advantages over the
more commonly used thermal bonding due to its much lower operation temperature
(30 degrees C lower than the material's Tg), lower load, as well as shorter
time. Lap shear test indicated bonding shear strength of up to 2.9 MPa. Leak
test based on the bubble emission technique showed that the bonded microfluidic
device can withstand at least 6 bars (87 psi) of internal pressure (gauge) in
the microchannel. This technique can be applied to other systems of polymer and
solvent.
"
142,"Design and Modeling of Micromechanical GaAs based Hot Plate for Gas
  Sensors","  For modern Gas sensors, high sensitivity and low power are expected. This
paper discusses design, simulation and fabrication of new Micromachined Thermal
Converters (MTCs) based on GaAs developed for Gas sensors. Metal oxide gas
sensors generally work in high temperature mode that is required for chemical
reactions to be performed between molecules of the specified gas and the
surface of sensing material. There is a low power consumption required to
obtain the operation temperatures in the range of 200 to 500 oC. High thermal
isolation of these devices solves consumption problem and can be made by
designing of free standing micromechanical hot plates. Mechanical stability and
a fast thermal response are especially significant parameters that can not be
neglected. These characteristics can be achieved with new concept of GaAs
thermal converter.
"
143,"Set-up and characterization of a humidity sensor realized in
  LTCC-technology","  A new type of integrated temperature and humidity sensor applying
LTCC-technology has been developed and characterized. In this approach, sensing
elements are implemented using heated metal resistors (Pt-elements), where one
is exposed to the humid environment that causes the sensor element to cool down
with increased humidity, while the other one is sealed from the environment.
Sensor design is based on FEA (Finite Element Analyses) where the critical
design parameters have been analyzed with regard to the performance
characteristic of the device. The set-up of sensor element will be shown and
the functional capability will be demonstrated by experimental results.
"
144,"Micromachined Polycrystalline Sige-Based Thermopiles for Micropower
  Generation on Human Body","  This paper presents a polycrystalline silicon germanium (poly-SiGe)
thermopile specially designed for thermoelectric generators used on human body.
Both the design of the single thermocouple and the arrangement of the
thermocouple array have been described. A rim structure has been introduced in
order to increase the temperature difference across the thermocouple junctions.
The modeling of the thermocouple and the thermopile has been performed
analytically and numerically. An output power of about 1 $\mu$W at an output
voltage of more than 1 V is expected from the current design of thermopiles in
a watch-size generator. The key material properties of the poly-SiGe have been
measured. The thermopile has been fabricated and tested. Experimental results
clearly demonstrate the advantage of the rim structure in increasing output
voltage. In presence of forced convection, the output voltage of a non-released
thermopile can increase from about 53 mV/K/cm2 to about 130 mV/K/cm2 after the
rim structure is formed. A larger output voltage from the thermopile is
expected upon process completion.
"
145,"Silicon microneedles array with biodegradable tips for transdermal drug
  delivery","  This paper presents the fabrication process, characterization results and
basic functionality of silicon microneedles array with biodegradable tips. In
order to avoid the main problems related to silicon microneedles : broking of
the top part of the needles inside the skin, a simple solution can be
fabrication of microneedles array with biodegradable tips. The silicon
microneedles array was fabricated by using reactive ion etching while the
biodegradable tips were performed using and anodization process that generates
selectively porous silicon only on the top part of the skin. The paper presents
also the results of in vitro release of calcein using microneedles array with
biodegradable tips
"
146,"2-D Analysis of Enhancement of Analytes Adsorption Due to Flow Stirring
  by Electrothermal Force in The Microcantilever Sensor","  Ac electrokinetic flows are commonly used for manipulating micron-scale
particles in a biosensor system. At the solid-liquid state there are two kinds
of processes in the reaction between analytes and ligands: the mass transport
process and the chemical reaction process. The mass transport process is
related to convection and diffusion. Total or partial limit of mass transport
would retard the diffusion from the bulk fluid to the interface of reaction.
This effect decreases the possibility of adsorption of analyte and ligand
because the chemical reaction is faster than the diffusion. In order to solve
this problem, we apply an ac electric field to induce a vortex field by the
electrothermal effect, which helps in increasing the rate of diffusion. By
using the finite element analysis software, COMSOL Multiphysics, we optimized
several parameters of the microelectrode structures and the position of the
reacting surface, i.e. the microcantilever, by a simplified 2-D model and a 3-D
model. It is successful in accelerating the reacting rate of the molecule which
is limited by mass transport. The factor of the efficiency is about 1.429 when
the operating voltage is 15 Vrms peak-to-peak. In addition, the surface
concentration of the complex on the microcantilever has been simulated.
"
147,"Development and Application of a Diaphragm Micro-Pump with Piezoelectric
  Device","  In this study, a new type of thin, compact, and light weighed diaphragm
micro-pump has been successfully developed to actuate the liquid by the
vibration of a diaphragm. The micro-diaphragm pump with two valves is
fabricated in an aluminum case by using highly accurate CNC machine, and the
cross-section dimension is 5mm x 8mm. Both valves and diaphragm are
manufactured from PDMS. The amplitude of vibration by a piezoelectric device
produces an oscillating flow which may change the chamber volume by changing
the curvature of a diaphragm. Several experimental set-ups for performance test
in a single micro-diaphragm pump, isothermal flow open system, and a closed
liquid cooling system is designed and implemented. The performance of one-side
actuating micro-diaphragm pump is affected by the design of check valves,
diaphragm, piezoelectric device, chamber volume, input voltage and frequency.
The measured maximum flow rate of present design is 72 ml/min at zero total
pump head in the range of operation frequency 70-180 Hz.
"
148,Simulation of valveless micropump and mode analysis,"  In this work, a 3-D simulation is performed to study for the solid-fluid
coupling effect driven by piezoelectric materials and utilizes asymmetric
obstacles to control the flow direction. The result of simulation is also
verified. For a micropump, it is crucial to find the optimal working frequency
which produce maximum net flow rate. The PZT plate vibrates under the first
mode, which is symmetric. Adjusting the working frequency, the maximum flow
rate can be obtained. For the micrpump we studied, the optimal working
frequency is 3.2K Hz. At higher working frequency, say 20K Hz, the fluid-solid
membrane may come out a intermediate mode, which is different from the first
mode and the second mode. It is observed that the center of the mode drifts.
Meanwhile, the result shows that a phase shift lagging when the excitation
force exists in the vibration response. Finally, at even higher working
frequency, say 30K Hz, a second vibration mode is observed.
"
149,Enhanced Sensing Characteristics in MEMS-based Formaldehyde Gas Sensor,"  This study has successfully demonstrated a novel self-heating formaldehyde
gas sensor based on a thin film of NiO sensing layer. A new fabrication process
has been developed in which the Pt micro heater and electrodes are deposited
directly on the substrate and the NiO thin film is deposited above on the micro
heater to serve as sensing layer. Pt electrodes are formed below the sensing
layer to measure the electrical conductivity changes caused by formaldehyde
oxidation at the oxide surface. Furthermore, the upper sensing layer and
NiO/Al2O3 co-sputtering significantly increases the sensitivity of the gas
sensor, improves its detection limit capability. The microfabricated
formaldehyde gas sensor presented in this study is suitable not only for
industrial process monitoring, but also for the detection of formaldehyde
concentrations in buildings in order to safeguard human health.
"
150,"Mems Q-Factor Enhancement Using Parametric Amplification: Theoretical
  Study and Design of a Parametric Device","  Parametric amplification is an interesting way of artificially increasing a
MEMS Quality factor and could be helpful in many kinds of applications. This
paper presents a theoretical study of this principle, based on Matlab/Simulink
simulations, and proposes design guidelines for parametric structures. A new
device designed with this approach is presented together with the corresponding
FEM simulation results.
"
151,"Biodegradable Polylactic Acid (PLA) Microstructures for Scaffold
  Applications","  In this research, we present a simple and cost effective soft lithographic
process to fabricate PLA scaffolds for tissue engineering. In which, the
negative photoresist JSR THB-120N was spun on a glass subtract followed by
conventional UV lithographic processes to fabricate the master to cast the PDMS
elastomeric mold. A thin poly(vinyl alcohol) (PVA) layer was used as a mode
release such that the PLA scaffold can be easily peeled off. The PLA precursor
solution was then cast onto the PDMS mold to form the PLA microstructures.
After evaporating the solvent, the PLA microstructures can be easily peeled off
from the PDMS mold. Experimental results show that the desired microvessels
scaffold can be successfully transferred to the biodegradable polymer PLA.
"
152,Solving functional reliability issue for an optical electrostatic switch,"  In this paper, we report the advantage of using AC actuating signal for
driving MEMS actuators instead of DC voltages. The study is based upon micro
mirror devices used in digital mode for optical switching operation. When the
pull-in effect is used, charge injection occurs when the micro mirror is
maintained in the deflected position. To avoid this effect, a geometrical
solution is to realize grounded landing electrodes which are electro-statically
separated from the control electrodes. Another solution is the use of AC signal
which eliminates charge injection particularly if a bipolar signal is used.
Long term experiments have demonstrated the reliability of such a signal
command to avoid injection of electric charges.
"
153,"Identification of Test Structures for Reduced Order Modeling of the
  Squeeze Film Damping in Mems","  In this study the dynamic behaviour of perforated microplates oscillating
under the effect of squeeze film damping is analyzed. A numerical approach is
adopted to predict the effects of damping and stiffness transferred from the
surrounding ambient air to oscillating structures ; the effect of hole's cross
section and plate's extension is observed. Results obtained by F.E.M. models
are compared with experimental measurements performed by an optical
interferometric microscope.
"
154,Out-of-Plane Cmos Compatible Magnetometers,"  Three-dimensional MEMS magnetometers with use of residual stresses in thin
multilayers cantilevers are presented. Half-loop cantilevers based on
Lorentz-force deflection convert magnetic flux in changes, thanks to
piezoresistive transducers mounted in Wheatstone bridge. Magnetic field in the
order of 10 Gauss was measured with a sensitivity of 0.015 mV/Gauss. A Finite
Element Model of the device has been developed with Ansys for static and
dynamic simulations. Novel out-of-plane ferromagnetic nickel plate magnetometer
is also presented.
"
155,Tuneable Capacitor based on dual picks profile of the sacrificial layer,"  In this paper, we present a novel dual gap tuneable capacitor process based
on the profile of the sacrificial layer. This profile involves a tri-layer
photo-resist process with only one mask level. This realization is based on a
special profile of the sacrificial layer designed by two picks. The mechanism
of the sacrificial layer realisation is dependent on resist thickness, resist
formulation (viscosity, type of polymer and/or solvent, additives...), design
of the patterned layer (size or width) and the conditions under which this
layer is prepared: thermal treatment, etch back processes... In this
communication we demonstrate influence of the later parameters and discuss how
a dual pick profile was achieved.
"
156,"Reduced 30% scanning time 3D multiplexer integrated circuit applied to
  large array format 20KHZ frequency inkjet print heads","  Enhancement of the number and array density of nozzles within an inkjet head
chip is one of the keys to raise the printing speed and printing resolutions.
However, traditional 2D architecture of driving circuits can not meet the
requirement for high scanning speed and low data accessing points when nozzle
numbers greater than 1000. This paper proposes a novel architecture of
high-selection-speed three-dimensional data registration for inkjet
applications. With the configuration of three-dimensional data registration,
the number of data accessing points as well as the scanning lines can be
greatly reduced for large array inkjet printheads with nozzles numbering more
than 1000. This IC (Integrated Circuit) architecture involves three-dimensional
multiplexing with the provision of a gating transistor for each ink firing
resistor, where ink firing resistors are triggered only by the selection of
their associated gating transistors. Three signals: selection (S), address (A),
and power supply (P), are employed together to activate a nozzle for droplet
ejection. The smart printhead controller has been designed by a 0.35 um CMOS
process with a total circuit area, 2500 x 500 microm2, which is 80% of the
cirucuit area by 2D configuration for 1000 nozzles. Experiment results
demonstrate the functionality of the fabricated IC in operation, signal
transmission and a potential to control more than 1000 nozzles with only 31
data access points and reduced 30% scanning time.
"
157,Analysis of Asymmetric Piezoelectric Composite Beam,"  This paper deals with the vibration analysis of an asymmetric composite beam
composed of glass a piezoelectric material. The Bernoulli's beam theory is
adopted for mechanical deformations, and the electric potential field of the
piezoelectric material is assumed such that the divergence-free requirement of
the electrical displacements is satisfied. The accuracy of the analytic model
is assessed by comparing the resonance frequencies obtained by the analytic
model with those obtained by the finite element method. The model developed can
be used as a tool for designing piezoelectric actuators such as micro-pumps.
"
158,A High-Q Microwave MEMS Resonator,"  A High-Q microwave (K band) MEMS resonator is presented, which empolys
substrate integrated waveguide (SIW) and micromachined via-hole arrays by ICP
process. Nonradiation dielectric waveguide (NRD) is formed by metal filled
via-hole arrays and grounded planes. The three dimensional (3D) high
resistivity silicon substrate filled cavity resonator is fed by current probes
using CPW line. This monolithic resonator results in low cost, high performance
and easy integration with planar cicuits. The measured quality factor is beyond
180 and the resonance frequency is 21GHz.It shows a good agreement with the
simulation results. The chip size is only 4.7mm x 4.6mm x 0.5mm. Finally, as an
example of applications, a filter using two SIW resonators is designed.
"
159,"Lamination And Microstructuring Technology for a Bio-Cell Multiwell
  array","  Microtechnology becomes a versatile tool for biological and biomedical
applications. Microwells have been established long but remained
non-intelligent up to now. Merging new fabrication techniques and handling
concepts with microelectronics enables to realize intelligent microwells
suitable for future improved cancer treatment. The described technology depicts
the basis for the fabrication of a elecronically enhanced microwell. Thin
aluminium sheets are structured by laser micro machining and laminated
successively to obtain registration tolerances of the respective layers of
5..10\^A$\mu$m. The microwells lasermachined into the laminate are with
50..80\^A$\mu$m diameter, allowing to hold individual cells within the well.
The individual process steps are described and results on the microstructuring
are given.
"
160,"Monotonic and fatigue testing of spring-bridged freestanding microbeams
  application for MEMS","  Microelectromechanical systems (MEMS) technologies are developing rapidly
with increasing study of the design, fabrication and commercialization of
microscale systems and devices. Accurate knowledge on the mechanical behaviors
of thin film materials used for MEMS is important for successful design and
development of MEMS. Here a novel electroplating spring-bridge micro-tensile
specimen integrates pin-pin align holes, misalignment compensate spring, load
sensor beam and freestanding thin film is demonstrated and fabricated. The
specimen is fit into a specially designed micro-mechanical apparatus to carry
out a series of monotonic tensile testing on sub-micron freestanding thin
films. Certain thin films applicable as structure or motion gears in MEMS were
tested including sputtered gold, copper and tantalum nitride thin films. Metal
specimens were fabricated by sputtering; for tantalum nitride film samples,
nitrogen gas was introduced into the chamber during sputtering tantalum films
on the silicon wafer. The sample fabrication method involves three steps of
lithography and two steps of electroplating copper to hold a dog bone
freestanding thin film. Using standard wet etching or lift off techniques, a
series of microtensile specimens were patterned in metal thin films, holes, and
seed layer for spring and frame structure on the underlying silicon oxide
coated silicon substrate. Two steps of electroplating processing to distinct
spring and frame portion of the test chip. Finally, chemical etched away the
silicon oxide to separated electroplated specimen and silicon substrate.
"
161,"Profile Control of a Borosilicate-Glass Groove Formed by Deep Reactive
  Ion Etching","  Deep reactive ion etching (DRIE) of borosilicate glass and profile control of
an etched groove are reported. DRIE was carried out using an anodically bonded
silicon wafer as an etching mask. We controlled the groove profile, namely
improving its sidewall angle, by removing excessively thick polymer film
produced by carbonfluoride etching gases during DRIE. Two fabrication processes
were experimentally compared for effective removal of the film : DRIE with the
addition of argon to the etching gases and a novel combined process in which
DRIE and subsequent ultrasonic cleaning in DI water were alternately carried
out. Both processes improved the sidewall angle, and it reached 85o independent
of the mask-opening width. The results showed the processes can remove
excessive polymer film on sidewalls. Accordingly, the processes are an
effective way to control the groove profile of borosilicate glass.
"
162,"Selection of High Strength Encapsulant for MEMS Devices Undergoing High
  Pressure Packaging","  Deflection behavior of several encapsulant materials under uniform pressure
was studied to determine the best encapsulant for MEMS device. Encapsulation is
needed to protect movable parts of MEMS devices during high pressure transfer
molded packaging process. The selected encapsulant material has to have surface
deflection of less than 5 ?m under 100 atm vertical loading. Deflection was
simulated using CoventorWare ver.2005 software and verified with calculation
results obtained using shell bending theory. Screening design was used to
construct a systematic approach for selecting the best encapsulant material and
thickness under uniform pressure up to 100 atm. Materials considered for this
study were polyimide, parylene C and carbon based epoxy resin. It was observed
that carbon based epoxy resin has deflection of less than 5 ?m for all
thickness and pressure variations. Parylene C is acceptable and polyimide is
unsuitable as high strength encapsulant. Carbon based epoxy resin is considered
the best encapsulation material for MEMS under high pressure packaging process
due to its high strength.
"
163,A Two-Step Etching Method to Fabricate Nanopores in Silicon,"  A cost effectively method to fabricate nanopores in silicon by only using the
conventional wet-etching technique is developed in this research. The main
concept of the proposed method is a two-step etching process, including a
premier double-sided wet etching and a succeeding track-etching. A special
fixture is designed to hold the pre-etched silicon wafer inside it such that
the track-etching can be effectively carried out. An electrochemical system is
employed to detect and record the ion diffusion current once the pre-etched
cavities are etched into a through nanopore. Experimental results indicate that
the proposed method can cost effectively fabricate nanopores in silicon.
"
164,A Reconfigurable Impedance Matching Network Employing RF-MEMS Switches,"  We propose the design of a reconfigurable impedance matching network for the
lower RF frequency band, based on a developed RF-MEMS technology. The circuit
is composed of RF-MEMS ohmic relays, metal-insulator-metal (MIM) capacitors and
suspended spiral inductors, all integrated on a high resistivity Silicon
substrate. The presented circuit is well-suited for all applications requiring
adaptive impedance matching between two in principle unknown cascaded
RF-circuits. The fabrication and testing of a monolithic integrated prototype
in RF-MEMS technology from ITC-irst is currently underway.
"
165,"Towards a Methodology for Analysis of Interconnect Structures for
  3D-Integration of Micro Systems","  Functional aspects as well as the influence of integration technology on the
system behavior have to be considered in the 3D integration design process of
micro systems. Therefore, information from different physical domains has to be
provided to designers. Due to the variety of structures and effects of
different physical domains, efficient modeling approaches and simulation
algorithms have to be combined. The paper describes a modular approach which
covers detailed analysis with PDE solvers and model generation for system level
simulation.
"
166,Modeling of a piezoelectric micro-scanner,"  Micro-scanners have been widely used in many optical applications. The
micro-scanner presented in this paper uses multimorph-type bending actuators to
tilt a square plate mirror. This paper presents a complete analytical model of
the piezoelectric micro-scanner. This theoretical model based on strength of
material equations calculates the force generated by the multimorphs on the
mirror, the profile of the structure and the angular deflection of the mirror.
The proposed model, used to optimize the design of the piezoelectric silicon
micro-scanner, is intended for further HDL integration, allowing in this way
system level simulation and optimization.
"
167,"A novel method for fatigue testing of MEMS devices containing movable
  elements","  In this paper we present an electronic circuit for position or capacitance
estimation of MEMS electrostatic actuators based on a switched capacitor
technique. The circuit uses a capacitive divider configuration composed by a
fixed capacitor and the variable capacitance of the electrostatic actuator for
generating a signal that is a function of the input voltage and capacitive
ratio. The proposed circuit can be used to actuate and to sense position of an
electrostatic MEMS actuator without extra sensing elements. This approach is
compatible with the requirements of most analog feedback systems and the
circuit topology of pulsed digital oscillators (PDO).
"
168,Noise and thermal stability of vibrating micro-gyrometers preamplifiers,"  The preamplifier is a critical component of gyrometer's electronics. Indeed
the resolution of the sensor is limited by its signal to noise ratio, and the
gyrometer's thermal stability is limited by its gain drift. In this paper, five
different kinds of preamplifiers are presented and compared. Finally, the
design of an integrated preamplifier is shown in order to increase the gain
stability while reducing its noise and size.
"
169,0-level Vacuum Packaging RT Process for MEMS Resonators,"  A new Room Temperature (RT) 0-level vacuum package is demonstrated in this
work, using amorphous silicon (aSi) as sacrificial layer and SiO2 as structural
layer. The process is compatible with most of MEMS resonators and Resonant
Suspended-Gate MOSFET [1] fabrication processes. This paper presents a study on
the influence of releasing hole dimensions on the releasing time and hole
clogging. It discusses mass production compatibility in terms of packaging
stress during back-end plastic injection process. The packaging is done at room
temperature making it fully compatible with IC-processed wafers and avoiding
any subsequent degradation of the active devices.
"
170,From MEMS Devices to Smart Integrated Systems,"  The smart integrated systems of tomorrow would demand a combination of
micromechanical components and traditional electronics. On-chip solutions will
be the ultimate goal. One way of making such systems is to implement the
mechanical parts in an ordinary CMOS process. This procedure has been used to
design an oscillator consisting of a resonating cantilever beam and a CMOS
Pierce feedback amplifier. The resonating frequency is changed if the beam is
bent by external forces. The paper describes central features of this procedure
and highlights the design considerations for the CMOS-MEMS oscillator. The
circuit is used as an example of a ""VLSI designer"" way of making future
integrated micromechanical and microelectronic systems on-chip. The possibility
for expansion to larger systems is reviewed.
"
171,Interconnect Challenges in Highly Integrated MEMS/ASIC Subsystems,"  Micromechanical devices like accelerometers or rotation sensors form an
increasing segment beneath the devices supplying the consumer market. A hybrid
integration approach to build smart sensor clusters for the precise detection
of movements in all spatial dimensions requires a large toolbox of interconnect
technologies, each with its own constraints regarding the total process
integration. Specific challenges described in this paper are post-CMOS
feedthroughs, front-to-front die contact arrays, vacuum-compliant lateral
interconnect and fine-pitch solder balling to finally form a Chip-Scale
System-in-Package (CSSiP).
"
172,"Experimental Characterization of the static behaviour of
  microcatntilevers electrostatically actuated","  This paper concerns the experimental validation of some mathematical models
previously developed by the authors, to predict the static behaviour of
microelectrostatic actuators, basically free-clamped microbeams. This layout is
currently used in RF-MEMS design operation or even in material testing at
microscale. The analysis investigates preliminarily the static behaviour of a
set of microcantilevers bending in-plane. This investigation is aimed to
distinguish the geometrical linear behaviour, exhibited under small
displacement assumption, from the geometrical nonlinearity, caused by large
deflection. The applied electromechanical force, which nonlinearly depends on
displacement, charge and voltage, is predicted by a coupled-field approach,
based on numerical methods and herewith experimentally validated, by means of a
Fogale Zoomsurf 3D. Model performance is evaluated on pull-in prediction and on
the curve displacement vs. voltage. In fact, FEM nonlinear solution performed
by a coupled-field approach, available on commercial codes, and by a FEM
non-incremental approach are compared with linear solution, for different
values of the design parameters.
"
173,Nanobiosensors based on individual olfactory receptors,"  In the SPOT-NOSED European project, nanoscale sensing elements bearing
olfactory receptors and grafted onto functionalized gold substrates are used as
odorant detectors to develop a new concept of nanobioelectronic nose, through
sensitive impedancemetric measurement of single receptor conformational change
upon ligand binding, with a better specificity and lower detection threshold
than traditional physical sensors.
"
174,Bridge configurations in piezoresistive two-axis accelerometers,"  In piezoresisitive two-axis accelerometers with two proof masses suspended by
cantilever beams, there are generally many ways to configure the Wheatstone
bridges. The configurations are different both with respect to functionality
and performance. The main distinction is between bridges that contain resistors
belonging to both proof masses, and the one bridge that doesn't. We compare the
different bridge configurations by analytical calculations of bridge
non-linearity, robustness towards manufacturing variations and electronic
noise. We consider accelerometers where the ratio between the sensitivity to
acceleration normal and parallel to the chip plane vary over a wide range. For
numerical examples we use representative values for p-type silicon. The
performance of the configuration with one bridge connected to each proof mass
is superior to those that combine resistors belonging to different proof
masses.
"
175,Online Sensor Testing through Superposition of Encoded Stimulus,"  Online monitoring remains an important requirement for a range of
microsystems. The solution based on the injection of an actuating test stimulus
into the bias structure of active devices holds great potential. This paper
presents an improved solution that aims to remove the measurand-induced signal
from the sensor output. It involves encoding the test stimulus and using a
covariance algorithm to reject the signal that does not contain the code. The
trade-off between the sine wave rejection ratio of the technique and the test
time response is studied and, in the case of a MEMS accelerometer, it is
demonstrated that the rejection is higher than 14dB for a test time of about
0.7s. Furthermore, the accuracy of the test signal can be evaluated to
guarantee the integrity of the online test output.
"
176,Modeling of T-Shaped Microcantilever Resonators,"  The extensive research and development of micromechanical resonators is
trying to allow the use of these devices for highly sensitive applications.
Microcantilevers are some of the simplest MEMS structure and had been proved to
be a good platform due to its excellent mechanical properties. A cantilever
working in dynamic mode, adjust its resonance frequency depending on changes in
both the spring constant (k) and mass (m) of the resonator. The aim of this
work was to model a cantilever structure to determine the optimal dimensions in
which the resonance frequency would be a function dominated by mass changes and
not stiffness changes. In order to validate the model a set of microcantilevers
were fabricated and characterized.
"
177,"Novel Bonding technologies for wafer-level transparent packaging of
  MOEMS","  Depending on the type of Micro-Electro-Mechanical System (MEMS), packaging
costs are contributing up to 80% of the total device cost. Each MEMS device
category, its function and operational environment will individually dictate
the packaging requirement. Due to the lack of standardized testing procedures,
the reliability of those MEMS packages sometimes can only be proven by taking
into consideration its functionality over lifetime. Innovation with regards to
cost reduction and standardization in the field of packaging is therefore of
utmost importance to the speed of commercialisation of MEMS devices. Nowadays
heavily driven by consumer applications the MEMS device market is forecasted to
enjoy a compound annual growth rate (CAGR) above 13%, which is when compared to
the IC device market, an outstanding growth rate. Nevertheless this forecasted
value can drift upwards or downwards depending on the rate of innovation in the
field of packaging. MEMS devices typically require a specific fabrication
process where the device wafer is bonded to a second wafer which effectively
encapsulates the MEMS structure. This method leaves the device free to move
within a vacuum or an inert gas atmosphere.
"
178,"Design and Fabrication of the Suspended High-Q Spiral Inductors with
  X-Beams","  In this paper, deep sub-micron CMOS process compatible high Q on chip spiral
inductors with air gap structure were designed and fabricated. In the design
the electromagnetic were used for electrical-characteristics and maximum
mechanical strength, respectively. The copper wires were capped with
electroless Ni plating to prevent the copper from oxidizing. A Si3N4/ SiO2
X-beam was designed to increase the mechanical strength of the inductor in air
gap. The enhancement of maximum mechanical strength of a spiral inductor with
X-beams is more than 4500 times. Among these structures, the measured maximum
quality factor (Q) of the suspending inductor and frequency at maximum Q are
improved from 5.2 and 1.6GHz of conventional spiral inductor to 7.3 and 2.1
GHz, respectively.
"
179,One MEMS Design Tool with Maximal Six Design Flows,"  This paper presents one MEMS design tool with total six design flows, which
makes it possible that the MEMS designers are able to choose the most suitable
design flow for their specific devices. The design tool is divided into three
levels and interconnected by six interfaces. The three levels are
lumped-element model based system level, finite element analysis based device
level and process level, which covers nearly all modeling and simulation
functions for MEMS design. The six interfaces are proposed to automatically
transmit the design data between every two levels, thus the maximal six design
flows could be realized. The interfaces take the netlist, solid model and
layout as the data inlet and outlet for the system, device and process level
respectively. The realization of these interfaces are presented and verified by
design examples, which also proves that the enough flexibility in the design
flow can really increase the design efficiency.
"
180,Studies of Polymer Deformation and Recovery in Hot Embossing,"  In large area micro hot embossing, the process temperature plays a critical
role to both the local fidelity of microstructure formation and global
uniformity. The significance of low temperature hot embossing is to improve
global flatness of embossed devices. This paper reports on experimental studies
of polymer deformation and relaxation in micro embossing when the process
temperatures are below or near its glass transition temperature (Tg). In this
investigation, an indentation system and a micro embosser were used to
investigate the relationship of microstructure formation versus process
temperature and load pressure. The depth of indentation was controlled and the
load force at a certain indentation depth was measured. Experiments were
carried out using 1 mm thick PMMA films with the process temperature ranging
from Tg-55 degrees C to Tg +20 degrees C. The embossed structures included a
single micro cavity and groups of micro cavity arrays. It was found that at
temperature of Tg-55 degrees C, elastic deformation dominated the formation of
microstructures and significant relaxation happened after embossing. From Tg-20
degrees C to Tg, plastic deformation dominated polymer deformation, and
permanent cavities could be formed on PMMA substrates without obvious
relaxation. However, the formation of protrusive structures as micro pillars
was not complete since there was little polymer flow. With an increase in
process temperature, microstructure could be formed under lower loading
pressure. Considering the fidelity of a single microstructure and global
flatness of embossed substrates, micro hot embossing at a low process
temperature, but with good fidelity, should be preferred.
"
181,"Evaluation of the thermal and hydraulic performances of a very thin
  sintered copper flat heat pipe for 3D microsystem packages","  The reported research work presents numerical studies validated by
experimental results of a flat micro heat pipe with sintered copper wick
structure. The objectives of this project are to produce and demonstrate the
efficiency of the passive cooling technology (heat pipe) integrated in a very
thin electronic substrate that is a part of a multifunctional 3-D electronic
package. The enhanced technology is dedicated to the thermal management of high
dissipative microsystems having heat densities of more than 10W/cm2. Future
applications are envisaged in the avionics sector. In this research 2D
numerical hydraulic model has been developed to investigate the performance of
a very thin flat micro heat pipe with sintered copper wick structure, using
water as a refrigerant. Finite difference method has been used to develop the
model. The model has been used to determine the mass transfer and fluid flow in
order to evaluate the limits of heat transport capacity as functions of the
dimensions of the wick and the vapour space and for various copper spheres
radii. The results are presented in terms of liquid and vapour pressures within
the heat pipe. The simulated results are validated by experiments and proved
that the method can be further used to predict thermal performance of the heat
pipe and to optimise its design.
"
182,Architecture for Integrated Mems Resonators Quality Factor Measurement,"  In this paper, an architecture designed for electrical measurement of the
quality factor of MEMS resonators is proposed. An estimation of the measurement
performance is made using PSPICE simulations taking into account the
component's non-idealities. An error on the measured Q value of only several
percent is achievable, at a small integration cost, for sufficiently high
quality factor values (Q > 100).
"
183,"Optimization of Cricket-inspired, Biomimetic Artificial Hair Sensors for
  Flow Sensing","  High density arrays of artificial hair sensors, biomimicking the extremely
sensitive mechanoreceptive filiform hairs found on cerci of crickets have been
fabricated successfully. We assess the sensitivity of these artificial sensors
and present a scheme for further optimization addressing the deteriorating
effects of stress in the structures. We show that, by removing a portion of
chromium electrodes close to the torsional beams, the upward lift at the edges
of the membrane due to the stress, will decrease hence increase the
sensitivity.
"
184,An Improved Scheme for Initial Ranging in OFDMA-based Networks,"  An efficient scheme for initial ranging has recently been proposed by X. Fu
et al. in the context of orthogonal frequency-division multiple-access (OFDMA)
networks based on the IEEE 802.16e-2005 standard. The proposed solution aims at
estimating the power levels and timing offsets of the ranging subscriber
stations (RSSs) without taking into account the effect of possible carrier
frequency offsets (CFOs) between the received signals and the base station
local reference. Motivated by the above problem, in the present work we design
a novel ranging scheme for OFDMA in which the ranging signals are assumed to be
misaligned both in time and frequency. Our goal is to estimate the timing
errors and CFOs of each active RSS. Specifically, CFO estimation is
accomplished by resorting to subspacebased methods while a least-squares
approach is employed for timing recovery. Computer simulations are used to
assess the effectiveness of the proposed solution and to make comparisons with
existing alternatives.
"
185,"Qtier-Rapor: Managing Spreadsheet Systems & Improving Corporate
  Performance, Compliance and Governance","  Much of what EuSpRIG discusses is concerned with the integrity of individual
spreadsheets. In businesses, interlocking spreadsheets are regularly used to
fill functional gaps in core administrative systems. The growth and deployment
of such integrated spreadsheet SYSTEMS raises the scale of issues to a whole
new level. The correct management of spreadsheet systems is necessary to ensure
that the business achieves its goals of improved performance and good corporate
governance, within the constraints of legislative compliance - poor management
will deliver the opposite. This paper is an anatomy of the real-life issues of
the commercial use of spreadsheets in business, and demonstrates how
Qtier-Rapor has been used to instil best practice in the use of integrated
commercial spreadsheet systems.
"
186,"From a set of parts to an indivisible whole. Part I: Operations in a
  closed mode","  This paper provides a description of a new method for information processing
based on holistic approach wherein analysis is a direct product of synthesis.
The core of the method is iterative averaging of all the elements of a system
according to all the parameters describing the elements. Contrary to common
logic, the iterative averaging of a system's elements does not result in
homogenization of the system; instead, it causes an obligatory subdivision of
the system into two alternative subgroups, leaving no outliers. Within each of
the formed subgroups, similarity coefficients between the elements reach the
value of 1, whereas similarity coefficients between the elements of different
subgroups equal a certain constant value greater than 0 but lower than 1. When
subjected to iterative averaging, any system consisting of three or more
elements of which at least two elements are not completely identical undergo
such a process of bifurcation that occurs non-linearly. Successive iterative
averaging of each of the forming subgroups eventually provides a hierarchical
system that reflects relationships between the elements of an input system
under analysis. We propose a definition of a natural hierarchy that can exist
only in conditions of closeness of a system and can be discovered upon
providing such an effect onto a system which allows its elements interact with
each other based on the principle of self-organization. Self-organization can
be achieved through an overall and total cross-averaging of a system's
elements. We demonstrate the application potentials of the proposed technology
on a number of examples, including a system of scattered points, randomized
datasets, as well as meteorological and demographical datasets.
"
187,"Evaluation and exploitation of knowledge robustness in knowledge-based
  systems","  Industrial knowledge is complex, difficult to formalize and very dynamic in
reason of the continuous development of techniques and technologies. The
verification of the validity of the knowledge base at the time of its
elaboration is not sufficient. To be exploitable, this knowledge must then be
able to be used under conditions (slightly) different from the conditions in
which it was formalized. So, it becomes vital for the company to permanently
evaluate the quality of the industrial knowledge implemented in the system.
This evaluation is founded on the concept of robustness of the knowledge
formalized by conceptual graphs. The evaluation method is supported by a
computerized tool.
"
188,Science mapping with asymmetrical paradigmatic proximity,"  We propose a series of methods to represent the evolution of a field of
science at different levels: namely micro, meso and macro levels. We use a
previously introduced asymmetric measure of paradigmatic proximity between
terms that enables us to extract structure from a large publications database.
We apply our set of methods on a case study from the complex systems community
through the mapping of more than 400 complex systems science concepts indexed
from a database as large as several millions of journal papers. We will first
summarize the main properties of our asymmetric proximity measure. Then we show
how salient paradigmatic fields can be embedded into a 2-dimensional
visualization into which the terms are plotted according to their relative
specificity and generality index. This meso-level helps us producing
macroscopic maps of the field of science studied featuring the former
paradigmatic fields.
"
189,The Role of Management Practices in Closing the Productivity Gap,"  There is no doubt that management practices are linked to the productivity
and performance of a company. However, research findings are mixed. This paper
provides a multi-disciplinary review of the current evidence of such a
relationship and offers suggestions for further exploration. We provide an
extensive review of the literature in terms of research findings from studies
that have been trying to measure and understand the impact that individual
management practices and clusters of management practices have on productivity
at different levels of analysis. We focus our review on Operations Management
(om) and Human Resource Management (hrm) practices as well as joint
applications of these practices. In conclusion, we can say that taken as a
whole, the research findings are equivocal. Some studies have found a positive
relationship between the adoption of management practices and productivity,
some negative and some no association whatsoever. We believe that the lack of
universal consensus on the effect of the adoption of complementary management
practices might be driven either by measurement issues or by the level of
analysis. Consequently, there is a need for further research. In particular,
for a multi-level approach from the lowest possible level of aggregation up to
the firm-level of analysis in order to assess the impact of management
practices upon the productivity of firms.
"
190,"Geographic Information Systems in Evaluation and Visualization of
  Construction Schedule","  Commercially available scheduling tools such as Primavera and Microsoft
Project fail to provide information pertaining to the spatial aspects of
construction project. A methodology using geographical information systems
(GIS) is developed to represent spatial aspects of the construction progress
graphically by synchronizing it with construction schedule. The spatial aspects
are depicted by 3D model developed in AutoCAD and construction schedule is
generated using Microsoft Excel. Spatial and scheduling information are linked
together into the GIS environment (ArcGIS). The GIS-based system developed in
this study may help in better understanding the schedule along with its spatial
aspects.
"
191,Some properties of the regular asynchronous systems,"  The asynchronous systems are the models of the asynchronous circuits from the
digital electrical engineering. An asynchronous system f is a multi-valued
function that assigns to each admissible input u a set f(u) of possible states
x in f(u). A special case of asynchronous system consists in the existence of a
Boolean function \Upsilon such that for any u and any x in f(u), a certain
equation involving \Upsilon is fulfilled. Then \Upsilon is called the generator
function of f (Moisil used the terminology of network function) and we say that
f is generated by \Upsilon. The systems that have a generator function are
called regular.
  Our purpose is to continue the study of the generation of the asynchronous
systems that was started in [2], [3].
"
192,"From a set of parts to an indivisible whole. Part II: Operations in an
  open comparative mode","  This paper describes a new method, HGV2C, for pattern analysis. The HGV2C
method involves the construction of a computer ego (CE) based on an individual
object that can be either a part of the system under analysis or a newly
created object based on a certain hypothesis. The CE provides a capability to
analyze data from a specific standpoint, e.g. from a viewpoint of a certain
object. The CE is constructed from two identical copies of a query object, and
its functioning mechanism involves: a hypothesis-parameter (HP) and
infothyristor (IT). HP is a parameter that is introduced into an existing set
of parameters. The HP value for one of the clones of a query object is set to
equal 1, whereas for another clone it is greater than 1. The IT is based on the
previously described algorithm of iterative averaging and performs three
functions: 1) computation of a similarity matrix for the group of three objects
including two clones of a query object and a target object; 2) division of the
group into two alternative subgroups; and 3) a successive increase of the HP
weight in the totality of all the parameters. Initially, both clones of the
query object appear together in one of the subgroups as all of their parameter
values, except the HP, are identical. At a certain point of the HP
multiplication, one of the clones moves to the group of the target object. A
respective number of the HP multiplications represents the dissimilarity (D)
between the query and target objects. The product of D multiplied by the
difference in HP values of the clones is strictly constant and linearly
increases as the difference in HP values of the clones decreases. This new
approach to knowledge representation is demonstrated on the example of
population pyramids of 220 countries.
"
193,Climate modification directed by control theory,"  Climate modification measures to counteract global warming receive some more
new attentions in these years. Most current researches only discuss the impact
of these measures to climate, but how to design such a climate regulator is
still unknown. This paper shows the control theory could give the systematic
direction for climate modification. But the control analyzing also reveals that
climate modifications should only be regarded as a last-ditch measure.
"
194,Diversity-Integration Trade-offs in MIMO Detection,"  In this work, a MIMO detection problem is considered. At first, we derive the
Generalized Likelihood Ratio Test (GLRT) for arbitrary transmitted signals and
arbitrary time-correlation of the disturbance. Then, we investigate design
criteria for the transmitted waveforms in both power-unlimited and
power-limited systems and we study the interplay among the rank of the
optimized code matrix, the number of transmit diversity paths and the amount of
energy integrated along each path. The results show that increasing the rank of
the code matrix allows generating a larger number of diversity paths at the
price of reducing the average signal-to-clutter level along each path.
"
195,Sub-$\mu$ structured Lotus Surfaces Manufacturing,"  Sub-micro structured surfaces allow modifying the behavior of polymer films
or components. Especially in micro fluidics a lotus-like characteristic is
requested for many applications. Structure details with a high aspect ratio are
necessary to decouple the bottom and the top of the functional layer. Unlike to
stochastic methods, patterning with a LIGA-mold insert it is possible to
structure surfaces very uniformly or even with controlled variations (e.g. with
gradients). In this paper we present the process chain to realize polymer
sub-micro structures with minimum lateral feature size of 400 nm and up to 4
micrometers high.
"
196,"Linear and Non Linear Behaviour of Mechanical Resonators for Optimized
  Inertial Electromagnetic Microgenerators","  The need for wearable or abandoned microsystems, as well as the trend to a
lower power consumption of electronic devices, make miniaturized renewable
energy generators a viable alternative to batteries. Among the different
alternatives, an interesting option is the use of inertial microgenerators for
energy scavenging from vibrations present in the environment. These devices
constitute perpetual energy sources without the need for refilling, thus being
well suited for abandoned sensors, wireless systems or microsystems which must
be embedded within the structure, without outside physical connections.
Different electromagnetic energy scavenging devices have been described in the
literature [1,2,3], based on the use of a velocity damped resonator, which is
well suited for harvesting of vibrational energy induced by the operation of
machines. These vibrations are characterized by a well defined frequency (in
the range between few Hz's and few kHz's) and low displacement amplitudes.
Adjusting the resonant frequency of the system to that of the vibrations allows
amplification of these low amplitude displacements. Moreover, for these
applications, the use of an electromagnetic device has the potential advantages
of a good level of compatibility with Si Microsystem technology, as well as the
possibility of relatively high electromechanical coupling with simple designs.
"
197,"Design And Fabrication of Condenser Microphone Using Wafer Transfer And
  Micro-electroplating Technique","  A novel fabrication process, which uses wafer transfer and
micro-electroplating technique, has been proposed and tested. In this paper,
the effects of the diaphragm thickness and stress, the air-gap thickness, and
the area ratio of acoustic holes to backplate on the sensitivity of the
condenser microphone have been demonstrated since the performance of the
microphone depends on these parameters. The microphone diaphragm has been
designed with a diameter and thickness of 1.9 mm and 0.6 $\mu$m, respectively,
an air-gap thickness of 10 $\mu$m, and a 24% area ratio of acoustic holes to
backplate. To obtain a lower initial stress, the material used for the
diaphragm is polyimide. The measured sensitivities of the microphone at the
bias voltages of 24 V and 12 V are -45.3 and -50.2 dB/Pa (at 1 kHz),
respectively. The fabricated microphone shows a flat frequency response
extending to 20 kHz.
"
198,Porous Alumina Based Capacitive MEMS RH Sensor,"  The aim of a joint research and development project at the BME and HWU is to
produce a cheap, reliable, low-power and CMOS-MEMS process compatible
capacitive type relative humidity (RH) sensor that can be incorporated into a
state-of-the-art, wireless sensor network. In this paper we discuss the
preparation of our new capacitive structure based on post-CMOS MEMS processes
and the methods which were used to characterize the thin film porous alumina
sensing layer. The average sensitivity is approx. 15 pF/RH% which is more than
a magnitude higher than the values found in the literature. The sensor is
equipped with integrated resistive heating, which can be used for maintenance
to reduce drift, or for keeping the sensing layer at elevated temperature, as
an alternative method for temperature-dependence cancellation.
"
199,Integrated RF MEMS/CMOS Devices,"  A maskless post-processing technique for CMOS chips is developed that enables
the fabrication of RF MEMS parallel-plate capacitors with a high quality factor
and a very compact size. Simulations and measured results are presented for
several MEMS/CMOS capacitors. A 2-pole coupled line tunable bandpass filter
with a center frequency of 9.5 GHz is designed, fabricated and tested. A tuning
range of 17% is achieved using integrated variable MEMS/CMOS capacitors with a
quality factor exceeding 20. The tunable filter occupies a chip area of 1.2 x
2.1 mm2.
"
200,Design Methodology and Manufacture of a Microinductor,"  Potential core materials to supersede ferrite in the 0.5-10 MHz frequency
range are investigated. The performance of electrodeposited nickel-iron,
cobalt-iron-copper alloys and the commercial alloy Vitrovac 6025 have been
assessed through their inclusion within a custom-made solenoid microinductor.
Although the present inductor, at 500 KHz, achieves 77% power efficiency for
24.7W/cm3 power density, an optimized process predicts a power efficiency of
97% for 30.83W/cm3 power density. The principle issues regarding microinductor
design and performance are discussed.
"
201,Megasonic Enhanced Electrodeposition,"  A novel way of filling high aspect ratio vertical interconnection (microvias)
with an aspect ratio of >2:1 is presented. High frequency acoustic streaming at
megasonic frequencies enables the decrease of the Nernst-diffusion layer down
to the sub-micron range, allowing thereby conformal electrodeposition in deep
grooves. Higher throughput and better control over the deposition properties
are possible for the manufacturing of interconnections and metal-based MEMS.
"
202,UV Direct-Writing of Metals on Polyimide,"  Conductive micro-patterned copper tracks were fabricated by UV direct-writing
of a nanoparticle silver seed layer followed by selective electroless copper
deposition. Silver ions were first incorporated into a hydrolyzed polyimide
surface layer by wet chemical treatment. A photoreactive polymer coating,
methoxy poly(ethylene glycol) (MPEG) was coated on top of the substrate prior
to UV irradiation. Electrons released through the interaction between the MPEG
molecules and UV photons allowed the reduction of the silver ions across the
MPEG/doped polyimide interface. The resultant silver seed layer has a cluster
morphology which is suitable for the initiation of electroless plating. Initial
results showed that the deposited copper tracks were in good agreement with the
track width on the photomask and laser direct-writing can also fabricate
smaller line width metal tracks with good accuracy. The facile fabrication
presented here can be carried out in air, at atmospheric pressure, and on
contoured surfaces.
"
203,Micro Embossing of Ceramic Green Substrates for Micro Devices,"  Multilayered ceramic substrates with embedded micro patterns are becoming
increasingly important, for example, in harsh environment electronics and
microfluidic devices. Fabrication of these embedded micro patterns, such as
micro channels, cavities and vias, is a challenge. This study focuses on the
process of patterning micro features on ceramic green substrates using micro
embossing. A ceramic green tape that possessed near-zero shrinkage in the x-y
plane was used, six layers of which were laminated as the embossing substrate.
The process parameters that impact on the pattern fidelity were investigated
and optimized in this study. Micro features with line-width as small as several
micrometers were formed on the ceramic green substrates. The dynamic
thermo-mechanical analysis indicated that extending the holding time at certain
temperature range would harden the green substrates with little effect on
improving the embossing fidelity. Ceramic substrates with embossed micro
patterns were obtain d after co-firing. The embedded micro channels were also
obtained by laminating the green tapes on the embossed substrates.
"
204,Characterization and Modeling of an Electro-thermal MEMS Structure,"  Thermal functional circuits are an interesting and perspectivic group of the
MEMS elements. A practical realization is called Quadratic Transfer
Characteristic (QTC) element which driving principle is the Seebeck-effect. In
this paper we present the analyses of a QTC element from different
perspectives. To check the real behavior of the device, we measured a few,
secondary properties of the structure which correspond to special behavior
because these properties can not be easily derived from the main
characteristics.
"
205,Measurement of Large Forces and Deflections in Microstructures,"  Properties of typical MEMS materials have been widely investigated.
Mechanical properties of MEMS structures depend not only on the bulk material
properties, but also structural factors. A measurement system has been made to
measure force/deflection on microstructures to examine some of the structural
properties. This is a stylus setup integrated with a load cell and a linear
actuator. First, the requirements for the measurement system were established.
Then the system was built up and characterized. We have successfully made
measurements on a typical micromechanical structure, a cantilever accelerometer
design. The stylus placement accuracy, the spring constant along the proof
mass, analysis of the force/deflection curve shape and destructive tests on the
cantilever have been investigated in our experiment and will be presented in
this paper.
"
206,Contactless Thermal Characterization of High Temperature Test Chamber,"  In this paper the methodology and the results of a contactless thermal
characterization of a high temperature test chamber will be introduced. The
test chamber is used for fatigue testing of different MEMS devices where the
homogenous temperature distribution within the close proximity from the heating
filaments is very important. Our aim was to characterize the evolving
temperature distribution inside the test chamber. In order to achieve smaller
time constant a new contactless sensor card was developed. The contactless
thermal characterization method introduced in this paper enables in situ heat
distribution measurement inside the test chamber during operation, with the
detection of potentially uneven heat distribution.
"
207,"Processing and Characterization of Precision Microparts from
  Nickel-based Materials","  The objective of this research was to study the influence of electroplating
parameters on electrodeposit characteristics for the production of nickel (Ni)
and nickel-iron (Ni-Fe) microparts by photoelectroforming. The research focused
on the most relevant parameter for industry, which is the current density,
because it determines the process time and the consumed energy. The results of
the Ni and Ni-Fe characterisations can be divided into two aspects closely
linked with each other ; the morphology and the hardness.
"
208,"Manufacturing of A micro probe using supersonic aided electrolysis
  process","  In this paper, a practical micromachining technology was applied for the
fabrication of a micro probe using a complex nontraditional machining process.
A series process was combined to machine tungsten carbide rods from original
dimension. The original dimension of tungsten carbide rods was 3mm ; the rods
were ground to a fixed-dimension of 50 micrometers using precision grinding
machine in first step. And then, the rod could be machined to a
middle-dimension of 20 micrometers by electrolysis. A final desired micro
dimension can be achieved using supersonic aided electrolysis.
High-aspect-ratio of micro tungsten carbide rod was easily obtained by this
process. Surface roughness of the sample with supersonic aided agitation was
compared with that with no agitation in electrolysis. The machined surface of
the sample is very smooth due to ionized particles of anode could be removed by
supersonic aided agitation during electrolysis. Deep micro holes can also be
achieved by the machined high-aspect-rati tungsten carbide rod using EDM
process. A micro probe of a ball shape at the end was processed by proposed
supersonic aided electrolysis machining process.
"
209,Large Area Roller Embossing of Multilayered Ceramic Green Composites,"  In this paper, we will report our achievements in developing large area
patterning of multilayered ceramic green composites using roller embossing. The
aim of our research is to pattern large area ceramic green composites using a
modified roller laminating apparatus, which is compatible with screen printing
machines, for integration of embossing and screen printing. The instrumentation
of our roller embossing apparatus, as shown in Figure1, consists of roller 1
and rollers 2. Roller 1 is heated up to the desired embossing temperature ;
roller 2 is, however, kept at room temperature. The mould is a nickel template
manufactured by plating nickel-based micro patterns (height : 50 $\mu$m) on a
nickel film (thickness : 70 $\mu$m) ; the substrate for the roller embossing is
a multilayered Heraeus Heralock HL 2000 ceramic green composite. Comparing with
the conventional simultaneous embossing, the advantages of roller embossing
include : (1) low embossing force ; (2) easiness of demoulding ; (3) localized
area in contact with heater ; and etc. We have demonstrated the capability of
large area roller embossing with a panel size of 150mmx 150mm on the mentioned
substrate. We have explored and confirmed the impact of parameters (feed speed,
temperature of roller and applied pressure) to the pattern quality of roller
embossing. Furthermore, under the optimized process parameters, we
characterized the variations of pattern dimension over the panel area, and
calculated a scaling factor in order to make the panel compatible with other
processes. Figure 2 shows the embossed patterns on a 150mmx 150mm green ceramic
panel.
"
210,"Study of mechanical response in embossing of ceramic green substrate by
  micro-indentation","  Micro-indentation test with a micro flat-end cone indenter was employed to
simulate micro embossing process and investigate the thermo-mechanical response
of ceramic green substrates. The laminated low temperature co-fired ceramic
green tapes were used as the testing material ; the correlations of indentation
depth versus applied force and applied stress at the temperatures of 25 degrees
C and 75degrees C were studied. The results showed that permanent indentation
cavities could be formed at temperatures ranging from 25 degrees C to 75
degrees C, and the depth of cavities created was applied force, temperature and
dwell time dependent. Creep occurred and made a larger contribution to the
plastic deformation at elevated temperatures and high peak loads. There was
instantaneous recovery during the unloading and retarded recovery in the first
day after indentation. There was no significant pile-up due to material flow
observed under compression at the temperature up to 75 degrees C. The plastic
deformation was the main cause for formation of cavity on the ceramic green
substrate under compression. The results can be used as a guideline for
embossing ceramic green substrates.
"
211,"Top-Down Behavioral Modeling Methodology of a Piezoelectric
  Microgenerator For Integrated Power Harvesting Systems","  In this study, we developed a top/down methodology for behavioral and
structural modeling of multi-domain microsystems. Then, we validated this
methodology through a study case : a piezoelectric microgenerator. We also
proved the effectiveness of VHDL-AMS language not only for modeling in
behavioral and structural levels but also in writing physical models that can
predict the experimental results. Finally, we validated these models by
presenting and discussing simulations results.
"
212,"Hybridization of Magnetism and Piezoelectricity for an Energy Scavenger
  based on Temporal Variation of Temperature","  Autonomous microsystems are confronted today to a major challenge : the one
of energy supply. Energy scavenging, i.e. collecting energy from the ambient
environment has been developed to answer this problematic. Various sources have
already been successfully used (solar, vibration). This article presents
temporal variations of temperature as a new source of exploitable energy. A
brief review will take place at the beginning, exposing the different
approaches used in the past. Then we will focus our attention on hybridization
of magnetism and piezoelectricity. A new kind of thermal generator is proposed
and a preliminary model is exposed. Conclusions will be drawn on the
suitability of this prototype and the improvements that are needed to increase
its potential.
"
213,"Simulation of an Electrostatic Energy Harvester at Large Amplitude
  Narrow and Wide Band Vibrations","  An electrostatic in-plane overlap varying energy harvester is modeled and
simulated using a circuit simulator. Both linear and nonlinear models are
investigated. The nonlinear model includes mechanical stoppers at the
displacement extremes. Large amplitude excitation signals, both narrow and wide
band, are used to emulate environmental vibrations. Nonlinear behavior is
significant at large displacement due to the impact on mechanical stoppers. For
a sinusoidal excitation the mechanical stoppers cause the output power to
flatten and weakly decrease. For a wide band excitation, the output power first
increases linearly with the power spectral density of the input signal, then
grows slower than linearly.
"
214,"Optimization and AMS Modeling for Design of an Electrostatic Vibration
  Energy Harvester's Conditioning Circuit with an Auto-Adaptive Process to the
  External Vibration Changes","  Electrostatic transducers for vibration energy scavenging have been an object
to numerous studies, but are still facing major issues relating to their
conditioning circuit. One of the most popular ones uses a charge pump and a
flyback circuit based on a Buck DC-DC converter (Fig. 1). A commutation between
the energy accumulation in the charge pump and the recharge of the buffer
capacitor Cres is assured by a switch which is the major bottleneck in the
energy harvester circuit. The commutation timing of the switch determines the
efficiency of the energy harvesting. In previous papers [1] the switch
commutates periodically with some fixed duty ratio. However, this solution is
not appropriate when the environment parameters, e.g. the vibration frequency,
change. We found that the switching should be ordered by the internal state of
the circuit, an not by some fixed timing scenario. We presents how to find the
optimal operation mode of the harvester. To validate the study, the system was
modeled using a mixed VHDL-AMS - ELDO model.
"
215,"Fabrication of Nanostructured PLGA Scaffolds Using Anodic Aluminum Oxide
  Templates","  PLGA (poly(lactic-co-glycolic acid)) is one of the most used biodegradable
and biocompatible materials. Nanostructured PLGA even has great application
potentials in tissue engineering. In this research, a fabrication technique for
nanostructured PLGA membrane was investigated and developed. In this novel
fabrication approach, an anodic aluminum oxide (AAO) film was use as the
template ; the PLGA solution was then cast on it ; the vacuum air-extraction
process was applied to transfer the nano porous pattern from the AAO membrane
to the PLGA membrane and form nanostures on it. The cell culture experiments of
the bovine endothelial cells demonstrated that the nanostructured PLGA membrane
can double the cell growing rate. Compared to the conventional chemical-etching
process, the physical fabrication method proposed in this research not only is
simpler but also does not alter the characteristics of the PLGA. The
nanostructure of the PLGA membrane can be well controlled by the AAO temperate.
"
216,"High Performance Microreactor for Rapid Fluid Mixing and Redox Reaction
  of Ascorbic Acid","  A novel micro device with a mechanism of split and recombination (SNR) for
rapid fluidic mixing and reaction, named a SNR micro-reactor, was designed,
fabricated and systematically analyzed. This SNR micro-reactor possessing an
in-plane dividing structure requires only simple fabrication. We investigated
this reactor and compared it numerically and experimentally with a
slanted-groove micromixer (SGM). From the numerical results the mixing indices
and mixing patterns demonstrated that the mixing ability of the SNR
micro-reactor was much superior to that of the SGM. From a mixing test with
phenolphthalein and sodium hydroxide solutions, the mixing lengths of the SNR
micro-reactor were less than 4 mm for a Reynolds number over a wide range (Re =
0.1 - 10). From a comparison of mixing lengths, the results revealed also that
the SNR micro-reactor surpassed the SGM in mixing performance by more than 200
%. As a reaction length is a suitable test of the performance of a reactor, we
introduced a redox reaction between ascorbic acid and iodine solutions to
assess the reaction capability of these micro devices ; the reaction lengths of
the SNR micro-reactor were much shorter than those of a SGM. The SNR
micro-reactor has consequently a remarkable efficiency for fluid mixing and
reaction.
"
217,"Cell Trapping Utilizing Insulator-based Dielectrophoresis in The
  Open-Top Microchannels","  The ability to manipulate or separate a biological small particle, such as a
living cell and embryo, is fundamental needed to many biological and medical
applications. The insulator-based dielectrophoresis (iDEP) trapping is composed
of conductless tetragon structures in micro-chip. In this study, a lower
conductive material of photoresist was adopted as a structure in open-top
microchannel instead of a metallic wire to squeeze the electric field in a
conducting solution, therefore, creating a high field gradient with a local
maximum. The microchip with the open-top microchannels was designed and
fabricated herein. The insulator-based DEP trapping microchip with the open-top
microchannels was designed and fabricated in this work. The cells trapped by
DEP force could be further treated or cultured in the open-top microchannel ;
however, those trapped in the microchip with enclosed microchannels could not
be proceeded easily.
"
218,Design and Analysis of a Chaotic Micromixer with Vortices Modulation,"  A novel design for vortex modulation of a passive chaotic micromixer, named a
circulation-disturbance micromixer (CDM), has been achieved and analyzed
experimentally and numerically. The systematic numerical analyses - topological
flow characteristics and particle tracking method - have been developed, that
enable visualization of detailed mixing patterns. To display the cross section
of mixing region of flows in our CDM, the biotin-streptavidin binding is
detected through the fluorescence resonance energy transfer (FRET) pair of
fluorescent proteins - R-phycoerythrin (RPE) and cross-linked allophycocyanin
(clAPC). We expect the diagnosis technique using FRET will be successfully
applied to biochemical analysis in microfluidic system.
"
219,Portable Valve-less Peristaltic Micro-pump Design and Fabrication,"  This paper is to describe a design and fabrication method for a valve-less
peristaltic micro-pump. The valve-less peristaltic micro-pump with three
membrane chambers in a serial is actuated by three piezoelectric (PZT)
actuators. With the fluidic flow design, liquid in the flow channel is pumped
to a constant flow speed ranged from 0.4 to 0.48 mm/s. In term of the maximum
flow rate of the micro-pump is about 365 mircoliters/min, when the applied
voltage is 24V and frequency 50 Hz. Photolithography process was used to
fabricate the micro-pump mold. PDMS molding and PDMS bonding method were used
to fabricate the micro-channel and actuator chambers. A portable drive
controller was designed to control three PZT actuators in a proper sequence to
drive the chamber membrane. Then, all parts were integrated into the portable
valve-less peristaltic micro-pump system.
"
220,"Microfluidic Device for Continuous Magnetophoretic Separation of Red
  Blood Cells","  This paper presents a microfluidic device for magnetophoretic separation red
blood cells from blood under contionous flow. The separation method consist of
continous flow of a blood sample (diluted in PBS) through a microfluidic
channel which presents on the bottom ""dots"" of feromagnetic layer. By appling a
magnetic field perpendicular on the flowing direction, the feromagnetic ""dots""
generates a gradient of magnetic field which amplifies the magnetic force. As a
result, the red blood cells are captured on the bottom of the microfluidic
channel while the rest of the blood is collected at the outlet. Experimental
results show that an average of 95 % of red blood cells are trapped in the
device
"
221,"Fabrication of Embedded Microvalve on PMMA Microfluidic Devices through
  Surface Functionalization","  The integration of a PDMS membrane within orthogonally placed PMMA
microfluidic channels enables the pneumatic actuation of valves within bonded
PMMA-PDMS-PMMA multilayer devices. Here, surface functionalization of PMMA
substrates via acid catalyzed hydrolysis and air plasma corona treatment were
investigated as possible techniques to permanently bond PMMA microfluidic
channels to PDMS surfaces. FTIR and water contact angle analysis of
functionalized PMMA substrates showed that air plasma corona treatment was most
effective in inducing PMMA hydrophilicity. Subsequent fluidic tests showed that
air plasma modified and bonded PMMA multilayer devices could withstand fluid
pressure at an operational flow rate of 9 mircoliters/min. The pneumatic
actuation of the embedded PDMS membrane was observed through optical microscopy
and an electrical resistance based technique. PDMS membrane actuation occurred
at pneumatic pressures of as low as 10kPa and complete valving occurred at
14kPa for 100 micrometers x 100 micrometers channel cross-sections.
"
222,Hot Roller Embossing for the Creation of Microfluidic Devices,"  We report on the hot roller embossing of polymer sheets for the creation of
microfluidic structures. Measurements conducted on 100 $\mu$m features showed
that the lateral dimensions could be replicated to within 2% tolerance, while
over 85% of mould depth was embossed. Feature sizes down to 50 $\mu$m and
feature depths up to 30 $\mu$m had been achieved. At lower temperatures,
asymmetric pile up of polymer material outside embossed regions was observed
with higher pile up occurring on the trailing side of the embossed regions.
"
223,"Geometrical Variation Analysis of an Electrothermally Driven Polysilicon
  Microactuator","  The analytical models that predict thermal and mechanical responses of
microactuator have been developed. These models are based on electro thermal
and thermo mechanical analysis of the microbeam. Also, Finite Element Analysis
(FEA) is used to evaluate microactuator tip deflection. Analytical and Finite
Element results are compared with experimental results in literature and show
good agreement in low input voltages. A dimensional variation of beam lengths,
beam lengths ratios and gap are introduced in analytical and FEA models to
explore microactuator performance. An electrothermally driven polysilicon
microactuator similar to Pan's actuator architecture has been studied in this
paper. This microactuator generates deflection through asymmetric heating of
the hot and cold polysilicon arms with different lengths. For this
microactuator architecture, an optimal beam length ratio equal to 0.46 has been
obtained to achieve maximum tip deflection. . As it was expected, the results
show decreasing air gap increase microactuator tip deflection. It is also found
that for microactuators with longer hot arms, microactuator tip deflections are
more sensitive to beam length ratios and air gap.
"
224,In-Plane Bistable Nanowire For Memory Devices,"  We present a nanomechanical device design to be used in a non-volatile
mechanical memory point. The structure is composed of a suspended slender
nanowire (width : 100nm, thickness 430nm length : 8 to 30$\mu$m) clamped at its
both ends. Electrodes are placed on each sides of the nanowire and are used to
actuate the structure (writing, erasing) and to measure the position through a
capactive bridge (reading). The structure is patterned by electron beam
lithography on a pre-stressed thermally grown silicon dioxide layer. When later
released, the stressed material relaxes and the beam buckles in a position of
lower energy. Such symmetric beams, called Euler beams, show two stable
deformed positions thus form a bistable structure. This paper will present the
fabrication, simulation and mechanical and electrical actuation of an in plane
bistable nanowire. Final paper will include a section on FEM simulations.
"
225,RF-MEMS Switched Varactors for Medium Power Applications,"  In RF (Radio Frequency) domain, one of the limitations of using MEMS (Micro
Electromechanical Systems) switching devices for medium power applications is
RF power. Failure phenomena appear even for 500 mW. A design of MEMS switched
capacitors with an enhanced topology is presented in this paper to prevent it.
This kind of device and its promising performances will serve to fabricate a
MEMS based phase shifter able to work under several watts.
"
226,Low-Drift Flow Sensor with Zero-Offset Thermopile-Based Power Feedback,"  A thermal flow sensor has been realised consisting of freely-suspended
silicon-rich silicon-nitride microchannels with an integrated Al/poly-Si++
thermopile in combination with up- and downstream Al heater resistors. The
inherently zero offset of the thermopile is exploited in a feedback loop
controlling the dissipated power in the heater resistors, eliminating
inevitable influences of resistance drift and mismatch of the thin-film metal
resistors. The control system cancels the flow-induced temperature difference
across the thermopile by controlling a power difference between both heater
resistors, thereby giving a measure for the flow rate. The flow sensor was
characterised for power difference versus water flow rates up to 1.5 ul/min,
being in good agreement with a thermal model of the sensor, and the correct
low-drift operation of the temperature-balancing control system has been
verified.
"
227,Single Chip Sensing of Multiple Gas Flows,"  The fabrication and experimental characterization of a thermal flow meter,
capable of detecting and measuring two independent gas flows with a single
chip, is described. The device is based on a 4 x 4 mm2 silicon chip, where a
series of differential micro-anemometers have been integrated together with
standard electronic components by means of postprocessing techniques. The
innovative aspect of the sensor is the use of a plastic adapter, thermally
bonded to the chip, to convey the gas flow only to the areas where the sensors
are located. The use of this inexpensive packaging procedure to include
different sensing structures in distinct flow channels is demonstrated.
"
228,"Comparison Between Damping Coefficients of Measured Perforated
  Micromechanical Test Structures and Compact Models","  Measured damping coefficients of six different perforated micromechanical
test structures are compared with damping coefficients given by published
compact models. The motion of the perforated plates is almost translational,
the surface shape is rectangular, and the perforation is uniform validating the
assumptions made for compact models. In the structures, the perforation ratio
varies from 24% - 59%. The study of the structure shows that the
compressibility and inertia do not contribute to the damping at the frequencies
used (130kHz - 220kHz). The damping coefficients given by all four compact
models underestimate the measured damping coefficient by approximately 20%. The
reasons for this underestimation are discussed by studying the various flow
components in the models.
"
229,"A piecewise-linear reduced-order model of squeeze-film damping for
  deformable structures including large displacement effects","  This paper presents a reduced-order model for the Reynolds equation for
deformable structure and large displacements. It is based on the model
established in [11] which is piece-wise linearized using two different methods.
The advantages and drawbacks of each method are pointed out. The pull-in time
of a microswitch is determined and compared to experimental and other
simulation data.
"
230,"RF-MEMS beam components : FEM modelling and experimental identification
  of pull-in in presence of residual stress","  In this paper an experimental validation of numerical approaches aimed to
predict the coupled behaviour of microbeams for out-of-plane bending tests is
performed. This work completes a previous investigation concerning in plane
microbeams bending. Often out-of-plane microcantilevers and clamped-clamped
microbeams suffer the presence of residual strain and stress, which affect the
value of pull-in voltage. In case of microcantilever an accurate modelling
includes the effect of the initial curvature due to microfabrication. In double
clamped microbeams a preloading applied by tensile stress is considered.
Geometrical onlinearity caused by mechanical coupling between axial and
flexural behaviour is detected and modelled. Experimental results demonstrate a
good agreement between FEM approaches proposed and tests. A fairly fast and
accurate prediction of pull-in condition is performed, thus numerical models
can be used to identify residual stress in microbridges by reverse analysis
from the measured value of pull-in voltage.
"
231,"Validation of compact models of microcantilever actuators for RF-MEMS
  application","  Microcantilever specimens for in-plane and out-ofplane bending tests are here
analyzed. Experimental validation of 2D and 3D numerical models is performed.
Main features of in-plane and out-of-plane layouts are then discussed.
Effectiveness of plane models to predict pull-in in presence of geometric
nonlinearity due to a large tip displacement and initial curvature of microbeam
is investigated. The paper is aimed to discuss the capability of 2D models to
be used as compact tools to substitute some model order reduction techniques,
which appear unsuitable in presence of both electromechanical and geometric
nonlinearities.
"
232,"Numerical Investigation of Laser-Assisted Nanoimprinting on a Copper
  Substrate from a Perspective of Heat Transfer Analysis","  The technique of laser-assisted nanoimprinting lithography (LAN) has been
proposed to utilize an excimer laser to irradiate through a quartz mold and
melts a thin polymer film on the substrate for micro- to nano-scaled
fabrications. In the present study, the novel concept of that copper was
adopted as the substrate instead of silicon, which is conventionally used, was
proposed. The micro/nano structures on the copper substrate could be fabricated
by chemical/electrochemical etching or electroforming ; following by the
patterns have been transferred onto the substrate using LAN process.
Alternatives of the substrate materials could lead versatile applications in
micro/nano-fabrication. To demonstrate the feasibility of this concept
numerically, this study introduced optical multiple reflection theory to
perform both analytical and numerical modeling during the process and to
predict the thermal response theoretically.
"
233,Micro-electroforming metallic bipolar electrodes for mini-DMFC stacks,"  This paper describes the development of metallic bipolar plate fabrication
using micro-electroforming process for mini-DMFC (direct methanol fuel cell)
stacks. Ultraviolet (UV) lithography was used to define micro-fluidic channels
using a photomask and exposure process. Micro-fluidic channels mold with 300
micrometers thick and 500 micrometers wide were firstly fabricated in a
negative photoresist onto a stainless steel plate. Copper micro-electroforming
was used to replicate the micro-fluidic channels mold. Following by sputtering
silver (Ag) with 1.2 micrometers thick, the metallic bipolar plates were
completed. The silver layer is used for corrosive resistance. The completed
mini-DMFC stack is a 2x2 cm2 fuel cell stack including a 1.5x1.5 cm2 MEA
(membrane electrode assembly). Several MEAs were assembly into mini-DMFC stacks
using the completed metallic bipolar plates. All test results showed the
metallic bipolar plates suitable for mini-DMFC stacks. The maximum output power
density is 9.3mW/cm2 and current density is 100 mA/cm2 when using 8 vol. %
methanol as fuel and operated at temperature 30 degrees C. The output power
result is similar to other reports by using conventional graphite bipolar
plates. However, conventional graphite bipolar plates have certain difficulty
to be machined to such micro-fluidic channels. The proposed
micro-electroforming metallic bipolar plates are feasible to miniaturize DMFC
stacks for further portable 3C applications.
"
234,"On the determination of Poisson's ratio of stressed monolayer and
  bilayer submicron thick films","  In this paper, the bulge test is used to determine the mechanical properties
of very thin dielectric membranes. Commonly, this experimental method permits
to determine the residual stress (s0) and biaxial Young's modulus (E/(1-u)).
Associating square and rectangular membranes with different length to width
ratios, the Poisson's ratio (u) can also be determined. LPCVD Si3N4 monolayer
and Si3N4/SiO2 bilayer membranes, with thicknesses down to 100 nm, have been
characterized giving results in agreement with literature for Si3N4, E = 212
$\pm$ 14 GPa, s0 = 420 $\pm$ 8 and u = 0.29.
"
235,"New high fill-factor triangular micro-lens array fabrication method
  using UV proximity printing","  A simple and effective method to fabricate a high fill-factor triangular
microlens array using the proximity printing in lithography process is
reported. The technology utilizes the UV proximity printing by controlling a
printing gap between the mask and substrate. The designed approximate triangle
microlens array pattern can be fabricated the high fill-factor triangular
microlens array in photoresist. It is due to the UV light diffraction to
deflect away from the aperture edges and produce a certain exposure in
photoresist material outside the aperture edges. This method can precisely
control the geometric profile of high fill factor triangular microlens array.
The experimental results showed that the triangular micro-lens array in
photoresist could be formed automatically when the printing gap ranged from 240
micrometers to 840 micrometers. The gapless triangular microlens array will be
used to increases of luminance for backlight module of liquid crystal displays.
"
236,"Design Optimization for an Electro-Thermally Actuated Polymeric
  Microgripper","  Thermal micro-actuators are a promising solution to the need for
large-displacement, gentle handling force, low-power MEMS actuators. Potential
applications of these devices are micro-relays, assembling and miniature
medical instrumentation. In this paper the development of thermal
microactuators based on SU-8 polymer is described. The paper presents the
development of a new microgripper which can realize a movement of the gripping
arms with possibility for positioning and manipulating of the gripped object.
Two models of polymeric microgripper electrothermo- mechanical actuated, using
low actuation voltages, designed for SU-8 polymer fabrication were presented.
The electro-thermal microgrippers were designed and optimized using finite
element simulations. Electro-thermo-mechanical simulations based on finite
element method were performed for each of the model in order to compare the
results. Preliminary experimental tests were carried out.
"
237,"Design And Fabrication of High Numerical Aperture And Low Aberration
  Bi-Convex Micro Lens Array","  Micro lens array is crucial in various kinds of optical and electronic
applications. A micro lens array with high numerical aperture (NA) and low
aberration is in particular needed. This research is aimed to design and
fabricate such a micro lens array with simple structure while keeps the same NA
of a same-diameter hemisphere lens. A bi-convex semispherical micro lens array,
with corresponding NA 0.379, by PDMS is first designed and analyzed.
Experiments are further conducted to fabricate the designed micro lens array by
the thermal reflow process. The formed profile is then sputtered with copper to
serve as the mold. The front and the rear micro lens array are fabricated by
plating PDMS to the mold and then assembled to form the designed micro lens
array.
"
238,Micromachined Inclinometer Based on Fluid Convection,"  This paper presents a numerical simulation and experimental results of a
one-dimensional thermal inclinometer with the cavity filled of gas and liquid.
The sensor principle consists of one heating resistor placed between two
detectors. When the resistor is electrically powered, it creates a symmetrical
temperature profile inside a micromachined silicon cavity. By applying a tilt
to the sensor, the profile shifts in the same direction of the sensible axis
corresponding to the horizontal one to one. The temperature profile and the
sensitivity according to the CO2 gas and mineral oil SAE50 have been studied
using numerical resolution of fluid dynamics equations with the computational
fluid dynamics (CFD) software package Fluent V6.2. We have shown that the
sensitivity of liquid sensors is higher than the gas sensors one. By using
micromachined silicon technique, a thermal inclinometer with one pair of
detectors placed at 300 um from the heater has been made. Experimental
measurements corroborate with the numerical simulation.
"
239,"Model Based Sensor System for Temperature Measurement in R744 Air
  Conditioning Systems","  The goal is the development of a novel principle for the temperature
acquisition of refrigerants in CO2 air conditioning systems. The new approach
is based on measuring the temperature inside a pressure sensor, which is also
needed in the system. On the basis of simulative investigations of different
mounting conditions functional relations between measured and medium
temperature will be derived.
"
240,"Integrated 3D Sound Intensity Sensor with Four-Wire Particle Velocity
  Sensors","  A new symmetrical four-wire sensor configuration has resulted in a fully
integrated sound intensity sensor with significant lower noise floor and
smaller size than its predecessors. An integrated sound pressure sensor was
further miniaturized by using a folded ""back chamber"" at both sides of the
chip.
"
241,"High Aspect Pattern Formation by Integration of Micro Inkjetting and
  Electroless Plating","  This paper reports on formation of high aspect micro patterns on low
temperature co-fired ceramic (LTCC) substrates by integrating micro inkjetting
with electroless plating. Micro inkjetting was realized by using an inkjetting
printer that ejects ink droplets from a printhead. This printhead consists of a
glass nozzle with a diameter of 50 micrometers and a piezoelectric transducer
that is coated on the nozzle. The silver colloidal solution was inkjetted on a
sintered CT800 ceramic substrate, followed by curing at 200 degrees C for 60
minutes. As a result, the silver trace with a thickness of 200 nm was obtained.
The substrate, with the ejected silver thin film as the seed layer, was then
immersed into a preinitiator solution to coat a layer of palladium for
enhancing the deposition of nickel. Electroless nickel plating was successfully
conducted at a rate of 0.39 micrometers /min, and the thickness of traces was
plated up to 84 micrometers. This study demonstrates that the integration of
inkjetting with plating is an effective method to form high aspect patterns at
the demand location.
"
242,A Nanostructual Microwave Probe Used for Atomic Force Microscope,"  In order to develop a new structure microwave probe, the fabrication of AFM
probe on the GaAs wafer was studied. A waveguide was introduced by evaporating
Au film on the top and bottom surfaces of the GaAs AFM probe. A tip having 8
micrometers high, and curvature radius about 50 nm was formed. The dimensions
of the cantilever are 250x30x15 micrometers. The open structure of the
waveguide at the tip of the probe was introduced by using FIB fabrication. AFM
topography of a grating sample was measured by using the fabricated GaAs
microwave probe. The fabricated probe was found having nanometer scale
resolution, and microwave emission was detected successfully at the tip of the
probe by approaching Cr-V steel and Au wire samples.
"
243,High Density out-of-Plane Microprobe Array,"  MEMS technology has been developed rapidly in the last few years. More and
more special micro structures were discussed in several publications. However,
all of the structures were produced by consist of the three fundamental
structures, which included bridge, cantilever and membrane structures. Even the
more complex structures were no exception. The cantilever with the property of
simple design and easy fabrication among three kinds of fundamental structure,
therefore, it was popular used in the design of MEMS device.
"
244,"Simulation of Coating -Visco-Elastic liquid in the Mico-Nip of Metering
  Size Press","  For a set of operating conditions and coating color formulations, undesirable
phenomena like color spitting and coating ribs may be triggered in the
Micro-nip during the coating process. Therefore, our interest in this work
focus on another parameter affect on the undesirable phenomena as the vortices
in the Micro-nip. The problem deals with the flow through the Micro-nip of
metering size press. The flow enters and exits at a tangential velocity of 20
m/s between two rollers with diameter 80 cm and 60 -m apart. In the upper and
bottom part of the domain the angular velocity is 314 rad /s. It has one
sub-domain. Previous studies focus on the Micro-nip without considering the
inertia and the viscoelasticity of the material. Roll coating is a technique
commonly used in the coating industry to meter a thin fluid film on a moving
substrate. During the film formation, the fluid is subjected to very high shear
and extensional rates over a very short period of time. The fluid domain
changes as a function of the hydrodynamic pressure within the nip as a result
of the deformable cover usually used on one of the rolls. The free surface also
adds more complexity to the flow due to the force equilibrium in the fluid gas
interface. Last of all, the rheological behavior of the coating fluid is
usually non-Newtonian, so the metering flow hydrodynamics is finally very
difficult to describe. It is concluded that the normal forces of micro-nip
increases with increasing the inhibitors. Therefore, it affects on the
smoothness and creates defects. On the other hand, it can be concluded that the
creation of big vortex in the middle of micro-nip affects on the coating liquid
behavior.
"
245,"A Microcantilever-based Gas Flow Sensor for Flow Rate and Direction
  Detection","  The purpose of this paper is to apply characteristics of residual stress that
causes cantilever beams to bend for manufacturing a micro-structured gas flow
sensor. This study uses a silicon wafer deposited silicon nitride layers,
reassembled the gas flow sensor with four cantilever beams that perpendicular
to each other and manufactured piezoresistive structure on each
micro-cantilever by MEMS technologies, respectively. When the cantilever beams
are formed after etching the silicon wafer, it bends up a little due to the
released residual stress induced in the previous fabrication process. As air
flows through the sensor upstream and downstream beam deformation was made,
thus the airflow direction can be determined through comparing the resistance
variation between different cantilever beams. The flow rate can also be
measured by calculating the total resistance variations on the four
cantilevers.
"
246,"Process nano scale mechanical properties measurement of thin metal films
  using a novel paddle cantilever test structure","  A new technique was developed for studying the mechanical behavior of
nano-scale thin metal films on substrate is presented. The test structure was
designed on a novel ""paddle"" cantilever beam specimens with dimensions as few
hundred nanometers to less than 10 nanometers. This beam is in triangle shape
in order to provide uniform plane strain distribution. Standard clean room
processing was used to prepare the paddle sample. The experiment can be
operated by using the electrostatic deflection on the paddle uniform
distributed stress cantilever beam and then measure the deposited thin metal
film materials on top of it. A capacitance technique was used to measurement on
the other side of the deflected plate to measure its deflection with respect to
the force. The measured strain was converted through the capacitance
measurement for the deflection of the cantilever. System performance on the
residual stress measurement of thin films are calculated with three different
forces on the ""paddle"" cantilever beam, including the force due to the film,
compliance force and electrostatic force.
"
247,High Q-factor CMOS-MEMS inductor,"  This study investigates a high Q-factor spiral inductor fabricated by the
CMOS (complementary metal oxide semiconductor) process and a post-process. The
spiral inductor is manufactured on silicon substrate using the 0.35 micrometers
CMOS process. In order to reduce the substrate loss and enhance the Q-factor of
the inductor, silicon substrate under the inductor is removed using a
post-process. The post-process uses RIE (reactive ion etching) to etch the
sacrificial layer of silicon dioxide, and then TMAH (tetra methyl ammonium
hydroxide) is employed to remove the underlying silicon substrate and obtain
the suspended spiral inductor. The advantage of the post process is compatible
with the CMOS process. The Agilent 8510C network analyzer and a Cascade probe
station are used to measure the performances of the spiral inductor.
Experiments indicate that the spiral inductor has a Q-factor of 15 at 11 GHz,
an inductance of 4 nH at 25.5 GHz and a self-resonance frequency of about 27
GHz.
"
248,Copper Electrodeposition for 3D Integration,"  Two dimensional (2D) integration has been the traditional approach for IC
integration. Due to increasing demands for providing electronic devices with
superior performance and functionality in more efficient and compact packages,
has driven the semiconductor industry to develop more advanced packaging
technologies. Three-dimensional (3D) approaches address both miniaturization
and integration required for advanced and portable electronic products.
Vertical integration proved to be essential in achieving a greater integration
flexibility of disparate technologies, reason for which a general trend of
transition from 2D to 3D integration is currently being observed in the
industry. 3D chip integration using through silicon via (TSV) copper is
considered one of the most advanced technologies among all different types of
3D packaging technologies. Copper electrodeposition is one of technologies that
enable the formation of TSV structures. Because of its well-known application
for copper damascene, it was believed that its transfer to filling TSV vias
would be easily adopted. However, as any new technology at its beginning, there
are several challenges that need to be addressed and resolved before becoming a
fully mature technology. This paper will address the TSV fill processes using
copper electrodeposition, the advantages as well as difficulties associated
with this technology and approaches taken to overcome them. Electrochemical
characterization of the organics behavior and their effect on via filling will
be presented. The effect of wafer design on process performance and throughput,
including necessary process optimizations that are required for achieving
void-free via filling while reducing the processing time, will be discussed.
"
249,Technologies for 3D Heterogeneous Integration,"  3D-Integration is a promising technology towards higher interconnect
densities and shorter wiring lengths between multiple chip stacks, thus
achieving a very high performance level combined with low power consumption.
This technology also offers the possibility to build up systems with high
complexity just by combining devices of different technologies. For ultra thin
silicon is the base of this integration technology, the fundamental processing
steps will be described, as well as appropriate handling concepts. Three main
concepts for 3D integration have been developed at IZM. The approach with the
greatest flexibility called Inter Chip Via - Solid Liquid Interdiffusion
(ICV-SLID) is introduced. This is a chip-to-wafer stacking technology which
combines the advantages of the Inter Chip Via (ICV) process and the
solid-liquid-interdiffusion technique (SLID) of copper and tin. The fully
modular ICV-SLID concept allows the formation of multiple device stacks. A test
chip was designed and the total process sequence of the ICV-SLID technology for
the realization of a three-layer chip-to-wafer stack was demonstrated. The
proposed wafer-level 3D integration concept has the potential for low cost
fabrication of multi-layer high-performance 3D-SoCs and is well suited as a
replacement for embedded technologies based on monolithic integration. To
address yield issues a wafer-level chip-scale handling is presented as well, to
select known-good dies and work on them with wafer-level process sequences
before joining them to integrated stacks.
"
250,Through Silicon Vias as Enablers for 3D Systems,"  This special session on 3D TSV's will highlight some of the fabrication
processes and used technologies to create vias from the frontside of an active
circuit to its backside and potential implementation solutions to form complex
systems leveraging these novel possibilities. General techniques for via
formation are discussed as well as advanced integration solutions leveraging
the power of 3D TSV's.
"
251,Fabrication of 3D Packaging TSV using DRIE,"  Emerging 3D chips stacking and MEMS/Sensors packaging technologies are using
DRIE (Deep Reactive Ion Etching) to etch through-silicon via (TSV) for advanced
interconnections. The interconnection step can be done prior to or post CMOS
manufacturing, each requiring different etch process performances. A review of
the DRIE capability in terms of etching profile, etch rate, etch depth has been
carried out. Excellent tool flexibility allows a wide range of basic and
complex profiles to be achieved. Unlike other techniques, DRIE has the
capability to etch feature sizes ranging from sub-micron to millimeter width.
The main specificity of the DRIE is that etch rate is sensitive to the total
exposed area and the aspect ratio. For the TSV applications, where the total
exposed area is lower than 10%, high etch rates are achievable. A study has
also been done to highlight the importance of via profile for the success of
the refilling step. In addition, due to the high flexibility of DRIE, we also
explore the capability of using this technique for wafer thinning and plasma
die separation.
"
252,"Study of First-Order Thermal Sigma-Delta Architecture for Convective
  Accelerometers","  This paper presents the study of an original closed-loop conditioning
approach for fully-integrated convective inertial sensors. The method is
applied to an accelerometer manufactured on a standard CMOS technology using an
auto-aligned bulk etching step. Using the thermal behavior of the sensor as a
summing function, a first order sigma-delta modulator is built. This
""electro-physical"" modulator realizes an analog-to-digital conversion of the
signal. Besides the feedback scheme should improve the sensor performance.
"
253,High Density Through Silicon Via (TSV),"  The Through Silicon Via (TSV) process developed by Silex provides down to 30
micrometers pitch for through wafer connections in up to 600 micrometers thick
substrates. Integrated with MEMS designs it enables significantly reduced die
size and true ""Wafer Level Packaging"" - features that are particularly
important in consumer market applications. The TSV technology also enables
integration of advanced interconnect functions in optical MEMS, sensors and
microfluidic devices. In addition the Via technology opens for very interesting
possibilities considering integration with CMOS processing. With several
companies using the process already today, qualified volume manufacturing in
place and a line-up of potential users, the process is becoming a standard in
the MEMS industry. We provide a introduction to the via formation process and
also present some on the novel solutions made available by the technology.
"
254,"Rejection of Power Supply Noise in Wheatstone Bridges : Application to
  Piezoresistive MEMS","  This paper deals with the design of MEMS using piezoresistivity as
transduction principle. It is demonstrated that when the sensor topology
doesn't allow a perfect matching of strain gauges, the resolution is limited by
the ability of the conditioning circuit (typically a Wheatstone bridge) to
reject power supply noise. As this ability is strongly reduced when an offset
voltage is present at the output of the bridge, the proposed architecture
implements a feedback loop to control MOS transistors inserted in the
Wheatstone bridge to compensate resistor mismatches. This feedback exhibits a
very good offset cancellation and therefore a better resolution is achieved.
"
255,Next Generation of TCAD Environments for MEMS Design,"  In this paper we present the latest version of the TCAD environment BICEP'S
(Braunschweigs Integrated CAD-Environment for Product Planning and Process
Simulation). By using a central process database, which allows the exchange of
all relevant process data it is able to overcome many of the mentioned
obstacles. The database and process planning tool can be used by process
developers to document changes in process settings and the influence of such
changes on the process result. This information can then be used by the
designers to set-up a simulation file for a detailed analysis of the impact of
such parameter changes on the requested design. This will be shown by the
example of silicon etching using an atomistic etch simulator.
"
256,Noise Analysis and Noise-based Optimization for Resonant MEMS Structures,"  This paper presents a detailed noise analysis and a noise-based optimization
procedure for resonant MEMS structures. A design for high sensitivity of MEMS
structures needs to take into account the noise shaping induced by damping
phenomena at micro scale. The existing literature presents detailed models for
the damping at microscale, but usually neglects them in the noise analysis
process, assuming instead a white spectrum approximation for the
mechano-thermal noise. The present work extends the implications of the complex
gas-solid interaction into the field of noise analysis for mechanical sensors,
and provides a semi-automatic procedure for behavioral macromodel extraction
and sensor optimization with respect to signal-to-noise ratio.
"
257,"Haptic sensing for MEMS with application for cantilever and Casimir
  effect","  This paper presents an implementation of the Cosserat theory into haptic
sensing technologies for real-time simulation of microstructures. Cosserat
theory is chosen instead of the classical theory of elasticity for a better
representation of stress, especially in the nonlinear regime. The use of
Cosserat theory leads to a reduction of the complexity of the modelling and
thus increases its capability for real time simulation which is indispensable
for haptic technologies. The incorporation of Cosserat theory into haptic
sensing technology enables the designer to simulate in real-time the components
in a virtual reality environment (VRE) which can enable virtual manufacturing
and prototyping. The software tool created as a result of this methodology
demonstrates the feasibility of the proposed model. As test demonstrators, a
cantilever microbeam and microbridge undergoing bending in VRE are presented.
"
258,On-Chip Hotplate for Temperature Control of Cmos Saw Resonators,"  Due to the sensitivity of the piezoelectric layer in surface acoustic wave
(SAW) resonators to temperature, a method of achieving device stability as a
function of temperature is required. This work presents the design, modeling
and characterization of integrated dual-serpentine polysilicon resistors as a
method for temperature control of CMOS SAW resonators. The design employs the
oven control temperature stabilization scheme where the device's temperature is
elevated to higher than Tmax to maintain constant device temperature. The
efficiency of the polysilicon resistor as a heating element was verified
through a 1-D partial differential equation model, 3-D CoventorWare finite
element simulations and measurements using Compix thermal camera. To verify
that the on-chip hotplate is effective as a temperature control method, both DC
and RF measurements of the heater together with the resonator were conducted.
Experimental results have indicated that the TCF of the CMOS SAW resonator of
-97.2 ppm/deg C has been reduced to -23.19 ppm/deg C when heated to 56 deg C.
"
259,Modelling methodology of MEMS structures based on Cosserat theory,"  Modelling MEMS involves a variety of software tools that deal with the
analysis of complex geometrical structures and the assessment of various
interactions among different energy domains and components. Moreover, the MEMS
market is growing very fast, but surprisingly, there is a paucity of modelling
and simulation methodology for precise performance verification of MEMS
products in the nonlinear regime. For that reason, an efficient and rapid
modelling approach is proposed that meets the linear and nonlinear dynamic
behaviour of MEMS systems.
"
260,"Low Voltage Totally Free Flexible RF MEMS Switch With Anti-Stiction
  System","  This paper concerns a new design of RF MEMS switch combined with an
innovative process which enable low actuation voltage (<5V) and avoid stiction.
First, the structure described with principal design issues, the corresponding
anti-stiction system is presented and FEM simulations are done. Then, a short
description of the process flow based on two non polymer sacrificial layers.
Finally, RF measurements are presented and preliminary experimental protocol
and results of anti-stiction validation is detailed. Resulting RF performances
are -30dB of isolation and -0.45dB of insertion loss at 10 GHz.
"
261,"High Quality Factor Silicon Cantilever Driven by PZT Actuator for
  Resonant Based Mass Detection","  A high quality factor (Q-factor) piezoelectric lead zirconat titanate (PZT)
actuated single crystal silicon cantilever was proposed in this paper for
resonant based ultra-sensitive mass detection. Energy dissipation from
intrinsic mechanical loss of the PZT film was successfully compressed by
separating the PZT actuator from resonant structure. Excellent Q-factor, which
is several times larger than conventional PZT cantilever, was achieved under
both atmospheric pressure and reduced pressures. For a 30 micrometer-wide 100
micrometer-long cantilever, Q-factor was measured as high as 1113 and 7279
under the pressure of 101.2 KPa and 35 Pa, respectively. Moreover, it was found
that high-mode vibration can be realized by the cantilever for the pursuit of
great Q-factor, while support loss became significant because of the increased
vibration amplitude at the actuation point. An optimized structure using
node-point actuation was suggested then to suppress corresponding energy
dissipation.
"
262,"A New Four States High Deflection Low Actuation Voltage Electrostatic
  Mems Switch for RF Applications","  This paper presents a new electrostatic MEMS (MicroElectroMechanical System)
based on a single high reliability totally free flexible membrane. Using four
electrodes, this structure enables four states which allowed large deflections
(4$\mu$m) with low actuation voltage (7,5V). This design presents also a good
contact force and improve the restoring force of the structure. As an example
of application, a Single Pole Double Throw (SPDT) for 24GHz applications, based
on this design, has been simulated.
"
263,Design and Fabrication of a Novel Micro Electromagnetic Actuator,"  The present study presents a new micro electromagnetic actuator utilizing a
PDMS membrane with a magnet. The actuator is integrated with micro coils to
electromagnetically actuate the membrane and results in a large deflection. The
micro electromagnetic actuator proposed in this study is easily fabricated and
is readily integrated with existing bio-medical chips due to its planar
structure.
"
264,Flexible Micro Thermoelectric Generator based on Electroplated Bi2Te3,"  We present and discuss the fabrication process and the performance of a
flexible micro thermoelectric generator with electroplated Bi2Te3 thermocouples
in a SU-8 mold. Demonstrator devices generate 278uWcm-2 at dTmeas=40K across
the experimental set up. Based on model calculations, a temperature difference
of dTG=21.4K across the generator is assumed. Due to the flexible design and
the chosen generator materials, the performance stays high even for curved
contact surfaces. The measurement results correlate well with the model based
design optimization predictions.
"
265,"A Novel Piezoelectric Microtransformer for Autonmous Sensors
  Applications","  This work relates to a novel piezoelectric transformer to be used in an
autonomous sensor unit, possibly in conjunction with a RF-MEMS retro-modulator.
"
266,"Optimization of efficiency and energy density of passive micro fuel
  cells and galvanic hydrogen generators","  A PEM micro fuel cell system is described which is based on self-breathing
PEM micro fuel cells in the power range between 1 mW and 1W. Hydrogen is
supplied with on-demand hydrogen production with help of a galvanic cell, that
produces hydrogen when Zn reacts with water. The system can be used as a
battery replacement for low power applications and has the potential to improve
the run time of autonomous systems. The efficiency has been investigated as
function of fuel cell construction and tested for several load profiles.
"
267,"Design and Fabrication of Acoustic Wave Actuated Microgenerator for
  Portable Electronic Devices","  The past few years have seen an increasing focus on energy harvesting issue,
including power supply for portable electric devices. Utilize scavenging
ambient energy from the environment could eliminate the need for batteries and
increase portable device lifetimes indefinitely. In addition, through MEMS
technology fabricated micro-generator could easy integrate with these small or
portable devices. Several different ambient sources, including solar, vibration
and temperature effect, have already exploited [1-3]. Each energy source should
be used in suitable environment, therefore to produce maximum efficiency. In
this paper, we present an acoustic wave actuated micro-generator for power
system by using the energy of acoustic waves, such as the sound from human
voices or speakerphone, to actuate a MEMS-type electromagnetic transducer. This
provides a longer device lifetime and greater power system convenience.
Moreover, it is convenient to integrate MEMS-based microgenerators with small
or porta le devices
"
268,Package Hermeticity Testing with Thermal Transient Measurements,"  The rapid incursion of new technologies such as MEMS and smart sensor device
manufacturing requires new tailor-made packaging designs. In many applications
these devices are exposed to humid environments. Since the penetration of
moisture into the package may result in internal corrosion or shift of the
operating parameters, the reliability testing of hermetically sealed packages
has become a crucial question in the semiconductor industry.
"
269,"Micro-tensile tests on micromachined metal on polymer specimens:
  elasticity, plasticity and rupture","  This study is focused on the mechanical characterization of materials used in
microelectronic and micro- electromechanical systems (MEMS) devices. In order
to determine their mechanical parameters, a new deformation bench test with
suitable micromachined specimens have been developed. Uniaxial tensile tests
were performed on ""low cost"" specimens, consisting in electroplated thin copper
films and structures, deposited on a polimide type substrate. Moreover, a
cyclic mechanical actuation via piezoelectric actuators was tested on the same
deformation bench. These experiments validate the device for performing dynamic
characterization of materials, and reliability studies of different
microstructures.
"
270,Open Ended Microwave Oven for Packaging,"  A novel open waveguide cavity resonator is presented for the combined
variable frequency microwave curing of bumps, underfills and encapsulants, as
well as the alignment of devices for fast flip-chip assembly, direct chip
attach (DCA) or wafer-scale level packaging (WSLP). This technology achieves
radio frequency (RF) curing of adhesives used in microelectronics,
optoelectronics and medical devices with potential simultaneous micron-scale
alignment accuracy and bonding of devices. In principle, the open oven cavity
can be fitted directly onto a flip-chip or wafer scale bonder and, as such,
will allow for the bonding of devices through localised heating thus reducing
the risk to thermally sensitive devices. Variable frequency microwave (VFM)
heating and curing of an idealised polymer load is numerically simulated using
a multi-physics approach. Electro-magnetic fields within a novel open ended
microwave oven developed for use in micro-electronics manufacturing
applications are solved using a de icated Yee scheme finite-difference
time-domain (FDTD) solver. Temperature distribution, degree of cure and thermal
stresses are analysed using an Unstructured Finite Volume method (UFVM)
multi-physics package. The polymer load was meshed for thermophysical analysis,
whilst the microwave cavity - encompassing the polymer load - was meshed for
microwave irradiation. The two solution domains are linked using a
cross-mapping routine. The principle of heating using the evanescent fringing
fields within the open-end of the cavity is demonstrated. A closed loop
feedback routine is established allowing the temperature within a lossy sample
to be controlled. A distribution of the temperature within the lossy sample is
obtained by using a thermal imaging camera.
"
271,Mechanical Fatigue on Gold MEMS Devices: Experimental Results,"  The effect of mechanical fatigue on structural performances of gold devices
is investigated. The pull-in voltage of special testing micro-systems is
monitored during the cyclical load application. The mechanical collapse is
identified as a dramatic loss of mechanical strength of the specimen. The
fatigue limit is estimated through the stair-case method by means of the
pull-in voltage measurements. Measurements are performed by means of the
optical interferometric technique.
"
272,Testability of Reversible Iterative Logic Arrays,"  Iterative Logic Arrays (ILAs) are ideal as VLSI sub-systems because of their
regular structure and its close resemblance with FPGAs (Field Programmable Gate
Arrays). Reversible circuits are of interest in the design of very low power
circuits where energy loss implied by high frequency switching is not of much
consideration. Reversibility is essential for Quantum Computing. This paper
examines the testability of Reversible Iterative Logic Arrays (ILAs) composed
of reversible k-CNOT gates. For certain ILAs it is possible to find a test set
whose size remains constant irrespective of the size of the ILA, while for
others it varies with array size. Former type of ILAs is known as
Constant-Testable, i.e. C-Testable. It has been shown that Reversible Logic
Arrays are C-Testable and size of test set is equal to number of entries in
cells truth table implying that the reversible ILAs are also Optimal-Testable,
i.e. O-Testable. Uniform-Testability, i.e. U-Testability has been defined and
Reversible Heterogeneous ILAs have been characterized as U-Testable. The test
generation problem has been shown to be related to certain properties of cycles
in a set of graphs derived from cell truth table. By careful analysis of these
cycles an efficient test generation technique that can be easily converted to
an ATPG program has been presented for both 1-D and 2D ILAs. The same
algorithms can be easily extended for n-Dimensional Reversible ILAs.
"
273,"Fast Monte Carlo Estimation of Timing Yield: Importance Sampling with
  Stochastic Logical Effort (ISLE)","  In the nano era in integrated circuit fabrication technologies, the
performance variability due to statistical process and circuit parameter
variations is becoming more and more significant. Considerable effort has been
expended in the EDA community during the past several years in trying to cope
with the so-called statistical timing problem. Most of this effort has been
aimed at generalizing the static timing analyzers to the statistical case. In
this paper, we take a pragmatic approach in pursuit of making the Monte Carlo
method for timing yield estimation practically feasible. The Monte Carlo method
is widely used as a golden reference in assessing the accuracy of other timing
yield estimation techniques. However, it is generally believed that it can not
be used in practice for estimating timing yield as it requires too many costly
full circuit simulations for acceptable accuracy. In this paper, we present a
novel approach to constructing an improvedMonte Carlo estimator for timing
yield which provides the same accuracy as the standard Monte Carlo estimator,
but at a cost of much fewer full circuit simulations. This improved estimator
is based on a novel combination of a variance reduction technique, importance
sampling, and a stochastic generalization of the logical effort formalism for
cheap but approximate delay estimation. The results we present demonstrate that
our improved yield estimator achieves the same accuracy as the standard Monte
Carlo estimator at a cost reduction reaching several orders of magnitude.
"
274,"Fusion d'images: application au contr\^ole de la distribution des
  biopsies prostatiques","  This paper is about the application of a 3D ultrasound data fusion technique
to the 3D reconstruction of prostate biopies in a reference volume. The method
is introduced and its evaluation on a series of data coming from 15 patients is
described.
"
275,"Fuzzy Feedback Scheduling of Resource-Constrained Embedded Control
  Systems","  The quality of control (QoC) of a resource-constrained embedded control
system may be jeopardized in dynamic environments with variable workload. This
gives rise to the increasing demand of co-design of control and scheduling. To
deal with uncertainties in resource availability, a fuzzy feedback scheduling
(FFS) scheme is proposed in this paper. Within the framework of feedback
scheduling, the sampling periods of control loops are dynamically adjusted
using the fuzzy control technique. The feedback scheduler provides QoC
guarantees in dynamic environments through maintaining the CPU utilization at a
desired level. The framework and design methodology of the proposed FFS scheme
are described in detail. A simplified mobile robot target tracking system is
investigated as a case study to demonstrate the effectiveness of the proposed
FFS scheme. The scheme is independent of task execution times, robust to
measurement noises, and easy to implement, while incurring only a small
overhead.
"
276,Neural Feedback Scheduling of Real-Time Control Tasks,"  Many embedded real-time control systems suffer from resource constraints and
dynamic workload variations. Although optimal feedback scheduling schemes are
in principle capable of maximizing the overall control performance of
multitasking control systems, most of them induce excessively large
computational overheads associated with the mathematical optimization routines
involved and hence are not directly applicable to practical systems. To
optimize the overall control performance while minimizing the overhead of
feedback scheduling, this paper proposes an efficient feedback scheduling
scheme based on feedforward neural networks. Using the optimal solutions
obtained offline by mathematical optimization methods, a back-propagation (BP)
neural network is designed to adapt online the sampling periods of concurrent
control tasks with respect to changes in computing resource availability.
Numerical simulation results show that the proposed scheme can reduce the
computational overhead significantly while delivering almost the same overall
control performance as compared to optimal feedback scheduling.
"
277,"From a set of parts to an indivisible whole. Part III: Holistic space of
  multi-object relations","  The previously described methodology for hierarchical grouping of objects
through iterative averaging has been used for simulation of cooperative
interactions between objects of a system with the purpose of investigation of
the conformational organization of the system. Interactions between objects
were analyzed within the space of an isotropic field of one of the objects
(drifter). Such an isotropic field of an individual object can be viewed as a
prototype of computer ego. It allows visualization of a holistic space of
multi-object relations (HSMOR) which has a complex structure depending on the
number of objects, their mutual arrangement in space, and the type of metric
used for assessment of (dis)similarities between the objects. In the course of
computer simulation of cooperative interactions between the objects, only those
points of the space were registered which corresponded to transitions in
hierarchical grouping. Such points appeared to aggregate into complex spatial
structures determining a unique internal organization of a respective HSMOR. We
describe some of the peculiarities of such structures, referred to by us as
attractor membranes, and discuss their properties. We also demonstrate the
peculiarities of the changing of intergroup similarities which occurs when a
drifter infinitely moves away from the fixed objects.
"
278,"Control-Scheduling Codesign: A Perspective on Integrating Control and
  Computing","  Despite rapid evolution, embedded computing systems increasingly feature
resource constraints and workload uncertainties. To achieve much better system
performance in unpredictable environments than traditional design approaches, a
novel methodology, control-scheduling codesign, is emerging in the context of
integrating feedback control and real-time computing. The aim of this work is
to provide a better understanding of this emerging methodology and to spark new
interests and developments in both the control and computer science
communities. The state of the art of control-scheduling codesign is captured.
Relevant research efforts in the literature are discussed under two categories,
i.e., control of computing systems and codesign for control systems. Critical
open research issues on integrating control and computing are also outlined.
"
279,Atlas-Based Prostate Segmentation Using an Hybrid Registration,"  Purpose: This paper presents the preliminary results of a semi-automatic
method for prostate segmentation of Magnetic Resonance Images (MRI) which aims
to be incorporated in a navigation system for prostate brachytherapy. Methods:
The method is based on the registration of an anatomical atlas computed from a
population of 18 MRI exams onto a patient image. An hybrid registration
framework which couples an intensity-based registration with a robust
point-matching algorithm is used for both atlas building and atlas
registration. Results: The method has been validated on the same dataset that
the one used to construct the atlas using the ""leave-one-out method"". Results
gives a mean error of 3.39 mm and a standard deviation of 1.95 mm with respect
to expert segmentations. Conclusions: We think that this segmentation tool may
be a very valuable help to the clinician for routine quantitative image
exploitation.
"
280,Problems of robustness for universal coding schemes,"  The Lempel-Ziv universal coding scheme is asymptotically optimal for the
class of all stationary ergodic sources. A problem of robustness of this
property under small violations of ergodicity is studied. A notion of
deficiency of algorithmic randomness is used as a measure of disagreement
between data sequence and probability measure. We prove that universal
compressing schemes from a large class are non-robust in the following sense:
if the randomness deficiency grows arbitrarily slowly on initial fragments of
an infinite sequence then the property of asymptotic optimality of any
universal compressing algorithm can be violated. Lempel-Ziv compressing
algorithms are robust on infinite sequences generated by ergodic Markov chains
when the randomness deficiency of its initial fragments of length $n$ grows as
$o(n)$.
"
281,Range Medians,"  We study a generalization of the classical median finding problem to batched
query case: given an array of unsorted $n$ items and $k$ (not necessarily
disjoint) intervals in the array, the goal is to determine the median in {\em
each} of the intervals in the array. We give an algorithm that uses $O(n\log n
+ k\log k \log n)$ comparisons and show a lower bound of $\Omega(n\log k)$
comparisons for this problem. This is optimal for $k=O(n/\log n)$.
"
282,"Collaborative Virtual Queue: Fair Management of Congested Departure
  Operations and Benefit Analysis","  Due to the stochastic nature of departure operations, working at full
capacity makes major US airports very sensitive to uncertainties. Consequently,
airport ground operations face critically congested taxiways and long runway
queues. In this report, we show how improved management of departure operations
from the ready-to-push-back time to the wheels-off time can potentially yield
significant benefits to airlines and air traffic services. We develop a
Collaborative Virtual Queue to enable better optimization capabilities during
congested situations while taking into account the laissez-faire competitive
environment. Results are evaluated using a departure system model, validated
using current statistics and previous studies. First, the Collaborative Virtual
Queue enables keeping aircraft away from runway queues, which increases
wheels-off time predictability. Second, holding aircraft enables last-minute
intra-airline flight switching. This creates new optimization capabilities for
airlines i.e. it gives airlines the flexibility to prioritize their flight
sequence in real-time. These capabilities are illustrated by the trade-off
between minimizing the average passenger waiting time and minimizing the level
of unfairness between aircraft of the same airline. For instance, airlines
could choose to decrease by up to 15% their average passenger waiting time by
prioritizing heavy planes over small planes when the taxiway system is
congested.
"
283,Random XML sampling the Boltzmann way,"  In this article we present the prototype of a framework capable of producing,
with linear complexity, uniformly random XML documents with respect to a given
RELAX NG grammar. The generation relies on powerful combinatorial methods
together with numerical and symbolic resolution of polynomial systems.
"
284,A General Framework for Sound and Complete Floyd-Hoare Logics,"  This paper presents an abstraction of Hoare logic to traced symmetric
monoidal categories, a very general framework for the theory of systems. Our
abstraction is based on a traced monoidal functor from an arbitrary traced
monoidal category into the category of pre-orders and monotone relations. We
give several examples of how our theory generalises usual Hoare logics (partial
correctness of while programs, partial correctness of pointer programs), and
provide some case studies on how it can be used to develop new Hoare logics
(run-time analysis of while programs and stream circuits).
"
285,Fixed-Point Design of Generalized Comb Filters: A Statistical Approach,"  This paper is concerned with the problem of designing computationally
efficient Generalized Comb Filters (GCF). Basically, GCF filters are
anti-aliasing filters that guarantee superior performance in terms of
selectivity and quantization noise rejection compared to classical comb
filters, when used as decimation filters in multistage architectures. Upon
employing a partial polyphase (PP) architecture proposed in a companion paper,
we develop a sensitivity analysis in order to investigate the effects of the
coefficients' quantization on the frequency response of the designed filters.
We show that the sensitivity of the filter response to errors in the
coefficients is dependent on the particular split of the decimation factor
between the two sub-filters constituting the PP architecture. The sensitivity
analysis is then used for developing a fixed-point implementation of a sample
filter from the class of GCF filters, used as reference filter throughout the
paper. Finally, we present computer simulations in order to evaluate the
performance of the designed fixed-point filters.
"
286,A First Step to Convolutive Sparse Representation,"  In this paper an extension of the sparse decomposition problem is considered
and an algorithm for solving it is presented. In this extension, it is known
that one of the shifted versions of a signal s (not necessarily the original
signal itself) has a sparse representation on an overcomplete dictionary, and
we are looking for the sparsest representation among the representations of all
the shifted versions of s. Then, the proposed algorithm finds simultaneously
the amount of the required shift, and the sparse representation. Experimental
results emphasize on the performance of our algorithm.
"
287,"A Reconfigurable Programmable Logic Block for a Multi-Style Asynchronous
  FPGA resistant to Side-Channel Attacks","  Side-channel attacks are efficient attacks against cryptographic devices.
They use only quantities observable from outside, such as the duration and the
power consumption. Attacks against synchronous devices using electric
observations are facilitated by the fact that all transitions occur
simultaneously with some global clock signal. Asynchronous control remove this
synchronization and therefore makes it more difficult for the attacker to
insulate \emph{interesting intervals}. In addition the coding of data in an
asynchronous circuit is inherently more difficult to attack. This article
describes the Programmable Logic Block of an asynchronous FPGA resistant
against \emph{side-channel attacks}. Additionally it can implement different
styles of asynchronous control and of data representation.
"
288,Enhanced Energy-Aware Feedback Scheduling of Embedded Control Systems,"  Dynamic voltage scaling (DVS) is one of the most effective techniques for
reducing energy consumption in embedded and real-time systems. However,
traditional DVS algorithms have inherent limitations on their capability in
energy saving since they rarely take into account the actual application
requirements and often exploit fixed timing constraints of real-time tasks.
Taking advantage of application adaptation, an enhanced energy-aware feedback
scheduling (EEAFS) scheme is proposed, which integrates feedback scheduling
with DVS. To achieve further reduction in energy consumption over pure DVS
while not jeopardizing the quality of control, the sampling period of each
control loop is adapted to its actual control performance, thus exploring
flexible timing constraints on control tasks. Extensive simulation results are
given to demonstrate the effectiveness of EEAFS under different scenarios.
Compared with the optimal pure DVS scheme, EEAFS saves much more energy while
yielding comparable control performance.
"
289,"Integrated Design and Implementation of Embedded Control Systems with
  Scilab","  Embedded systems are playing an increasingly important role in control
engineering. Despite their popularity, embedded systems are generally subject
to resource constraints and it is therefore difficult to build complex control
systems on embedded platforms. Traditionally, the design and implementation of
control systems are often separated, which causes the development of embedded
control systems to be highly time-consuming and costly. To address these
problems, this paper presents a low-cost, reusable, reconfigurable platform
that enables integrated design and implementation of embedded control systems.
To minimize the cost, free and open source software packages such as Linux and
Scilab are used. Scilab is ported to the embedded ARM-Linux system. The drivers
for interfacing Scilab with several communication protocols including serial,
Ethernet, and Modbus are developed. Experiments are conducted to test the
developed embedded platform. The use of Scilab enables implementation of
complex control algorithms on embedded platforms. With the developed platform,
it is possible to perform all phases of the development cycle of embedded
control systems in a unified environment, thus facilitating the reduction of
development time and cost.
"
290,"Performance-Aware Power Management in Embedded Controllers with
  Multiple-Voltage Processors","  The goal of this work is to minimize the energy dissipation of embedded
controllers without jeopardizing the quality of control (QoC). Taking advantage
of the dynamic voltage scaling (DVS) technology, this paper develops a
performance-aware power management scheme for embedded controllers with
processors that allow multiple voltage levels. The periods of control tasks are
adapted online with respect to the current QoC, thus facilitating additional
energy reduction over standard DVS. To avoid the waste of CPU resources as a
result of the discrete voltage levels, a resource reclaiming mechanism is
employed to maximize the CPU utilization and also to improve the QoC.
Simulations are conducted to evaluate the performance of the proposed scheme.
Compared with the optimal standard DVS scheme, the proposed scheme is shown to
be able to save remarkably more energy while maintaining comparable QoC.
"
291,"Design of a Fractional Order PID Controller Using Particle Swarm
  Optimization Technique","  Particle Swarm Optimization technique offers optimal or suboptimal solution
to multidimensional rough objective functions. In this paper, this optimization
technique is used for designing fractional order PID controllers that give
better performance than their integer order counterparts. Controller synthesis
is based on required peak overshoot and rise time specifications. The
characteristic equation is minimized to obtain an optimum set of controller
parameters. Results show that this design method can effectively tune the
parameters of the fractional order controller.
"
292,"Approximation of a Fractional Order System by an Integer Order Model
  Using Particle Swarm Optimization Technique","  System identification is a necessity in control theory. Classical control
theory usually considers processes with integer order transfer functions. Real
processes are usually of fractional order as opposed to the ideal integral
order models. A simple and elegant scheme is presented for approximation of
such a real world fractional order process by an ideal integral order model. A
population of integral order process models is generated and updated by PSO
technique, the fitness function being the sum of squared deviations from the
set of observations obtained from the actual fractional order process. Results
show that the proposed scheme offers a high degree of accuracy.
"
293,"A Swarm Intelligence Based Scheme for Complete and Fault-tolerant
  Identification of a Dynamical Fractional Order Process","  System identification refers to estimation of process parameters and is a
necessity in control theory. Physical systems usually have varying parameters.
For such processes, accurate identification is particularly important. Online
identification schemes are also needed for designing adaptive controllers. Real
processes are usually of fractional order as opposed to the ideal integral
order models. In this paper, we propose a simple and elegant scheme of
estimating the parameters for such a fractional order process. A population of
process models is generated and updated by particle swarm optimization (PSO)
technique, the fitness function being the sum of squared deviations from the
actual set of observations. Results show that the proposed scheme offers a high
degree of accuracy even when the observations are corrupted to a significant
degree. Additional schemes to improve the accuracy still further are also
proposed and analyzed.
"
294,"The Application of Stochastic Optimization Algorithms to the Design of a
  Fractional-order PID Controller","  The Proportional-Integral-Derivative Controller is widely used in industries
for process control applications. Fractional-order PID controllers are known to
outperform their integer-order counterparts. In this paper, we propose a new
technique of fractional-order PID controller synthesis based on peak overshoot
and rise-time specifications. Our approach is to construct an objective
function, the optimization of which yields a possible solution to the design
problem. This objective function is optimized using two popular bio-inspired
stochastic search algorithms, namely Particle Swarm Optimization and
Differential Evolution. With the help of a suitable example, the superiority of
the designed fractional-order PID controller to an integer-order PID controller
is affirmed and a comparative study of the efficacy of the two above algorithms
in solving the optimization problem is also presented.
"
295,"A Deterministic Model for Analyzing the Dynamics of Ant System Algorithm
  and Performance Amelioration through a New Pheromone Deposition Approach","  Ant Colony Optimization (ACO) is a metaheuristic for solving difficult
discrete optimization problems. This paper presents a deterministic model based
on differential equation to analyze the dynamics of basic Ant System algorithm.
Traditionally, the deposition of pheromone on different parts of the tour of a
particular ant is always kept unvarying. Thus the pheromone concentration
remains uniform throughout the entire path of an ant. This article introduces
an exponentially increasing pheromone deposition approach by artificial ants to
improve the performance of basic Ant System algorithm. The idea here is to
introduce an additional attracting force to guide the ants towards destination
more easily by constructing an artificial potential field identified by
increasing pheromone concentration towards the goal. Apart from carrying out
analysis of Ant System dynamics with both traditional and the newly proposed
deposition rules, the paper presents an exhaustive set of experiments performed
to find out suitable parameter ranges for best performance of Ant System with
the proposed deposition approach. Simulations reveal that the proposed
deposition rule outperforms the traditional one by a large extent both in terms
of solution quality and algorithm convergence. Thus, the contributions of the
article can be presented as follows: i) it introduces differential equation and
explores a novel method of analyzing the dynamics of ant system algorithms, ii)
it initiates an exponentially increasing pheromone deposition approach by
artificial ants to improve the performance of algorithm in terms of solution
quality and convergence time, iii) exhaustive experimentation performed
facilitates the discovery of an algebraic relationship between the parameter
set of the algorithm and feature of the problem environment.
"
296,"Tuning PID and FOPID Controllers using the Integral Time Absolute Error
  Criterion","  Particle swarm optimization (PSO) is extensively used for real parameter
optimization in diverse fields of study. This paper describes an application of
PSO to the problem of designing a fractional-order
proportional-integral-derivative (FOPID) controller whose parameters comprise
proportionality constant, integral constant, derivative constant, integral
order (lambda) and derivative order (delta). The presence of five optimizable
parameters makes the task of designing a FOPID controller more challenging than
conventional PID controller design. Our design method focuses on minimizing the
Integral Time Absolute Error (ITAE) criterion. The digital realization of the
deigned system utilizes the Tustin operator-based continued fraction expansion
scheme. We carry out a simulation that illustrates the effectiveness of the
proposed approach especially for realizing fractional-order plants. This paper
also attempts to study the behavior of fractional PID controller vis-a-vis that
of its integer order counterpart and demonstrates the superiority of the former
to the latter.
"
297,"A Study of the Grunwald-Letnikov Definition for Minimizing the Effects
  of Random Noise on Fractional Order Differential Equations","  Of the many definitions for fractional order differintegral, the
Grunwald-Letnikov definition is arguably the most important one. The necessity
of this definition for the description and analysis of fractional order systems
cannot be overstated. Unfortunately, the Fractional Order Differential Equation
(FODE) describing such a systems, in its original form, highly sensitive to the
effects of random noise components inevitable in a natural environment. Thus
direct application of the definition in a real-life problem can yield erroneous
results. In this article, we perform an in-depth mathematical analysis the
Grunwald-Letnikov definition in depth and, as far as we know, we are the first
to do so. Based on our analysis, we present a transformation scheme which will
allow us to accurately analyze generalized fractional order systems in presence
of significant quantities of random errors. Finally, by a simple experiment, we
demonstrate the high degree of robustness to noise offered by the said
transformation and thus validate our scheme.
"
298,"Complete Identification of a Dynamic Fractional Order System Under
  Non-ideal Conditions Using Fractional Differintegral Definitions","  This contribution deals with identification of fractional-order dynamical
systems. System identification, which refers to estimation of process
parameters, is a necessity in control theory. Real processes are usually of
fractional order as opposed to the ideal integral order models. A simple and
elegant scheme of estimating the parameters for such a fractional order process
is proposed. This method employs fractional calculus theory to find equations
relating the parameters that are to be estimated, and then estimates the
process parameters after solving the simultaneous equations. The data used for
the calculations are intentionally corrupted to simulate real-life conditions.
Results show that the proposed scheme offers a very high degree of accuracy
even for erroneous data.
"
299,"A Novel Approach for Complete Identification of Dynamic Fractional Order
  Systems Using Stochastic Optimization Algorithms and Fractional Calculus","  This contribution deals with identification of fractional-order dynamical
systems. System identification, which refers to estimation of process
parameters, is a necessity in control theory. Real processes are usually of
fractional order as opposed to the ideal integral order models. A simple and
elegant scheme of estimating the parameters for such a fractional order process
is proposed. This method employs fractional calculus theory to find equations
relating the parameters that are to be estimated, and then estimates the
process parameters after solving the simultaneous equations. The said
simultaneous equations are generated and updated using particle swarm
optimization (PSO) technique, the fitness function being the sum of squared
deviations from the actual set of observations. The data used for the
calculations are intentionally corrupted to simulate real-life conditions.
Results show that the proposed scheme offers a very high degree of accuracy
even for erroneous data.
"
300,Adaptive Fault Masking With Incoherence Scoring,"  An adaptive voting algorithm for digital media was introduced in this study.
Availability was improved by incoherence scoring in voting mechanism of
Multi-Modular Redundancy. Regulation parameters give the algorithm flexibility
of adjusting priorities in decision process. Proposed adaptive voting algorithm
was shown to be more aware of fault status of redundant modules
"
301,String Art: Circle Drawing Using Straight Lines,"  An algorithm to generate the locus of a circle using the intersection points
of straight lines is proposed. The pixels on the circle are plotted independent
of one another and the operations involved in finding the locus of the circle
from the intersection of straight lines are parallelizable. Integer only
arithmetic and algorithmic optimizations are used for speedup. The proposed
algorithm makes use of an envelope to form a parabolic arc which is consequent
transformed into a circle. The use of parabolic arcs for the transformation
results in higher pixel errors as the radius of the circle to be drawn
increases. At its current state, the algorithm presented may be suitable only
for generating circles for string art.
"
302,"The Impact of New Technologies in Public Financial Management and
  Performance: Agenda for Public Financial Management Reformance in the Context
  of Global Best Practices","  Information and Communication Technologies (ICT) has practically penetrated
into all spheres of life. Therefore a closer look at the impact of ICT in
public financial management and performance is highly justified. Public finance
is defined as a field of economics concerned with paying for collective or
governmental activities, and with the administration and design of those
activities. Activities will be viewed as services or more precisely as public
services. We believe that there is need to consider performance from the
perspective of effective performance and the perceived performance. In fact the
real or effective performance might not correspond to the perceived
performance. A service can be considered from the perspective of the
decision-maker, who in our case could be a government or a collectivity. ICT
can be employed in the three phases that concern the decision-maker: design,
implementation and evaluation. The beneficiaries of a service can employ ICT in
any of the three phases - awareness, exploitation and assessment - for
guarantying a high level of efficiency. Each phase in the environment of a
service will be presented as well as illustrations of how ICT can be employed
in order to improve the end-result of each one of them. We believe that a high
efficiency of each phase will produce a high global efficiency. It should be
noted however that the effectiveness of any system is highly dependent on the
human engagement in the system. Therefore, the impact of ICT in public
financial management will be felt only if the decision-makers and the end-users
of the services engage themselves in the success of the system. Instead of
giving a catalog of services, the focus has been on the model (or methodology)
to adopt in designing services for which ICT could enhance the implementation.
"
303,Pancake Flipping with Two Spatulas,"  In this paper we study several variations of the \emph{pancake flipping
problem}, which is also well known as the problem of \emph{sorting by prefix
reversals}. We consider the variations in the sorting process by adding with
prefix reversals other similar operations such as prefix transpositions and
prefix transreversals. These type of sorting problems have applications in
interconnection networks and computational biology. We first study the problem
of sorting unsigned permutations by prefix reversals and prefix transpositions
and present a 3-approximation algorithm for this problem. Then we give a
2-approximation algorithm for sorting by prefix reversals and prefix
transreversals. We also provide a 3-approximation algorithm for sorting by
prefix reversals and prefix transpositions where the operations are always
applied at the unsorted suffix of the permutation. We further analyze the
problem in more practical way and show quantitatively how approximation ratios
of our algorithms improve with the increase of number of prefix reversals
applied by optimal algorithms. Finally, we present experimental results to
support our analysis.
"
304,Measures for classification and detection in steganalysis,"  Still and multi-media images are subject to transformations for compression,
steganographic embedding and digital watermarking. In a major program of
activities we are engaged in the modeling, design and analysis of digital
content. Statistical and pattern classification techniques should be combined
with understanding of run length, transform coding techniques, and also
encryption techniques.
"
305,Developments in ROOT I/O and trees,"  For the last several months the main focus of development in the ROOT I/O
package has been code consolidation and performance improvements. Access to
remote files is affected both by bandwidth and latency. We introduced a
pre-fetch mechanism to minimize the number of transactions between client and
server and hence reducing the effect of latency. We will review the
implementation and how well it works in different conditions (gain of an order
of magnitude for remote file access). We will also review new utilities,
including a faster implementation of TTree cloning (gain of an order of
magnitude), a generic mechanism for object references, and a new entry list
mechanism tuned both for small and large number of selections. In addition to
reducing the coupling with the core module and becoming its owns library
(libRIO) (as part of the general restructuration of the ROOT libraries), the
I/O package has been enhanced in the area of XML and SQL support, thread
safety, schema evolution, TTreeFormula, and many other areas. We will also
discuss various ways, ROOT will be able to benefit from multi-core architecture
to improve I/O performances.
"
306,Fault Masking By Probabilistic Voting,"  In this study, we introduced a probabilistic voter, regarding symbol
probabilities in decision process besides majority consensus. Conventional
majority voter is independent of functionality of redundant modules. In our
study, proposed probabilistic voter is designed corresponding to functionality
of the redundant module. We tested probabilistic voter for 3 and 5 redundant
modules with random transient errors inserted the wires and it was seen from
simulation results that Multi-Modular Redundancy (M-MR) with Probabilistic
Voting (PV) had been shown better availability performance than conventional
majority voter.
"
307,"Information science and technology as applications of the physics of
  signalling","  Adopting the scientific method a theoretical model is proposed as foundation
for information science and technology, extending the existing theory of
signaling: a fact f becomes known in a physical system only following the
success of a test f, tests performed primarily by human sensors and applied to
(physical) phenomena within which further tests may be performed. Tests are
phenomena and classify phenomena. A phenomenon occupies both time and space,
facts and inferences having physical counterparts which are phenomena of
specified classes. Identifiers such as f are conventional, assigned by humans;
a fact (f', f'') reports the success of a test of generic class f', the outcome
f'' of the reported application classifying the successful test in more detail.
Facts then exist only within structures of a form dictated by constraints on
the structural design of tests. The model explains why responses of real time
systems are not uniquely predictable and why restrictions, on concurrency in
performing inferences within them, are needed. Improved methods, based on the
model and applicable throughout the software life-cycle, are summarised in the
paper. No report of similar work has been found in the literature.
"
308,Self-assembly of the discrete Sierpinski carpet and related fractals,"  It is well known that the discrete Sierpinski triangle can be defined as the
nonzero residues modulo 2 of Pascal's triangle, and that from this definition
one can easily construct a tileset with which the discrete Sierpinski triangle
self-assembles in Winfree's tile assembly model. In this paper we introduce an
infinite class of discrete self-similar fractals that are defined by the
residues modulo a prime p of the entries in a two-dimensional matrix obtained
from a simple recursive equation. We prove that every fractal in this class
self-assembles using a uniformly constructed tileset. As a special case we show
that the discrete Sierpinski carpet self-assembles using a set of 30 tiles.
"
309,"Finite-size effects in the dependency networks of free and open-source
  software","  We propose a continuum model for the degree distribution of directed networks
in free and open-source software. The degree distributions of links in both the
in-directed and out-directed dependency networks follow Zipf's law for the
intermediate nodes, but the heavily linked nodes and the poorly linked nodes
deviate from this trend and exhibit finite-size effects. The finite-size
parameters make a quantitative distinction between the in-directed and
out-directed networks. For the out-degree distribution, the initial condition
for a dynamic evolution corresponds to the limiting count of the most heavily
liked nodes that the out-directed network can finally have. The number of nodes
contributing out-directed links grows with every generation of software
release, but this growth ultimately saturates towards a terminal value due to
the finiteness of semantic possibilities in the network.
"
310,An Alternative Cracking of The Genetic Code,"  We Propose 22 unique Solutions to the Genetic Code. An Alternative Cracking,
from the Perspective of a Mathematician.
"
311,"Hierarchical Triple-Modular Redundancy (H-TMR) Network For Digital
  Systems","  Hierarchical application of Triple-Modular Redundancy (TMR) increases fault
tolerance of digital Integrated Circuit (IC). In this paper, a simple
probabilistic model was proposed for analysis of fault masking performance of
hierarchical TMR networks. Performance improvements obtained by second order
TMR network were theoretically compared with first order TMR network.
"
312,MicroSim: Modeling the Swedish Population,"  This article presents a unique, large-scale and spatially explicit
microsimulation model that uses official anonymized register data collected
from all individuals living in Sweden. Individuals are connected to households
and workplaces and represent crucial links in the Swedish social contact
network. This enables significant policy experiments in the domain of epidemic
outbreaks. Development of the model started in 2004 at the Swedish Institute
for Infectious Disease Control (SMI) in Solna, Sweden with the goal of creating
a tool for testing the effects of intervention policies. These interventions
include mass vaccination, targeted vaccination, isolation and social
distancing. The model was initially designed for simulating smallpox outbreaks.
In 2006, it was modified to support simulations of pandemic influenza. All nine
millions members of the Swedish population are represented in the model. This
article is a technical description of the simulation model; the input data, the
simulation engine and the basic object types.
"
313,On Why and What of Randomness,"  This paper has several objectives. First, it separates randomness from
lawlessness and shows why even genuine randomness does not imply lawlessness.
Second, it separates the question -why should I call a phenomenon random? (and
answers it in part one) from the patent question -What is a random sequence?
-for which the answer lies in Kolmogorov complexity (which is explained in part
two). While answering the first question the note argues why there should be
four motivating factors for calling a phenomenon random: ontic, epistemic,
pseudo and telescopic, the first two depicting genuine randomness and the last
two false. Third, ontic and epistemic randomness have been distinguished from
ontic and epistemic probability. Fourth, it encourages students to be applied
statisticians and advises against becoming armchair theorists but this is
interestingly achieved by a straight application of telescopic randomness.
Overall, it tells (the teacher) not to jump to probability without explaining
randomness properly first and similarly advises the students to read (and
understand) randomness minutely before taking on probability.
"
314,Directing RF Terminals Using TELNET Applications,"  The present study aims to emphasize the way in which the TELNET protocol for
directing the mobile terminals is used and works. The paper is structured in
three parts: the first two parts are a theoretic presentation of the TELNET
protocol, respectively of the mobile terminals. The third part contains an
application of the way in which a mobile terminal can be programmed using the
TELNET protocol.
"
315,Quantum Information Science and Nanotechnology,"  In this note is touched upon an application of quantum information science
(QIS) in nanotechnology area. The laws of quantum mechanics may be very
important for nano-scale objects. A problem with simulating of quantum systems
is well known and quantum computer was initially suggested by R. Feynman just
as the way to overcome such difficulties. Mathematical methods developed in QIS
also may be applied for description of nano-devices. Few illustrative examples
are mentioned and they may be related with so-called fourth generation of
nanotechnology products.
"
316,"Analysis of the Relationships among Longest Common Subsequences,
  Shortest Common Supersequences and Patterns and its application on Pattern
  Discovery in Biological Sequences","  For a set of mulitple sequences, their patterns,Longest Common Subsequences
(LCS) and Shortest Common Supersequences (SCS) represent different aspects of
these sequences profile, and they can all be used for biological sequence
comparisons and analysis. Revealing the relationship between the patterns and
LCS,SCS might provide us with a deeper view of the patterns of biological
sequences, in turn leading to better understanding of them. However, There is
no careful examinaton about the relationship between patterns, LCS and SCS. In
this paper, we have analyzed their relation, and given some lemmas. Based on
their relations, a set of algorithms called the PALS (PAtterns by Lcs and Scs)
algorithms are propsoed to discover patterns in a set of biological sequences.
These algorithms first generate the results for LCS and SCS of sequences by
heuristic, and consequently derive patterns from these results. Experiments
show that the PALS algorithms perform well (both in efficiency and in accuracy)
on a variety of sequences. The PALS approach also provides us with a solution
for transforming between the heuristic results of SCS and LCS.
"
317,Airport Gate Assignment A Hybrid Model and Implementation,"  With the rapid development of airlines, airports today become much busier and
more complicated than previous days. During airlines daily operations,
assigning the available gates to the arriving aircrafts based on the fixed
schedule is a very important issue, which motivates researchers to study and
solve Airport Gate Assignment Problems (AGAP) with all kinds of
state-of-the-art combinatorial optimization techniques. In this paper, we study
the AGAP and propose a novel hybrid mathematical model based on the method of
constraint programming and 0 - 1 mixed-integer programming. With the objective
to minimize the number of gate conflicts of any two adjacent aircrafts assigned
to the same gate, we build a mathematical model with logical constraints and
the binary constraints. For practical considerations, the potential objective
of the model is also to minimize the number of gates that airlines must lease
or purchase in order to run their business smoothly. We implement the model in
the Optimization Programming Language (OPL) and carry out empirical studies
with the data obtained from online timetable of Continental Airlines, Houston
Gorge Bush Intercontinental Airport IAH, which demonstrate that our model can
provide an efficient evaluation criteria for the airline companies to estimate
the efficiency of their current gate assignments.
"
318,Generating Hierarchically Modular Networks via Link Switching,"  This paper introduces a method to generate hierarchically modular networks
with prescribed node degree list by link switching. Unlike many existing
network generating models, our method does not use link probabilities to
achieve modularity. Instead, it utilizes a user-specified topology to determine
relatedness between pairs of nodes in terms of edge distances and links are
switched to increase edge distances. To measure the modular-ness of a network
as a whole, a new metric called Q2 is proposed. Comparisons are made between
the Q [15] and Q2 measures. We also comment on the effect of our modularization
method on other network characteristics such as clustering, hierarchy, average
path length, small-worldness, degree correlation and centrality. An application
of this method is reported elsewhere [12]. Briefly, the generated networks are
used as test problems to explore the effect of modularity and degree
distribution on evolutionary search algorithms.
"
319,Boolean Logic with Fault Tolerant Coding,"  Error detectable and error correctable coding in Hamming space was researched
to discover possible fault tolerant coding constellations, which can implement
Boolean logic with fault tolerant property. Basic logic operators of the
Boolean algebra were developed to apply fault tolerant coding in the logic
circuits. It was shown that application of three-bit fault tolerant codes have
provided the digital system skill of auto-recovery without need for designing
additional-fault tolerance mechanisms.
"
320,Analysis of some properties for a basic Petri net model,"  The formalism of the models with Petri networks provides a sound theoretical
base, supported by powerful mathematical methods able to extract information
necessary for the formalism and simulation of the real system that provides
features of competition and synchronization. The paper presents a model based
on a Petri net, in order to extract information relative to the technological
producing process of a food additive.
"
321,Pipeline Leak Detection Techniques,"  Leak detection systems range from simple, visual line walking and checking
ones to complex arrangements of hard-ware and software. No one method is
universally applicable and operating requirements dictate which method is the
most cost effective. The aim of the paper is to review the basic techniques of
leak detection that are currently in use. The advantages and disadvantages of
each method are discussed and some indications of applicability are outlined.
"
322,Computer Systems to Oil Pipeline Transporting,"  Computer systems in the pipeline oil transporting that the greatest amount of
data can be gathered, analyzed and acted upon in the shortest amount of time.
Most operators now have some form of computer based monitoring system employing
either commercially available or custom developed software to run the system.
This paper presented the SCADA systems to oil pipeline in concordance to the
Romanian environmental reglementations.
"
323,ShopList: Programming PDA applications for Windows Mobile using C#,"  This paper is focused on a C# and Sql Server Mobile 2005 application to keep
evidence of a shop list. The purpose of the application is to offer to the user
an easier way to manage his shopping options.
"
324,"Mapping of transrectal ultrasonographic prostate biopsies: quality
  control and learning curve assessment by image processing","  Objective: Mapping of transrectal ultrasonographic (TRUS) prostate biopsies
is of fundamental importance for either diagnostic purposes or the management
and treatment of prostate cancer, but the localization of the cores seems
inaccurate. Our objective was to evaluate the capacities of an operator to plan
transrectal prostate biopsies under 2-dimensional TRUS guidance using a
registration algorithm to represent the localization of biopsies in a reference
3-dimensional ultrasonographic volume.
  Methods: Thirty-two patients underwent a series of 12 prostate biopsies under
local anesthesia performed by 1 operator using a TRUS probe combined with
specific third-party software to verify that the biopsies were indeed conducted
within the planned targets. RESULTS: The operator reached 71% of the planned
targets with substantial variability that depended on their localization (100%
success rate for targets in the middle and right parasagittal parts versus 53%
for targets in the left lateral base). Feedback from this system after each
series of biopsies enabled the operator to significantly improve his dexterity
over the course of time (first 16 patients: median score, 7 of 10 and cumulated
median biopsy length in targets of 90 mm; last 16 patients, median score, 9 of
10 and a cumulated median length of 121 mm; P = .046).
  Conclusions: In addition to being a useful tool to improve the distribution
of prostate biopsies, the potential of this system is above all the preparation
of a detailed ""map"" of each patient showing biopsy zones without substantial
changes in routine clinical practices.
"
325,5-axis High Speed Milling Optimisation,"  Manufacturing of free form parts relies on the calculation of a tool path
based on a CAD model, on a machining strategy and on a given numerically
controlled machine tool. In order to reach the best possible performances, it
is necessary to take into account a maximum of constraints during tool path
calculation. For this purpose, we have developed a surface representation of
the tool paths to manage 5-axis High Speed Milling, which is the most
complicated case. This model allows integrating early in the step of tool path
computation the machine tool geometrical constraints (axis ranges, part holder
orientation), kinematical constraints (speed and acceleration on the axes,
singularities) as well as gouging issues between the tool and the part. The aim
of the paper is to optimize the step of 5-axis HSM tool path calculation with a
bi-parameter surface representation of the tool path. We propose an example of
integration of the digital process for tool path computation, ensuring the
required quality and maximum productivity
"
326,Usinage de poches en UGV - Aide au choix de strat\'egies,"  The paper deals with associating the optimal machining strategy to a given
pocket geometry, within the context of High-Speed Machining (HSM) of
aeronautical pockets. First we define different classes of pocket features
according to geometrical criteria. Following, we propose a method allowing to
associate a set of capable tools to the features. Each capable tool defines a
machined zone with a specific geometry. The last part of the paper is thus
dedicated to associate the optimal machining strategy to a given geometry
within the context of HSM. Results highlight that analyses must be conducted in
a dynamical as well as a geometrical viewpoint. In particular, it becomes
necessary to integrate dynamical specifities associated to the behavior of the
couple machine/NC unit in the tool path calculation.
"
327,Self-Assembly of a Statistically Self-Similar Fractal,"  We demonstrate existence of a tile assembly system that self-assembles the
statistically self-similar Sierpinski Triangle in the Winfree-Rothemund Tile
Assembly Model. This appears to be the first paper that considers self-assembly
of a random fractal, instead of a deterministic fractal or a finite, bounded
shape. Our technical contributions include a way to remember, and use,
unboundedly-long prefixes of an infinite coding sequence at each stage of
fractal construction; a tile assembly mechanism for nested recursion; and a
definition of ""almost-everywhere local determinism,"" to describe a tileset
whose assembly is locally determined, conditional upon a zeta-dimension zero
set of (infinitely many) ""input"" tiles. This last is similar to the definition
of randomized computation for Turing machines, in which an algorithm is
deterministic relative to an oracle sequence of coin flips that provides advice
but does not itself compute. Keywords: tile self-assembly, statistically
self-similar Sierpinski Triangle.
"
328,First Person Singular,"  Brian Rotman argues that (one) ""mind"" and (one) ""god"" are only conceivable,
literally, because of (alphabetic) literacy, which allowed us to designate each
of these ghosts as an incorporeal, speaker-independent ""I"" (or, in the case of
infinity, a notional agent that goes on counting forever). I argue that to have
a mind is to have the capacity to feel. No one can be sure which organisms
feel, hence have minds, but it seems likely that one-celled organisms and
plants do not, whereas animals do. So minds originated before humans and before
language --hence, a fortiori, before writing, whether alphabetic or
ideographic.
"
329,P vs NP Problem in the field anthropology,"  An attempt of a new kind of complexity anthropology is considered.
"
330,Internet: Romania vs. Europe,"  This paper presents various access ways to Internet for home users, both for
those who are low consumers (consumed time online or traffic monthly value), or
large consumers (unlimited connection). The main purpose of the work consists
in making a comparison between the situation of the Internet in Romania and
other countries in Europe such as Hungary (more western than Romania, so a
little more developed, still an Eastern country comparing to the more developed
countries in Western Europe and others well developed such as England, Italy,
France, and to those in development such as Poland, and at the periphery of
Europe such as Ukraine.
"
331,"Les technologies de l'information et de la communication au niveau
  mondial et en Roumanie dans les dernieres annees","  The level of development of the electronic communication market and of the
information technology, the indicators regarding the penetration of Internet
and the level of penetration of the connections in wide band, the integration
degree of the Tic application in the business area are crucial for the
development of an informational society and for the creation of a society based
on knowledge. Though the levels are still reduced their positive evolution
reflects the attenuation of the gaps by Romania, comparatively to other
countries.
"
332,On the Ambiguity of Commercial Open Source,"  Open source and commercial applications used to be two separate worlds. The
former was the work of amateurs who had little interest in making a profit,
while the latter was only profit oriented and was produced by big companies.
Nowadays open source is a threat and an opportunity to serious businesses of
all kinds, generating good profits while delivering low costs products to
customers. The competition between commercial and open source software has
impacted the industry and the society as a whole. But in the last years, the
markets for commercial and open source software are converging rapidly and it
is interesting to resume and discuss the implications of this new paradigm,
taking into account arguments pro and against it.
"
333,Quantum theory can be collectively verified,"  No theory of physics has been collectively scientifically verified in an
experiment so far. It is pointed out that probabilistic structure of quantum
theory can be collectively scientifically verified in an experiment. It is also
argued that experimentalist point of view quantum theory is a complete theory.
"
334,"Output Width Signal Control In Asynchronous Digital Systems Using
  External Clock Signal","  In present paper, I propose a method for resolving the timing delays for
output signals from an asynchronous sequential system. It will be used an
example of an asynchronous sequential system that will set up an output signal
when an input signal will be set up. The width of the output signal depends on
the input signal width, and in this case it is very short. There are many
synthesis methods, like using a RC group system, a monostable system in design
of the asynchronous digital system or using an external clock signal, CK. In
this paper will be used an external clock signal, CK.
"
335,"Output Width Signal Control In Asynchronous Digital Systems Using
  Monostable Circuits","  In present paper, I propose a method for resolving the timing delays for
output signals from an asynchronous sequential system. It will be used an
example of an asynchronous sequential system that will set up an output signal
when an input signal will be set up. The width of the output signal depends on
the input signal width, and in this case it is very short. There are many
synthesis methods, like using a RC group system, a monostable system in design
of the asynchronous digital system or using an external clock signal, CK. In
this paper will be used a monostable circuit.
"
336,Mesh,"  Whether you just want to take a peek of a remote computer status, or you want
to install the latest version of a software on several workstations, you can do
all of this from your computer. The networks are growing, the time spent
administering the workstations increases and the number of repetitive tasks is
going sky high. But here comes MESH to take that load off your shoulders. And
because of SMS commands you can take this ""command center"" wherever you will
go. Just connect a GSM phone to the computer (using a cable, IrDA or Bluetooth)
and lock/restart/shutdown computers from your LAN with the push of a cell phone
button. You can even create your own SMS commands. This is MESH - the network
administrator's Swiss knife
"
337,"The Role of Self-Forensics in Vehicle Crash Investigations and Event
  Reconstruction","  This paper further introduces and formalizes a novel concept of
self-forensics for automotive vehicles, specified in the Forensic Lucid
language. We argue that self-forensics, with the forensics taken out of the
cybercrime domain, is applicable to ""self-dissection"" of intelligent vehicles
and hardware systems for automated incident and anomaly analysis and event
reconstruction by the software with or without the aid of the engineering teams
in a variety of forensic scenarios. We propose a formal design, requirements,
and specification of the self-forensic enabled units (similar to blackboxes) in
vehicles that will help investigation of incidents and also automated reasoning
and verification of theories along with the events reconstruction in a formal
model. We argue such an analysis is beneficial to improve the safety of the
passengers and their vehicles, like the airline industry does for planes.
"
338,"Predictability of PV power grid performance on insular sites without
  weather stations: use of artificial neural networks","  The official meteorological network is poor on the island of Corsica: only
three sites being about 50 km apart are equipped with pyranometers which enable
measurements by hourly and daily step. These sites are Ajaccio (seaside),
Bastia (seaside) and Corte (average altitude of 486 meters). This lack of
weather station makes difficult the predictability of PV power grid
performance. This work intends to study a methodology which can predict global
solar irradiation using data available from another location for daily and
hourly horizon. In order to achieve this prediction, we have used Artificial
Neural Network which is a popular artificial intelligence technique in the
forecasting domain. A simulator has been obtained using data available for the
station of Ajaccio that is the only station for which we have a lot of data: 16
years from 1972 to 1987. Then we have tested the efficiency of this simulator
in two places with different geographical features: Corte, a mountainous region
and Bastia, a coastal region. On daily horizon, the relocation has implied
fewer errors than a naive prediction method based on the persistence (RMSE=1468
Vs 1383Wh/m2 to Bastia and 1325 Vs 1213Wh/m2 to Corte). On hourly case, the
results were still satisfactory, and widely better than persistence (RMSE=138.8
Vs 109.3 Wh/m2 to Bastia and 135.1 Vs 114.7 Wh/m2 to Corte). The last
experiment was to evaluate the accuracy of our simulator on a PV power grid
localized at 10 km from the station of Ajaccio. We got errors very suitable
(nRMSE=27.9%, RMSE=99.0 W.h) compared to those obtained with the persistence
(nRMSE=42.2%, RMSE=149.7 W.h).
"
339,Multimedia Aplication for Solving a Sudoku Game,"  This article explains the way in which, with the help of Action Script 3 in
combination with Flash, a method of solving Sudoku game was implemented,
through searching for the certain numbers and after that trying to guess for
the squares where there are two possible numbers.
"
340,Mathematical Models in Danube Water Quality,"  The mathematical shaping in the study of water quality has become a branch of
environmental engineering. The comprehension and effective application of
mathematical models in studying environmental phenomena keep up with the
results in the domain of mathematics and the development of specialized
software as well. Integrated software programs simulate and predict extreme
events, propose solutions, analyzing and processing data in due time. This
paper presents a browsing through some mathematical categories of processing
the statistical data, examples and their analysis concerning the degree of
water pollution downstream the river Danube.
"
341,"Proposition d'une methode de qualification et de selection d'un logiciel
  d'analyse et de suivi du referencement dans les moteurs de recherche","  In order to measure website visibility in search engines, there are softwares
for analytics and referencing follow-up. They permit to quantify website's
efficacity of referencing and optimize its positionning in search engines. With
regard to search engines' algorithms' evolution and centralization of Key
Performance Indicators for Marketing decision making, it becomes hard to find
solutions to effectively lead a lot of projects for referencing. That's why we
have built a methodology in order compare, evaluate and choose a software for
analytics and referencing follow-up in search engines.
"
342,XML Technologies in Computer Assisted Learning and Testing Systems,"  The learning and assessment activities have undergone major changes due to
the development of modern technologies. The computer-assisted learning and
testing has proven a number of advantages in the development of modern
educational system. The paper suggests a solution for the computer-assisted
testing, which uses XML technologies, a solution that could make the basis for
developing a learning computer-assisted system.
"
343,"Designing a Framework to Develop WEB Graphical Interfaces for ORACLE
  Databases - Web Dialog","  The present article aims to describe a project consisting in designing a
framework of applications used to create graphical interfaces with an Oracle
distributed database. The development of the project supposed the use of the
latest technologies: database Oracle server, Tomcat web server, JDBC (Java
library used for accessing a database), JSP and Tag Library (for the
development of graphical interfaces).
"
344,"The Effectiveness of Computer Assisted Classes for English as a Second
  Language","  The present study aims to evaluate the efficiency of the computer assisted
English classes and to emphasize the necessity of developing sound
methodological strategies adjusted to the new technology. It also present the
benefits of using the computer in the pre-school and elementary school classes,
highlighted by a report on the comparative observation of four groups of
children studying English in a computer assisted environment.
"
345,Informatics Issues Used in the Production Dashboard,"  The aim of this paper is to present some computer aspects regarding the
implementation and the employing of a dashboard in relation to the production
activity. The paper begins with the theoretical presentation of the managerial
perspective regarding the necessity of using the dashboard. The main functions
of the dashboard in the production activity and the way it is employed are
presented in the second part of the paper.
"
346,Flexible frontiers for text division into rows,"  This paper presents an original solution for flexible hand-written text
division into rows. Unlike the standard procedure, the proposed method avoids
the isolated characters extensions amputation and reduces the recognition error
rate in the final stage.
"
347,"Specific Characteristics of Applying the Paired Comparison Method for
  Parameterization of Consumer Wants","  The article describes the main problems concerned with using expert
assessment method in consumer preference researches. The author proved the
expediency of using a 3-point measurement scale. The author suggested an
algorithm for controlling the judgments' consistency that includes analyzing
and correcting the input estimates in real-time mode.
"
348,Decision Support Systems Architectures,"  This paper presents the main components of the decision assisting systems.
Further on three types of architectures of these systems are described,
analyzed, and respectively compared, namely: the network architecture, the
centralized architecture and the hierarchical architecture.
"
349,"Adobe AIR, Bringing Rich Internet Applications to the Desktop","  Rich Internet Applications are the new trend in software development today.
Adobe AIR offers the possibility to create cross-platform desktop applications
using popular Web technologies like HTML, JavaScript, Flash and Flex. This
article is focused on presenting the advantages that this new environment has
to offer for the web development community and how quickly you can develop a
desktop application using Adobe AIR.
"
350,"Upon the Modeling and the Optimization of the Debiting Process through
  Computer Aided Non-Conventional Technologies","  The debiting process of the remarkable properties materials can be managed
through unconventional technologies as the complex electrical erosion. We
present the modeling of the previous experimental results to obtain a
mathematical dependence of the output parameters (processing time, surface
quality) on the input parameters (voltage or current). All the experimental
data are memorized on a database and for each particular debiting process a new
dependence is built. Because all the experiments applied in the Romanian
laboratories or practical applications of the nonconventional technological
processes in the factories were based on the particular conditions of one
activity, this papers presents the technical implementation of a computer-aided
solution that keeps all previous experimental data, optimizes the processing
conditions and eventual manage the driving gear. The flow-chart we present in
this paper offers a solution for practitioners to reduce the electrical
consumption while a technological processing of special materials is necessary.
The computer program and the database can be easily adapted to any
technological processes (conventional or not).
"
351,PayPal in Romania,"  The present paper refers to the usefulness of online payment through PayPal
and to the development of this payment manner in Romania. PayPal is an example
of a payment intermediary service that facilitates worldwide e-commerce.
"
352,"The morpho-topographic and cartographic analysis of the archaeological
  site Cornesti ""Iarcuri"", Timis County, Romania, using computer sciences
  methods (GIS and Remote Sensing techniques)","  The archaeological site Cornesti ""Iarcuri"" is the largest earth fortification
in Romania, made out of four concentric compounds, spreading over 1780
hectares. It is known since 1700, but it had only a few small attempts of
systematic research, the fortress gained interest only after the publishing of
some satellite images by Google Earth. It is located in an area of high fields
and it occupies three interfluves and contains two streams. Our paper contains
a geomorphologic, topographic and cartographic analysis of the site in order to
determine the limits, the structure, the morphology, the construction technique
and the functionality of such a fortification.Our research is based on
satellite image analysis, on archaeological topography, on soil, climate and
vegetation analysis as a way to offer a complex image, through this
interdisciplinary study of landscape archaeology. Through our work we try not
to date the site as this objective will be achieved only after completing the
systematic excavations which started in 2007, but only to analyze the
co-relationship with the environment.
"
353,What Does Artificial Life Tell Us About Death?,"  Short philosophical essay
"
354,Towards Activity Context using Software Sensors,"  Service-Oriented Computing delivers the promise of configuring and
reconfiguring software systems to address user's needs in a dynamic way.
Context-aware computing promises to capture the user's needs and hence the
requirements they have on systems. The marriage of both can deliver ad-hoc
software solutions relevant to the user in the most current fashion. However,
here it is a key to gather information on the users' activity (that is what
they are doing). Traditionally any context sensing was conducted with hardware
sensors. However, software can also play the same role and in some situations
will be more useful to sense the activity of the user. Furthermore they can
make use of the fact that Service-oriented systems exchange information through
standard protocols. In this paper we discuss our proposed approach to sense the
activity of the user making use of software.
"
355,"A Note on Mathematical Modelling of Practical Multicampaign Assignment
  and Its Computational Complexity","  Within personalized marketing, a recommendation issue known as multicampaign
assignment is to overcome a critical problem, known as the multiple
recommendation problem which occurs when running several personalized campaigns
simultaneously. This paper mainly deals with the hardness of multicampaign
assignment, which is treated as a very challenging problem in marketing. The
objective in this problem is to find a customer-campaign matrix which maximizes
the effectiveness of multiple campaigns under some constraints. We present a
realistic response suppression function, which is designed to be more
practical, and explain how this can be learned from historical data. Moreover,
we provide a proof that this more realistic version of the problem is NP-hard,
thus justifying to use of heuristics presented in previous work.
"
356,The Nondeterministic Waiting Time Algorithm: A Review,"  We present briefly the Nondeterministic Waiting Time algorithm. Our technique
for the simulation of biochemical reaction networks has the ability to mimic
the Gillespie Algorithm for some networks and solutions to ordinary
differential equations for other networks, depending on the rules of the
system, the kinetic rates and numbers of molecules. We provide a full
description of the algorithm as well as specifics on its implementation. Some
results for two well-known models are reported. We have used the algorithm to
explore Fas-mediated apoptosis models in cancerous and HIV-1 infected T cells.
"
357,"Mathematical Modeling of Aerodynamic Space -to - Surface Flight with
  Trajectory for Avoid Intercepting Process for Safety and Security Issues","  The research project has been made for mathematical modeling of aerospace
system Space-to-Surface for avoid intercepting process by flight objects
Surface-to-Air. The research has been completed and created mathematical models
which used for research and statistical analysis. In mathematical modeling has
been including a few models: Model of atmosphere, Model of speed of sound,
Model of flight head in space, Model of flight in atmosphere, Models of
navigation and guidance, Model and statistical analysis of approximation of
aerodynamic characteristics. Modeling has been created for a Space-to-Surface
system defined for an optimal trajectory in terminal phase. The modeling
includes models for simulation atmosphere, aerodynamic flight and navigation by
an infrared system. The modeling simulation includes statistical analysis of
the modeling results.
"
358,On the Expressiveness of Line Drawings,"  Can expressiveness of a drawing be traced with a computer? In this study a
neural network (perceptron) and a support vector machine are used to classify
line drawings. To do this the line drawings are attributed values according to
a kinematic model and a diffusion model for the lines they consist of. The
values for both models are related to looking times. Extreme values according
to these models, that is both extremely short and extremely long looking times,
are interpreted as indicating expressiveness. The results strongly indicate
that expressiveness in this sense can be detected, at least with a neural
network.
"
359,How does certainty enter into the mind?,"  Any problem is concerned with the mind, but what do minds make a decision on?
Here we show that there are three conditions for the mind to make a certain
answer. We found that some difficulties in physics and mathematics are in fact
introduced by infinity, which can not be rightly expressed by minds. Based on
this point, we suggest a general observation system, where we use region (a
type of infinity) to substitute for infinitesimal (another type of infinity)
and thus get a consistent image with the mind. Furthermore, we declare that
without world pictures we can never have ideas to any expressive events, which
is the primary condition for a wave function like mind to collapse to a series
of numbers. A following observation by expanding algorithm brings the final
collapse: classifying the numbers and coming up with a certain yes or no
answer.
"
360,"Estimation of urinary stone composition by automated processing of CT
  images","  The objective of this article was developing an automated tool for routine
clinical practice to estimate urinary stone composition from CT images based on
the density of all constituent voxels. A total of 118 stones for which the
composition had been determined by infrared spectroscopy were placed in a
helical CT scanner. A standard acquisition, low-dose and high-dose acquisitions
were performed. All voxels constituting each stone were automatically selected.
A dissimilarity index evaluating variations of density around each voxel was
created in order to minimize partial volume effects: stone composition was
established on the basis of voxel density of homogeneous zones. Stone
composition was determined in 52% of cases. Sensitivities for each compound
were: uric acid: 65%, struvite: 19%, cystine: 78%, carbapatite: 33.5%, calcium
oxalate dihydrate: 57%, calcium oxalate monohydrate: 66.5%, brushite: 75%.
Low-dose acquisition did not lower the performances (P < 0.05). This entirely
automated approach eliminates manual intervention on the images by the
radiologist while providing identical performances including for low-dose
protocols.
"
361,Towards a General Definition of Biometric Systems,"  A foundation for closing the gap between biometrics in the narrower and the
broader perspective is presented trough a conceptualization of biometric
systems in both perspectives. A clear distinction between verification,
identification and classification systems is made as well as shown that there
are additional classes of biometric systems. In the end a Unified Modeling
Language model is developed showing the connections between the two
perspectives.
"
362,On the Interesting World of Fractals and Their Applications to Music,"  In this paper we have defined one function that has been used to construct
different fractals having fractal dimensions between 1.58 and 2. Also, we tried
to calculate the amount of increment of fractal dimension in accordance with
the base of the number systems. Further, interestingly enough, these very
fractals could be a frame of lyrics for the musicians, as we know that the
fractal dimension of music is around 1.65 and varies between a high of 1.68 and
a low of 1.60. Further, at the end we conjecture that the switching from one
music fractal to another is nothing but enhancing a constant amount fractal
dimension which might be equivalent to a kind of different sets of musical
notes in various orientations.
"
363,Dynamically Generated Interfaces in XML Based Architecture,"  Providing on-line services on the Internet will require the definition of
flexible interfaces that are capable of adapting to the user's characteristics.
This is all the more important in the context of medical applications like home
monitoring, where no two patients have the same medical profile. Still, the
problem is not limited to the capacity of defining generic interfaces, as has
been made possible by UIML, but also to define the underlying information
structures from which these may be generated. The DIATELIC project deals with
the tele-monitoring of patients under peritoneal dialysis. By means of XML
abstractions, termed as ""medical components"", to represent the patient's
profile, the application configures the customizable properties of the
patient's interface and generates a UIML document dynamically. The interface
allows the patient to feed the data manually or use a device which allows
""automatic data acquisition"". The acquired medical data is transferred to an
expert system, which analyses the data and sends alerts to the medical staff.
In this paper we show how UIML can be seen as one component within a global XML
based architecture.
"
364,Multiple antenna technologies,"  Multiple antenna technologies have received high attention in the last few
decades for their capabilities to improve the overall system performance.
Multiple-input multiple-output systems include a variety of techniques capable
of not only increase the reliability of the communication but also impressively
boost the channel capacity. In addition, smart antenna systems can increase the
link quality and lead to appreciable interference reduction.
"
365,"An Empirical Comparative Study of Checklist based and Ad Hoc Code
  Reading Techniques in a Distributed Groupware Environment","  Software inspection is a necessary and important tool for software quality
assurance. Since it was introduced by Fagan at IBM in 1976, arguments exist as
to which method should be adopted to carry out the exercise, whether it should
be paper based or tool based, and what reading technique should be used on the
inspection document. Extensive works have been done to determine the
effectiveness of reviewers in paper based environment when using ad hoc and
checklist reading techniques. In this work, we take the software inspection
research further by examining whether there is going to be any significant
difference in defect detection effectiveness of reviewers when they use either
ad hoc or checklist reading techniques in a distributed groupware environment.
Twenty final year undergraduate students of computer science, divided into ad
hoc and checklist reviewers groups of ten members each were employed to inspect
a medium sized java code synchronously on groupware deployed on the Internet.
The data obtained were subjected to tests of hypotheses using independent T
test and correlation coefficients. Results from the study indicate that there
are no significant differences in the defect detection effectiveness, effort in
terms of time taken in minutes and false positives reported by the reviewers
using either ad hoc or checklist based reading techniques in the distributed
groupware environment studied.
"
366,Through-Wall Tracking Using Variance-Based Radio Tomography Networks,"  This paper presents a new method for imaging, localizing, and tracking motion
behind walls in real-time. The method takes advantage of the motion-induced
variance of received signal strength measurements made in a wireless
peer-to-peer network. Using a multipath channel model, we show that the signal
strength on a wireless link is largely dependent on the power contained in
multipath components that travel through space containing moving objects. A
statistical model relating variance to spatial locations of movement is
presented and used as a framework for the estimation of a motion image. From
the motion image, the Kalman filter is applied to recursively track the
coordinates of a moving target. Experimental results for a 34-node through-wall
imaging and tracking system over a 780 square foot area are presented.
"
367,"Prostate Biopsy Assistance System with Gland Deformation Estimation for
  Enhanced Precision","  Computer-assisted prostate biopsies became a very active research area during
the last years. Prostate tracking makes it possi- ble to overcome several
drawbacks of the current standard transrectal ultrasound (TRUS) biopsy
procedure, namely the insufficient targeting accuracy which may lead to a
biopsy distribution of poor quality, the very approximate knowledge about the
actual location of the sampled tissues which makes it difficult to implement
focal therapy strategies based on biopsy results, and finally the difficulty to
precisely reach non-ultrasound (US) targets stemming from different modalities,
statistical atlases or previous biopsy series. The prostate tracking systems
presented so far are limited to rigid transformation tracking. However, the
gland can get considerably deformed during the intervention because of US probe
pres- sure and patient movements. We propose to use 3D US combined with
image-based elastic registration to estimate these deformations. A fast elastic
registration algorithm that copes with the frequently occurring US shadows is
presented. A patient cohort study was performed, which yielded a statistically
significant in-vivo accuracy of 0.83+-0.54mm.
"
368,"Co-Channel Interference Cancellation in OFDM Networks using Coordinated
  Symbol Repetition and Soft Decision MLE CCI Canceler","  In this paper, a new scheme of downlink co-channel interference (CCI)
cancellation in OFDM cellular networks is introduced for users at the
cell-edge. Coordinated symbol transmission between base stations (BS) is
operated where the same symbol is transmitted from different BS on different
sub-carriers. At the mobile station (MS) receiver, we introduce a soft decision
maximum likelihood CCI canceler and a modified maximum ratio combining (M-MRC)
to obtain an estimate of the transmitted symbols. Weights used in the combining
method are derived from the channels coefficients between the cooperated BS and
the MS. Simulations show that the proposed scheme works well under
frequency-selective channels and frequency non-selective channels. A gain of 9
dB and 6 dB in SIR is obtained under multipath fading and flat-fading channels,
respectively.
"
369,A methodology for semi-automatic classification schema building,"  This paper describe a methodology for semi-automatic classification schema
definition (a classification schema is a taxonomy of categories useful for
automatic document classification). The methodology is based on: (i) an
extensional approach useful to create a typology starting from a document base,
and (ii) an intensional approach to build the classification schema starting
from the typology. The extensional approach uses clustering techniques to group
together documents on the basis of a similarity measure, whereas the
intensional approach uses different operations (aggregation, reduction,
generalization specialization) to define classes. keywords: ontology,
classification schema, fundamentum divisionis, cluster analysis classification
task.
"
370,"Use of L-system mathematics for making new subfamily members of
  olfactory receptor full length genes, OR1D2, OR1D4 and OR1D5","  Ligands for only two human olfactory receptors are known. One of them, OR1D2,
binds to Bourgeonal [Malnic B, Godfrey P-A, Buck L-B (2004) The human olfactory
receptor gene family. Proc. Natl. Acad. Sci U. S. A. 101: 2584-2589 and Erratum
in: Proc Natl Acad Sci U. S. A. (2004) 101: 7205]. OR1D2, OR1D4 and OR1D5 are
three full length olfactory receptors present in an olfactory locus in human
genome. These receptors are more than 80% identical in DNA sequences and have
108 base pair mismatches among them. We have used L-system mathematics and have
been able to show a closely related subfamily of OR1D2, OR1D4 and OR1D5.
"
371,Some Thoughts on Hypercomputation,"  Hypercomputation is a relatively new branch of computer science that emerged
from the idea that the Church--Turing Thesis, which is supposed to describe
what is computable and what is noncomputable, cannot possible be true. Because
of its apparent validity, the Church--Turing Thesis has been used to
investigate the possible limits of intelligence of any imaginable life form,
and, consequently, the limits of information processing, since living beings
are, among others, information processors. However, in the light of
hypercomputation, which seems to be feasibly in our universe, one cannot impose
arbitrary limits to what intelligence can achieve unless there are specific
physical laws that prohibit the realization of something. In addition,
hypercomputation allows us to ponder about aspects of communication between
intelligent beings that have not been considered before
"
372,Assessment of a percutaneous iliosacral screw insertion simulator,"  BACKGROUND: Navigational simulator use for specialized training purposes is
rather uncommon in orthopaedic and trauma surgery. However, it reveals
providing a valuable tool to train orthopaedic surgeons and help them to plan
complex surgical procedures. PURPOSE: This work's objective was to assess
educational efficiency of a path simulator under fluoroscopic guidance applied
to sacroiliac joint percutaneous screw fixation. MATERIALS AND METHODS: We
evaluated 23 surgeons' accuracy inserting a guide-wire in a human cadaver
experiment, following a pre-established procedure. These medical trainees were
defined in three prospective respects: novice or skilled; with or without
theoretical knowledge; with or without surgical procedure familiarity. Analysed
criteria for each tested surgeon included the number of intraoperative X-rays
taken in order to achieve the surgical procedure as well as an iatrogenic index
reflecting the surgeon's ability to detect any hazardous trajectory at the time
of performing said procedure. RESULTS: An average number of 13 X-rays was
required for wire implantation by the G1 group. G2 group, assisted by the
simulator use, required an average of 10 X-rays. A substantial difference was
especially observed within the novice sub-group (N), with an average of 12.75
X-rays for the G1 category and an average of 8.5 X-rays for the G2 category. As
far as the iatrogenic index is concerned, we were unable to observe any
significant difference between the groups.
"
373,Can we debug the Universe?,"  Roughly, the Church-Turing thesis is a hypothesis that describes exactly what
can be computed by any real or feasible conceptual computing device. Generally
speaking, the computational metaphor is the idea that everything, including the
universe itself, has a computational nature. However, if the Church-Turing
thesis is not valid, then does it make sense to expect the construction of a
computer program capable of simulating the whole Universe? In the lights of
hypercomputation, the scientific discipline that is about computing beyond the
Church-Turing barrier, the most natural answer to this question is: No. This
note is a justification of this answer and its deeper meaning based on
arguments from physics, the philosophy of the mind, and, of course,
(hyper)computability theory.
"
374,"Knowledge Extraction for Discriminating Male and Female in Logical
  Reasoning from Student Model","  The learning process is a process of communication and interaction between
the teacher and his students on one side and between the students and each
others on the other side. Interaction of the teacher with his students has a
great importance in the process of learning and education. The pattern and
style of this interaction is determined by the educational situation, trends
and concerns, and educational characteristics. Classroom interaction has an
importance and a big role in increasing the efficiency of the learning process
and raising the achievement levels of students. Students need to learn skills
and habits of study, especially at the university level. The effectiveness of
learning is affected by several factors that include the prevailing patterns of
interactive behavior in the classroom. These patterns are reflected in the
activities of teacher and learners during the learning process. The
effectiveness of learning is also influenced by the cognitive and non cognitive
characteristics of teacher that help him to succeed, the characteristics of
learners, teaching subject, and the teaching methods. This paper presents a
machine learning algorithm for extracting knowledge from student model. The
proposed algorithm utilizes the inherent characteristic of genetic algorithm
and neural network for extracting comprehensible rules from the student
database. The knowledge is used for discriminating male and female levels in
logical reasoning as a part of an expert system course.
"
375,An Alternative To Common Content Management Techniques,"  Content management systems use various strategies to store and manage
information. One of the most usual methods encountered in commercial products
is to make use of the file system to store the raw content information, while
the associated metadata is kept synchronized in a relational database
management system. This strategy has its advantages but we believe it also has
significant limitations which should be addressed and eventually solved. In
this paper we propose an alternative method of storing and managing content
aiming at finding solutions for current limitations both in terms of functional
and nonfunctional requirements.
"
376,Radio Transmission Performance of EPCglobal Gen-2 RFID System,"  In this paper, we analyze the performance of the encoding and the modulation
processes in the downlink and uplink of the EPCglobal Gen2 system through the
analysis and simulation. Furthermore, the synchronization issues on time and
frequency domain and the preamble architecture are evaluated. Through the
simulation in the uplink, we find that the detection probability of FM0 and
Miller coding approaches 1 at 13dB Eb/N0.
"
377,One-bit stochastic resonance storage device,"  The increasing capacity of modern computers, driven by Moore's Law, is
accompanied by smaller noise margins and higher error rates. In this paper we
propose a memory device, consisting of a ring of two identical overdamped
bistable forward-coupled oscillators, which may serve as a building block in a
larger scale solution to this problem. We show that such a system is capable of
storing one bit and its performance improves with the addition of noise. The
proposed device can be regarded as asynchronous, in the sense that stored
information can be retrieved at any time and, after a certain synchronization
time, the probability of erroneous retrieval does not depend on the
interrogated oscillator. We characterize memory persistence time and show it to
be maximized for the same noise range that both minimizes the probability of
error and ensures synchronization. We also present experimental results for a
hard-wired version of the proposed memory, consisting of a loop of two Schmitt
triggers. We show that this device is capable of storing one bit and does so
more efficiently in the presence of noise.
"
378,"Diffusion Controlled Reactions, Fluctuation Dominated Kinetics, and
  Living Cell Biochemistry","  In recent years considerable portion of the computer science community has
focused its attention on understanding living cell biochemistry and efforts to
understand such complication reaction environment have spread over wide front,
ranging from systems biology approaches, through network analysis (motif
identification) towards developing language and simulators for low level
biochemical processes. Apart from simulation work, much of the efforts are
directed to using mean field equations (equivalent to the equations of
classical chemical kinetics) to address various problems (stability,
robustness, sensitivity analysis, etc.). Rarely is the use of mean field
equations questioned. This review will provide a brief overview of the
situations when mean field equations fail and should not be used. These
equations can be derived from the theory of diffusion controlled reactions, and
emerge when assumption of perfect mixing is used.
"
379,Rule-based Modelling and Tunable Resolution,"  We investigate the use of an extension of rule-based modelling for cellular
signalling to create a structured space of model variants. This enables the
incremental development of rule sets that start from simple mechanisms and
which, by a gradual increase in agent and rule resolution, evolve into more
detailed descriptions.
"
380,"Le travail coop\'eratif comme vecteur d'\'evolution de nos syst\`emes
  d'information","  This article focuses on presenting the cooperative tool DIMS, it is a
platform that can manage the participants's daily life, by providing
flexibility and speed in the organization's tasks. The main interest lies in
the possibility of organizing information system in a logical network, across
multiple physical sites, incorporating a specific protocol to making possible
the inter-server communication. This protocol, based on the Jabber standard,
that meets the needs of working everyday matters, allowing the distribution of
research and access to resources of the WAN. The technological objective
concerns the evolution of an information system architectures, where the
application may be a comprehensive set of tools and services on a distributed
network of remote sites. The challenge for users is the perception of a
collective role to each individual who cooperates in compliance with safety
rules and unlimited access to technical information.
"
381,AnAmeter: The First Steps to Evaluating Adaptation,"  This paper presents the online AnAmeter framework that helps characterize the
different types of adaptations a system features by helping the evaluator fill
in a simple form. The provided information is then processed to obtain a
quantitative evaluation of three parameters called global, semi-global and
local adaptation degrees. By characterizing and quantifying adaptation,
AnAmeter provides the first steps towards the evaluation of the quality of a
system's adaptation. AnAmeter is an open tool available as freeware on the web
and has been applied to a selection of well known systems. To build this
evaluation grid we also collected a number of systems that cover the full range
of adaptation types.
"
382,Discussion on Supervisory Control by Solving Automata Equation,"  In this paper we consider the supervisory control problem through language
equation solving. The equation solving approach allows to deal with more
general topologies and to find a largest supervisor which can be used as a
reservoir for deriving an optimal controller. We introduce the notions of
solutions under partial controllability and partial observability, and we show
how supervisory control problems with partial controllability and partial
observability can be solved by employing equation solving methods.
"
383,"Alternate methods of evaluation for web sites concordant to IAS/IFRS
  Standards","  This work has as the principal theme, the study, analysis and implementation
of the methodology for use the web sites in e-commerce. The authors try to deal
with particular methodological and applied aspects inherent in the analysis of
data from the interaction of man-Internet (Web-mining). The research
methodology of this work will be focused on a prevalent optic multidisciplinary
research based on the pillars of data mining and Web mining. The explosion of
Internet and electronic commerce has made the most of business to have its own
website. A company may engage internal costs for the development and operation
of their website. The website can be designed for internal access (in which
case it can be used for presentation and data storage company policies with
references of customers) or for external access (they are created and used for
promotional and advertising products and services company). The objective of
this research, primarily concerns the definition of a repertoire of tools in
analyzing e-business through the development process for web-usage mining; 2nd
objective is oriented to management, recognizing and evaluating the web-sites
in accountancy, as property intangible, which is a special case and very little
studied in economic literature financial specialty, the authors try to achieve
a national and international accounting treatment of the creation and
development of web-sites.
"
384,Web Based Cross Language Plagiarism Detection,"  As the Internet help us cross language and cultural border by providing
different types of translation tools, cross language plagiarism, also known as
translation plagiarism are bound to arise. Especially among the academic works,
such issue will definitely affect the student's works including the quality of
their assignments and paper works. In this paper, we propose a new approach in
detecting cross language plagiarism. Our web based cross language plagiarism
detection system is specially tuned to detect translation plagiarism by
implementing different techniques and tools to assist the detection process.
Google Translate API is used as our translation tool and Google Search API,
which is used in our information retrieval process. Our system is also
integrated with the fingerprint matching technique, which is a widely used
plagiarism detection technique. In general, our proposed system is started by
translating the input documents from Malay to English, followed by removal of
stop words and stemming words, identification of similar documents in corpus,
comparison of similar pattern and finally summary of the result. Three
least-frequent 4-grams fingerprint matching is used to implement the core
comparison phase during the plagiarism detection process. In K-gram fingerprint
matching technique, although any value of K can be considered, yet K = 4 was
stated as an ideal choice. This is because smaller values of K (i.e., K = 1, 2,
or 3), do not provide good discrimination between sentences. On the other hand,
the larger the values of K (i.e., K = 5, 6, 7...etc), the better discrimination
of words in one sentence from words in another.
"
385,"Speed Control of Multi Level Inverter Designed DC Series Motor with
  Neuro-Fuzzy Controllers","  This paper describes the speed control of a DC series motor for an accurate
and high-speed performance. A neural network based controlling operation with
fuzzy modeling is suggested in this paper. The driver units of these machines
are designed with a Multi-level inverter operation and are controlled by a
common current control mechanism for an accurate and efficient driving
technique for DC series motor. The neuro-fuzzy logic control technique is
introduced to eliminate uncertainties in the plant parameters of the DC Series
motors, and also considered as potential candidate for different applications
to prove adequacy of the proposed control algorithm through simulations. The
simulation result with such an approach is made and observed efficient over
other controlling technique.
"
386,Analysis on the Study of QoS-Aware Web Services Discovery,"  Web service technology has gained more important role in developing
distributed applications and systems on the Internet. Rapid growth of published
Web services makes their discovery more and more difficult. There exist many
web services which exhibit similar functional characteristics. It is imperative
to provide service consumers with facilities for selecting required web
services according to their non-functional characteristics or QoS. The
QoS-based web service discovery mechanisms will play an essential role in SOA,
as e-Business applications want to use services that most accurately meet their
requirements. However, representing and storing the values of QoS attributes
are problematic, as the current UDDI was not designed to accommodate these
emerging requirements. To solve the problems of storing QoS in UDDI and
aggregating QoS values using the tModel approach. The aim is to study these
approaches and other existing QoS tModel representation for their efficiency
and consistency in service discovery. This paper discusses a broad range of
research issues such as web service discovery or web service selection based on
QoS in the E-Business domain.
"
387,Advanced Technology in Speech Disorder Therapy of Romanian Language,"  One of the key challenges of the society development is related to public
health and one of its specific targets includes better treatments of diseases.
It is true that there are affections which by their nature do not endanger the
life of a person, but they may have negative implications during his/her
lifetime. Various language or speech disorders are part of this category.
Discovered and treated in time, they can be corrected, most often in childhood.
Because the Romanian language is a phonetic one that has its own special
linguistic particularities, there is a real need to develop advanced
information systems, which can be used to assist and help specialists in
different speech disorders therapy. The aim of this paper is to present a few
CBTS developed for the treatment of various language and speech disorders
specific to the Romanian language.
"
388,"The Effected Oxide Capacitor in CMOS Structure of Integrated Circuit
  Level 5 Micrometer Technology","  This article is present the effected oxide capacitor in CMOS structure of
integrated circuit level 5 micrometer technology. It has designed and basic
structure of MOS diode. It establish with aluminum metallization layer by
sputtering method, oxide insulator layer mode from silicon dioxide, n+ and p+
semiconductor layer, it has high capacitance concentrate. From the MOS diode
structure silicon dioxide thickness 0.5 micrometer, it will get capacitance
between aluminum metal layer and p+ semiconductor at 28.62 pF, the capacitance
between aluminum metal layer and n+ semiconductor at 29.55 pF. In this article
establish second metal layer for measurement density values of first aluminum
metal layer with second aluminum metal layer, it has density values at 16 pF.
"
389,Teaching Result Analysis Using Rough Sets and Data Mining,"  The development of IT and WWW provides different teaching strategies, which
are chosen by teachers. Students can acquire knowledge through different
learning models. The problem based learning is a popular teaching strategy for
teachers. Based on the educational theory, students increase their learning
motivation, which can increase learning effectiveness. In this paper, we
propose a concept map for each student and staff. This map finds the result of
the subjects and also recommends a sequence of remedial teaching. Here, rough
set theory is used for dealing with uncertainty in the hidden pattern of data.
For each competence the lower and upper approximations are calculated based on
the brainstorm maps.
"
390,"Retail Market analysis in targeting sales based on Consumer Behaviour
  using Fuzzy Clustering - A Rule Based Mode","  Product Bundling and offering products to customers is of critical importance
in retail marketing. In general, product bundling and offering products to
customers involves two main issues, namely identification of product taste
according to demography and product evaluation and selection to increase sales.
The former helps to identify, analyze and understand customer needs according
to the demo-graphical characteristics and correspondingly transform them into a
set of specifications and offerings for people. The latter, concerns with how
to determine the best product strategy and offerings for the customer in
helping the retail market to improve their sales. Existing research has focused
only on identifying patterns for a particular dataset and for a particular
setting. This work aims to develop an explicit decision support for the
retailers to improve their product segmentation for different settings based on
the people characteristics and thereby promoting sales by efficient knowledge
discovery from the existing sales and product records. The work presents a
framework, which models an association relation mapping between the customers
and the clusters of products they purchase in an existing location and helps in
finding rules for a new location. The methodology is based on the integration
of popular data mining approaches such as clustering and association rule
mining. It focuses on the discovery of rules that vary according to the
economic and demographic characteristics and concentrates on marketing of
products based on the population.
"
391,Pseudorandomness in Central Force Optimization,"  Central Force Optimization is a deterministic metaheuristic for an
evolutionary algorithm that searches a decision space by flying probes whose
trajectories are computed using a gravitational metaphor. CFO benefits
substantially from the inclusion of a pseudorandom component (a numerical
sequence that is precisely known by specification or calculation but otherwise
arbitrary). The essential requirement is that the sequence is uncorrelated with
the decision space topology, so that its effect is to pseudorandomly distribute
probes throughout the landscape. While this process may appear to be similar to
the randomness in an inherently stochastic algorithm, it is in fact
fundamentally different because CFO remains deterministic at every step. Three
pseudorandom methods are discussed (initial probe distribution, repositioning
factor, and decision space adaptation). A sample problem is presented in detail
and summary data included for a 23-function benchmark suite. CFO's performance
is quite good compared to other highly developed, state-of-the-art algorithms.
Includes corrections 02-03-2010.
"
392,Piecewise Certificates of Positivity for matrix polynomials,"  We show that any symmetric positive definite homogeneous matrix polynomial
$M\in\R[x_1,...,x_n]^{m\times m}$ admits a piecewise semi-certificate, i.e. a
collection of identites $M(x)=\sum_jf_{i,j}(x)U_{i,j}(x)^TU_{i,j}(x)$ where
$U_{i,j}(x)$ is a matrix polynomial and $f_{i,j}(x)$ is a non negative
polynomial on a semi-algebraic subset $S_i$, where $\R^n=\cup_{i=1}^r S_i$.
This result generalizes to the setting of biforms. Some examples of
certificates are given and among others, we study a variation around the Choi
counterexample of a positive semi-definite biquadratic form which is not a sum
of squares. As a byproduct we give a representation of the famous non negative
sum of squares polynomial $x^4z^2+z^4y^2+y^4x^2-3 x^2y^2z^2$ as the determinant
of a positive semi-definite quadratic matrix polynomial.
"
393,"Design and Analysis of a Spurious Switching Suppression Technique
  Equipped Low Power Multiplier with Hybrid Encoding Scheme","  Multiplication is an arithmetic operation that is mostly used in Digital
Signal Processing (DSP) and communication applications. Efficient
implementation of the multipliers is required in many applications. The design
and analysis of Spurious Switching Suppression Technique (SSST) equipped low
power multiplier with hybrid encoding is presented in this paper. The proposed
encoding technique reduces the number of switching activity and dynamic power
consumption by analyzing the bit patterns in the input data. In this proposed
encoding scheme, the operation is executed depends upon the number of 1s and
its position in the multiplier data. The architecture of the proposed
multiplier is designed using a low power full adder which consumes less power
than the other adder architectures. The switching activity of the proposed
multiplier has been reduced by 86 percent and 46percent compared with
conventional and Booth multiplier respectively. It is observed from the device
level simulation using TANNER 12.6 EDA that the power consumption of the
proposed multiplier has been reduced by 87 percent and 26 percent compared with
conventional and Booth multiplier.
"
394,"Measurement of Nuchal Translucency Thickness for Detection of
  Chromosomal Abnormalities using First Trimester Ultrasound Fetal Images","  The Nuchal Translucency thickness measurement is made to identify the Down
Syndrome in screening first trimester fetus and presented in this paper. The
mean shift analysis and canny operators are utilized for segmenting the nuchal
translucency region and the exact thickness has been estimated using Blob
analysis. It is observed from the results that the fetus in the 14th week of
Gestation is expected to have a nuchal translucency thickness of 1.87 plus or
minus 0.25mm.
"
395,"An Efficient Inter Carrier Interference Cancellation Schemes for OFDM
  Systems","  Orthogonal Frequency Division Multiplexing (OFDM) has recently been used
widely in wireless communication systems. OFDM is very effective in combating
intersymbol interference and can achieve high data rate in frequency selective
channel. For OFDM communication systems, the frequency offsets in mobile radio
channels distort the orthogonality between subcarriers resulting in Inter
Carrier Interference (ICI). ICI causes power leakage among subcarriers thus
degrading the system performance. A wellknown problem of OFDM is its
sensitivity to frequency offset between the transmitted and received carrier
frequencies. There are two deleterious effects caused by frequency offset one
is the reduction of signal amplitude in the output of the filters matched to
each of the carriers and the second is introduction of ICI from the other
carriers. This research work investigates three effective methods for combating
the effects of ICI: ICI Self Cancellation (SC), Maximum Likelihood (ML)
estimation, and Extended Kalman Filter (EKF) method. These three methods are
compared in terms of bit error rate performance and bandwidth efficiency.
Through simulations, it is shown that the three techniques are effective in
mitigating the modulation schemes, the ML and EKF methods perform better than
the SC method.
"
396,High Precision HalfWave Rectifier Circuit In Dual Phase Output Mode,"  This paper present high precision halfwave rectifier circuit in dual phase
output mode by 0.5 micrometer CMOS technology, plus or minus 1.5 V low voltage,
it has received input signal and sent output current signal, respond in high
frequency. The main structure compound with CMOS inverter circuit, common
source circuit, and current mirror circuit. Simulation and confirmation quality
of working by PSpice program, then it able to operating at maximum frequency
about 100 MHz, maximum input current range about 400 \mu Ap p, high precision
output signal, low power dissipation, and uses a little transistor.
"
397,"High Precision MultiWave Rectifier Circuit Operating in Low Voltage 1.5
  Volt Current Mode","  This article is present high precision multiwave rectifier circuit operating
in low voltage plus or minus 1.5 Volt current modes by CMOS technology 0.5
\mum, receive input and give output in current mode, respond at high frequency
period. The structure compound with high speed current comparator circuit,
current mirror circuit, and CMOS inverter circuit. PSpice program used for
confirmation the performance of testing. The PSpice program shows operating of
circuit is able to working at maximum input current 400 \muAp p, maximum
frequency responding 200 MHz, high precision and low power losses, and
non-precision zero crossing output signal.
"
398,"Sinusoidal Frequency Doublers Circuit With Low Voltage 1.5 Volt CMOS
  Inverter","  This paper is present sinusoidal frequency doublers circuit with low voltage
1.5 volt CMOS inverter. Main structure of circuit has three parts that is CMOS
inverter circuit, differential amplifier circuit, and square root circuit. This
circuit has designed to receive input voltage and give output voltage use few
MOS transistor, easy to understand, non complex of circuit, high precision, low
error and low power. The Simulation of circuit has MOS transistor functional in
active and saturation period. PSpice programmed has used to confirmation of
testing and simulation.
"
399,Mortality and Longevity Valuation - A Quantitative Approach,"  This paper examines several computer algorithms designed to assess mortality
and longevity risk.
"
400,Heat Sink Performance Analysis through Numerical Technique,"  The increase in dissipated power per unit area of electronic components sets
higher demands on the performance of the heat sink. Also if we continue at our
current rate of miniaturisation, laptops and other electronic devices can get
heated up tremendously. Hence we require a better heat dissipating system to
overcome the excess heat generating problem of using nanoelectronics, which is
expected to power the next generation of computers. To handle the excessive and
often unpredictable heating up of high performance electronic components like
microprocessors, we need to predict the temperature profile of the heat sink
used. This also helps us to select the best heat sink for the operating power
range of any microprocessor. Understanding the temperature profile of a heat
sink and a microprocessor helps us to handle its temperature efficiently for a
range of loads. In this work, a method to estimate the normal response of a
heat sink to various loads of a microprocessor is explained.
"
401,"A Study of VLSI Technology, Wafers and Impact on Nanotechnology","  This paper presents a detailed study of the present VLSI technological
aspects, importance and their replacement or combination with the
Nanotechnology in the VLSI world of silicon semiconductors. Here authors bring
out the nanotechnology in Silicon world which invariably means shrinking
geometry of CMOS devices to nano scale. This also refers to a new world of
nanotechnology where chemists are working in manufacturing of carbon nanotubes
, nano devices of varius materials of nano dimensions without even knowing how
this could change the whole world of Si and CMOS technology and the world we
live in.
"
402,Hybrid Workflow Policy Management for Heart Disease Identification,"  As science technology grows, medical application is becoming more complex to
solve the physiological problems within expected time. Workflow management
systems (WMS) in Grid computing are promising solution to solve the
sophisticated problem such as genomic analysis, drug discovery, disease
identification, etc. Although existing WMS can provide basic management
functionality in Grid environment, consideration of user requirements such as
performance, reliability and interaction with user is missing. In this paper,
we propose hybrid workflow management system for heart disease identification
and discuss how to guarantee different user requirements according to user SLA.
The proposed system is applied to Physio-Grid e-health platform to identify
human heart disease with ECG analysis and Virtual Heart Simulation (VHS)
workflow applications.
"
403,Solar Still Coupled With Solar Collector and Storage Tank,"  Acute shortage of good, clean drinking water is a major problem for most
developing countries of the world. In most cases, ponds, streams, wells and
rivers are often polluted that they are unsafe for direct use as drinking water
>.Often water sources are brackish and or contain harmful bacteria. Therefore
cannot be used for drinking .In addition there are many coastal locations where
sea water is abundant but potable water is not available. Solar distillation is
one of the important methods of utilizing solar energy for the supply of
potable water to small communities where natural supply of fresh water is
inadequate or of poor quality .In this direction an experimental performance
analysis was carried out on a single basin still compared with FPC coupled one.
Test were carried out for different water samples namely borewell water, sea
water, river water for a water depth of 20 mm
"
404,"Modeling of 2D and 3D Assemblies Taking Into Account Form Errors of
  Plane Surfaces","  The tolerancing process links the virtual and the real worlds. From the
former, tolerances define a variational geometrical language (geometric
parameters). From the latter, there are values limiting those parameters. The
beginning of a tolerancing process is in this duality. As high precision
assemblies cannot be analyzed with the assumption that form errors are
negligible, we propose to apply this process to assemblies with form errors
through a new way of allowing to parameterize forms and solve their assemblies.
The assembly process is calculated through a method of allowing to solve the 3D
assemblies of pairs of surfaces having form errors using a static equilibrium.
We have built a geometrical model based on the modal shapes of the ideal
surface. We compute for the completely deterministic contact points between
this pair of shapes according to a given assembly process. The solution gives
an accurate evaluation of the assembly performance. Then we compare the results
with or without taking into account the form errors. When we analyze a batch of
assemblies, the problem is to compute for the nonconformity rate of a pilot
production according to the functional requirements. We input probable errors
of surfaces (position, orientation, and form) in our calculus and we evaluate
the quality of the results compared with the functional requirements. The pilot
production then can or cannot be validated.
"
405,"Caract\'erisation des d\'efauts d'une surface sph\'erique par
  d\'ecomposition modale","  The [ISO 1101] standard specifies the form errors with geometrical tolerances
using the zone concept.To complete this concept, we present a generic method
which adapts to any geometry and allows to describe any kind of errors. Thus,we
can dissociate the part errors according to reference categories: position,
orientation,form, waviness and roughnesses. Starting from a cloud of poinds
representing the error measurement, the ""modal"" method decompose, like Fourier
series,this error in a sum of sorted errors according to the ircomplexity
degree (a number of ""wavinesses""). In addition, we propose to show, on a simple
example, that according to error complexity to be characterized, an
interpolation by the modal method allows to optimize the measuring strategy.
"
406,"A proposition of 3D inertial tolerancing to consider the statistical
  combination of the location and orientation deviations","  Tolerancing of assembly mechanisms is a major interest in the product life
cycle. One can distinguish several models with growing complexity, from
1-dimensional (1D) to 3-dimensional (3D) (including form deviations), and two
main tolerancing assumptions, the worst case and the statistical hypothesis.
This paper presents an approach to 3D statistical tolerancing using a new
acceptance criterion. Our approach is based on the 1D inertial acceptance
criterion that is extended to 3D and form acceptance. The modal
characterisation is used to describe the form deviation of a geometry as the
combination of elementary deviations (location, orientation and form). The
proposed 3D statistical tolerancing is applied on a simple mechanism with lever
arm. It is also compared to the traditional worst-case tolerancing using a
tolerance zone.
"
407,"Optimization of a Classical Stamping Progression by Modal Correction of
  Anisotropy Ears","  This work is a development from the Inetforsmep European project. We proposed
to realize a global optimization of a deep drawing industrial progression (made
of several stages) for a cup manufacture. The objectives of the process were
the thickness decrease and the geometrical parameters (especially the height).
This paper improves on this previous work in the aim of mastering the contour
error. From the optimal configuration, we expect to cut down the amount of the
needed material and the number of forming operations. Our action is focused on
the appearance of unexpected undulations (ears) located on the rim of the cups
during forming due to a nonuniform crystallographic texture. Those undulations
can cause a significant amount of scraps, productivity loss, and cost during
manufacture. In this paper, this phenomenon causes the use of four forming
operations for the cup manufacture. The aim is to cut down from four to two
forming stages by defining an optimal blank (size and shape). The advantage is
to reduce the cost of the tool manufacturing and to minimize the needed
material (by suppressing the part flange). The chosen approach consists in
defining a particular description of the ears' part by modal decomposition and
then simulating several blank shapes and sizes generated by discrete cosine
transformation (DCT). The use of a numerical simulation for the forming
operation and the design of an experiment technique allow mathematical links
between the ears' formation and the DCT coefficients. An optimization is then
possible by using mathematical links. This original approach leads the ears'
amplitude to be reduced by a factor of 10, with only 15 numerical experiments.
Moreover, we have limited the number of forming stages from 4 to 2 with a
minimal material use.
"
408,Inertial tolerancing and capability indices in an assembly production,"  Traditional tolerancing considers the conformity of a batch when the batch
satisfies the specifications. The characteristic is considered for itself and
not according to its incidence in the assembly. Inertial tolerancing proposes
another alternative of tolerancing in order to guarantee the final assembly
characteristic. The inertia I2 = \sqrt{\delta^2 + \sigma^2} is not toleranced
by a tolerance interval but by a scalar representing the maximum inertia that
the characteristic should not exceed. We detail how to calculate the inertial
tolerances according to two cases, one aims to guarantee an inertia of the
assembly characteristic the other a tolerance interval on the assembly
characteristic by a Cpk capability index, in the particular but common case of
uniform tolerances or more general with non uniform tolerances. An example will
be detailed to show the results of the different tolerancing methods.
"
409,"ICT in Universities of the Western Himalayan Region of India II: A
  Comparative SWOT Analysis","  This study presents a comparative SWOT analysis to comprehend the pattern of
development of ICT within six universities of western Himalayan region of
India. With the objective of achieving quality and excellence in higher
education system in the region, this study provides a basis to decision makers
to exploit opportunities and minimize the external threats. The SWOT analysis
of different universities, placed under three categories, has been undertaken
within the four-tier framework used earlier by the authors. Guided by the
initiatives of National Mission on Education through ICT (NMEICT) for SWOT
analysis, findings of this paper reveal, relative consistency of these three
categories of universities, with the earlier study. A few suggestions, as
opportunities, with an emphasis on problem solving orientation in higher
education, have been made to strengthen the leadership of universities in the
field of ICT.
"
410,"Modified EESM Based Link Adaptation Algorithm for Multimedia
  Transmission in Multicarrier Systems","  The previous link adaptation algorithms on ofdm based systems use equal
modulation order for all sub carrier index within a block. For multimedia
transmission using ofdm as the modulation technique, unequal constellation is
used within one ofdm subcarrier block, a set of subcarriers for audio and
another set for video transmissions. A generic model has been shown for such a
transmission and link adaptation algorithm has been proposed using EESM
(Effective Exponential SNR mapping) method as basic method. Mathematical model
has been derived for the channel based on bivariate Gaussian distribution in
which the amplitude varies two dimensionally in the same envelope. From the
Moment generating function of bivariate distribution, Probability of error has
been theoretically derived. Results have been shown for BER performance of an
ofdm system using unequal constellation. BER performances have been shown for
different values of correlation parameter and fading figure.
"
411,"Understanding Formulation of Social Capital in Online Social Network
  Sites (SNS)","  Online communities are the gatherings of like-minded people, brought together
in cyberspace by shared interests. The shared interest has hidden social
capital aspects and can be of bridging or bonding type. Creating such
communities is not a big challenge but sustaining member's participation is.
This study examines the formation and maintenance of social capital in social
network sites. In addition to assessing bonding and bridging social capital, we
explore a dimension of social capital that assesses one's ability to stay
connected with members of a previously inhabited community, which we call
maintained social capital. Such dimension is enacted here in terms of
Hypothesis.
"
412,"New Insights from an Analysis of Social Influence Networks under the
  Linear Threshold Model","  We study the spread of influence in a social network based on the Linear
Threshold model. We derive an analytical expression for evaluating the expected
size of the eventual influenced set for a given initial set, using the
probability of activation for each node in the social network. We then provide
an equivalent interpretation for the influence spread, in terms of acyclic path
probabilities in the Markov chain obtained by reversing the edges in the social
network influence graph. We use some properties of such acyclic path
probabilities to provide an alternate proof for the submodularity of the
influence function. We illustrate the usefulness of the analytical expression
in estimating the most influential set, in special cases such as the
UILT(Uniform Influence Linear Threshold), USLT(Uniform Susceptance Linear
Threshold) and node-degree based influence models. We show that the PageRank
heuristic is either provably optimal or performs very well in the above models,
and explore its limitations in more general cases. Finally, based on the
insights obtained from the analytical expressions, we provide an efficient
algorithm which approximates the greedy algorithm for the influence
maximization problem.
"
413,"Design of Current Controller for Two Quadrant DC Motor Drive by Using
  Model Order Reduction Technique","  In this paper, design of current controller for a two quadrant DC motor drive
was proposed with the help of model order reduction technique. The calculation
of current controller gain with some approximations in the conventional design
process is replaced by proposed model order reduction method. The model order
reduction technique proposed in this paper gives the better controller gain
value for the DC motor drive. The proposed model order reduction method is a
mixed method, where the numerator polynomial of reduced order model is obtained
by using stability equation method and the denominator polynomial is obtained
by using some approximation technique preceded in this paper. The designed
controllers responses were simulated with the help of MATLAB to show the
validity of the proposed method.
"
414,"An Intelligent System For Effective Forest Fire Detection Using Spatial
  Data","  The explosive growth of spatial data and extensive utilization of spatial
databases emphasize the necessity for the automated discovery of spatial
knowledge. In modern times, spatial data mining has emerged as an area of
voluminous research. Forest fires are a chief environmental concern, causing
economical and ecological damage while endangering human lives across the
world. The fast or early detection of forest fires is a vital element for
controlling such phenomenon. The application of remote sensing is at present a
significant method for forest fires monitoring, particularly in vast and remote
areas. Different methods have been presented by researchers for forest fire
detection. The motivation behind this research is to obtain beneficial
information from images in the forest spatial data and use the same in the
determination of regions at the risk of fires by utilizing Image Processing and
Artificial Intelligence techniques. This paper presents an intelligent system
to detect the presence of forest fires in the forest spatial data using
Artificial Neural Networks. The digital images in the forest spatial data are
converted from RGB to XYZ color space and then segmented by employing
anisotropic diffusion to identify the fire regions. Subsequently, Radial Basis
Function Neural Network is employed in the design of the intelligent system,
which is trained with the color space values of the segmented fire regions.
Extensive experimental assessments on publicly available spatial data
illustrated the efficiency of the proposed system in effectively detecting
forest fires.
"
415,IT in Power Sector A KPCL Implementation,"  In this paper we investigate the extent of Information Technology penetration
in Power sector, taking KPCL, Karnataka Power Corporation Ltd., a premier power
generating, a state owned public sector organization as an example. Any
organization to flourish, adoption of Information Technology is inevitable in
the days of fast changing technological advancements. It is not merely the
investment on IT which helps but adoption of right IT solutions and the optimum
use of the same does matter and becomes most critical. A strong infrastructure
coupled with modern technical and management concepts has helped KPCL to meet
the challenges of the rising energy demands of Karnataka.
"
416,"Comparative Results: Group Search Optimizer and Central Force
  Optimization","  This note compares the performance of two multidimensional search and
optimization algorithms: Group Search Optimizer and Central Force Optimization.
GSO is a new state-of-the-art algorithm that has gained some notoriety,
consequently providing an excellent yardstick for measuring the performance of
other algorithms. CFO is a novel deterministic metaheuristic that has performed
well against GSO in previous tests. The CFO implementation reported here
includes architectural improvements in errant probe retrieval and decision
space adaptation that result in even better performance. Detailed results are
provided for the twenty-three function benchmark suite used to evaluate GSO.
CFO performs better than or essentially as well as GSO on twenty functions and
nearly as well on one of the remaining three. Includes update 24 February 2010.
"
417,"Effect of different substrates on Compact stacked square Microstrip
  Antenna","  Selection of the most suitable substrate for a Microstrip antenna is a matter
of prime importance. This is because many limitations of the microstrip antenna
such as high return loss, low gain and low efficiency can be overcome by
selecting an appropriate substrate for fabrication of the antenna, without
shifting the resonant frequency significantly. The substate properties such as
its dielectric constant, loss tangent have a pronounced effect on the antenna
characteristics. Some of the critical properties that are to be taken care of
while selecting a dielectric are homogeneity, moisture absorption and adhesion
of metal- foil cladding. In this paper a comprehensive study of the effect of
variation of substrate material on the antenna properties has been presented.
"
418,"Limited Memory Prediction for Linear Systems with Different types of
  Observation","  This paper is concerned with distributed limited memory prediction for
continuous-time linear stochastic systems with multiple sensors. A distributed
fusion with the weighted sum structure is applied to the optimal local limited
memory predictors. The distributed prediction algorithm represents the optimal
linear fusion by weighting matrices under the minimum mean square criterion.
The algorithm has the parallel structure and allows parallel processing of
observations making it reliable since the rest faultless sensors can continue
to the fusion estimation if some sensors occur faulty. The derivation of
equations for error cross-covariances between the local predictors is the key
of this paper. Example demonstrates effectiveness of the distributed limited
memory predictor.
"
419,"FPGA Based Sinusoidal Pulse Width Modulated Waveform Generation for
  Solar (PV) Rural Home Power Inverter","  With the increasing concern about global environmental protection and energy
demand due to rapid growth of population in developing countries and the
diminishing trend of resources of conventional grid supply, the need to produce
freely available pollution free natural energy such as solar/wind energy has
been drawing increasing interest in every corner of the world. In an effort to
utilize these energies effectively through Power converter, a great deal of
research is being carried out by different researchers / scientist and
engineers at different places in the world to meet the increasing demand of
load. The study presents methodology to integrate solar (PV) energy (which is
freely available in every corner of the world) with grid source and supplement
the existing grid power in rural houses during its cut off or restricted supply
period. In order to get consistency in supply a DG is also added as a standby
source in the proposed integration of network. The software using novel Direct
PWM modulation strategy and its soft control features extend the flexibility to
control converter (inverter) parameters like voltage, frequency, number of
samples of PWM pulses constituting sine-wave without changing any hardware
configuration in the circuit. The system simulation of PWM Pulse generation has
been done on a XILINX based FPGA Spartan 3E board using VHDL code. The test on
simulation of PWM generation program after synthesis and compilation were
recorded and verified on a prototype sample.
"
420,SVM Model for Identification of human GPCRs,"  G-protein coupled receptors (GPCRs) constitute a broad class of cell-surface
receptors in eukaryotes and they possess seven transmembrane a-helical domains.
GPCRs are usually classified into several functionally distinct families that
play a key role in cellular signalling and regulation of basic physiological
processes. We can develop statistical models based on these common features
that can be used to classify proteins, to predict new members, and to study the
sequence-function relationship of this protein function group. In this study,
SVM based classification model has been developed for the identification of
human gpcr sequences. Sequences of Level 1 subfamilies of Class A rhodopsin is
considered as case study. In the present study, an attempt has been made to
classify GPCRs on the basis of species. The present study classifies human gpcr
sequences with rest of the species available in GPCRDB. Classification is based
on specific information derived from the n-terminal and extracellular loops of
the sequences, some physicochemical properties and amino acid composition of
corresponding gpcr sequences. Our method classifies Level 1 subfamilies of
GPCRs with 94% accuracy.
"
421,Optimized reversible BCD adder using new reversible logic gates,"  Reversible logic has received great attention in the recent years due to
their ability to reduce the power dissipation which is the main requirement in
low power digital design. It has wide applications advanced computing, low
power CMOS design, Optical information processing, DNA computing, bio
information, quantum computation and nanotechnology. This paper presents an
optimized reversible BCD adder using a new reversible gate. A comparative
result is presented which shows that the proposed design is more optimized in
terms of number of gates, number of garbage outputs and quantum cost than the
existing designs.
"
422,Determining the quality evaluation procedures using the expert systems,"  At this time, quality is a strategic instrument of the entities' global
management, but it is also a determining element of their competitive spirit.
The importance given to quality is abundantly found in the preoccupations of
the European Union's Minister Board, by elaborating documents with a high
impact over the quality of products/ services in special, and organizations in
general. We live in an era, when the evolution of the social life puts the
accent more and more on quality, resulted from various processes, at the level
of various domains of the economical and social development.
"
423,The Role of the XBRL Standard in Optimizing the Financial Reporting,"  When the financial information is difficult to produce, interpret, compare
and analyze, we are put in the situation to face inconvenient consequences with
negative repercussions, such as: the investor can give up the investment (with
negative consequences on the risk equity market), the banks may not give loans,
an auditor may not consider the financial statements as being credible etc.
These facts allow the introduction of this paper's main objective, the
eXtensible Business Reporting Language (XBRL) which is an open standard,
independent and international for the treatments, opportunity, correctness,
efficiency and minor costs of the financial and economical information. The
XBRL will be analyzed in the second part of the paper, the history of this
electronic communication language will be described, as there will also be
described the promoting organizations, the base technology (the WEB and XML
architecture which will be the next stage of the internet programming), and the
role it has within the chain of reporting between the XBRL consortium and the
international accounting organizations IASB-CI. This taxonomy serves clearly
every accounting and extra- accounting information made by the company. This
information which is treated in present by resorting to various formats or
structures (most times incompatible between them and the owners) will be
standardized with the XBRL.
"
424,"Equal Power Distribution and Dynamic Subcarrier Assignment in OFDM Using
  Minimum Channel Gain Flow with Robust Optimization Uncertain Demand","  In this paper, the minimum channel gain flow with uncertainty in the demand
vector is examined. The approach is based on a transformation of uncertainty in
the demand vector to uncertainty in the gain vector. OFDM systems are known to
overcome the impairment of the wireless channel by splitting the given system
bandwidth into parallel sub-carriers, on which data-symbols can be transmitted
simultaneously. This enables the possibility of enhancing the system's
performance by deploying adaptive mechanisms, namely power distribution and
dynamic sub-carrier assignments. The performances of maximizing the minimum
throughput have been analyzed by MATLAB codes.
"
425,Optimal Control Strategies in Delayed Sharing Information Structures,"  The $n$-step delayed sharing information structure is investigated. This
information structure comprises of $K$ controllers that share their information
with a delay of $n$ time steps. This information structure is a link between
the classical information structure, where information is shared perfectly
between the controllers, and a non-classical information structure, where there
is no ""lateral"" sharing of information among the controllers. Structural
results for optimal control strategies for systems with such information
structures are presented. A sequential methodology for finding the optimal
strategies is also derived. The solution approach provides an insight for
identifying structural results and sequential decomposition for general
decentralized stochastic control problems.
"
426,Low-complexity Fusion Filtering for Continuous-Discrete Systems,"  In this paper, low-complexity distributed fusion filtering algorithm for
mixed continuous-discrete multisensory dynamic systems is proposed. To
implement the algorithm a new recursive equations for local cross-covariances
are derived. To achieve an effective fusion filtering the covariance
intersection (CI) algorithm is used. The CI algorithm is useful due to its
low-computational complexity for calculation of a big number of
cross-covariances between local estimates and matrix weights. Theoretical and
numerical examples demonstrate the effectiveness of the covariance intersection
algorithm in distributed fusion filtering.
"
427,Nonlinear System Identification and Behavioral Modeling,"  This paper has been withdrawn by the author
"
428,"Processing of Communication Signal Using Operational Transconductance
  Amplifier","  This paper proposes a signal processing methodology of communication system
and realized that circuits using operational transconductance amplifier (OTA).
Two important classes of communication circuit, delta modulator and compander
have been designed using that procedure. In the first implementation coded
pulse modulation system is demonstrated which employ sampling, quantizing and
coding to convert analog waveforms to digital signals while the second gives
data compression and expansion in digital communication system. The proposed
compander circuit is realized with operational transconductance amplifier and
diode. Required power supply to operate the circuit is 3.5V. Performance of the
circuits realized with OTAs has been demonstrated through SPICE simulation.
"
429,"Central Force Optimization Applied to the PBM Suite of Antenna
  Benchmarks","  Central Force Optimization (CFO) is a new nature-inspired deterministic
multi-dimensional search and optimization metaheuristic based on the metaphor
of gravitational kinematics. CFO is applied to the PBM antenna benchmark suite
and the results compared to published performance data for other optimization
algorithms. CFO acquits itself quite well. CFO's gradient-like nature is
discussed, and it is speculated that a ""generalized hyperspace derivative""
might be defined for optimization problems as a new mathematical construct
based on the Unit Step function. What appears to be a sufficient but not
necessary condition for local trapping, oscillation in the probe average
distance curve, is discussed in the context of the theory of gravitational
""resonant returns"" that gives rise to strikingly similar oscillatory curves. It
is suggested that the theory may be applicable to CFO as an aid to
understanding trapping and to developing effective mitigation techniques,
possibly based on a concept of ""energy"" in CFO space. It also is suggested that
CFO may be re-formulated as a ""total energy"" model by analogizing conservation
of energy for orbiting masses in physical space.
"
430,Flexible Lyapunov Functions and Applications to Fast Mechatronic Systems,"  The property that every control system should posses is stability, which
translates into safety in real-life applications. A central tool in systems
theory for synthesizing control laws that achieve stability are control
Lyapunov functions (CLFs). Classically, a CLF enforces that the resulting
closed-loop state trajectory is contained within a cone with a fixed,
predefined shape, and which is centered at and converges to a desired
converging point. However, such a requirement often proves to be
overconservative, which is why most of the real-time controllers do not have a
stability guarantee. Recently, a novel idea that improves the design of CLFs in
terms of flexibility was proposed. The focus of this new approach is on the
design of optimization problems that allow certain parameters that define a
cone associated with a standard CLF to be decision variables. In this way
non-monotonicity of the CLF is explicitly linked with a decision variable that
can be optimized on-line. Conservativeness is significantly reduced compared to
classical CLFs, which makes \emph{flexible CLFs} more suitable for
stabilization of constrained discrete-time nonlinear systems and real-time
control. The purpose of this overview is to highlight the potential of flexible
CLFs for real-time control of fast mechatronic systems, with sampling periods
below one millisecond, which are widely employed in aerospace and automotive
applications.
"
431,Verifying Recursive Active Documents with Positive Data Tree Rewriting,"  This paper proposes a data tree-rewriting framework for modeling evolving
documents. The framework is close to Guarded Active XML, a platform used for
handling XML repositories evolving through web services. We focus on automatic
verification of properties of evolving documents that can contain data from an
infinite domain. We establish the boundaries of decidability, and show that
verification of a {\em positive} fragment that can handle recursive service
calls is decidable. We also consider bounded model-checking in our data
tree-rewriting framework and show that it is $\nexptime$-complete.
"
432,"Parameter-Free Deterministic Global Search with Central Force
  Optimization","  This note describes a parameter-free implementation of Central Force
Optimization for deterministic multidimensional search and optimization. The
user supplies only one input: the objective function to be maximized, nothing
more. The CFO equations of motion are simplified by assigning specific values
to CFO's basic parameters, and this particular algorithmic implementation also
includes hardwired internal parameters so that none is user-specified. The
algorithm's performance is tested against a widely used suite of twenty three
benchmark functions and compared to other state-of-the-art algorithms. CFO
performs very well indeed. Includes important update 20 March 2010 addressing
the issue of different probes coalescing into one.
"
433,"New clustering method to decrease probability of failure nodes and
  increasing the lifetime in WSNs","  Clustering in wireless sensor networks is one of the crucial methods for
increasing of network lifetime. There are many algorithms for clustering. One
of the important cluster based algorithm in wireless sensor networks is LEACH
algorithm. In this paper we proposed a new clustering method for increasing of
network lifetime. We distribute several sensors with a high-energy for managing
the cluster head and to decrease their responsibilities in network. The
performance of the proposed algorithm via computer simulation was evaluated and
compared with other clustering algorithms. The simulation results show the high
performance of the proposed clustering algorithm.
"
434,Current Conveyor Based Multifunction Filter,"  The paper presents a current conveyor based multifunction filter. The
proposed circuit can be realized as low pass, high pass, band pass and
elliptical notch filter. The circuit employs two balanced output current
conveyors, four resistors and two grounded capacitors, ideal for integration.
It has only one output terminal and the number of input terminals may be used.
Further, there is no requirement for component matching in the circuit. The
parameter resonance frequency (\omega_0) and bandwidth (\omega_0 /Q) enjoy
orthogonal tuning. The complementary metal oxide semiconductor (CMOS)
realization of the current conveyor is given for the simulation of the proposed
circuit. A HSPICE simulation of circuit is also studied for the verification of
theoretical results. The non-ideal analysis of CCII is also studied.
"
435,Creating A Model HTTP Server Program Using java,"  HTTP Server is a computer programs that serves webpage content to clients. A
webpage is a document or resource of information that is suitable for the World
Wide Web and can be accessed through a web browser and displayed on a computer
screen. This information is usually in HTML format, and may provide navigation
to other webpage's via hypertext links. WebPages may be retrieved from a local
computer or from a remote HTTP Server. WebPages are requested and served from
HTTP Servers using Hypertext Transfer Protocol (HTTP). WebPages may consist of
files of static or dynamic text stored within the HTTP Server's file system.
Client-side scripting can make WebPages more responsive to user input once in
the client browser. This paper encompasses the creation of HTTP server program
using java language, which is basically supporting for HTML and JavaScript.
"
436,QoS Based Dynamic Web Services Composition & Execution,"  The use of web services has dominated software industry. Existing
technologies of web services are extended to give value added customized
services to customers through composition. Automated web service composition is
a very challenging task. This paper proposed the solution of existing problems
and proposed a technique by combination of interface based and functionality
based rules. The proposed framework also solves the issues related to
unavailability of updated information and inaccessibility of web services from
repository/databases due to any fault/failure. It provides updated information
problem by adding aging factor in repository/WSDB (Web Services Database) and
inaccessibility is solved by replication of WSDB. We discussed data
distribution techniques and proposed our framework by using one of these
strategies by considering quality of service issues. Finally, our algorithm
eliminates the dynamic service composition and execution issues, supports web
service composition considering QoS (Quality of Service), efficient data
retrieval and updation, fast service distribution and fault tolerance.
"
437,"A New Variable Threshold and Dynamic Step Size Based Active Noise
  Control System for Improving Performance","  Several approaches have been introduced in literature for active noise
control (ANC) systems. Since FxLMS algorithm appears to be the best choice as a
controller filter, researchers tend to improve performance of ANC systems by
enhancing and modifying this algorithm. In this paper, modification is done in
the existing FxLMS algorithm that provides a new structure for improving the
tracking performance and convergence rate. The secondary signal y(n) is dynamic
thresholded by Wavelet transform to improve tracking. The convergence rate is
improved by dynamically varying the step size of the error signal.
"
438,Knowledge Management,"  This paper discusses the important process of knowledge and its management,
and differences between tacit and explicit knowledge and understanding the
culture as a key issue for the successful implementation of knowledge
management, in addition to, this paper is concerned with the four-stage model
for the evolution of information technology (IT) support for knowledge
management in law firms.
"
439,Wireless IP Telephony,"  The convergence of traditional telecommunications and the Internet is
creating new network-based service delivery opportunities for
telecommunications companies carriers, service providers, and network equipment
providers. Voice over Wireless IP is one of the most exciting new developments
emerging within the telephony market. It is set to revolutionize the delivery
of mobile voice Services and provide exciting new opportunities for operators
and service providers alike. This survey discusses principal of Wireless IP
Telephony.
"
440,"Model Based Ceramic tile inspection using Discrete Wavelet Transform and
  Euclidean Distance","  Visual inspection of industrial products is used to determine the control
quality for these products. This paper deals with the problem of visual
inspection of ceramic tiles industry using Wavelet Transform. The third level
the coefficients of two dimensions Haar Discrete Wavelet Transform (HDWT) is
used in this paper to process the images and feature extraction. The proposed
algorithm consists of two main phases. The first phase is to compute the
wavelet transform for an image free of defects which known as reference image,
and the image to be inspected which known as test image. The second phase is
used to decide whether the tested image is defected or not using the Euclidean
distance similarity measure. The experimentation results of the proposed
algorithm give 97% for correct detection of ceramic defects.
"
441,"FP-tree and COFI Based Approach for Mining of Multiple Level Association
  Rules in Large Databases","  In recent years, discovery of association rules among itemsets in a large
database has been described as an important database-mining problem. The
problem of discovering association rules has received considerable research
attention and several algorithms for mining frequent itemsets have been
developed. Many algorithms have been proposed to discover rules at single
concept level. However, mining association rules at multiple concept levels may
lead to the discovery of more specific and concrete knowledge from data. The
discovery of multiple level association rules is very much useful in many
applications. In most of the studies for multiple level association rule
mining, the database is scanned repeatedly which affects the efficiency of
mining process. In this research paper, a new method for discovering multilevel
association rules is proposed. It is based on FP-tree structure and uses
cooccurrence frequent item tree to find frequent items in multilevel concept
hierarchy.
"
442,"Dual-hop transmissions with fixed-gain relays over Generalized-Gamma
  fading channels","  In this paper, a study on the end-to-end performance of dual-hop wireless
communication systems equipped with fixed-gain relays and operating over
Generalized-Gamma (GG) fading channels is presented. A novel closed form
expression for the moments of the end-to-end signal-to-noise ratio (SNR) is
derived. The average bit error probability for coherent and non-coherent
modulation schemes as well as the end-to-end outage probability of the
considered system are also studied. Extensive numerically evaluated and
computer simulations results are presented that verify the accuracy of the
proposed mathematical analysis.
"
443,Simulating Grover's Quantum Search in a Classical Computer,"  The rapid progress of computer science has been accompanied by a
corresponding evolution of computation, from classical computation to quantum
computation. As quantum computing is on its way to becoming an established
discipline of computing science, much effort is being put into the development
of new quantum algorithms. One of quantum algorithms is Grover algorithm, which
is used for searching an element in an unstructured list of N elements with
quadratic speed-up over classical algorithms. In this work, Quantum Computer
Language (QCL) is used to make a Grover's quantum search simulation in a
classical computer
"
444,Automated selection of LEDs by luminance and chromaticity coordinate,"  The increased use of LEDs for lighting purposes has led to the development of
numerous applications requiring a pre-selection of LEDs by their luminance and
/ or their chromaticity coordinate. This paper demonstrates how a manual
pre-selection process can be realized using a relatively simple configuration.
Since a manual selection service can only be commercially viable as long as
only small quantities of LEDs need to be sorted, an automated solution suggests
itself. This paper introduces such a solution, which has been developed by
Harzoptics in close cooperation with Rundfunk Gernrode. The paper also
discusses current challenges in measurement technology as well as market
trends.
"
445,"Scalable, Time-Responsive, Digital, Energy-Efficient Molecular Circuits
  using DNA Strand Displacement","  We propose a novel theoretical biomolecular design to implement any Boolean
circuit using the mechanism of DNA strand displacement. The design is scalable:
all species of DNA strands can in principle be mixed and prepared in a single
test tube, rather than requiring separate purification of each species, which
is a barrier to large-scale synthesis. The design is time-responsive: the
concentration of output species changes in response to the concentration of
input species, so that time-varying inputs may be continuously processed. The
design is digital: Boolean values of wires in the circuit are represented as
high or low concentrations of certain species, and we show how to construct a
single-input, single-output signal restoration gate that amplifies the
difference between high and low, which can be distributed to each wire in the
circuit to overcome signal degradation. This means we can achieve a digital
abstraction of the analog values of concentrations. Finally, the design is
energy-efficient: if input species are specified ideally (meaning absolutely 0
concentration of unwanted species), then output species converge to their ideal
concentrations at steady-state, and the system at steady-state is in (dynamic)
equilibrium, meaning that no energy is consumed by irreversible reactions until
the input again changes.
  Drawbacks of our design include the following. If input is provided
non-ideally (small positive concentration of unwanted species), then energy
must be continually expended to maintain correct output concentrations even at
steady-state. In addition, our fuel species - those species that are
permanently consumed in irreversible reactions - are not ""generic""; each gate
in the circuit is powered by its own specific type of fuel species. Hence
different circuits must be powered by different types of fuel. Finally, we
require input to be given according to the dual-rail convention, so that an
input of 0 is specified not only by the absence of a certain species, but by
the presence of another. That is, we do not construct a ""true NOT gate"" that
sets its output to high concentration if and only if its input's concentration
is low. It remains an open problem to design scalable, time-responsive,
digital, energy-efficient molecular circuits that additionally solve one of
these problems, or to prove that some subset of their resolutions are mutually
incompatible.
"
446,Towards brain-inspired computing,"  We present introductory considerations and analysis toward computing
applications based on the recently introduced deterministic logic scheme with
random spike (pulse) trains [Phys. Lett. A 373 (2009) 2338-2342]. Also, in
considering the questions, ""Why random?"" and ""Why pulses?"", we show that the
random pulse based scheme provides the advantages of realizing multivalued
deterministic logic. Pulse trains are realized by an element called
orthogonator. We discuss two different types of orthogonators, parallel
(intersection-based) and serial (demultiplexer-based) orthogonators. The last
one can be slower but it makes sequential logic design straightforward. We
propose generating a multidimensional logic hyperspace [Physics Letters A 373
(2009) 1928-1934] by using the zero-crossing events of uncorrelated Gaussian
electrical noises available in the chips. The spike trains in the hyperspace
are non-overlapping, and are referred to as neuro-bits. To demonstrate this
idea, we generate 3-dimensional hyperspace bases using 2 Gaussian noises as
sources for neuro-bits, respectively. In such a scenario, the detection of
different hyperspace basis elements may have vastly differing delays. We show
that it is possible to provide an identical speed for all the hyperspace bases
elements using correlated noise sources, and demonstrate this for the 2
neuro-bits situations. The key impact of this paper is to demonstrate that a
logic design approach using such neuro-bits can yield a fast, low power
processing and environmental variation tolerant means of designing computer
circuitry. It also enables the realization of multi-valued logic, significantly
increasing the complexity of computer circuits by allowing several neuro-bits
to be transmitted on a single wire.
"
447,Plagiarism Detection using ROUGE and WordNet,"  With the arrival of digital era and Internet, the lack of information control
provides an incentive for people to freely use any content available to them.
Plagiarism occurs when users fail to credit the original owner for the content
referred to, and such behavior leads to violation of intellectual property. Two
main approaches to plagiarism detection are fingerprinting and term occurrence;
however, one common weakness shared by both approaches, especially
fingerprinting, is the incapability to detect modified text plagiarism. This
study proposes adoption of ROUGE and WordNet to plagiarism detection. The
former includes ngram co-occurrence statistics, skip-bigram, and longest common
subsequence (LCS), while the latter acts as a thesaurus and provides semantic
information. N-gram co-occurrence statistics can detect verbatim copy and
certain sentence modification, skip-bigram and LCS are immune from text
modification such as simple addition or deletion of words, and WordNet may
handle the problem of word substitution.
"
448,The output distribution of important LULU-operators,"  Two procedures to compute the output distribution phi_S of certain stack
filters S (so called erosion-dilation cascades) are given. One rests on the
disjunctive normal form of S and also yields the rank selection probabilities.
The other is based on inclusion-exclusion and e.g. yields phi_S for some
important LULU-operators S. Properties of phi_S can be used to characterize
smoothing properties of S. One of the methods discussed also allows for the
calculation of the reliability polynomial of any positive Boolean function
(e.g. one derived from a connected graph).
"
449,Design of A Low Power Low Voltage CMOS Opamp,"  In this paper a CMOS operational amplifier is presented which operates at 2V
power supply and 1microA input bias current at 0.8 micron technology using non
conventional mode of operation of MOS transistors and whose input is depended
on bias current. The unique behaviour of the MOS transistors in subthreshold
region not only allows a designer to work at low input bias current but also at
low voltage. While operating the device at weak inversion results low power
dissipation but dynamic range is degraded. Optimum balance between power
dissipation and dynamic range results when the MOS transistors are operated at
moderate inversion. Power is again minimised by the application of input
dependant bias current using feedback loops in the input transistors of the
differential pair with two current substractors. In comparison with the
reported low power low voltage opamps at 0.8 micron technology, this opamp has
very low standby power consumption with a high driving capability and operates
at low voltage. The opamp is fairly small (0.0084 mm 2) and slew rate is more
than other low power low voltage opamps reported at 0.8 um technology [1,2].
Vittoz at al [3] reported that slew rate can be improved by adaptive biasing
technique and power dissipation can be reduced by operating the device in weak
inversion. Though lower power dissipation is achieved the area required by the
circuit is very large and speed is too small. So, operating the device in
moderate inversion is a good solution. Also operating the device in
subthreshold region not only allows lower power dissipation but also a lower
voltage operation is achieved.
"
450,Arithmetic Operations in Multi-Valued Logic,"  This paper presents arithmetic operations like addition, subtraction and
multiplications in Modulo-4 arithmetic, and also addition, multiplication in
Galois field, using multi-valued logic (MVL). Quaternary to binary and binary
to quaternary converters are designed using down literal circuits. Negation in
modular arithmetic is designed with only one gate. Logic design of each
operation is achieved by reducing the terms using Karnaugh diagrams, keeping
minimum number of gates and depth of net in to consideration. Quaternary
multiplier circuit is proposed to achieve required optimization. Simulation
result of each operation is shown separately using Hspice.
"
451,"Bit Error Rate Performance Analysis on Modulation Techniques of Wideband
  Code Division Multiple Access","  In the beginning of 21st century there has been a dramatic shift in the
market dynamics of telecommunication services. The transmission from base
station to mobile or downlink transmission using M-ary Quadrature Amplitude
modulation (QAM) and Quadrature phase shift keying (QPSK) modulation schemes
are considered in Wideband-Code Division Multiple Access (W-CDMA) system. We
have done the performance analysis of these modulation techniques when the
system is subjected to Additive White Gaussian Noise (AWGN) and multipath
Rayleigh fading are considered in the channel. The research has been performed
by using MATLAB 7.6 for simulation and evaluation of Bit Error Rate (BER) and
Signal-To-Noise Ratio (SNR) for W-CDMA system models. It is shows that the
analysis of Quadrature phases shift key and 16-ary Quadrature Amplitude
modulations which are being used in wideband code division multiple access
system, Therefore, the system could go for more suitable modulation technique
to suit the channel quality, thus we can deliver the optimum and efficient data
rate to mobile terminal.
"
452,"A Mobile Message Scheduling and Delivery System using m-Learning
  framework","  Wireless data communications in form of Short Message Service (SMS) and
Wireless Access Protocols (WAP) browsers have gained global popularity, yet,
not much has been done to extend the usage of these devices in electronic
learning (e-learning) and information sharing. This project explores the
extension of e learning into wireless/ handheld (W/H) computing devices with
the help of a mobile learning (m-learning) framework. This framework provides
the requirements to develop m-learning application that can be used to share
academic and administrative information among people within the university
campus. A prototype application has been developed to demonstrate the important
functionality of the proposed system in simulated environment. This system is
supposed to work both in bulk SMS and interactive SMS delivery mode. Here we
have combined both Short Message Service (SMS) and Wireless Access Protocols
(WAP) browsers. SMS is used for Short and in time information delivery and WAP
is used for detailed information delivery like course content, training
material, interactive evolution tests etc. The push model is used for sending
personalized multicasting messages to a group of mobile users with a common
profile thereby improving the effectiveness and usefulness of the cntent
delivered. Again pull mechanism can be applied for sending information as SMS
when requested by end user in interactive SMS delivery mode. The main strength
of the system is that, the actual SMS delivery application can be hosted on a
mobile device, which can operate even when the device is on move.
"
453,"Performance Analysis of Best suited Adaptive Equalization Algorithm for
  Optical Communication","  Fiber optics is one of the highest bandwidth communication channel types in
the current communication industry. The paper is to analyze a typical optical
channel and perform channel equalization using an adaptive modified DFE with
Activity Detection Guidance and Tap Decoupling algorithm. Evaluation can be
made on the employment of the DFE algorithm and with enhancements, like
Fractionally-Spaced equalization and Activity Detection Guidance, to improve
its stability, steady-state error performance and convergence rate. The
successful implementation of the Adaptive FS-DFE with ADG and TD technique
offers an excellent alternative to linear equalization, which is known to be of
little benefit for optical channels because of exorbitant noise enhancement.
The FSE technique, when combined with the DFE, would offer improved
effectiveness to amplitude distortion. As the impulse response of a typical
optical link would have regions that are essentially zero, the employment of
the activity detection scheme with Tap Decoupling would further enhance the
steady-state error performance and convergence rate.
"
454,Web-Based Learning and Training for Virtual Metrology Lab,"  The use of World Web Wide for distance education has received increasing
attention over the past decades. The real challenge of adapting this technology
for engineering education and training is to facilitate the laboratory
experiments via Internet. In the sciences, measurement plays an important role.
The accuracy of the measurement, as well as the units, help scientists to
better understand phenomena occurring in nature. This paper introduces
Metrology educators to the use and adoption of Java-applets in order to create
virtual, online Metrology laboratories for students. These techniques have been
used to successfully form a laboratory course which augments the more
conventional lectures in concepts of Metrology course at Faculty of
Engineering, Albaha University, KSA. Improvements of the package are still
undergoing to incorporate Web-based technologies (Internet home page, HTML,
Java programming etc...). This Web-based education and training has been
successfully class-tested within an undergraduate preliminary year engineering
course and students reported a positive experience with its use. The use of
these labs should be self-explanatory and their reliable operation has been
thoroughly tested.
"
455,"Variable Threshold MOSFET Approach (Through Dynamic Threshold MOSFET)
  For Universal Logic Gates","  In this article, we proposed a Variable threshold MOSFET(VTMOS)approach which
is realized from Dynamic Threshold MOSFET(DTMOS), suitable for sub-threshold
digital circuit operation. Basically the principle of sub- threshold logics is
operating MOSFET in sub-threshold region and using the leakage current in that
region for switching action, there by drastically decreasing power. To reduce
the power consumption of sub-threshold circuits further, a novel body biasing
technique termed VTMOS is introduced .VTMOS approach is realized from DTMOS
approach. Dynamic threshold MOS (DTMOS) circuits provide low leakage and high
current drive, compared to CMOS circuits, operated at lower voltages. The VTMOS
is based on operating the MOS devices with an appropriate substrate bias which
varies with gate voltage, by connecting a positive bias voltage between gate
and substrate for NMOS and negative bias voltage between gate and substrate for
PMOS. With VTMOS, there is a considerable reduction in operating current and
power dissipation, while the remaining characteristics are almost the same as
those of DTMOS. Results of our investigations show that VTMOS circuits improves
the power up to 50% when compared to CMOS and DTMOS circuits, in sub- threshold
region.. The performance analysis and comparison of VTMOS, DTMOS and CMOS is
made and test results of Power dissipation, Propagation delay and Power delay
product are presented to justify the superiority of VTMOS logic over
conventional sub-threshold logics using Hspice Tool. The dependency of these
parameters on frequency of operation has also been investigated.
"
456,Effect of Weighting Scheme to QoS Properties in Web Service Discovery,"  Specifying QoS properties can limit the selection of some good web services
that the user will have considered; this is because the algorithm used strictly
ensures that there is a match between QoS properties of the consumer with that
of the available services. This is to say that, a situation may arise that some
services might not have all that the user specifies but are rated high in those
they have. With some tradeoffs specified in form of weight, these services will
be made available to the user for consideration. This assertion is from the
fact that, the user's requirements for the specified QoS properties are of
varying degree i.e. he will always prefer one ahead of the other. This can be
captured in form of weight i.e. the one preferred most will have the highest
weight. If a consumer specifies light weight for those QoS properties that a
web service is deficient in and high weight for those it has, this will
minimize the difference between them. Hence the service can be returned.
"
457,"An Improved Fixed Switching Frequency Direct Torque Control of Induction
  Motor Drives Fed by Direct Matrix Converter","  A few papers have been interested by the fixed switching frequency direct
torque control fed by direct matrix converters, where we can find just the use
of direct torque controlled space vector modulated method. In this present
paper, we present an improved method used for a fixed switching frequency
direct torque control (DTC) using a direct matrix converter (DMC). This method
is characterized by a simple structure, a fixed switching frequency which
causes minimal torque ripple and a unity input power factor. Using this
strategy, we combine the direct matrix converters advantages with those of
direct torque control (DTC) schemes. The used technique for constant frequency
is combined with the input current space vector to create the switching table
of direct matrix converter (DMC). Simulation results clearly demonstrate a
better dynamic and steady state performances of the proposed method.
"
458,Predictive Gain Estimation - A mathematical analysis,"  In case of realization of successful business, gain analysis is essential. In
this paper we have cited some new techniques of gain expectation on the basis
of neural property of perceptron. Support rule and Sequence mining based
artificial intelligence oriented practices have also been done in this context.
In the view of above fuzzy and statistical based gain sensing is also pointed
out.
"
459,"Kinematic modelling of a 3-axis NC machine tool in linear and circular
  interpolation","  Machining time is a major performance criterion when it comes to high-speed
machining. CAM software can help in estimating that time for a given strategy.
But in practice, CAM-programmed feed rates are rarely achieved, especially
where complex surface finishing is concerned. This means that machining time
forecasts are often more than one step removed from reality. The reason behind
this is that CAM routines do not take either the dynamic performances of the
machines or their specific machining tolerances into account. The present
article seeks to improve simulation of high-speed NC machine dynamic behaviour
and machining time prediction, offering two models. The first contributes
through enhanced simulation of three-axis paths in linear and circular
interpolation, taking high-speed machine accelerations and jerks into account.
The second model allows transition passages between blocks to be integrated in
the simulation by adding in a polynomial transition path that caters for the
true machining environment tolerances. Models are based on respect for path
monitoring. Experimental validation shows the contribution of polynomial
modelling of the transition passage due to the absence of a leap in
acceleration. Simulation error on the machining time prediction remains below
1%.
"
460,Instantaneous noise-based logic,"  We show two universal, Boolean, deterministic logic schemes based on binary
noise timefunctions that can be realized without time-averaging units. The
first scheme is based on a new bipolar random telegraph wave scheme and the
second one makes use of the recent noise-based logic which is conjectured to be
the brain's method of logic operations [Physics Letters A 373 (2009)
2338-2342]. Error propagation and error removal issues are also addressed.
"
461,A Computational Algorithm for Metrical Classification of Verse,"  The science of versification and analysis of verse in Sanskrit is governed by
rules of metre or chandas. Such metre-wise classification of verses has
numerous uses for scholars and researchers alike, such as in the study of poets
and their style of Sanskrit poetical works. This paper presents a comprehensive
computational scheme and set of algorithms to identify the metre of verses
given as Sanskrit (Unicode) or English E-text (Latin Unicode). The paper also
demonstrates the use of euphonic conjunction rules to correct verses in which
these conjunctions, which are compulsory in verse, have erroneously not been
implemented.
"
462,Modelling of Human Glottis in VLSI for Low Power Architectures,"  The Glottal Source is an important component of voice as it can be considered
as the excitation signal to the voice apparatus. Nowadays, new techniques of
speech processing such as speech recognition and speech synthesis use the
glottal closure and opening instants. Current models of the glottal waves
derive their shape from approximate information rather than from exactly
measured data. General method concentrate on assessment of the glottis opening
using optical, acoustical methods, or on visualization of the larynx position
using ultrasound, computer tomography or magnetic resonance imaging techniques.
In this work, circuit model of Human Glottis using MOS is designed by
exploiting fluid volume velocity to current, fluid pressure to voltage, and
linear and nonlinear mechanical impedances to linear and nonlinear electrical
impedances. The glottis modeled as current source includes linear, non-linear
impedances to represent laminar and turbulent flow respectively, in vocal
tract. The MOS modelling and simulation results of glottal circuit has been
carried out on BSIM 3v3 model in TSMC 0.18 micrometer technology using ELDO
simulator.
"
463,Adaptive Neuro-Fuzzy Extended Kalman Filtering for Robot Localization,"  Extended Kalman Filter (EKF) has been a popular approach to localization a
mobile robot. However, the performance of the EKF and the quality of the
estimation depends on the correct a priori knowledge of process and measurement
noise covariance matrices (Qk and Rk, respectively). Imprecise knowledge of
these statistics can cause significant degradation in performance. This paper
proposed the development of an Adaptive Neuro- Fuzzy Extended Kalman Filtering
(ANFEKF) for localization of robot. The Adaptive Neuro-Fuzzy attempts to
estimate the elements of Qk and Rk matrices of the EKF algorithm, at each
sampling instant when measurement update step is carried out. The ANFIS
supervises the performance of the EKF with the aim of reducing the mismatch
between the theoretical and actual covariance of the innovation sequences. The
free parameters of ANFIS are trained using the steepest gradient descent (SD)
to minimize the differences of the actual value of the covariance of the
residual with its theoretical value as much possible. The simulation results
show the effectiveness of the proposed algorithm.
"
464,"A General Simulation Framework for Supply Chain Modeling: State of the
  Art and Case Study","  Nowadays there is a large availability of discrete event simulation software
that can be easily used in different domains: from industry to supply chain,
from healthcare to business management, from training to complex systems
design. Simulation engines of commercial discrete event simulation software use
specific rules and logics for simulation time and events management.
Difficulties and limitations come up when commercial discrete event simulation
software are used for modeling complex real world-systems (i.e. supply chains,
industrial plants). The objective of this paper is twofold: first a state of
the art on commercial discrete event simulation software and an overview on
discrete event simulation models development by using general purpose
programming languages are presented; then a Supply Chain Order Performance
Simulator (SCOPS, developed in C++) for investigating the inventory management
problem along the supply chain under different supply chain scenarios is
proposed to readers.
"
465,Propose a Fuzzy Queuing Maximal Benefit Location Problem,"  This paper presents a fuzzy queuing location model for congested system. In a
queuing system there are different criteria that are not constant such as
service rate, service rate demand, queue length, the occupancy probability of a
service center and Probability of joining the queue line. In this paper with
fuzzifying all of these variables, will try to reach an accurate real problem.
Finally we change the problem to a single objective function and as far as this
model is in NP-Hard classification we will use genetic algorithm for solving it
and ant colony for comparison is used for their results and run time.
"
466,"An approximate analytical (structural) superposition in terms of two, or
  more, ""alfa""-circuits of the same topology: Pt.1 - description of the
  superposition","  One-ports named ""f-circuits"", composed of similar conductors described by a
monotonic polynomial, or quasi-polynomial (i.e. with positive but not
necessarily integer, powers) characteristic i = f(v) are studied, focusing on
the algebraic map f --> F. Here F(.) is the input conductivity characteristic;
i.e., iin = F(vin) is the input current. The ""power-law"" ""alfa-circuit""
introduced in [1], for which f(v) ~ v^""alfa"", is an important particular case.
By means of a generalization of a parallel connection, the f-circuits are
constructed from the alfa-circuits of the same topology, with different ""alfa"",
so that the given topology is kept, and 'f' is an additive function of the
connection. We observe and consider an associated, generally approximated, but,
in all of the cases studied, always high-precision, specific superposition.
This superposition is in terms of f --> F, and it means that F(.) of the
connection is close to the sum of the input currents of the independent
""alfa""-circuits, all connected in parallel to the same source. In other words,
F(.) is well approximated by a linear combination of the same degrees of the
independent variable as in f(.), i.e. the map of the characteristics f --> F is
close to a linear one. This unexpected result is useful for understanding
nonlinear algebraic circuits, and is missed in the classical theory.
  The cases of f(v) = D1v + D2v^2 and f(v) = D1v + D3v^3, are analyzed in
examples. Special topologies when the superposition must be ideal, are also
considered. In the second part [2] of the work the ""circuit mechanism"" that is
responsible for the high precision of the superposition, in the most general
case, will be explained.
"
467,"Intelligent Technologies in Model Base Management System Design
  Automation","  The article describes the prospects of model base management system design
automation for decision support systems and suggests the toolbox scheme for
design automation based on intelligent technologies.
"
468,"An approximate analytical (structural) superposition in terms of two, or
  more, ""alfa""-circuits of the same topology: Pt. 2 - the ""internal circuit
  mechanism""","  This is the second part, after [1], of the research devoted to analysis of
1-ports composed of similar conductors (""f-circuits"") described by the
characteristic i = f(v) of a polynomial type. This analysis is performed by
means of the power-law ""alfa""-circuits"" introduced in [2], for which f(v) ~
v^""alfa"". The f-circuits are constructed from the ""alfa""-circuits of the same
topology, with the proper ""alfa"", so that the given topology is kept, and 'f'
is an additive function of the connection. Explaining the situation described
in detail in [1], we note and analyze a simple ""circuit mechanism"" that causes
the difference between the input current of the f-circuit and the sum of the
input currents of the f-circuits before the composition to be relatively small.
The case of two degrees, f(v) = Dmv^m + Dnv^n, m unequal n, is treated in the
main proofs. Some simulations are presented, and some boundaries for the error
of the superposition are found. The cases of f(.) being a polynomial of the
third or fourth degrees are finally briefly considered.
"
469,Plagiarism Detection Using Graph-Based Representation,"  Plagiarism of material from the Internet is a widespread and growing problem.
Several methods used to detect the plagiarism and similarity between the source
document and suspected documents such as fingerprint based on character or
n-gram. In this paper, we discussed a new method to detect the plagiarism based
on graph representation; however, Preprocessing for each document is required
such as breaking down the document into its constituent sentences. Segmentation
of each sentence into separated terms and stop word removal. We build the graph
by grouping each sentence terms in one node, the resulted nodes are connected
to each other based on order of sentence within the document, all nodes in
graph are also connected to top level node ""Topic Signature"". Topic signature
node is formed by extracting the concepts of each sentence terms and grouping
them in such node. The main advantage of the proposed method is the topic
signature which is main entry for the graph is used as quick guide to the
relevant nodes. which should be considered for the comparison between source
documents and suspected one. We believe the proposed method can achieve a good
performance in terms of effectiveness and efficiency.
"
470,"Crosstalk Noise Modeling for RC and RLC interconnects in Deep Submicron
  VLSI Circuits","  The crosstalk noise model for noise constrained interconnects optimization is
presented for RC interconnects. The proposed model has simple closed-form
expressions, which is capable of predicting the noise amplitude and the noise
pulse width of an RC interconnect as well as coupling locations (near-driver
and near-receiver) on victim net. This paper also presents a crosstalk noise
model for both identical and non identical coupled
resistance-inductance-capacitance (RLC) interconnects, which is developed based
on a decoupling technique exhibiting an average error of 6.8% as compared to
SPICE. The crosstalk noise model, together with a proposed concept of effective
mutual inductance, is applied to evaluate the effectiveness of the shielding
technique.
"
471,"Prediction of Retained Capacity and EODV of Li-ion Batteries in LEO
  Spacecraft Batteries","  In resent years ANN is widely reported for modeling in different areas of
science including electro chemistry. This includes modeling of different
technological batteries such as lead acid battery, Nickel cadmium batteries
etc. Lithium ion batteries are advance battery technology which satisfy most of
the space mission requirements. Low earth orbit (LEO)space craft batteries
undergo large number of charge discharge cycles (about 25000 cycles)compared to
other ground level or space applications. This study is indented to develop ANN
model for about 25000 cycles, cycled under various temperature, Depth Of
Discharge (DOD) settings with constant charge voltage limit to predict the
retained capacity and End of Discharge Voltage (EODV). To extract firm
conclusion and distinguish the capability of ANN method, the predicted values
are compared with experimental result by statistical method and Bland Altman
plot.
"
472,"Modelling and Design of a Microstrip Band-Pass Filter Using Space
  Mapping Techniques","  Determination of design parameters based on electromagnetic simulations of
microwave circuits is an iterative and often time-consuming procedure. Space
mapping is a powerful technique to optimize such complex models by efficiently
substituting accurate but expensive electromagnetic models, fine models, with
fast and approximate models, coarse models. In this paper, we apply two space
mapping, an explicit space mapping as well as an implicit and response residual
space mapping, techniques to a case study application, a microstrip band-pass
filter. First, we model the case study application and optimize its design
parameters, using explicit space mapping modelling approach. Then, we use
implicit and response residual space mapping approach to optimize the filter's
design parameters. Finally, the performance of each design methods is
evaluated. It is shown that the use of above-mentioned techniques leads to
achieving satisfactory design solutions with a minimum number of
computationally expensive fine model evaluations.
"
473,"Clustering of Content Supporting Computer Mediated Courseware
  Development","  Computer Mediated Courseware (CMC) has been developed so far for individual
courses considering single or multiple text books. A group of courseware can be
developed by using multiple text books and in this case, it is a requirement to
cluster the contents of different books to form a generalized clustered
content. No work has been found to develop courseware applying generalized
clustered content. We have proposed a clustering of content supporting computer
mediated courseware development based on data mining techniques to construct a
hierarchical general structure of a group of courseware combining the
individual structure of a set of books. The clustering will help the courseware
developer to dynamically allocate contents to develop different courses using a
group of books. The authors have applied this methodology for different level
of courses on database. The methodology is generalized and can be applied to
any other courses.
"
474,Video shot boundary detection using motion activity descriptor,"  This paper focus on the study of the motion activity descriptor for shot
boundary detection in video sequences. We interest in the validation of this
descriptor in the aim of its real time implementation with reasonable high
performances in shot boundary detection. The motion activity information is
extracted in uncompressed domain based on adaptive rood pattern search (ARPS)
algorithm. In this context, the motion activity descriptor was applied for
different video sequence.
"
475,"Evaluation of Burst Loss Rate of an Optical Burst Switching (OBS)
  Network with Wavelength Conversion Capability","  This paper presents a new analytical model for calculating burst loss rate
(BLR) in a slotted optical burst switched network. The analytical result leads
to a framework which provides guidelines for optical burst switched networks.
Wavelength converter is used for burst contention resolution. The effect of
several design parameters such as burst arrival probability, wavelength
conversion capability, number of slots per burst and number of wavelengths is
incorporated on the above performance measure. We also extend the analytical
result of BLR for different types of service classes where each service class
has a reserved number of wavelengths in a network with fixed number of
wavelengths. We also introduce an algorithm to calculate the resultant number
of wavelength for each service classes depending on the various scenarios.
"
476,Performance Evaluation of SCM-WDM System Using Different Linecoding,"  This paper investigates the theoretical performance analysis for a subcarrier
multiplexed (SCM) wavelength division multiplexing (WDM) optical transmission
system in presence of optical beat interference (OBI) which occurs during the
photo detection process. We have presented a comparison for improving the
performance of SCM-WDM system in presence of OBI. Non-return-to zero (NRZ),
Manchester and Miller code (MC) line coding are used for performance
investigation of SCM-WDM system. A suitable signal bandwidth is selected and
200 KHz is considered as channel bandwidth. Power spectrum of signal and cross
component for those line coding are analyzed. Comparison results are evaluated
in terms of signal to OBI ratio for the three linecoding schemes which is
called signal to interference ratio (SIR). It is found that there is a
significant increase in the SIR by employing Miller code compared to NRZ and
Manchester for the same data rate. For example, for a number of subcarriers of
10, the achievable SIR is about -24 dB for Miller coded system compared to -46
dB for NRZ coded system and -49 dB for Manchester coded system. The results are
found to be satisfactorily agreed with the expected results.
"
477,Modelling and Implementation of ITWS: An ultimate solution to ITS,"  Casualties due to traffic accidents are increasing day by day. Think of this
message being displayed on your computer screen while you were driving ""there's
a possibility of collision with a car in the next few minutes if you go on
driving with this speed and direction"". Our research is intended towards
developing collision avoidance architecture for the latest Intelligent
Transport System. The exchange of safety messages among vehicles and with
infrastructure devices poses major challenges. Specially, safety messages have
to be adaptively distributed within a certain range of a basically unbounded
system. These messages are to be well coordinated and processed via different
algorithms. The purpose of the paper is to discuss the ITWS (intelligent
transportation warning system), we have discussed the Assisted Global
Positioning System(AGPS) system providing additional positioning information at
variable conditions. We have also discussed study the Data fusion and kalaman
filter in details. The performance of kalman filter and output are discussed.
Hardware realization of this model is achieved through software defined radio
(SDR).
"
478,"The New Embedded System Design Methodology For Improving Design Process
  Performance","  Time-to-market pressure and productivity gap force vendors and researchers to
improve embedded system design methodology. Current used design method,
Register Transfer Level (RTL), is no longer be adequate to comply with embedded
system design necessity. It needs a new methodology for facing the lack of RTL.
In this paper, a new methodology of hardware embedded system modeling process
is designed for improving design process performance using Transaction Level
Modeling (TLM). TLM is a higher abstraction design concept model above RTL
model. Parameters measured include design process time and accuracy of design.
For implementing RTL model used Avalon and Wishbone buses, both are System on
Chip bus. Performance improvement measured by comparing TLM and RTL model
process. The experiment results show performance improvements for Avalon RTL
using new design methodology are 1,03 for 3-tiers, 1,47 for 4-tiers and 1,69
for 5-tiers. Performance improvements for Wishbone RTL are 1,12 for 3-tiers,
1,17 for 4-tiers and 1,34 for 5-tiers. These results show the trend of design
process improvement.
"
479,"Implementation of the Six Channel Redundancy to achieve fault tolerance
  in testing of satellites","  This paper aims to implement the six channel redundancy to achieve fault
tolerance in testing of satellites with acoustic spectrum. We mainly focus here
on achieving fault tolerance. An immediate application is the microphone data
acquisition and to do analysis at the Acoustic Test Facility (ATF) centre,
National Aerospace Laboratories. It has an 1100 cubic meter reverberation
chamber in which a maximum sound pressure level of 157 dB is generated. The six
channel Redundancy software with fault tolerant operation is devised and
developed. The data are applied to program written in C language. The program
is run using the Code Composer Studio by accepting the inputs. This is tested
with the TMS 320C 6727 DSP, Pro Audio Development Kit (PADK).
"
480,Tunable Multifunction Filter Using Current Conveyor,"  The paper presents a current tunable multifunction filter using current
conveyor. The proposed circuit can be realized as on chip tunable low pass,
high pass, band pass and elliptical notch filter. The circuit employs two
current conveyors, one OTA, four resistors and two grounded capacitors, ideal
for integration. It has only one output terminal and the number of input
terminals may be used. Further, there is no requirement for component matching
in the circuit. The resonance frequency ({\omega}0) and bandwidth ({\omega}0
/Q) enjoy orthogonal tuning. The cutoff frequency of the filter is tunable by
changing the bias current, which makes it on chip tunable filter. The circuit
is realized by using commercially available current conveyor AD844 and OTA
LM13700. A HSPICE simulation of circuit is also studied for the verification of
theoretical results.
"
481,CrystalGPU: Transparent and Efficient Utilization of GPU Power,"  General-purpose computing on graphics processing units (GPGPU) has recently
gained considerable attention in various domains such as bioinformatics,
databases and distributed computing. GPGPU is based on using the GPU as a
co-processor accelerator to offload computationally-intensive tasks from the
CPU. This study starts from the observation that a number of GPU features (such
as overlapping communication and computation, short lived buffer reuse, and
harnessing multi-GPU systems) can be abstracted and reused across different
GPGPU applications. This paper describes CrystalGPU, a modular framework that
transparently enables applications to exploit a number of GPU optimizations.
Our evaluation shows that CrystalGPU enables up to 16x speedup gains on
synthetic benchmarks, while introducing negligible latency overhead.
"
482,Defuzzification Method for a Faster and More Accurate Control,"  Today manufacturers are using fuzzy logic in everything from cameras to
industrial process control. Fuzzy logic controllers are easier to design and so
are cheaper to produce. Fuzzy logic captures the impreciseness inherent in most
input data. Electromechanical controllers respond better to imprecise input if
their behavior was modeled on spontaneous human reasoning. In a conventional
PID controller, what is modeled is the system or process being controlled,
whereas in the Fuzzy logic controller, the focus is the human operator
behavior. In the first case, the system is modeled analytically by a set of
differential equations and their solutions tells the PID controllers how to
adjust the system's control parameters for each type of behavior required 3. In
the Fuzzy controller these adjustments are handled by a Fuzzy rule based expert
system. A logical model of the thinking process a person might go through in
the course of manipulating the system. This shift in focus from process to
person involved changes the entire approach to automatic control problems.
"
483,A Discussion of Thin Client Technology for Computer Labs,"  Computer literacy is not negotiable for any professional in an increasingly
computerised environment. Educational institutions should be equipped to
provide this new basic training for modern life. Accordingly, computer labs are
an essential medium for education in almost any field. Computer labs are one of
the most popular IT infrastructures for technical training in primary and
secondary schools, universities and other educational institutions all over the
world. Unfortunately, a computer lab is expensive, in terms of both initial
purchase and annual maintenance costs, and especially when we want to run the
latest software. Hence, research efforts addressing computer lab efficiency,
performance or cost reduction would have a worldwide repercussion. In response
to this concern, this paper presents a survey on thin client technology for
computer labs in educational environments. Besides setting out the advantages
and drawbacks of this technology, we aim to refute false prejudices against
thin clients, identifying a set of educational scenarios where thin clients are
a better choice and others requiring traditional solutions.
"
484,"Detecting communities of triangles in complex networks using spectral
  optimization","  The study of the sub-structure of complex networks is of major importance to
relate topology and functionality. Many efforts have been devoted to the
analysis of the modular structure of networks using the quality function known
as modularity. However, generally speaking, the relation between topological
modules and functional groups is still unknown, and depends on the semantic of
the links. Sometimes, we know in advance that many connections are transitive
and, as a consequence, triangles have a specific meaning. Here we propose the
study of the modular structure of networks considering triangles as the
building blocks of modules. The method generalizes the standard modularity and
uses spectral optimization to find its maximum. We compare the partitions
obtained with those resulting from the optimization of the standard modularity
in several real networks. The results show that the information reported by the
analysis of modules of triangles complements the information of the classical
modularity analysis.
"
485,On the Module of Internet Banking System,"  Because of the speed, flexibility, and efficiency that it offers, the
Internet has become the means for conducting growing numbers of transactions
between suppliers and large international corporations. In this way, the
Internet has opened new markets to the world and has accelerated the diffusion
of knowledge. The meaning of Internet markets or online business has been
widely used in these days. The success of the business depends on its
flexibility, availability and security. Since that the web-based systems should
have a special way to design the system and implement it. Nowadays, the
Internet Banking System widely used and the banks looking to provide the best
quality system with highly available, fast response, secure and safe to use.
The Unified Modelling Language (UML) is the uniquely language which is used to
analyse and design any system. In this paper, the UML diagrams has been
proposed to illustrate the design phase for any banking system. The authors,
presented two types of architecture which is used for the Internet Banking
System.
"
486,"Reduction in iron losses in Indirect Vector-Controlled IM Drive using
  FLC","  This paper describes the use of fuzzy logic controller for efficiency
optimization control of a drive while keeping good dynamic response. At
steady-state light-load condition, the fuzzy controller adaptively adjusts the
excitation current with respect to the torque current to give the minimum total
copper and iron loss. The measured input power such that, for a given load
torque and speed, the drive settles down to the minimum input power, i.e.,
operates at maximum efficiency. The low-frequency pulsating torque due to
decrementation of flux is compensated in a feed forward manner. If the load
torque or speed commands changes, the efficiency search algorithm is abandoned
and the rated flux is established to get the best dynamic response. The drive
system with the proposed efficiency optimization controller has been simulated
with lossy models of converter and machine, and its performance has been
thoroughly investigated.
"
487,E-Speed Governors For Public Transport Vehicles,"  An accident is unexpected, unusual, unintended and identifiable external
event which occurs at any place and at any time. The major concern faced by the
government and traffic officials is over speeding at limited speed zones like
hospitals, schools or residential places leading to causalities and more deaths
on the roads. Hence the speed of the vehicles is to be regulated and confined
to the limits as prescribed by the traffic regulations. In this paper we
propose a solution in the form of providing E-speed governor fitted with a
wireless communication system consisting of a Rx which receives the information
regarding the speed regulation for their zones. The TX will be made highly
intelligent and decide when receiver should be made active to regulate the
speed and unwarranted honking from the vehicles which can be deactivated in the
silent zones.
"
488,Effective Query Retrieval System In Mobile Business Environment,"  Web Based Query Management System (WBQMS) is a methodology to design and to
implement Mobile Business, in which a server is the gateway to connect
databases with clients which sends requests and receives responses in a
distributive manner. The gateway, which communicates with mobile phone via GSM
Modem, receives the coded queries from users and sends packed results back. The
software which communicates with the gateway system via SHORT MESSAGE, packs
users' requests, IDs and codes, and sends the package to the gateway; then
interprets the packed data for the users to read on a page of GUI. Whenever and
wherever they are, the customer can query the information by sending messages
through the client device which may be mobile phone or PC. The mobile clients
can get the appropriate services through the mobile business architecture in
distributed environment. The messages are secured through the client side
encoding mechanism to avoid the intruders. The gateway system is programmed by
Java, while the software at clients by J2ME and the database is created by
Oracle for reliable and interoperable services.
"
489,"A Novel Algorithm for Informative Meta Similarity Clusters Using Minimum
  Spanning Tree","  The minimum spanning tree clustering algorithm is capable of detecting
clusters with irregular boundaries. In this paper we propose two minimum
spanning trees based clustering algorithm. The first algorithm produces k
clusters with center and guaranteed intra-cluster similarity. The radius and
diameter of k clusters are computed to find the tightness of k clusters. The
variance of the k clusters are also computed to find the compactness of the
clusters. The second algorithm is proposed to create a dendrogram using the k
clusters as objects with guaranteed inter-cluster similarity. The algorithm is
also finds central cluster from the k number of clusters. The first algorithm
uses divisive approach, where as the second algorithm uses agglomerative
approach. In this paper we used both the approaches to find Informative Meta
similarity clusters.
"
490,Parallel QR decomposition in LTE-A systems,"  The QR Decomposition (QRD) of communication channel matrices is a fundamental
prerequisite to several detection schemes in Multiple-Input Multiple-Output
(MIMO) communication systems. Herein, the main feature of the QRD is to
transform the non-causal system into a causal system, where consequently
efficient detection algorithms based on the Successive Interference
Cancellation (SIC) or Sphere Decoder (SD) become possible. Also, QRD can be
used as a light but efficient antenna selection scheme. In this paper, we
address the study of the QRD methods and compare their efficiency in terms of
computational complexity and error rate performance. Moreover, a particular
attention is paid to the parallelism of the QRD algorithms since it reduces the
latency of the matrix factorization.
"
491,Radio Frequency Identifiers: What are the Possibilities?,"  This paper defines the components of radio frequency identifiers (RFID). It
also explores the different areas and sectors where RFID can be beneficial. The
paper discusses the uses and advantages of RFID in deference, consumer packaged
goods (CPG), healthcare, logistics, manufacturing, and retail.
"
492,"Towards MKM in the Large: Modular Representation and Scalable Software
  Architecture","  MKM has been defined as the quest for technologies to manage mathematical
knowledge. MKM ""in the small"" is well-studied, so the real problem is to scale
up to large, highly interconnected corpora: ""MKM in the large"". We contend that
advances in two areas are needed to reach this goal. We need representation
languages that support incremental processing of all primitive MKM operations,
and we need software architectures and implementations that implement these
operations scalably on large knowledge bases.
  We present instances of both in this paper: the MMT framework for modular
theory-graphs that integrates meta-logical foundations, which forms the base of
the next OMDoc version; and TNTBase, a versioned storage system for XML-based
document formats. TNTBase becomes an MMT database by instantiating it with
special MKM operations for MMT.
"
493,sTeXIDE: An Integrated Development Environment for sTeX Collections,"  Authoring documents in MKM formats like OMDoc is a very tedious task. After
years of working on a semantically annotated corpus of sTeX documents (GenCS),
we identified a set of common, time-consuming subtasks, which can be supported
in an integrated authoring environment. We have adapted the modular Eclipse IDE
into sTeXIDE, an authoring solution for enhancing productivity in contributing
to sTeX based corpora. sTeXIDE supports context-aware command completion,
module management, semantic macro retrieval, and theory graph navigation.
"
494,"On the Utility of Directional Information for Repositioning Errant
  Probes in Central Force Optimization","  Central Force Optimization is a global search and optimization algorithm that
searches a decision space by flying ""probes"" whose trajectories are
deterministically computed using two equations of motion. Because it is
possible for a probe to fly outside the domain of feasible solutions, a simple
errant probe retrieval method has been used previously that does not include
the directional information contained in a probe's acceleration vector. This
note investigates the effect of adding directionality to the ""repositioning
factor"" approach. As a general proposition, it appears that doing so does not
improve convergence speed or accuracy. In fact, adding directionality to the
original errant probe retrieval scheme appears to be highly inadvisable.
Nevertheless, there may be alternative probe retrieval schemes that do benefit
from directional information, and the results reported here may assist in or
encourage their development.
"
495,Rectangular and Circular Antenna Design on Thick Substrate,"  Millimeter wave technology being an emerging area is still very undeveloped.
A substantial research needs to be done in this area as its applications are
numerous. In the present endeavor, a rectangular patch antenna is designed on
thick substrate and simulated using SONNET software, also a novel analysis
technique is developed for circular patch antenna for millimeter wave
frequency. The antenna is designed at 39 GHz on thick substrate and has been
analyzed and simulated.The results of the theoretical analysis are in good
agreement with the simulated results.
"
496,"Mutual Coupling Reduction in Two-Dimensional Array of Microstrip
  Antennas Using Concave Rectangular Patches","  Using concave rectangular patches, a new solution to reduce mutual coupling
and return loss in two-dimensional array of microstrip antennas is proposed.
The effect of width and length concavity on mutual coupling and return loss is
studied. Also, the patch parameters as well as the amounts of width and length
concavity are optimized using an enhanced genetic algorithm. Simulation results
show that the resulting array antenna has low amounts of mutual coupling and
return loss.
"
497,"Improving GPS Precision and Processing Time using Parallel and
  Reduced-Length Wiener Filters","  Increasing GPS precision at low cost has always been a challenge for the
manufacturers of the GPS receivers. This paper proposes the use of a Wiener
filter for increasing precision in substitution of traditional GPS/INS fusion
systems, which require expensive inertial systems. In this paper, we first
implement and compare three GPS signal processing schemes: a Kalman filter, a
neural network and a Wiener filter and compare them in terms of precision and
the processing time. To further reduce the processing time of Wiener filter, we
propose parallel and reduced-length implementations. Finally, we calculate the
sampling frequency that would be required in every Wiener scheme in order to
obtain the same total processing time as the Kalman filter and the neural
network.
"
498,Novel method for planar microstrip antenna matching impedance,"  Because all microstrip antennas have to be matched to the standard generator
impedance or load, the input impedance matching method for antenna is
particularly important. In this paper a new methodology in achieving matching
impedance of a planar microstrip antenna for wireless application is described.
The method is based on embedding an Interdigital capacitor. The fine results
obtained by using a microstrip Interdigital capacitor for matching antenna
impedance led to an efficient method to improve array antenna performance. In
fact, a substantial saving on the whole surfaces as well as enhancement of the
gain, the directivity and the power radiated was achieved.
"
499,"To Optimally Design Microstrip Nonuniform Transmission Lines as Lowpass
  Filters","  A method is proposed to optimally design the Microstrip Nonuniform
Transmission Line (MNTLs) as lowpass filters. Some electrical and physical
restrictions are used to design MNTLs. To optimally design the MNTLs, their
strip width is expanded as truncated Fourier series, firstly. Then, the optimum
values of the coefficients of the series are obtained through an optimization
approach. The performance of the proposed structure is studied by design and
fabrication of two lowpass filters of cutoff frequency 2.0 GHz.
"
500,"Implementing and Evaluating a Wireless Body Sensor System for Automated
  Physiological Data Acquisition at Home","  Advances in embedded devices and wireless sensor networks have resulted in
new and inexpensive health care solutions. This paper describes the
implementation and the evaluation of a wireless body sensor system that
monitors human physiological data at home. Specifically, a waist-mounted
triaxial accelerometer unit is used to record human movements. Sampled data are
transmitted using an IEEE 802.15.4 wireless transceiver to a data logger unit.
The wearable sensor unit is light, small, and consumes low energy, which allows
for inexpensive and unobtrusive monitoring during normal daily activities at
home. The acceleration measurement tests show that it is possible to classify
different human motion through the acceleration reading. The 802.15.4 wireless
signal quality is also tested in typical home scenarios. Measurement results
show that even with interference from nearby IEEE 802.11 signals and microwave
ovens, the data delivery performance is satisfactory and can be improved by
selecting an appropriate channel. Moreover, we found that the wireless signal
can be attenuated by housing materials, home appliances, and even plants.
Therefore, the deployment of wireless body sensor systems at home needs to take
all these factors into consideration.
"
501,"Simple ROI untuk justifikasi investasi proyek Data Warehouse pada
  perguruan tinggi swasta","  Decreasing new students for private high education push the management
particularly for high level management for making an information which can help
them to make decisions in order for competition with other high educations. One
of way out by building with information technology approaching like data
warehouse for data handling and making the best decisions. Simple ROI is used
for project justification. Based on ROI value between 1,850.13% and cash flow
Rp. 22,081,297,308 then can be concluded that project data warehouse
development in private high education can be implemented with the particular
assumptions.
"
502,"Rancangan Infrastruktur E-Bisnis Business Intelligence Pada Perguruan
  Tinggi","  In order to compete with others, high education need complete their
infrastructure with Information technology support. High level management as a
decision maker need something that can boost the system to compete with other
high education, they need IT knowledge that can support them to view the future
and can help the whole system to improve their services. Business Intelligence
is one of term of Decision Support System which can help the management by
something that they can forecast and decide. High Education need infrastructure
design to make good foundation for business intelligent implementation which
will be implemented on internet or e-business.
"
503,"Sistem Pengambilan Keputusan Penanganan Bencana Alam Gempa Bumi Di
  Indonesia","  After Aceh's quake many earthquakes have struck Indonesia alternately and
even other disasters have been a threat for every citizen in this country.
Actually an everyday occurrence on earth and more than 3 million earthquakes
occur every year, about 8,000 a day, or one every 11 seconds in Indonesia there
are 5 to 30 quakes prediction everyday. Government's responsibility to protect
the citizen has been done by making National body of disaster management.
Preparing, saving and distribution logistic become National body of disaster
management's responsibility to build information management. Many law's
products have been produced as a government's responsibility to give secure
life for the citizen. We can not prevent them totally, we have to learn to live
with them and need to be prepared all the time, need to learn how to mitigate
risk of losses in such events by managing crisis and emergencies correctly.
After disaster happens respond must be rapidly and at an optimal level to save
lives and help to victims. DSS is information technology environment which can
be used to help human in order to learn from past earthquake, record it, learn
and plan for future mitigation and hope will reduce the disaster risk in the
future. Using web technology for DSS will give value added where not only make
a strategic decision for the decision maker, but for others who need national
earthquake information like citizen, scholars, researches and people around the
world.
"
504,"Efektifitas Teknologi Informasi Dalam Proses Belajar Mengajar Pada
  Universitas Budi Luhur","  In general, however, IT will empower students to have greater control over
the learning process, with all the benefits associated with active learning and
personal responsibility. Not only will students decide when to learn and how to
learn, increasingly they will also decide what to learn and how that learning
is to be certified. Traditionally, higher education institutions have combined
several functions in their faculty. Faculty are architects as they design
learning programs; navigators as they help advise students in their course of
study; instructors when they lecture; mentors when they help students form a
sense of connectedness to the world; and evaluators and certifiers as they
decide to grant students grades or degrees.
"
505,"Pemanfaatan Teknologi Sistem Informasi Geografis Untuk Menunjang
  Pembangunan Daerah","  The territory development will depend on that territory itself, where the
word of autonomy for each province or territory will give contribution how
Indonesian will responsible for development their territory. In order to
develop territory, the information technology can be used as a boost or tools
to give and deliver the best information and Geographic Information System is
one of the information technology tools which can be used to push every each
territory to speed the territory development. As a tool Geographic Information
System has an ability to save, process, analysis and deliver information right
in time and help the decision maker to make better decision.
"
506,"Pembobolan website KPU (Komisi Pemilihan Umum) Apakah melanggar UU RI
  no.36 tahun 1999 tentang telekomunikasi ?","  Information Technology KPU (Indonesia Electoral Commision) is a project in
supporting democratization process in Indonesia. It is a part of General
Election program of KPU-Indonesian Government. The aim of IT KPU is to build
the transparency of the ballot result to the public (citizen and international
world) and as the embrio of e government in Indonesia. It also has the aim for
influence the citizen with Information Technology and the use of computer.
"
507,Virtual On-demand Test Lab using Cloud based Architecture,"  Over a past few decades, VM's or Virtual machines have sort of gained a lot
of momentum, especially for large scale enterprises where the need for resource
optimization & power save is humongous, without compromising with performance
or quality. They are a perfect environment to experiment with new
applications/technologies in a completely secure and closed environment. This
paper discusses how the VM technology can be leveraged to solve day to day
requirement of an odd hundreds or thousands of people, organization-wide, with
new computational resources using a cluster of heterogeneous low or high-end
machines, independent of underlying OS, thereby maximizing resource
utilization. It takes into account both opensource (like VirtualBox) & other
proprietary technologies (like VMWare Workstations) available till date to
propose a viable solution using cloud computing concept. The ease of
scalability to multiple folds for optimizing performance & catering to an even
larger set are some of the salient features of this approach. Using the
snapshot feature, the state of any VM instance could be saved & served back
again on request. Now, this implementation is also served by VMWare ESX server
but again it's a costly solution & requires dedicated high-end machines to work
with.
"
508,High Speed Reconfigurable FFT Design by Vedic Mathematics,"  The Fast Fourier Transform (FFT) is a computationally intensive digital
signal processing (DSP) function widely used in applications such as imaging,
software-defined radio, wireless communication, instrumentation. In this paper,
a reconfigurable FFT design using Vedic multiplier with high speed and small
area is presented. Urdhava Triyakbhyam algorithm of ancient Indian Vedic
Mathematics is utilized to improve its efficiency. In the proposed
architecture, the 4x4 bit multiplication operation is fragmented reconfigurable
FFT modules. The 4x4 multiplication modules are implemented using small 2x2bit
multipliers. Reconfigurability at run time is provided for attaining power
saving. The reconfigurable FFT has been designed, optimized and implemented on
an FPGA based system. This reconfigurable FFT is having the high speed and
small area as compared to the conventional FFT.
"
509,"Effect of Distributed Shield Insertion on Crosstalk in Inductively
  Coupled VLSI Interconnects","  Crosstalk in VLSI interconnects is a major constrain in DSM and UDSM
technology. Among various strategies followed for its minimization, shield
insertion between Aggressor and Victim is one of the prominent options. This
paper analyzes the extent of crosstalk in inductively coupled interconnects and
minimizes the same through distributed shield insertion. Comparison is drawn
between signal voltage and crosstalk voltage in three different conditions i.e.
prior to shield insertion, after shield insertion and after additional ground
tap insertion at shield terminal.
"
510,MIMO Detection Algorithms for High Data Rate Wireless Transmission,"  Motivated by MIMO broad-band fading channel model, in this section a
comparative study is presented regarding various uncoded adaptive and
non-adaptive MIMO detection algorithms with respect to BER/PER performance, and
hardware complexity. All the simulations are conducted within MIMO-OFDM
framework and with a packet structure similar to that of IEEE 802.11a/g
standard. As the comparison results show, the RLS algorithm appears to be an
affordable solution for wideband MIMO system targeting at Giga-bit wireless
transmission. So MIMO can overcome huge processing power required for MIMO
detection by using optimizing channel coding and MIMO detection.
"
511,"The Forecasting of 3G Market in India Based on Revised Technology
  Acceptance Model","  3G, processor of 2G services, is a family of standards for mobile
telecommunications defined by the International Telecommunication Union [1]. 3G
services include wide-area wireless voice telephone, video calls, and wireless
data, all in a mobile environment. It allows simultaneous use of speech and
data services and higher data rates.3G is defined to facilitate growth,
increased bandwidth and support more diverse applications. The focus of this
study is to examine the factors affecting the adoption of 3G services among
Indian people. The study adopts the revised Technology Acceptance Model by
adding five antecedents-perceived risks, cost of adoption, perceived service
quality, subjective norms, and perceived lack of knowledge. Data have collected
from more than 400 school/college/Institution students & employees of various
Government/Private sectors using interviews & various convenience sampling
procedures and analyzed using MS excel and MATLAB. Result shows that perceived
usefulness has the most significant influence on attitude towards using 3G
services, which is consistent with prior studies. Of the five antecedents,
perceived risk and cost of adoption are found to be significantly influencing
attitude towards use. The outcome of this study would be beneficial to private
and public telecommunication organizations, various service providers, business
community, banking services and people of India. Research findings and
suggestions for future research are also discussed.
"
512,Continuous history variable for programmable quantum processors,"  In this brief note is discussed application of continuous quantum history
(""trash"") variable for simplification of scheme of programmable quantum
processor. Similar scheme may be tested also in other models of the theory of
quantum algorithms and complexity, because provides modification of a standard
operation: quantum function evaluation.
"
513,A Community Membership Life Cycle Model,"  Web 2.0 is transforming the internet: Information consumers become
information producers and consumers at the same time. In virtual places like
Facebook, Youtube, discussion boards and weblogs diversificated topics, groups
and issues are propagated and discussed. Today an internet user is a member of
lots of communities at different virtual places. ""Real life"" group membership
and group behavior has been analyzed in science intensively in the last
decades. Most interestingly, to our knowledge, user roles and behavior have not
been adapted to the modern internet. In this work, we give a short overview of
traditional community roles. We adapt those models and apply them to virtual
online communities. We suggest a community membership life cycle model
describing roles a user can take during his membership in a community. Our
model is systematic and generic; it can be adapted to concrete communities in
the web. The knowledge of a community's life cycle allows influencing the group
structure: Stage transitions can be supported or harmed, e.g. to strengthen the
binding of a user to a site and keep communities alive.
"
514,"Analysis of Microprocessor Based Protective Re-lay's (MBPR) Differential
  Equation Algorithms","  This paper analyses and explains from the systems point of view,
microprocessor based protective relay (MBPR) systems with emphasis on
differential equation algorithms. Presently, the application of protective
relaying in power systems, using MBPR systems, based on the differential
equation algorithm is valued more than the protection relaying based on any
other type of algorithm, because of advantages in accuracy and implementation.
MBPR differential equation approach can tolerate some errors caused by power
system abnormality such as DC offset. This paper shows that the algorithm is a
system description based and it is immune from distortions such as DC-offset.
Differential equation algorithms implemented in MBPR are widely used in the
protection of transmission and distribution lines, transformers, buses, motors,
etc. The parameters from the system, utilized in these algorithms, are obtained
from the power system current i(t) or voltage v(t), which are abnormal values
under fault or distortion situations. So, an error study for the algorithm is
considered necessary.
"
515,Algorithm and Implementation of the Blog-Post Supervision Process,"  A web log or blog in short is a trendy way to share personal entries with
others through website. A typical blog may consist of texts, images, audios and
videos etc. Most of the blogs work as personal online diaries, while others may
focus on specific interest such as photographs (photoblog), art (artblog),
travel (tourblog), IT (techblog) etc. Another type of blogging called
microblogging is also very well known now-a-days which contains very short
posts. Like the developed countries, the users of blogs are gradually
increasing in the developing countries e.g. Bangladesh. Due to the nature of
open access to all users, some people misuse it to spread fake news to achieve
individual or political goals. Some of them also post vulgar materials that
make an embarrass situation for other bloggers. Even, sometimes it indulges the
reputation of the victim. The only way to overcome this problem is to bring all
the posts under supervision of the blog moderator. But it totally contradicts
with blogging concepts. In this paper, we have implemented an algorithm that
would help to prevent the offensive entries from being posted. These entries
would go through a supervision process to justify themselves as legal posts.
From the analysis of the result, we have shown that this approach can eliminate
the chaotic situations in blogosphere at a great extent. Our experiment shows
that about 90% of offensive posts can be detected and stopped from being
published using this approach.
"
516,"Gender Based Emotion Recognition System for Telugu Rural Dialects Using
  Hidden Markov Models","  Automatic emotion recognition in speech is a research area with a wide range
of applications in human interactions. The basic mathematical tool used for
emotion recognition is Pattern recognition which involves three operations,
namely, pre-processing, feature extraction and classification. This paper
introduces a procedure for emotion recognition using Hidden Markov Models
(HMM), which is used to divide five emotional states: anger, surprise,
happiness, sadness and neutral state. The approach is based on standard speech
recognition technology using hidden continuous markov model by selection of low
level features and the design of the recognition system. Emotional Speech
Database from Telugu Rural Dialects of Andhra Pradesh (TRDAP) was designed
using several speaker's voices comprising the emotional states. The accuracy of
recognizing five different emotions for both genders of classification is 80%
for anger-emotion which is achieved by using the best combination of
39-dimensioanl feature vector for every frame (13 MFCCs, 13 Delta Coefficients
and 13 Acceleration Coefficients) and a classifier using HMM. This outcome very
much matches with that acquired with the same database with subjective
evaluation by human judges. Both gender-dependent and gender-independent
experiments are conducted on TRDAP emotional speech database.
"
517,"Search Engine Optimization Techniques Practiced in Organizations: A
  Study of Four Organizations","  Web spammers used Search Engine Optimization (SEO) techniques to increase
search-ranking of web sites. In this paper we have study the essentials SEO
techniques, such as; directory submission, keyword generation and link
exchanges. The impact of SEO techniques can be applied as marketing technique
and to get top listing in major search engines like Google, Yahoo, and MSN. Our
study focuses on these techniques from four different companies' perspectives
of United Kingdom and Pakistan. According to the these companies, these
techniques are low cost and high impacts in profit, because mostly customers
focus on major search engine to find different products on internet, so SEO
technique provides best opportunity to grow their business. This paper also
describes the pros and cons of using these searh engine optimization techniques
in above four companies. We have concluded that these techniques are essential
to increase their business profit and minimize their marketing cost.
"
518,Analytical Study on Internet Banking System,"  The Internet era is a period in the information age in which communication
and commerce via the Internet became a central focus for businesses, consumers,
government, and the media. The Internet era also marks the convergence of the
computer and communications industries and their associated services and
products. Nowadays, the availability of the Internet make it widely used for
everyday life. In order to led business to success, the business and specially
the services should provide comfort use to its costumer. The bank system is one
of the most important businesses who may use the website. The using for the
web-based systems should contain special requirements to achieve the business
goal. Since that the paper will present the functional and non-functional for
the web-based banking system.
"
519,Optimization of reversible sequential circuits,"  In recent years reversible logic has been considered as an important issue
for designing low power digital circuits. It has voluminous applications in the
present rising nanotechnology such as DNA computing, Quantum Computing, low
power VLSI and quantum dot automata. In this paper we have proposed optimized
design of reversible sequential circuits in terms of number of gates, delay and
hardware complexity. We have designed the latches with a new reversible gate
and reduced the required number of gates, garbage outputs, and delay and
hardware complexity. As the number of gates and garbage outputs increase the
complexity of reversible circuits, this design will significantly enhance the
performance. We have proposed reversible D-latch and JK latch which are better
than the existing designs available in literature.
"
520,"Critical Success factors for Enterprise Resource Planning implementation
  in Indian Retail Industry: An Exploratory study","  Enterprise resource Planning (ERP) has become a key business driver in
today's world. Retailers are also trying to reap in the benefits of the ERP. In
most large Indian Retail Industry ERP systems have replaced nonintegrated
information systems with integrated and maintainable software. Retail ERP
solution integrates demand and supply effectively to help improve bottom line.
The implementation of ERP systems in such firms is a difficult task. So far,
ERP implementations have yielded more failures than successes. Very few
implementation failures are recorded in the literature because few companies
wish to publicize their implementation failure. This paper explores and
validates the existing literature empirically to find out the critical success
factors that lead to the success of ERP in context to Indian retail industry.
The findings of the results provide valuable insights for the researchers and
practitioners who are interested in implementing Enterprise Resource Planning
systems in retail industry, how best they can utilize their limited resources
and to pay adequate attention to those factors that are most likely to have an
impact upon the implementation of the ERP system.
"
521,"Scheduling Periodic Real-Time Tasks with Heterogeneous Reward
  Requirements","  We study the problem of scheduling periodic real-time tasks so as to meet
their individual minimum reward requirements. A task generates jobs that can be
given arbitrary service times before their deadlines. A task then obtains
rewards based on the service times received by its jobs. We show that this
model is compatible to the imprecise computation models and the increasing
reward with increasing service models. In contrast to previous work on these
models, which mainly focus on maximize the total reward in the system, we aim
to fulfill different reward requirements by different tasks, which offers
better fairness and allows fine-grained tradeoff between tasks. We first derive
a necessary and sufficient condition for a system, along with reward
requirements of tasks, to be feasible. We also obtain an off-line feasibility
optimal scheduling policy. We then studies a sufficient condition for a policy
to be feasibility optimal or achieves some approximation bound. This condition
can serve as a guideline for designing on-line scheduling policy and we obtains
a greedy policy based on it. We prove that the on-line policy is feasibility
optimal when all tasks have the same periods and also obtain an approximation
bound for the policy under general cases.
"
522,"Channel Sounding for the Masses: Low Complexity GNU 802.11b Channel
  Impulse Response Estimation","  New techniques in cross-layer wireless networks are building demand for
ubiquitous channel sounding, that is, the capability to measure channel impulse
response (CIR) with any standard wireless network and node. Towards that goal,
we present a software-defined IEEE 802.11b receiver and CIR estimation system
with little additional computational complexity compared to 802.11b reception
alone. The system implementation, using the universal software radio peripheral
(USRP) and GNU Radio, is described and compared to previous work. By overcoming
computational limitations and performing direct-sequence spread-spectrum
(DS-SS) matched filtering on the USRP, we enable high-quality yet inexpensive
CIR estimation. We validate the channel sounder and present a drive test
campaign which measures hundreds of channels between WiFi access points and an
in-vehicle receiver in urban and suburban areas.
"
523,The Dynamics of Vehicular Networks in Urban Environments,"  Vehicular Ad hoc NETworks (VANETs) have emerged as a platform to support
intelligent inter-vehicle communication and improve traffic safety and
performance. The road-constrained, high mobility of vehicles, their unbounded
power source, and the emergence of roadside wireless infrastructures make
VANETs a challenging research topic. A key to the development of protocols for
inter-vehicle communication and services lies in the knowledge of the
topological characteristics of the VANET communication graph. This paper
explores the dynamics of VANETs in urban environments and investigates the
impact of these findings in the design of VANET routing protocols. Using both
real and realistic mobility traces, we study the networking shape of VANETs
under different transmission and market penetration ranges. Given that a number
of RSUs have to be deployed for disseminating information to vehicles in an
urban area, we also study their impact on vehicular connectivity. Through
extensive simulations we investigate the performance of VANET routing protocols
by exploiting the knowledge of VANET graphs analysis.
"
524,WLAN PIDS,"  This paper discuss two structures of WLAN system fit to Passenger Information
Display System which is partly of subway.
"
525,Contributions of PDM Systems in Organizational Technical Data Management,"  Product Data Management (PDM) claims of producing desktop and web based
systems to maintain the organizational data to increase the quality of products
by improving the process of development, business process flows, change
management, product structure management, project tracking and resource
planning. Moreover PDM helps in reducing the cost and effort required in
engineering. This paper discusses PDM desktop and web based system, needed
information and important guidelines for PDM system development, functional
requirements, basic components in detail and some already implemented PDM
Sys-tems. In the end paper investigates and briefly concludes major currently
faced challenges to Product Data Management (PDM) community.
"
526,Semantic Oriented Intelligent Electronic Learning,"  In this research paper we describe semantic oriented information engineering
and knowledge management based solution towards E-Learning systems. We also try
to justify the importance of proposed solution with respect to the E-Learning
Approaches .i.e., Behavior, Objectivism, Cognitive and Construction. Moreover
we briefly describe E-Learning, information engineering, knowledge management
and some old and newly available technologies supporting development of
E-Learning Systems in this research paper.
"
527,Web to Semantic Web & Role of Ontology,"  In this research paper we are briefly presenting current major web problems
and introducing semantic web technologies with the claim of solving existing
web's problems. Furthermore we are describing Ontology as the main building
block of semantic web and focusing on its contributions to semantic web
progress and current limitations.
"
528,How Does Ontology Contribute in Semantic Web Development?,"  This paper investigates and briefly describes the major currently existing
problems with World Wide Web .i.e., Information filtration and Security became
the main reasons of semantic web's invention. The semantic web claims of
providing the semantic based solutions towards current web problems. Semantic
web have introduced and relies on a main building block ""Ontology"" to provide
the information in machine processable semantic models and produce semantically
modelled knowledge representation systems. This paper also describes the role,
construction process and the contributions of ontology in providing some in
time proposed and implemented solutions. Furthermore paper concludes with the
currently existing limitations in Ontology and the areas which need
improvements.
"
529,"I-SOAS towards Product Data Management (PDM) based Application's
  Problems","  In this research paper we address the importance of Product Data Management
(PDM) with respect to the industrial contributional point of view and its major
objectives. Moreover we also present some currently available major challenges
to the Product Data Management based communities, and targeting those
challenges we discuss an already proposed conceptual architectural based
helpful approach and briefly describe how this approach can be helpful in
solving the PDM communities faced problems.
"
530,Efficient Wrapper/TAM Co-Optimization for SOC Using Rectangle Packing,"  The testing time for a system-on-chip(SOC) largely depends on the design of
test wrappers and the test access mechanism(TAM).Wrapper/TAM co-optimization is
therefore necessary to minimize SOC testing time . In this paper, we propose an
efficient algorithm to construct wrappers that reduce testing time for cores.
We further propose a new approach for wrapper/TAM co-optimization based on
two-dimensional rectangle packing. This approach considers the diagonal length
of the rectangles to emphasize on both TAM widths required by a core and its
corresponding testing time.
"
531,"Implementation of the Trigonometric LMS Algorithm using Original Cordic
  Rotation","  The LMS algorithm is one of the most successful adaptive filtering
algorithms. It uses the instantaneous value of the square of the error signal
as an estimate of the mean-square error (MSE). The LMS algorithm changes
(adapts) the filter tap weights so that the error signal is minimized in the
mean square sense. In Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS), two
new versions of LMS algorithms, same formulations are performed as in the LMS
algorithm with the exception that filter tap weights are now expressed using
trigonometric and hyperbolic formulations, in cases for TLMS and HLMS
respectively. Hence appears the CORDIC algorithm as it can efficiently perform
trigonometric, hyperbolic, linear and logarithmic functions. While
hardware-efficient algorithms often exist, the dominance of the software
systems has kept those algorithms out of the spotlight. Among these hardware-
efficient algorithms, CORDIC is an iterative solution for trigonometric and
other transcendental functions. Former researches worked on CORDIC algorithm to
observe the convergence behavior of Trigonometric LMS (TLMS) algorithm and
obtained a satisfactory result in the context of convergence performance of
TLMS algorithm. But revious researches directly used the CORDIC block output in
their simulation ignoring the internal step-by-step rotations of the CORDIC
processor. This gives rise to a need for verification of the convergence
performance of the TLMS algorithm to investigate if it actually performs
satisfactorily if implemented with step-by-step CORDIC rotation. This research
work has done this job. It focuses on the internal operations of the CORDIC
hardware, implements the Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS)
algorithms using actual CORDIC rotations. The obtained simulation results are
highly satisfactory and also it shows that convergence behavior of HLMS is much
better than TLMS.
"
532,"Wrapper/TAM Co-Optimization and Test Scheduling for SOCs Using Rectangle
  Bin Packing Considering Diagonal Length of Rectangles","  This paper describes an integrated framework for SOC test automation. This
framework is based on a new approach for Wrapper/TAM co-optimization based on
rectangle packing considering the diagonal length of the rectangles to
emphasize on both TAM widths required by a core and its corresponding testing
time. In this paper, we propose an efficient algorithm to construct wrappers
that reduce testing time for cores. We then use rectangle packing to develop an
integrated scheduling algorithm that incorporates power constraints in the test
schedule. The test power consumption is important to consider since exceeding
the system's power limit might damage the system.
"
533,"Wrapper/TAM Co-Optimization and constrained Test Scheduling for SOCs
  Using Rectangle Bin Packing","  This paper describes an integrated framework for SOC test automation. This
framework is based on a new approach for Wrapper/TAM co-optimization based on
rectangle packing considering the diagonal length of the rectangles to
emphasize on both TAM widths required by a core and its corresponding testing
time .In this paper, an efficient algorithm has been proposed to construct
wrappers that reduce testing time for cores. Rectangle packing has been used to
develop an integrated scheduling algorithm that incorporates power constraints
in the test schedule. The test power consumption is important to consider since
exceeding the system's power limit might damage the system.
"
534,Towards Autopoietic Computing,"  A key challenge in modern computing is to develop systems that address
complex, dynamic problems in a scalable and efficient way, because the
increasing complexity of software makes designing and maintaining efficient and
flexible systems increasingly difficult. Biological systems are thought to
possess robust, scalable processing paradigms that can automatically manage
complex, dynamic problem spaces, possessing several properties that may be
useful in computer systems. The biological properties of self-organisation,
self-replication, self-management, and scalability are addressed in an
interesting way by autopoiesis, a descriptive theory of the cell founded on the
concept of a system's circular organisation to define its boundary with its
environment. In this paper, therefore, we review the main concepts of
autopoiesis and then discuss how they could be related to fundamental concepts
and theories of computation. The paper is conceptual in nature and the emphasis
is on the review of other people's work in this area as part of a longer-term
strategy to develop a formal theory of autopoietic computing.
"
535,A Note on the Membrane Computer,"  Inspired by the emergent membrane computing (P Systems) concepts, some
efforts are carried out introducing simulation models, some are software
oriented, and others are hardware, yet all are applied with the current vision
of the conventional computers, based on ""Von Neumann architecture"", which is a
sequential design in its essence. We think that these models will need ""as a
consequent result"" to a new architecture exposing a true parallel design, in
this paper; we try to investigate and introduce a global view for how it would
be like to have such architecture. The main goal is to point out to this
direction broadly, suggesting that it might be useful considering some aspects,
like the need for a new definition of an operating system and its programs,
which will eventually lead to a higher scope: the membrane computer.
"
536,Home Automation,"  In this paper I briefly discuss the importance of home automation system.
Going in to the details I briefly present a real time designed and implemented
software and hardware oriented house automation research project, capable of
automating house's electricity and providing a security system to detect the
presence of unexpected behavior.
"
537,Aero Fighter - 2D Gaming,"  Designing and developing quality based computer game is always a challenging
task for developers. In this paper I briefly discuss aero fighting war game
based on simple 2D gaming concepts and developed in C & C++ programming
languages, using old bitmapping concepts. Going into the details of the game
development, I discuss the designed strategies, flow of game and implemented
prototype version of game, especially for beginners of game programming.
"
538,"Biochemical Filter with Sigmoidal Response: Increasing the Complexity of
  Biomolecular Logic","  The first realization of a designed, rather than natural, biochemical filter
process is reported and analyzed as a promising network component for
increasing the complexity of biomolecular logic systems. Key challenge in
biochemical logic research has been achieving scalability for complex network
designs. Various logic gates have been realized, but a ""toolbox"" of analog
elements for interconnectivity and signal processing has remained elusive.
Filters are important as network elements that allow control of noise in signal
transmission and conversion. We report a versatile biochemical filtering
mechanism designed to have sigmoidal response in combination with
signal-conversion process. Horseradish peroxidase-catalyzed oxidation of
chromogenic electron donor by hydrogen peroxide, was altered by adding
ascorbate, allowing to selectively suppress the output signal, modifying the
response from convex to sigmoidal. A kinetic model was developed for evaluation
of the quality of filtering. The results offer improved capabilities for design
of scalable biomolecular information processing systems.
"
539,Testing of Bridging Faults in AND-EXOR based Reversible Logic Circuits,"  Reversible circuits find applications in many areas of Computer Science
including Quantum Computation. This paper examines the testability of an
important subclass of reversible logic circuits that are composed of k-wire
controlled NOT (k-CNOT with k >/- 1) gates. A reversible k-CNOT gate can be
implemented using an irreversible k-input AND gate and an EXOR gate. A
reversible k-CNOT circuit where each k-CNOT gate is realized using irreversible
k-input AND and EXOR gate, has been considered. One of the most commonly used
Single Bridging Fault model (both wired-AND and wired-OR) has been assumed to
be type of fault for such circuits. It has been shown that an (n+p)-input
AND-EXOR based reversible logic circuit with p observable outputs, can be
tested for single bridging faults (SBF) using (3n + \lefthalfcap log2p
\righthalfcap + 2) tests.
"
540,"ARMrayan Multimedia Mobile CMS: a Simplified Approach towards
  Content-Oriented Mobile Application Designing","  The ARMrayan Multimedia Mobile CMS (Content Management System) is the first
mobile CMS that gives the opportunity to users for creating multimedia J2ME
mobile applications with their desired content, design and logo; simply,
without any need for writing even a line of code. The low-level programming and
compatibility problems of the J2ME, along with UI designing difficulties, makes
it hard for most people -even programmers- to broadcast their content to the
widespread mobile phones used by nearly all people. This system provides
user-friendly, PC-based tools for creating a tree index of pages and inserting
multiple multimedia contents (e.g. sound, video and picture) in each page for
creating a J2ME mobile application. The output is a stand-alone Java mobile
application that has a user interface, shows texts and pictures and plays music
and videos regardless of the type of devices used as long as the devices
support the J2ME platform. Bitmap fonts have also been used thus Middle Eastern
languages can be easily supported on all mobile phone devices. We omitted
programming concepts for users in order to simplify multimedia content-oriented
mobile application designing for use in educational, cultural or marketing
centers. Ordinary operators can now create a variety of multimedia mobile
applications such as tutorials, catalogues, books, and guides in minutes rather
than months. Simplicity and power has been the goal of this CMS. In this paper,
we present the software engineered-designed concepts of the ARMrayan MCMS along
with the implementation challenges faces and solutions adapted.
"
541,A Mobile Application for Smart House Remote Control System,"  At the start of the second decade of 21th century, the time has come to make
the Smart Houses a reality for regular use. The different parts of a Smart
House are researched but there are still distances from an applicable system,
using the modern technology. In this paper we present an overview of the Smart
House subsystems necessary for controlling the house using a mobile application
efficiently and securely. The sequence diagram of the mobile application
connecting to the server application and also the use-cases possible are
presented. The challenges faced in designing the mobile application and
illustrating the updated house top plane view in that application, are
discussed and solutions are adapted for it. Finally the designed mobile
application was implemented and the important sections of it were described,
such as the interactive house top view map which indicates the status of the
devices using predefined icons. The facilities to manage the scheduled tasks
and defined rules are also implemented in this mobile application that was
developed for use in Windows Mobile platform. This application has the
capability of connecting to the main server using GPRS mobile internet and SMS.
This system is expected to be an important step towards a unified system
structure that can be used efficiently in near future regular houses.
"
542,"Heuristic approach to optimize the number of test cases for simple
  circuits","  In this paper a new solution is proposed for testing simple stwo stage
electronic circuits. It minimizes the number of tests to be performed to
determine the genuinity of the circuit. The main idea behind the present
research work is to identify the maximum number of indistinguishable faults
present in the given circuit and minimize the number of test cases based on the
number of faults that has been detected. Heuristic approach is used for test
minimization part, which identifies the essential tests from overall test
cases. From the results it is observed that, test minimization varies from 50%
to 99% with the lowest one corresponding to a circuit with four gates .Test
minimization is low in case of circuits with lesser input leads in gates
compared to greater input leads in gates for the boolean expression with same
number of symbols. Achievement of 99% reduction is due to the fact that the
large number of tests find the same faults. The new approach is implemented for
simple circuits. The results show potential for both smaller test sets and
lower cpu times.
"
543,Nations At War I: Why do we keep building weapons?,"  This paper is the first in series of four papers that present an analytical
approach to war using game theory. We try to explore why is it that ""true
peace"" can't be achieved and all or any efforts we make towards that goal will
have huge road-blocks. A fairly simplistic and non technical overview of our
approach is given in this paper using prisoner's dilemma.
"
544,"Statistical Modelling of ft to Process Parameters in 30 nm Gate Length
  Finfets","  This paper investigates the effect of process variations on unity gain
frequency (ft) in 30 nm gate length FinFET by performing extensive TCAD
simulations. Six different geometrical parameters, channel doping, source/drain
doping and gate electrode work function are studied for their sensitivity on
ft. It is found that ft is more sensitive to gate length, underlap, gate-oxide
thickness, channel and Source/Drain doping and less sensitive to source/drain
width and length, and work function variations. Statistical modelling has been
performed for ft through design of experiment with respect to sensitive
parameters. The model has been validated through a comparison between random
set of experimental data simulations and predicted values obtained from the
model.
"
545,Control of Noise in Chemical and Biochemical Information Processing,"  We review models and approaches for error-control in order to prevent the
buildup of noise when gates for digital chemical and biomolecular computing
based on (bio)chemical reaction processes are utilized to realize stable,
scalable networks for information processing. Solvable rate-equation models
illustrate several recently developed methodologies for gate-function
optimization. We also survey future challenges and possible new research
avenues.
"
546,"Server Consolidation: An Approach to make Data Centers Energy Efficient
  and Green","  Data centers are the building blocks of IT business organizations providing
the capabilities of centralized repository for storage, management, networking
and dissemination of data. With the rapid increase in the capacity and size of
data centers, there is a continuous increase in the demand for energy
consumption. These data centers not only consume a tremendous amount of energy
but are riddled with IT inefficiencies. All data center are plagued with
thousands of servers as major components. These servers consume huge energy
without performing useful work. In an average server environment, 30% of the
servers are ""dead"" only consuming energy, without being properly utilized.
Their utilization ratio is only 5 to 10 percent. This paper focuses on the use
of an emerging technology called virtualization to achieve energy efficient
data centers by providing a solution called server consolidation. It increases
the utilization ratio up to 50% saving huge amount of energy. Server
consolidation helps in implementing green data centers to ensure that IT
infrastructure contributes as little as possible to the emission of green house
gases, and helps to regain power and cooling capacity, recapture resilience and
dramatically reducing energy costs and total cost of ownership.
"
547,Level Shifter Design for Low Power Applications,"  With scaling of Vt sub-threshold leakage power is increasing and expected to
become significant part of total power consumption In present work three new
configurations of level shifters for low power application in 0.35{\mu}m
technology have been presented. The proposed circuits utilize the merits of
stacking technique with smaller leakage current and reduction in leakage power.
Conventional level shifter has been improved by addition of three NMOS
transistors, which shows total power consumption of 402.2264pW as compared to
0.49833nW with existing circuit. Single supply level shifter has been modified
with addition of two NMOS transistors that gives total power consumption of
108.641pW as compared to 31.06nW. Another circuit, contention mitigated level
shifter (CMLS) with three additional transistors shows total power consumption
of 396.75pW as compared to 0.4937354nW. Three proposed circuit's shows better
performance in terms of power consumption with a little conciliation in delay.
Output level of 3.3V has been obtained with input pulse of 1.6V for all
proposed circuits.
"
548,Automatic Short -Answer Grading System (ASAGS),"  Automatic assessment needs short answer based evaluation and automated
assessment. Various techniques used are Ontology, Semantic similarity matching
and Statistical methods. An automatic short answer assessment system is
attempted in this paper. Through experiments performed on a data set, we show
that the semantic ASAGS outperforms methods based on simple lexical matching;
resulting is up to 59 percent with respect to the traditional vector-based
similarity metric.
"
549,Reusing optical supports using a simple software,"  In this paper we show how it is possible to reuse optical supports (CDs,
DVDs, etc.) without using chemical or physical transformation, only employing a
software that can easily run on domestic computers. This software can make
obsolete optical supports useful again, converting de facto WEEE (Waste
electric and electronic equipment) into EEE (Electric and electronic
equipment). A massive use of such a software can lead to a significant change
in EEE every-day use, reducing its production to sustainable levels.
"
550,Towards Greener and Safer Mines,"  Miniaturised sensors and networking are technical proven concepts. Both the
technologies are proven and various components e.g., sensors, controls, etc.
are commercially available. Technology scene in above areas presents enormous
possibilities for developing innovative applications for real life situations.
Mining operations in many countries have lot of scope for improving
environmental and safety measures. Efforts have been made to develop a system
to efficiently monitor a particular environment by deploying a wireless sensor
network using commercially available components. Wireless Sensor Network has
been integrated with telecom network through a gateway using a suitable
topology which can be selected at the application layer. The developed system
demonstrates a way to connect wireless sensor network to external network which
enables the distant administrator to access real time data and act expediently
from long-distance to improve the environmental situation or prevent a
disaster. Potentially, it can be used to avoid the awful situations leading to
terrible environment in underground mines. Keywords: Wireless sensor network,
Mine safety, Environment monitoring and telecom.
"
551,"Biopsym : a learning environment for transrectal ultrasound guided
  prostate biopsies","  This paper describes a learning environment for image-guided prostate
biopsies in cancer diagnosis; it is based on an ultrasound probe simulator
virtually exploring real datasets obtained from patients. The aim is to make
the training of young physicians easier and faster with a tool that combines
lectures, biopsy simulations and recommended exercises to master this medical
gesture. It will particularly help acquiring the three-dimensional
representation of the prostate needed for practicing biopsy sequences. The
simulator uses a haptic feedback to compute the position of the virtual probe
from three-dimensional (3D) ultrasound recorded data. This paper presents the
current version of this learning environment.
"
552,Usability Meets Instant Gratification on the Semantic Web,"  This paper presents a semantic wiki prototype application named SHAWN [later
WikSAR] that allows structuring concepts within a wiki environment. To entice
the use of Semantic Web technologies applications need to offer both high
usability and instant gratification. Concept creation is exceptionally easy in
SHAWN since metadata as well as plain text is entered within a single edit box
on each wiki page in a self-explaining fashion. The entered data is immediately
used for rendering sophisticated navigational means on the wiki. By editing
simple wiki pages ontologies emerge.
"
553,The emergence of the physical world from information processing,"  This paper links the conjecture that the physical world is a virtual reality
to the findings of modern physics. What is usually the subject of science
fiction is here proposed as a scientific theory open to empirical evaluation.
We know from physics how the world behaves, and from computing how information
behaves, so whether the physical world arises from ongoing information
processing is a question science can evaluate. A prima facie case for the
virtual reality conjecture is presented. If a photon is a pixel on a
multi-dimensional grid that gives rise to space, the speed of light could
reflect its refresh rate. If mass, charge and energy all arise from processing,
the many conservation laws of physics could reduce to a single law of dynamic
information conservation. If the universe is a virtual reality, then its big
bang creation could be simply when the system was booted up. Deriving core
physics from information processing could reconcile relativity and quantum
theory, with the former how processing creates the space-time operating system
and the latter how it creates energy and matter applications.
"
554,iCare: A Mobile Health Monitoring System for the Elderly,"  This paper describes a mobile health monitoring system called iCare for the
elderly. We use wireless body sensors and smart phones to monitor the wellbeing
of the elderly. It can offer remote monitoring for the elderly anytime anywhere
and provide tailored services for each person based on their personal health
condition. When detecting an emergency, the smart phone will automatically
alert pre-assigned people who could be the old people's family and friends, and
call the ambulance of the emergency centre. It also acts as the personal health
information system and the medical guidance which offers one communication
platform and the medical knowledge database so that the family and friends of
the served people can cooperate with doctors to take care of him/her. The
system also features some unique functions that cater to the living demands of
the elderly, including regular reminder, quick alarm, medical guidance, etc.
iCare is not only a real-time health monitoring system for the elderly, but
also a living assistant which can make their lives more convenient and
comfortable.
"
555,The Use of Rapid Digital Game Creation to Learn Computational Thinking,"  Computational Thinking (CT) has been described as a universally applicable
ability such as reading and writing. In this paper, we describe an innovative
pedagogy using Rapid Digital Game Creation (RDGC) for learning CT skills. RDGC
involves the rapid building of digital games with high-level software that
requires little or no programming knowledge. We analyze how RDGC supports
various CT concepts and how it may be mapped to equivalent Java concepts by
building the same game using both RDGC and Java. We discuss the potential
benefits of this approach for attracting computing majors, as a precursor to
learning formal programming languages, for learning domain knowledge, and for
bridging the digital divide. We present the implications of this work for
teachers and researchers.
"
556,VHDL Implementation and Verification of ARINC-429 Core,"  Modern Avionics are controlled by sophisticated mission components in the
Aircraft. The control function is implemented via a standard ARINC-429 bus
interface. It is a two-wire point-topoint serial data bus for control
communications in Avionics. The bus operates 12.5 or 100kb/sec, the
implementation is envisaged for one transmits and receive channel respectively.
Further the code can be modified for more no of independent Tx and Rx channels.
An on chip memory allotment on the FPGA will provide a buffer bank for storing
the incoming or outgoing data. For this purpose SRAM based FPGAs are utilized.
This flexible ARINC429 solution gives exactly what is needed for real time
applications. The IP can be programmed to send an interrupt to the host and
also prepare it to process the data. Majority of the hardware function of
digital natures are embedded into a single FPGA by saving in terms of PCB board
space, power consumption and volume results. This paper deals with the
development, implementation, simulation, and verification of ARINC_429 formats.
The IP core development is described in VHDL.
"
557,Simulating space and time,"  This chapter asks if a virtual space-time could appear to those within it as
our space-time does to us. A processing grid network is proposed to underlie
not just matter and energy, but also space and time. The suggested ""screen"" for
our familiar three dimensional world is a hyper-sphere surface simulated by a
grid network. Light and matter then travel, or are transmitted, in the
""directions"" of the grid architecture. The processing sequences of grid nodes
create time, as the static states of movies run together emulate events. Yet
here what exists are not the static states, but the dynamic processing between
them. Quantum collapse is the irreversible event that gives time its direction.
In this model, empty space is null processing, directions are node links, time
is processing cycles, light is a processing wave, objects are wave tangles and
energy is the processing transfer rate. It describes a world where empty space
is not empty, space warps, time dilates, and everything began when this virtual
universe ""booted up"".
"
558,"Using the C4ISR Architecture Framework as a Tool to Facilitate VV&A for
  Simulation Systems within the Military Application Domain","  To harmonize the individual architectures of the different commands,
services, and agencies dealing with the development and procurement of Command,
Control, Communications, Computing, Surveillance, Reconnaissance, and
Intelligence (C4ISR) systems, the C4ISR Architecture Framework was developed
based on existing and matured modeling techniques and methods. Within a short
period, NATO adapted this method family as the NATO Consultation, Command, and
Control (C3) System Architecture Framework to harmonize the efforts of the
different nations. Based on these products, for every system to be fielded to
be used in the US Armed Forces, a C4I Support Plan (C4ISP) has to be developed
enabling the integration of the special system into the integrated C4I
Architecture. The tool set proposed by these architecture frameworks connects
operational views of the military user, system views of the developers, and the
technical views for standards and integration methods needed to make the
network centric system of systems work. The tools are therefore logically a
valuable backbone for Verification, Validation, and Accreditation (VV&A). Their
application is not limited to C4ISR systems; they can be used to define
requirements and connected solutions and algorithms of Modeling and Simulation
(M&S) systems as well. Especially for M&S systems to be used in connection with
C4ISR system, the use of the C4ISR Architecture Framework would not only be a
help, but can nearly be seen to be necessary to avoid double work and foster
reuse and interoperability from the first stages of a project on. To enable the
reader to build his own picture, the respective tools used and their
application in the context of VV&A will be explained in form of an overview.
"
559,"Decision Support Systems - Technical Prerequisites and Military
  Requirements","  Decision Support Systems in the sense of online alternative course of action
(ACAO) development and analysis as well as tools for online Development of
Doctrine and Tactics Techniques, and Procedures (DTTP) for support to
operations make it possible to evaluate and forecast the command and control
processes and the performance capabilities of the friendly and enemy forces and
other decision relevant factors, support the military commander (brigade and
higher) and his staff in their headquarter by increasing their ability to
identify own opportunities, support all phases of the command and control
process, use computer based, automatic and closed models, that can be adapted
to the current situation.
  Objective of the paper is to present the results of studies conducted in
Germany on behalf of the German Ministry of Defense with the objective to work
out the conceptual basis for decision support systems and to evaluate, how this
technique will influence the command and control system of the army of the
federal armed forces. In addition, international works are considered as well.
In this paper, technical and operational requirements are derived and described
in detail that have to be met in order to support the warfighter by integrated
means of applied Operations Research ranging from simple optimization
algorithms to complex simulation federations comprising different systems.
"
560,The Light of Existence,"  This chapter derives the properties of light from the properties of
processing, including its ability to be both a wave and a particle, to respond
to objects it doesn't physically touch, to take all paths to a destination, to
choose a route after it arrives, and to spin both ways at once as it moves.
Here a photon is an entity program spreading as a processing wave of instances.
It becomes a ""particle"" if any part of it overloads the grid network that runs
it, causing the photon program to reboot and restart at a new node. The
""collapse of the wave function"" is how quantum processing creates what we call
a physical photon. This informational approach gives insights into issues like
the law of least action, entanglement, superposition, counterfactuals, the
holographic principle and the measurement problem. The conceptual cost is that
physical reality is a quantum processing output, i.e. virtual.
"
561,Extending ArXiv.org to Achieve Open Peer Review and Publishing,"  Today's peer review process for scientific articles is unnecessarily opaque
and offers few incentives to referees. Likewise, the publishing process is
unnecessarily inefficient and its results are only rarely made freely available
to the public. Here we outline a comparatively simple extension of arXiv.org,
an online preprint archive widely used in the mathematical and physical
sciences, that addresses both of these problems. Under the proposal, editors
invite referees to write public and signed reviews to be attached to the posted
preprints, and then elevate selected articles to ""published"" status.
"
562,"Functional Categories of Support to Operations in Military Information
  Systems","  In order to group the functional requirements for support to operations by
modern information systems systematically, the NATO Code of best Practise
(COBP) for C2 Assessment defines three domain areas: Battlespace Visualization,
Decision Making, and Battle Management Functions. In addition, within an domain
overlapping information grid of the information system, necessary functions for
assessing and disseminating the information are capsulated. For all three
domains, including the overlapping information grid, the respective
requirements for functional support have to be met be future command, control,
communications, and intelligence (C3I) systems.
  The paper describes the functional categories of the three domains having
been defined for article 5 operations, extends them to meet the requirements
for operations other than war (OOTW), and gives some examples how modules of
simulation systems can deliver respective support functions. In addition,
references defining migration procedures for legacy systems to enable a smooth
change from the old to the new C3I paradigm are given.
"
563,"Avoiding another Green Elephant - A Proposal for the Next Generation HLA
  based on the Model Driven Architecture","  When looking through the proceedings of the recent Simulation
Interoperability Workshops, a lot of papers - some of them even awarded by the
committee - are dealing with alternative concepts outside or beyond the High
Level Architecture (HLA): Web Services, the extensible Markup Language (XML),
Java Beans, Simple Object Access Protocol (SOAP), etc. Similarly, requirements
driven by interoperability issues have resulted in the need to use meta
modeling, adaptive models, and common repositories. The use of the Unified
Modeling Language (UML) as a model description language is also rapidly
becoming a standard. All these concepts have relations to the HLA, but they are
not part of it. There seems to be the danger that HLA is overrun by respective
developments of the free market and will become irrelevant finally. ... This
paper introduces the MDA concept and shows, how the HLA can be integrated to
become a standard stub for simulation applications of legacy systems, systems
under development, and systems of the future.
"
564,Communication model of emuStudio emulation platform,"  Within the paper a description of communication model of plug-in based
emuStudio emulation platform is given. The platform mentioned above allows the
emulation of whole computer systems, configurable to the level of its
components, represented by the plug-in modules of the platform. Development
tasks still are in progress at the home institution of the authors. Currently
the platform is exploited for teaching purposes within subjects aimed at
machine-oriented languages and computer architectures. Versatility of the
platform, given by its plug-in based architecture is a big advantage, when used
as a teaching support tool. The paper briefly describes the emuStudio platform
at its introductory part and then the mechanisms of inter-module communication
are described.
"
565,Towards a Spiking Neural P Systems OS,"  This paper is an attempt to incorporate the idea of spiking neural P systems
as an early seed into the area of Operating System Design, regarding their
capability to solve some classical computer science problems. It is reflecting
the power of such systems to simulate well known parallel computational models,
like logic gates, arithmetic operation, and sorting. In these devices, the time
(when the neurons fire and/or spike) plays an essential role. For instance, the
result of a computation is the time between the moments when a specified neuron
spikes. Seen as number computing devices, SN P systems are shown to be
computationally complete, and with such capabilities, arithmetic operations,
logic, and timing, some first steps could be taken towards an OS design.
"
566,Linked Environment Data for the Life Sciences,"  Environment Agencies from Europe and the US are setting up a network of
Linked Environment Data and are looking to crosslink it with Linked Data
contributions from the life sciences.
"
567,"Fundamentals of Semantic Web Technologies in Medical Environments: a
  case in breast cancer risk estimation","  Risk estimation of developing breast cancer poses as the first prevention
method for early diagnosis. Furthermore, data integration from different
departments involved in the process plays a key role. In order to guarantee
patient safety, the whole process should be orchestrated and monitored
automatically. Support for the solution will be a linked data cloud, composed
by all the departments that take part in the process, combined with rule
engines.
"
568,semanticSBML 2.0 - A Collection of Online Services for SBML Models,"  semanticSBML 2.0 is an online collection of services for the work with
biochemical network models in SBML format.
"
569,"The differences between natural and artificial life. Towards a
  definition of life","  It is high time to openly and without finalism define the dangerous but
needed term 'purposeful information', whose quantity is an Eigen information
value. Using the term 'biological information' in its stead forces one into an
uncomfortable detour. I propose such a definition based on the generalized
notions of 'information' and 'encoding'. Next, the properties of the
spontaneous process of collecting purposeful information are investigated. In
effect, the properties of this process: the goal 'continuation of existence',
reproduction and Darwinian mechanism are derived which suggest, that it is the
natural life process. A 'natural identity criterion' appears in this process
for an evolving object, that is connected to a 'small change tendency'.
Likewise, 'hereditary information' is defined. Artificial life is constructed
by living objects, is a part of natural life process and its properties are not
an effect of its internal restrictions but of external assumptions.
"
570,"Instantaneous, non-squeezed, noise-based logic","  Noise-based logic, by utilizing its multidimensional logic hyperspace, has
significant potential for low-power parallel operations in beyond-Moore-chips.
However universal gates for Boolean logic thus far had to rely on either time
averaging to distinguish signals from each other or, alternatively, on squeezed
logic signals, where the logic-high was represented by a random process and the
logic-low was a zero signal. A major setback is that squeezed logic variables
are unable to work in the hyperspace, because the logic-low zero value sets the
hyperspace product vector to zero. This paper proposes Boolean universal logic
gates that alleviate such shortcomings. They are able to work with non-squeezed
logic values where both the high and low values are encoded into nonzero,
bipolar, independent random telegraph waves. Non-squeezed universal Boolean
logic gates for spike-based brain logic are also shown. The advantages vs.
disadvantages of the two logic types are compared.
"
571,Online traffic state estimation based on floating car data,"  Besides the traditional data collection by stationary detectors, recent
advances in wireless and sensor technologies have promoted new potentials for a
vehicle-based data collection and local dissemination of information. By means
of microscopic traffic simulations we study the problem of online estimation of
the current traffic situation based on floating car data. Our focus is on the
estimation on the up- and downstream jam fronts determining the extension of
traffic congestion. We study the impact of delayed information transmission by
short-range communication via wireless LAN in contrast to instantaneous
information transmission to the roadside units by means of mobile radio. The
delayed information transmission leads to systematic estimation errors which
cannot be compensated for by a higher percentage of probe vehicles. Additional
flow measurements from stationary detectors allow for a model-based prediction
which is effective for much lower floating car percentages than 1%.
"
572,Solving a real-life large-scale energy management problem,"  This paper introduces a three-phase heuristic approach for a large-scale
energy management and maintenance scheduling problem. The problem is concerned
with scheduling maintenance and refueling for nuclear power plants up to five
years into the future, while handling a number of scenarios for future demand
and prices. The goal is to minimize the expected total production costs. The
first phase of the heuristic solves a simplified constraint programming model
of the problem, the second performs a local search, and the third handles
overproduction in a greedy fashion.
  This work was initiated in the context of the ROADEF/EURO Challenge 2010, a
competition organized jointly by the French Operational Research and Decision
Support Society, the European Operational Research Society, and the European
utility company Electricite de France. In the concluding phase of the
competition our team ranked second in the junior category and sixth overall.
  After correcting an implementation bug in the program that was submitted for
evaluation, our heuristic solves all ten real-life instances, and the solutions
obtained are all within 2.45% of the currently best known solutions. The
results given here would have ranked first in the original competition.
"
573,Software Oriented Data Monitoring System,"  This project ""Software Oriented Data Monitoring System"" deals with real time
monitoring of patients' parameters like body temperature, heart rate etc. The
parameters are checked at regular intervals and Short Messaging Service (SMS)
is sent to concerned doctor regarding the measured values. If the obtained
parameters are above or below critical values, an alert SMS is also sent to the
concerned doctor. This system is very much useful in hospitals, which saves the
valuable time of the doctor who otherwise will have to monitor the patients
throughout the day. Here the analog data from the sensors is first converted
into digital form and is fed to the parallel port of the computer. This data
obtained is converted into useful parameters, which is monitored and checked
for safe limits. Appropriate SMS is sent to the doctor depending on whether the
request is from an alert or routine signal. This is possible by interfacing a
mobile phone (Siemens c35i) to the serial port of the computer. The SMS is sent
from the computer using proper AT commands.
"
574,A simple circuit with dynamic logic architecture of basic logic gates,"  We report experimental results obtained with a circuit possessing dynamic
logic architecture based on one of the theoretical schemes proposed by H. Peng
and collaborators in 2008. The schematic diagram of the electronic circuit and
its implementation to get different basic logic gates are displayed and
discussed. In particular, we show explicitly how to get the electronic NOR,
NAND, and XOR gates. The proposed electronic circuit is easy to build because
it employs only resistors, operational amplifiers and comparators
"
575,"Universal regular autonomous asynchronous systems: omega-limit sets,
  invariance and basins of attraction","  The asynchronous systems are the non-deterministic real time-binary models of
the asynchronous circuits from electrical engineering. Autonomy means that the
circuits and their models have no input. Regularity means analogies with the
dynamical systems, thus such systems may be considered to be the real time
dynamical systems with a 'vector field' {\Phi}:{0,1}^2 \rightarrow {0,1}^2.
Universality refers to the case when the state space of the system is the
greatest possible in the sense of the inclusion. The purpose of the paper is
that of defining, by analogy with the dynamical systems theory, the
{\omega}-limit sets, the invariance and the basins of attraction of the
universal regular autonomous asynchronous systems.
"
576,"The dependence on the initial states and the transitivity of the regular
  autonomous asynchronous systems","  The asynchronous systems are non-deterministic real time, binary valued
models of the asynchronous circuits from electronics. Autonomy means that there
is no input and regularity means analogies with the (real) dynamical systems.
We introduce the concepts of dependence on the initial states and of
transitivity for these systems.
"
577,The model of the ideal rotary element of Morita,"  Reversible computing is a concept reflecting physical reversibility. Until
now several reversible systems have been investigated. In a series of papers
Kenichi Morita defines the rotary element RE, that is a reversible logic
element. By reversibility, he understands that 'every computation process can
be traced backward uniquely from the end to the start. In other words, they are
backward deterministic systems'. He shows that any reversible Turing machine
can be realized as a circuit composed of RE's only. Our purpose in this paper
is to use the asynchronous systems theory and the real time for the modeling of
the ideal rotary element
"
578,A Chronology of Torah Cryptography,"  Regarding some papers and notes submitted to, or presented at, the second
congress of the International Torah Codes Society in Jerusalem, Israel, June
2000.
"
579,Use of root in vehicular accident reconstruction,"  The purpose of this article is to introduce the reader to the ROOT data
analysis software package, and demonstrate how it may be used to complement
one's accident reconstruction analyses.
"
580,"Computer Aided Tolerancing Based on Analysis and Synthetizes of
  Tolerances Method","  The tolerancing step has a great importance in the design process. It
characterises the relationship between the different sectors of the product
life cycle: Design, Manufacturing and Control. We can distinguish several
methods to assist the tolerancing process in the design. Based on arithmetic
and statistical method, this paper presents a new approach of analysis and
verification of tolerances. The chosen approach is based on the Worst Case
Method as an arithmetic method and Monte Carlo method as a statistical method.
In this paper, we compare these methods and we present our main approach, which
is validated using an example of 1 D tolerancing.
"
581,Cluster quantum computer on the basis of quasi-part,"  The present paper deals with the possibility of creation of the quantum
computer in which the role of q-bits is played by quasi-particles. In such a
computer, the elementary computation block should represent a cluster created
on the basis of the paramagnetic molecules. The latter form heterogeneous spin
states in the cluster owing to the presence of interelectron correlations.
"
582,"Support of Interactive 3D/4D Presentations by the Very First Ever Made
  Virtual Laboratories of Antennas","  Based on the experience we have gained so far, as independent reviewers of
Radioengineering journal, we thought that may be proved useful to publicly
share with the interested author, especially the young one, some practical
implementations of our ideas for the interactive representation of data using
3D/4D movement and animation, in an attempt to motivate and support her/him in
the development of similar dynamic presentations, when s/he is looking for a
way to locate the stronger aspects of her/his research results in order to
prepare a clear, most appropriate for publication, static presentation figure.
For this purpose, we selected to demonstrate a number of presentations, from
the simplest to the most complicated, concerning well-known antenna issues with
rather hard to imagine details, as it happens perhaps in cases involving
Spherical Coordinates and Polarization, which we created to enrich the very
first ever made Virtual Laboratories of Antennas, that we distribute over the
Open Internet through our website Virtual Antennas. These presentations were
developed in a general way, without using antenna simulators, to handle output
text and image data from third-party CAS Computer Algebra Systems, such as the
Mathematica commercial software we use or the Maxima FLOSS we track its
evolution.
"
583,"Numerical investigation of a solar greenhouse tunnel drier for drying of
  copra","  A numerical investigation of a solar greenhouse tunnel drier (SGTD) has been
performed. In the present study, the geometry of the tunnel roof is assumed
semi-circular which is covered with a UV (200\mu) stabilized polyethylene film.
The simulated SGTD reduces moisture of copra from 52.2% to 8% in 55 h under
full load conditions. A system of partial differential equations describing
heat and moisture transfer during drying copra in the solar greenhouse dryer
was developed and this system of non-linear partial differential equations was
solved numerically using the finite difference method (FDM). The numerical
solution was programmed in Compaq Visual FORTRAN version 6.5. The simulated
results reasonably agreed with the experimental data for solar drying copra.
This model can be used to provide the design data and is also essential for
optimal design of the dryer. For instance the user is able to change the
radiation properties of the roof cover for different materials of roof cover.
"
584,P ORTOLAN: a Model-Driven Cartography Framework,"  Processing large amounts of data to extract useful information is an
essential task within companies. To help in this task, visualization techniques
have been commonly used due to their capacity to present data in synthesized
views, easier to understand and manage. However, achieving the right
visualization display for a data set is a complex cartography process that
involves several transformation steps to adapt the (domain) data to the
(visualization) data format expected by visualization tools. To maximize the
benefits of visualization we propose Portolan, a generic model-driven
cartography framework that facilitates the discovery of the data to visualize,
the specification of view definitions for that data and the transformations to
bridge the gap with the visualization tools. Our approach has been implemented
on top of the Eclipse EMF modeling framework and validated on three different
use cases.
"
585,An Autonomous Long Range Monitoring System For Emergency Operators,"  Miniaturization and portability of new electronics lead up to wearable
devices embedded within garments: a European program called ProeTEX developed
multi-purpose sensors integrated within emergency operators' garments in order
to monitor their health state and the surrounding environment. This work deals
with the development of an autonomous Long Range communication System (LRS),
suitable to transmit data between operators' equipment and the local command
post, where remote monitoring software is set up. The LRS infrastructure is
based on Wi-Fi protocol and modular architecture. Field tests carried out on
the developed prototype showed a high reliability in terms of correctly
exchanged data and recovering capabilities in case of temporary disconnection,
due to the operator's movements.
"
586,"Transient Stability Assessment of Smart Power System using Complex
  Networks Framework","  In this paper, a new methodology for stability assessment of a smart power
system is proposed. The key to this assessment is an index called betweenness
index which is based on ideas from complex network theory. The proposed
betweenness index is an improvement of previous works since it considers the
actual real power flow through the transmission lines along the network.
Furthermore, this work initiates a new area for complex system research to
assess the stability of the power system.
"
587,"Modelling dynamic route choice of pedestrians to assess the criticality
  of building evacuation","  This paper presents an event-driven way finding algorithm for pedestrians in
an evacuation scenario, which operates on a graph-based structure. The
motivation of each pedestrian is to leave the facility. The events used to
redirect pedestrians include the identification of a jam situation and/or
identification of a better route than the current. This study considers two
types of pedestrians: familiar and unfamiliar with the facility. Four
strategies are modelled to cover those groups. The modelled strategies are the
shortest path (local and global); They are combined with a quickest path
approach, which is based on an observation principle. In the quickest path
approach, pedestrians take their decisions based on the observed environment
and are routed dynamically in the network using an appropriate cost benefit
analysis function. The dynamic modelling of route choice with different
strategies and types of pedestrians considers the manifold of in uences which
appears in the real system and raises questions about the criticality of an
evacuation process. To address this question criteria are elaborated. The
criteria we focus on in this contribution are the evacuation time, the
individual times spent in jam, the jam size evolution and the overall jam size
itself. The in uences of the different strategies on those evaluation criteria
are investigated. The sensibility of the system to disturbances (e.g. broken
escape route) is also analysed. Keywords: pedestrian dynamics, routing,
quickest path, evacuation, jam, critical state
"
588,"Medical Image Denoising using Adaptive Threshold Based on Contourlet
  Transform","  Image denoising has become an essential exercise in medical imaging
especially the Magnetic Resonance Imaging (MRI). This paper proposes a medical
image denoising algorithm using contourlet transform. Numerical results show
that the proposed algorithm can obtained higher peak signal to noise ratio
(PSNR) than wavelet based denoising algorithms using MR Images in the presence
of AWGN.
"
589,Generating contour lines using different elevation data file formats,"  In terrain mapping, there are so many ways to measure and estimate the
terrain measurements like contouring, vertical profiling, hill shading,
hypsometric tinting, perspective view, etc. Here in this paper we are using the
contouring techniques to generate the contours for the different digital
elevation data like DEM, HGT, IMG etc. The elevation data is captured in dem,
hgt and img formats of the same projected area and the contour is generated
using the existing techniques and applications. The exact differences, errors
of elevation (contour) intervals, slopes and heights are analyzed and
recovered.
"
590,Distributed k-Core Decomposition,"  Among the novel metrics used to study the relative importance of nodes in
complex networks, k-core decomposition has found a number of applications in
areas as diverse as sociology, proteinomics, graph visualization, and
distributed system analysis and design. This paper proposes new distributed
algorithms for the computation of the k-core decomposition of a network, with
the purpose of (i) enabling the run-time computation of k-cores in ""live""
distributed systems and (ii) allowing the decomposition, over a set of
connected machines, of very large graphs, that cannot be hosted in a single
machine. Lower bounds on the algorithms complexity are given, and an exhaustive
experimental analysis on real-world graphs is provided.
"
591,"Performance evaluation of FD-SOI Mosfets for different metal gate work
  function","  Fully depleted (FD) Silicon on Insulator (SOI) metal oxide Field Effect
Transistor (MOSFET) Is the Leading Contender for Sun 65nm Regime. This paper
presents a study of effects of work functions of metal gate on the performance
of FD-SOI MOSFET. Sentaurus TCAD simulation tool is used to investigate the
effect of work function of gates ont he performance FDSOI MOSFET. Specific
channel length of the device that had been concentrated is 25nm. From
simulation we observed that by changing the work function of the metal gates of
FD-SOI MOSFET we can change the threshold voltage. Hence by using this
technique we can set the appropriate threshold voltage of FD-SOI MOSFET at same
voltage and we can decrease the leakage current, gate tunneling current and
short channel effects and increase drive current.
"
592,"Simulation and Performance Analysis of Adaptive Filtering Algorithms in
  Noise Cancellation","  Noise problems in signals have gained huge attention due to the need of
noise-free output signal in numerous communication systems. The principal of
adaptive noise cancellation is to acquire an estimation of the unwanted
interfering signal and subtract it from the corrupted signal. Noise
cancellation operation is controlled adaptively with the target of achieving
improved signal to noise ratio. This paper concentrates upon the analysis of
adaptive noise canceller using Recursive Least Square (RLS), Fast Transversal
Recursive Least Square (FTRLS) and Gradient Adaptive Lattice (GAL) algorithms.
The performance analysis of the algorithms is done based on convergence
behavior, convergence time, correlation coefficients and signal to noise ratio.
After comparing all the simulated results we observed that GAL performs the
best in noise cancellation in terms of Correlation Coefficient, SNR and
Convergence Time. RLS, FTRLS and GAL were never evaluated and compared before
on their performance in noise cancellation in terms of the criteria we
considered here.
"
593,Evolutionary Foundations of Mathematics,"  We propose a simple cognitive model where qualitative and quantitative com-
parisons enable animals to identify objects, associate them with their
properties held in memory and make naive inference. Simple notions like
equivalence re- lations, order relations are used. We then show that such
processes are at the root of human mathematical reasoning by showing that the
elements of totally ordered sets satisfy the Peano axioms. The process through
which children learn counting is then formalized. Finally association is
modeled as a Markov process leading to a stationary distribution.
"
594,"Access Control Mechanisms for Semantic Web services-A Discussion on
  Requirements & Future Directions","  Semantic Web is an open, distributed, and dynamic environment where access to
resources cannot be controlled in a safe manner unless the access decision
takes into account during discovery of web services. Security becomes the
crucial factor for the adoption of the semantic based web services. An access
control means that the users must fulfill certain conditions in order to gain
access over web services. Access control is important in both perspectives i.e.
legal and security point of view. This paper discusses important requirements
for effective access control in semantic web services which have been extracted
from the literature surveyed. I have also discussed open research issues in
this context, focusing on access control policies and models in this paper.
"
595,"Comparative analysis of the accuracy of the distance to the observed
  object for geometric methods","  The article presents a comparative analysis of the accuracy of the distance
to the observed object for geometric methods in noisy observations of
bearings-only information.
"
596,Impact of Limited Feedback on MIMO-OFDM Systems using Joint Beamforming,"  In multi input multi output antenna systems, beamforming is a technique for
guarding against the negative effects of fading. However, this technique
requires the transmitter to have perfect knowledge of the channel which is
often not available a priori. A solution to overcome this problem is to design
the beamforming vector using a limited number of feedback bits sent from the
receiver to the transmitter. In the case of limited feedback, the beamforming
vector is limited to lie in a codebook that is known to both the transmitter
and receiver.When the feedback is strictly limited, important issues are how to
quantize the information needed at the transmitter and how much improvement in
associated performance can be obtained as a function of the amount of feedback
available.In this paper channel quantization schema using simple approach to
codebook design (random vector quantization)is illustrated. Performance results
show that even with a few bits of feedback, performance can be close to that
with perfect channel knowledge at the transmitter.
"
597,"Design of Thin-Film-Transistor (TFT) arrays using current mirror
  circuits for Flat Panel Detectors (FPDs)","  We designed 4x4 matrix TFTs arrays using current mirror amplifiers.
Advantages of current mirror amplifiers are they need less requiring switches
and the conversion time is short. The TFTs arrays 4x4 matrix using current
mirror circuits have been fabricated and tested with success. The TFTs array
directly can process signals coming from 16 pixels in the same node. This
enables us to make the summation of the light intensities of close pixels
during a reading.
"
598,"Development of Active Pixel Photodiode Sensors for Gamma Camera
  Application","  We designed new photodiodes sensors including current mirror amplifiers.
These photodiodes have been fabricated using a CMOS 0.6 micrometers process
from Austria Micro System (AMS). The Photodiode areas are respectiveley 1mm x
1mm and 0.4mm x 0.4mm with fill factor 98 % and total chip area is 2 square
millimetres. The sensor pixels show a logarithmic response in illumination and
are capable of detecting very low blue light (less than 0.5 lux) . These
results allow to use our sensor in new Gamma Camera solid-state concept.
"
599,"A Novel Method for Calculating Demand Not Served for Transmission
  Expansion Planning","  Restructuring of the power market introduced demand uncertainty in
transmission expansion planning (TEP), which in turn also requires an accurate
estimation of demand not served (DNS). Unfortunately, the graph theory based
minimum-cut maximum-flow (MCMF) approach does not ensure that electrical laws
are followed. Nor can it be used for calculating DNS at individual buses. In
this letter, we propose a generalized load flow based methodology for
calculating DNS. This procedure is able to calculate simultaneously generation
not served (GNS) and wheeling loss (WL). Importantly, the procedure is able to
incorporate the effect of I2R losses, excluded in MCMF approach. Case study on
a 5-bus IEEE system shows the effectiveness of the proposed approach over
existing method.
"
600,"Book review: Katy B\""orner, Atlas of Science: Visualizing What We Know.
  Cambridge, MA/ London UK: The MIT Press, 2010","  Katy B\""orner has written a wonderful book about visualization that makes our
field of scientometrics accessible to much larger audiences. The book is to be
read in relation to the ongoing series of exhibitions entitled ""Places &
Spaces: Mapping Science"" currently touring the world. The book also provides
the scholarly background to the exhibitions. It celebrates scientometrics as
the discipline in the background that enables us to visualize the evolution of
knowledge as the acumen of human civilization.
"
601,A Knowledge base model for complex forging die machining,"  Recent evolutions on forging process induce more complex shape on forging
die. These evolutions, combined with High Speed Machining (HSM) process of
forging die lead to important increase in time for machining preparation. In
this context, an original approach for generating machining process based on
machining knowledge is proposed in this paper. The core of this approach is to
decompose a CAD model of complex forging die in geometric features.
Technological data and topological relations are aggregated to a geometric
feature in order to create machining features. Technological data, such as
material, surface roughness and form tolerance are defined during forging
process and dies design. These data are used to choose cutting tools and
machining strategies. Topological relations define relative positions between
the surfaces of the die CAD model. After machining features identification
cutting tools and machining strategies currently used in HSM of forging die,
are associated to them in order to generate machining sequences. A machining
process model is proposed to formalize the links between information imbedded
in the machining features and the parameters of cutting tools and machining
strategies. At last machining sequences are grouped and ordered to generate the
complete die machining process. In this paper the identification of geometrical
features is detailed. Geometrical features identification is based on machining
knowledge formalization which is translated in the generation of maps from STL
models. A map based on the contact area between cutting tools and die shape
gives basic geometrical features which are connected or not according to the
continuity maps. The proposed approach is illustrated by an application on an
industrial study case which was accomplished as part of collaboration.
"
602,"Relaxing Tight Frame Condition in Parallel Proximal Methods for Signal
  Restoration","  A fruitful approach for solving signal deconvolution problems consists of
resorting to a frame-based convex variational formulation. In this context,
parallel proximal algorithms and related alternating direction methods of
multipliers have become popular optimization techniques to approximate
iteratively the desired solution. Until now, in most of these methods, either
Lipschitz differentiability properties or tight frame representations were
assumed. In this paper, it is shown that it is possible to relax these
assumptions by considering a class of non necessarily tight frame
representations, thus offering the possibility of addressing a broader class of
signal restoration problems. In particular, it is possible to use non
necessarily maximally decimated filter banks with perfect reconstruction, which
are common tools in digital signal processing. The proposed approach allows us
to solve both frame analysis and frame synthesis problems for various noise
distributions. In our simulations, it is applied to the deconvolution of data
corrupted with Poisson noise or Laplacian noise by using (non-tight) discrete
dual-tree wavelet representations and filter bank structures.
"
603,Publicity of the intimate text (the blog studying and publication),"  One of the important problems of a modern society - communications. At all
readiness of this question both humanitarian, and engineering science, process
of transfer and information reception remains in the centre of attention of
researchers. The dialogue phenomenon in a network becomes the significant
factor of such attention. The fact of the publication of blogs and increasing
popularity of bloggers is connected, in our opinion, with an increasing
openness of a blog sphere (each record can be commented any user), and
accordingly, the Internet as a whole.
"
604,Re-thinking Enrolment in Identity Card Schemes,"  Many countries around the world have initiated national ID card programs in
the last decade. These programs are considered of strategic value to
governments due to its contribution in enhancing existing identity management
systems. Considering the total cost of such programs which goes up to billions
of dollars, the success in attaining their objectives is a crucial element in
the agendas of political systems in countries worldwide. Our experience in the
field shows that many of such projects have been challenged to deliver their
primary objectives of population enrolment, and therefore resulted in failing
to meet deadlines and keeping up with budgetary constraints. The purpose of
this paper is to explain the finding of a case study action research aimed to
introduce a new approach to how population are enrolled in national ID
programs. This is achieved through presenting a case study of a business
process reengineering initiative undertaken in the UAE national ID program. The
scope of this research is limited to the enrolment process within the program.
This article also intends to explore the possibilities of significant results
with the new proposed enrolment approach with the application of BPR. An
overview of the ROI study has been developed to illustrate such efficiencies.
"
605,"A Comparative Study Between a Micromechanical Cantilever Resonator and
  MEMS-based Passives for Band-pass Filtering Application","  Over the past few years, significant growth has been observed in using MEMS
based passive components in the RF microelectronics domain, especially in
transceiver components. This is due to some excellent properties of the MEMS
devices like low loss, excellent isolation etc. in the microwave frequency
domain where the on-chip passives normally tend to become leakier and degrades
the transceiver performance. This paper presents a comparative analysis between
MEMS-resonator based and MEMS-passives based band-pass filter configurations
for RF applications, along with their design, simulation, fabrication and
characterization. The filters were designed to have a center frequency of 455
kHz, meant for use as the intermediate frequency (IF) filter in superheterodyne
receivers. The filter structures have been fabricated in PolyMUMPs process, a
three-polysilicon layer surface micromachining process.
"
606,Characterization of 3D surface topography in 5-axis milling,"  Within the context of 5-axis free-form machining, CAM software offers various
ways of tool-path generation, depending on the geometry of the surface to be
machined. Therefore, as the manufactured surface quality results from the
choice of the machining strategy and machining parameters, the prediction of
surface roughness in function of the machining conditions is an important issue
in 5-axis machining. The objective of this paper is to propose a simulation
model of material removal in 5-axis based on the N-buffer method and
integrating the Inverse Kinematics Transformation. The tooth track is linked
with the velocity giving the surface topography resulting from actual machining
conditions. The model is assessed thanks to a series of sweeping over planes
according to various tool axis orientations and cutting conditions. 3D surface
topography analyses are performed through the new areal surface roughness
parameters proposed by recent standards.
"
607,"Optimisation de la taille de la s\'erie: illustration par un cas
  industriel de sous-traitance m\'ecanique","  Reducing costs of manufactured products is one of the key issues of
companies. Bar turning companies (mechanical subcontracting companies) are
faced with the following dilemma: use a pull strategy or use a push strategy.
Instinctively these companies produce more than demand required by customers.
This strategy allows them to respond to requests forecasts and reduce their
cost of changeover time. These companies make a bet on sales opportunities and
think to realize an additional profit. We have tried to find in this study to
provide elements to know the limits of this strategy. Our proposal focuses on
developing a model to support the decision taking into account the mix of
opportunities, economic constraints and mean constraints. This model features
the particular importance of high rates of ownership and the risk of not
selling.
  R\'eduire les co\^uts de revient des produits fabriqu\'es est une des
probl\'ematiques essentielles des entreprises d'aujourd'hui. Les entreprises de
d\'ecolletage (entreprises de sous-traitance m\'ecanique) sont confront\'ees au
dilemme suivant : produire juste la demande client ou produire plus.
Instinctivement ces entreprises, dont les temps de changement de s\'erie sont
\'elev\'es, cherchent \`a produire plus que la demande exig\'ee par le client.
Cette strat\'egie leur permet de r\'epondre \`a des demandes pr\'evisionnelles
et r\'eduire ainsi le co\^ut de revient des produits. Ces entreprises
r\'ealisent un pari sur les opportunit\'es de vente et pensent r\'ealiser un
gain suppl\'ementaire en r\'ealisant des stocks. Nous avons cherch\'e dans
cette \'etude \`a fournir des \'el\'ements de d\'ecision pour conna\^itre les
limites de cette r\`egle de gestion. Notre proposition porte sur le
d\'eveloppement d'un mod\`ele d'aide \`a la d\'ecision prenant en
consid\'eration le mixte entre opportunit\'es commerciales, contraintes
\'economiques et contraintes de moyen. Ce mod\`ele souligne l'importance
particuli\`ere du taux de possession et du risque de non vente.
"
608,A new DFM approach to combine machining and additive manufacturing,"  Design For Manufacturing (DFM) approaches aim to integrate manufacturability
aspects during the design stage. Most of DFM approaches usually consider only
one manufacturing process, but products competitiveness may be improved by
designing hybrid modular products, in which products are seen as 3-D puzzles
with modules realized aside by the best manufacturing process and further
gathered. A new DFM system is created in order to give quantitative information
during the product design stage of which modules will benefit in being machined
and which ones will advantageously be realized by an additive process (such as
Selective Laser Sintering or laser deposition). A methodology for a
manufacturability evaluation in case of a subtractive or an additive
manufacturing process is developed and implemented in a CAD software. Tests are
carried out on industrial products from automotive industry.
"
609,"Adaptive Monte Carlo applied to uncertainty estimation in a five axis
  machine tool link errors identification","  Knowledge of a machine tool axis to axis location errors allows compensation
and correcting actions to be taken to enhance its volumetric accuracy. Several
procedures exist, involving either lengthy individual test for each geometric
error or faster single tests to identify all errors at once. This study focuses
on the closed kinematic Cartesian chain method which uses a single setup test
to identify the eight link errors of a five axis machine tool. The
identification is based on volumetric error measurements for different poses
with a non-contact measuring instrument called CapBall, developed in house. In
order to evaluate the uncertainty on each identified error, a multi-output
Monte Carlo approach is implemented. Uncertainty sources in the measurement and
identification chain - such as sensors output, machine drift and frame
transformation uncertainties - can be included in the model and propagated to
the identified errors. The estimated uncertainties are finally compared to
experimental results to assess the method. It shows that the effect of the
drift, a disturbance, must be simulated as a function of time the Monte Carlo
approach. The machine drift is found to be an important uncertainty in sources
for the machine tested.
"
610,The impact of energy constraints on the medium access,"  Contemporary mobile devices are battery powered and due to their shrinking
size and increasing complexity operate on a tight energy budget. Thus, energy
consumption is becoming one of the major concerns regarding the current and
upcoming wireless communication systems. On the other hand, the available
bandwidth resources are limited and modern applications are throughput
demanding, leading thus to strong competition for the medium. In this
direction, we consider a stochastic contention based medium access scheme,
where the devices may choose to turn off for some time in order to save energy.
We perform an analysis for a slotted ALOHA scenario and we show that the energy
constraints, if properly exploited, may reduce contention for the medium. Our
results give valuable insights on the energy--throughput tradeoff for any
contention based system.
"
611,A novel methodology for antenna design and optimization: Variable Zo,"  This paper describes ""Variable Zo,"" a novel and proprietary approach to
antenna design and optimization. The new methodology is illustrated by applying
it to the design of a resistively-loaded bowtie antenna and to two broadband
Yagi-Uda arrays. Variable Zo is applicable to any antenna design or
optimization methodology. Using it will result in generally better antenna
designs across any user-specified set of performance objectives.
"
612,Cloud Computing Future Framework for e-management of NGO's,"  Cloud computing is an emerging new computing paradigm for delivering
computing services. This computing approach relies on a number of existing
technologies, e.g., the Internet, virtualization, grid computing, Web services,
etc. Cloud Computing aims to provide scalable and inexpensive on-demand
computing infrastructures with good quality of service levels. It represents a
shift away from computing as a product that is purchased, to computing as a
service that is delivered to consumers from the cloud. It helps an organization
in saving costs and creating new business opportunities.This paper provides a
framework, Education Cloud for the e- management of NGO's. The Education Cloud
can transform a nonprofit, or an entire sector of nonprofits, achieves its
mission and creates lasting impact in its communities. This paper also presents
the case study of Kalgidhar trust, Baru Sahib, Himachal Pradesh, NGO which is
using the education as the tool to solve the social issues.
"
613,"Feed drive modelling for the simulation of tool path tracking in
  multi-axis High Speed Machining","  Within the context of High Speed Machining, it is essential to manage the
trajectory generation to achieve both high surface quality and high
productivity. As feed drives are one part of the set Machine tool - Numerical
Controller, it is necessary to improve their performances to optimize feed
drive dynamics during trajectory follow up. Hence, this paper deals with the
modelling of the feed drive in the case of multi axis machining. This model can
be used for the simulation of axis dynamics and tool-path tracking to tune
parameters and optimize new frameworks of command strategies. A procedure of
identification based on modern NC capabilities is presented and applied to
industrial HSM centres. Efficiency of this modelling is assessed by
experimental verifications on various representative trajectories. After
implementing a Generalized Predictive Control, reliable simulations are
performed thanks to the model. These simulations can then be used to tune
parameters of this new framework according to the tool-path geometry.
"
614,"Evaluation of servo, geometric and dynamic error sources on five axis
  high-speed machine tool","  Many sources of errors exist in the manufacturing process of complex shapes.
Some approximations occur at each step from the design geometry to the machined
part. The aim of the paper is to present a method to evaluate the effect of
high speed and high dynamic load on volumetric errors at the tool center point.
The interpolator output signals and the machine encoder signals are recorded
and compared to evaluate the contouring errors resulting from each axis
follow-up error. The machine encoder signals are also compared to the actual
tool center point position as recorded with a non-contact measuring instrument
called CapBall to evaluate the total geometric errors. The novelty of the work
lies in the method that is proposed to decompose the geometric errors in two
categories: the quasi-static geometric errors independent from the speed of the
trajectory and the dynamic geometric errors, dependent on the programmed feed
rate and resulting from the machine structure deflection during the
acceleration of its axes. The evolution of the respective contributions for
contouring errors, quasi-static geometric errors and dynamic geomet- ric errors
is experimentally evaluated and a relation between programmed feed rate and
dynamic errors is highlighted.
"
615,"UWB Array Design Using Variable Zo Technology and Central Force
  Optimization","  This note applies Variable Zo technology to the design of an Ultra Wideband
(UWB) Yagi-Uda array optimized using Central Force Optimization. Variable Zo is
a novel and proprietary approach to antenna design and optimization that treats
the feed system characteristic impedance, Zo, as a design variable instead of a
fixed design parameter as is traditionally done. Variable Zo is applicable to
any antenna design or optimization methodology, and using it will generally
produce better antenna designs across any user-specified set of performance
objectives.
"
616,The Impact of Information Technology in Nigeria's Banking Industry,"  Today, information technology (IT) has become a key element in economic
development and a backbone of knowledge-based economies in terms of operations,
quality delivery of services and productivity of services. Therefore, taking
advantage of information technologies (IT) is an increasing challenge for
developing countries. There is now growing evidence that Knowledge-driven
innovation is a decisive factor in the competitiveness of nations, industries,
organizations and firms. Organizations like the banking sector have benefited
substantially from e-banking, which is one among the IT applications for
strengthening the competitiveness. This paper presents the current trend in the
application of IT in the banking industries in Nigeria and gives an insight
into how quality banking has been enhanced via IT. The paper further reveals
that the deployment of IT facilities in the Nigerian Banking industry has
brought about fundamental changes in the content and quality of banking
business in the country. This analysis and clarification of how Nigerian Banks
have used IT to reengineer their operations is detailed through literature
review and observation. Three categories of variables that relate to the use
and implementation of information technology devices were considered in this
paper. These include the nature and degree of adoption of innovative
technologies; degree of utilization of the identified technologies; and the
impact of the adoption of IT devices on the bank operations.
"
617,System Support for Managing Invalid Bindings,"  Context-aware adaptation is a central aspect of pervasive computing
applications, enabling them to adapt and perform tasks based on contextual
information. One of the aspects of context-aware adaptation is reconfiguration
in which bindings are created between application component and remote services
in order to realize new behaviour in response to contextual information.
Various research efforts provide reconfiguration support and allow the
development of adaptive context-aware applications from high-level
specifications, but don't consider failure conditions that might arise during
execution of such applications, making bindings between application and remote
services invalid. To this end, we propose and implement our design approach to
reconfiguration to manage invalid bindings. The development and modification of
adaptive context-aware applications is a complex task, and an issue of an
invalidity of bindings further complicates development efforts. To reduce the
development efforts, our approach provides an application-transparent solution
where the issue of the invalidity of bindings is handled by our system,
Policy-Based Contextual Reconfiguration and Adaptation (PCRA), not by an
application developer. In this paper, we present and describe our approach to
managing invalid bindings and compare it with other approaches to this problem.
We also provide performance evaluation of our approach.
"
618,Knowledge Audit Framework,"  KAF consists of a process and some templates to guide the planning and
execution of audits of knowledge resources, with emphasis on sharing. KAF is
based on methodological blueprint provided by the Data Audit Framework
(DAF)conceived by the JISC-funded DAFD project.KAF enables organisations to
find out what knowledge resources are associated with the project, and how they
are shared.KAF is available in two versionsKAF-g (generic, domain independent)
KAF-se (targets systems enegineering knowledge)
"
619,A New System Architecture for Pervasive Computing,"  We present new system architecture, a distributed framework designed to
support pervasive computing applications. We propose a new architecture
consisting of a search engine and peripheral clients that addresses issues in
scalability, data sharing, data transformation and inherent platform
heterogeneity. Key features of our application are a type-aware data transport
that is capable of extract data, and present data through handheld devices (PDA
(personal digital assistant), mobiles, etc). Pervasive computing uses web
technology, portable devices, wireless communications and nomadic or ubiquitous
computing systems. The web and the simple standard HTTP protocol that it is
based on, facilitate this kind of ubiquitous access. This can be implemented on
a variety of devices - PDAs, laptops, information appliances such as digital
cameras and printers. Mobile users get transparent access to resources outside
their current environment. We discuss our system's architecture and its
implementation. Through experimental study, we show reasonable performance and
adaptation for our system's implementation for the mobile devices.
"
620,Classification of Emergency Scenarios,"  In most of today's emergency scenarios information plays a crucial role.
Therefore, information has to be constantly collected and shared among all
rescue team members and this requires new innovative technologies. In this
paper a classification of emergency scenarios is presented, describing their
special characteristics and common strategies employed by rescue units to
handle them. Based on interviews with professional firefighters, requirements
for new systems are listed. The goal of this article is to support developers
designing new systems by providing them a deeper look into the work of first
responders.
"
621,Modeling Smart Grid using Generalized Stochastic Petri Net,"  Building smart grid for power system is a major challenge for safe, automated
and energy efficient usage of electricity. The full implementation of the smart
grid will evolve over time. However, before a new set of infrastructures are
invested to build the smart grid, proper modeling and analysis is needed to
avoid wastage of resources. Modeling also helps to identify and prioritize
appropriate systems parameters. In this paper, an all comprehensive model of
smart grid have been proposed using Generalized Stochastic Petri Nets (GSPN).
The model is used to analyze the constraints and deliverables of the smart
power grid of future.
"
622,Simplicity Effects in the Experience of Near-Miss,"  Near-miss experiences are one of the main sources of intense emotions.
Despite people's consistency when judging near-miss situations and when
communicating about them, there is no integrated theoretical account of the
phenomenon. In particular, individuals' reaction to near-miss situations is not
correctly predicted by rationality-based or probability-based optimization. The
present study suggests that emotional intensity in the case of near-miss is in
part predicted by Simplicity Theory.
"
623,Emotion in good luck and bad luck: predictions from simplicity theory,"  The feeling of good or bad luck occurs whenever there is an emotion contrast
between an event and an easily accessible counterfactual alternative. This
study suggests that cognitive simplicity plays a key role in the human ability
to experience good and bad luck after the occurrence of an event.
"
624,A structural model of intuitive probability,"  Though the ability of human beings to deal with probabilities has been put
into question, the assessment of rarity is a crucial competence underlying much
of human decision-making and is pervasive in spontaneous narrative behaviour.
This paper proposes a new model of rarity and randomness assessment, designed
to be cognitively plausible. Intuitive randomness is defined as a function of
structural complexity. It is thus possible to assign probability to events
without being obliged to consider the set of alternatives. The model is tested
on Lottery sequences and compared with subjects' preferences.
"
625,"Quality Evaluation of Conceptual Level Object Multidimensional Data
  Model","  The advancement of technology facilitates explosive growth of mobile usage in
the last decade. Numerous applications have been developed to support its
usage. However, gap in technology exists in obtaining correct and trusted
values for evaluation indexes of the precise amount of usage. The claims of
loss in revenue by the service providers could be more due to unexpected
behaviour of the hardware. A similar mistrust is often observed in the users of
the services. A trustworthy subscription scheme is in demand for consumers
whereas revenue needs to be assured of the service providers. Multiple
Authorizations by Multiple Owners (MAMO) has already been introduced as a
technology to build trust in the third party billing system. In this paper,
MAMO is extended to ensure trustworthiness of the parameters for subscription.
Along with call transaction data are reconciled to assure the proper revenue
generation.
"
626,Food Redistribution as Optimization,"  In this paper we study the simultaneous problems of food waste and hunger in
the context of the possible solution of food (waste) rescue and redistribution.
To this end, we develop an empirical model that can be used in Monte Carlo
simulations to study the dynamics of the underlying problem. Our model's
parameters are derived from a unique data set provided by a large food bank and
food rescue organization in north central Colorado. We find that food supply is
a non-parametric heavy-tailed process that is well-modeled with an extreme
value peaks-over-threshold model. Although the underlying process is
stochastic, the basic approach of food rescue and redistribution appears to be
feasible both at small and large scales. The ultimate efficacy of this model is
intimately tied to the rate at which food expires and hence the ability to
preserve and quickly transport and redistribute food. The cost of the
redistribution is tied to the number and density of participating suppliers,
and costs can be reduced (and supply increased) simply by recruiting additional
donors to participate. Our results show that with sufficient funding and
manpower, a significant amount of food can be rescued from the waste stream and
used to feed the hungry.
"
627,"A Novel VSWR-Protected and Controllable CMOS Class E Power Amplifier for
  Bluetooth Applications","  This paper describes the design of a differential class-E PA for Bluetooth
applications in 0.18um CMOS technology with load mismatch protection and power
control features. The breakdown induced by load mismatch can be avoided by
attenuating the RF power to the final stage during over voltage conditions.
Power control is realized by means of ""open loop"" techniques to regulate the
power supply voltage, and a novel controllable bias network with temperature
compensated is proposed, which allows a moderate power control slope (dB/V) to
be achieved. Post-layout Simulation results show that the level of output power
can be controlled in 2dBm steps; especially the output power in every step is
quite insensitive to temperature variations.
"
628,Complexity,"  The term complexity derives etymologically from the Latin plexus, which means
interwoven. Intuitively, this implies that something complex is composed by
elements that are difficult to separate. This difficulty arises from the
relevant interactions that take place between components. This lack of
separability is at odds with the classical scientific method - which has been
used since the times of Galileo, Newton, Descartes, and Laplace - and has also
influenced philosophy and engineering. In recent decades, the scientific study
of complexity and complex systems has proposed a paradigm shift in science and
philosophy, proposing novel methods that take into account relevant
interactions.
"
629,"Framework to Integrate Business Intelligence and Knowledge Management in
  Banking Industry","  In this digital age organizations depend upon the technologies to provide
customer-centric solutions by understanding well about their customers'
behaviour and continuously improving business process of the organization.
Business intelligence (BI) applications will play a vital role at this stage by
discovering the knowledge hidden in internal as well as external sources. On
the other hand, Knowledge Management (KM) will enhance the organisations
performance by providing collaborative tools to learn, create and share the
knowledge among the employees. The main intention of the BI is to enhance the
employees' knowledge with information that allows them to make decisions to
achieve its organisational strategies. However only twenty percent of data
exist in structured form, majority of banks knowledge is in unstructured or
minds of its employees. Organizations are needed to integrate KM with Knowledge
which is discovered from data and information. The purpose of this paper is to
discuss the need of business insiders in the process of knowledge discovery and
distribution, to make BI more relevant to business of the bank. We have also
discussed about the BI/KM applications in banking industry and provided a
framework to integrate BI and KM in banking industry.
"
630,The Aware Cricket Ground,"  The most profound technologies are those that disappear. They weave
themselves into fabrics of everyday life until they are indistinguishable from
it [1]. This research work is a mere effort for automated decision making
during sports of most common interest leveraging ubiquitous computing.
Primarily cricket has been selected for the first implementation of the idea. A
positioning system is used for locating the objects moving in the field. Main
objectives of the research are to help achieve the following goals. 1) Make
Decisions where human eye can make error due to human limitations. 2) Simulate
the Match activity during and after the game in a 3D computerized Graphics
system. 3) Make various types of game and performance analysis of a certain
team or a player.
"
631,Outsourcing Competence,"  The topic of this paper, competences needed for outsourcing, is organized by
first providing a generic competence scheme, which is subsequently instantiated
to the area of sourcing and outsourcing. Sourcing and outsourcing are
positioned as different areas of activity, neither one of which is subsumed
under the other one. It is argued that competences relevant for outsourcing are
mainly community based rather than evidence based. Subjective ability and
objective ability are distinguished as categories, together making up ability,
which are distinct but not necessarily disjoint from competence. Conjectural
ability is introduced as a form of subjective ability. A person's competence
profile includes competences as well as abilities, including subjective ones.
Competence assessment and acquisition as well as the impact of assessed
competence on practical work is described. The analysis of competence and
ability thus developed is used as standpoint from which to extract a
specification of an audience for a theory of outsourcing, yet to be written.
Moreover, it allows to formulate requirements for and in preparation of the
development of an outsourcing theory. Formulating these requirements is done
under the assumption that a person's awareness of a theory of outsourcing is
expected to strengthen that person's outsourcing competence profile.
"
632,"Towards an interoperable information infrastructure providing decision
  support for genomic medicine","  Genetic dispositions play a major role in individual disease risk and
treatment response. Genomic medicine, in which medical decisions are refined by
genetic information of particular patients, is becoming increasingly important.
Here we describe our work and future visions around the creation of a
distributed infrastructure for pharmacogenetic data and medical decision
support, based on industry standards such as the Web Ontology Language (OWL)
and the Arden Syntax.
"
633,A binary noisy channel to model errors in printing process,"  To model printing noise a binary noisy channel and a set of controlled gates
are introduced. The channel input is an image created by a halftoning algorithm
and its output is the printed picture. Using this channel robustness to noise
between halftoning algorithms can be studied. We introduced relative entropy to
describe immunity of the algorithm to noise and tested several halftoning
algorithms.
"
634,Fault Tolerant Matrix Pencil Method for Direction of Arrival Estimation,"  Continuing to estimate the Direction-of-arrival (DOA) of the signals
impinging on the antenna array, even when a few elements of the underlying
Uniform Linear Antenna Array (ULA) fail to work will be of practical interest
in RADAR, SONAR and Wireless Radio Communication Systems. This paper proposes a
new technique to estimate the DOAs when a few elements are malfunctioning. The
technique combines Singular Value Thresholding (SVT) based Matrix Completion
(MC) procedure with the Direct Data Domain (D^3) based Matrix Pencil (MP)
Method. When the element failure is observed, first, the MC is performed to
recover the missing data from failed elements, and then the MP method is used
to estimate the DOAs. We also, propose a very simple technique to detect the
location of elements failed, which is required to perform MC procedure. We
provide simulation studies to demonstrate the performance and usefulness of the
proposed technique. The results indicate a better performance, of the proposed
DOA estimation scheme under different antenna failure scenarios.
"
635,Biological Computation as the Revolution of Complex Engineered Systems,"  Provided that there is no theoretical frame for complex engineered systems
(CES) as yet, this paper claims that bio-inspired engineering can help provide
such a frame. Within CES bio-inspired systems play a key role. The disclosure
from bio-inspired systems and biological computation has not been sufficiently
worked out, however. Biological computation is to be taken as the processing of
information by living systems that is carried out in polynomial time, i.e.,
efficiently; such processing however is grasped by current science and research
as an intractable problem (for instance, the protein folding problem). A remark
is needed here: P versus NP problems should be well defined and delimited but
biological computation problems are not. The shift from conventional
engineering to bio-inspired engineering needs bring the subject (or problem) of
computability to a new level. Within the frame of computation, so far, the
prevailing paradigm is still the Turing-Church thesis. In other words,
conventional engineering is still ruled by the Church-Turing thesis (CTt).
However, CES is ruled by CTt, too. Contrarily to the above, we shall argue here
that biological computation demands a more careful thinking that leads us
towards hypercomputation. Bio-inspired engineering and CES thereafter, must
turn its regard toward biological computation. Thus, biological computation can
and should be taken as the ground for engineering complex non-linear systems.
Biological systems do compute in terms of hypercomputation, indeed. If so, then
the focus is not algorithmic or computational complexity but
computation-beyond-the-Church-Turing-barrier. We claim that we need a new
computational theory that encompasses biological processes wherein the
Turing-Church thesis is but a particular case.
"
636,An Efficient Approach towards Mitigating Soft Errors Risks,"  Smaller feature size, higher clock frequency and lower power consumption are
of core concerns of today's nano-technology, which has been resulted by
continuous downscaling of CMOS technologies. The resultant 'device shrinking'
reduces the soft error tolerance of the VLSI circuits, as very little energy is
needed to change their states. Safety critical systems are very sensitive to
soft errors. A bit flip due to soft error can change the value of critical
variable and consequently the system control flow can completely be changed
which leads to system failure. To minimize soft error risks, a novel
methodology is proposed to detect and recover from soft errors considering only
'critical code blocks' and 'critical variables' rather than considering all
variables and/or blocks in the whole program. The proposed method shortens
space and time overhead in comparison to existing dominant approaches.
"
637,The Axiomatic Foundation of Space in GFO,"  Space and time are basic categories of any top-level ontology. They are
fundamental assumptions for the mode of existence of those individuals which
are said to be in space and time. In the present paper the ontology of space in
the General Formal Ontology (GFO) is expounded. This ontology is represented as
a theory BT (Brentano Theory), which is specified by a set of axioms formalized
in first-order logic. This theory uses four primitive relations: SReg(x) (x is
space region), spart(x, y) (x is spatial part of y), sb(x, y) (x is spatial
boundary of y), and scoinc(x, y) (x and y spatially coincide). This ontology is
inspired by ideas of Franz Brentano. The investigation and exploration of Franz
Brentano's ideas on space and time began about twenty years ago by work of R.M.
Chisholm, B. Smith and A. Varzi. The present paper takes up this line of
research and makes a further step in establishing an ontology of space which is
based on rigorous logical methods and on principles of the new philosophical
approach of integrative realism.
"
638,Generation of Test Vectors for Sequential Cell Verification,"  For Application Specific Integrated Circuits (ASIC) and System-on-Chip (SOC)
designs, Cell - Based Design (CBD) is the most prevalent practice as it
guarantees a shorter design cycle, minimizes errors and is easier to maintain.
In modern ASIC design, standard cell methodology is practiced with sizable
libraries of cells, each containing multiple implementations of the same logic
functionality, in order to give the designer differing options based on area,
speed or power consumption. For such library cells, thorough verification of
functionality and timing is crucial for the overall success of the chip, as
even a small error can prove fatal due to the repeated use of the cell in the
design. Both formal and simulation based methods are being used in the industry
for cell verification. We propose a method using the latter approach that
generates an optimized set of test vectors for verification of sequential
cells, which are guaranteed to give complete Single Input Change transition
coverage with minimal redundancy. Knowledge of the cell functionality by means
of the State Table is the only prerequisite of this procedure.
"
639,Gas turbine diagnostic system,"  The creation of the systems models is very actual at present time, because it
allow to simulate the work of some complex equipment without any additional
spends. The given model of gas turbine is allowed to test and optimize the
software for gas turbine automation systems, study station personal, like
operators and engineers and will be useful for diagnostics and prediction tasks
to analyze the efficiency of the gas turbine.
"
640,"A Stochastic Net Model for Controlling Bullwhip Effect in Virtual
  Multi-Tier Retail Network","  Supply Chain operation is an integrated business process starting from
primary supplier to end user and the process produce products, services and
information. A successful chain will explore technology, lean operations, and
quality management by adding value for customers and stakeholders. It is a
strategic alliance among the partnering enterprises without geographical
boundary. Every chain has its own unique set of market demands and operating
challenges. Retailing is one such service domain of Supply Chain vulnerable to
bullwhip effects. Demand uncertainty is one of the root causes of Bullwhip
effects. This paper calls for modeling of a demand driven multi-tier stochastic
Retail Chain to work against the Bullwhip effect. The proposed model of the
operational chain will ensure significant return of share to the retailer
through the sophisticated transaction management, real-time inventory
management and the ability to track all inventory movements.
"
641,"Quantum/Relativistic Computation of Security and Efficiency of
  Electrical Power System for a Day-Ahead","  An algorithm for Electric Power System (EPS) quantum/relativistic security
and efficiency computation for a day-ahead via perturbative renormalization of
the EPS, finding the computation flowcharts, verification and validation is
built in this paper.
"
642,Solar Power Systems Web Monitoring,"  All over the world the peak demand load is increasing and the load factor is
decreasing year-by-year. The fossil fuel is considered insufficient thus solar
energy systems are becoming more and more useful, not only in terms of
installation but monitoring of these systems is very crucial. Monitoring
becomes very important when there are a large number of solar panels.
Monitoring would allow early detection if the output falls below required level
or one of the solar panel out of 1000 goes down. In this study the target is to
monitor and control a developed solar panel by using available internet
foundation. This web-enabled software will provide more flexibility over the
system such as transmitting data from panel to the host computer and
disseminating information to relevant stake holders barring any geographical
barrier. The software would be built around web server with dynamic HTML and
JAVA, this paper presents the preliminary design of the proposed system.
"
643,Architecture and Design of Medical Processor Units for Medical Networks,"  This paper introduces analogical and deductive methodologies for the design
medical processor units (MPUs). From the study of evolution of numerous earlier
processors, we derive the basis for the architecture of MPUs. These specialized
processors perform unique medical functions encoded as medical operational
codes (mopcs). From a pragmatic perspective, MPUs function very close to CPUs.
Both processors have unique operation codes that command the hardware to
perform a distinct chain of subprocesses upon operands and generate a specific
result unique to the opcode and the operand(s). In medical environments, MPU
decodes the mopcs and executes a series of medical sub-processes and sends out
secondary commands to the medical machine. Whereas operands in a typical
computer system are numerical and logical entities, the operands in medical
machine are objects such as such as patients, blood samples, tissues, operating
rooms, medical staff, medical bills, patient payments, etc. We follow the
functional overlap between the two processes and evolve the design of medical
computer systems and networks.
"
644,"New noise-based logic representations to avoid some problems with time
  complexity","  Instantaneous noise-based logic can avoid time-averaging, which implies
significant potential for low-power parallel operations in
beyond-Moore-law-chips. However, the universe (uniform superposition) will be
zero with high probability (non-zero with exponentially low probability) in the
random-telegraph-wave representation thus the operations with the universe
would require exponential time-complexity. To fix this deficiency, we modify
the amplitudes of the signals of the L and H states and achieve an exponential
speedup compared to the old situation. Another improvement concerns the
identification of a single product (hyperspace) state. We introduce a time
shifted noise-based logic, which is constructed by shifting each reference
signal with a small time delay. This modification implies an exponential
speedup of single hyperspace vector identification compared to the former case
and it requires the same, O(N) complexity as in quantum computing.
"
645,"Using Scalp Electrical Biosignals to Control an Object by Concentration
  and Relaxation Tasks: Design and Evaluation","  In this paper we explore the use of electrical biosignals measured on scalp
and corresponding to mental relaxation and concentration tasks in order to
control an object in a video game. To evaluate the requirements of such a
system in terms of sensors and signal processing we compare two designs. The
first one uses only one scalp electroencephalographic (EEG) electrode and the
power in the alpha frequency band. The second one uses sixteen scalp EEG
electrodes and machine learning methods. The role of muscular activity is also
evaluated using five electrodes positioned on the face and the neck. Results
show that the first design enabled 70% of the participants to successfully
control the game, whereas 100% of the participants managed to do it with the
second design based on machine learning. Subjective questionnaires confirm
these results: users globally felt to have control in both designs, with an
increased feeling of control in the second one. Offline analysis of face and
neck muscle activity shows that this activity could also be used to distinguish
between relaxation and concentration tasks. Results suggest that the
combination of muscular and brain activity could improve performance of this
kind of system. They also suggest that muscular activity has probably been
recorded by EEG electrodes.
"
646,"A Novel Approach for Periodic Assessment of Business Process
  Interoperability","  Business collaboration networks provide collaborative organizations a
favorable context for automated business process interoperability. This paper
aims to present a novel approach for assessing interoperability of process
driven services by considering the three main aspects of interoperation:
potentiality, compatibility and operational performance. It presents also a
software tool that supports the proposed assessment method. In addition to its
capacity to track and control the evolution of interoperation degree in time,
the proposed tool measures the required effort to reach a planned degree of
interoperability. Public accounting of financial authority is given as an
illustrative case study of interoperability monitoring in public collaboration
network.
"
647,"Reference Model for Performance Management in Service-Oriented Virtual
  Organization Breeding Environments","  Performance management (PM) is a key function of virtual organization (VO)
management. A large set of PM indicators has been proposed and evaluated within
the context of virtual breeding environments (VBEs). However, it is currently
difficult to describe and select suitable PM indicators because of the lack of
a common vocabulary and taxonomies of PM indicators. Therefore, there is a need
for a framework unifying concepts in the domain of VO PM. In this paper, a
reference model for VO PM is presented in the context of service-oriented VBEs.
In the proposed reference model, both a set of terms that could be used to
describe key performance indicators, and a set of taxonomies reflecting various
aspects of PM are proposed. The proposed reference model is a first attempt and
a work in progress that should not be supposed exhaustive.
"
648,Social Protocols for Agile Virtual Teams,"  Despite many works on collaborative networked organizations (CNOs), CSCW,
groupware, workflow systems and social networks, computer support for virtual
teams is still insufficient, especially support for agility, i.e. the
capability of virtual team members to rapidly and cost efficiently adapt the
way they interact to changes. In this paper, requirements for computer support
for agile virtual teams are presented. Next, an extension of the concept of
social protocol is proposed as a novel model supporting agile interactions
within virtual teams. The extended concept of social protocol consists of an
extended social network and a workflow model.
"
649,"Modeling Virtual Organization Architecture with the Virtual Organization
  Breeding Methodology","  While Enterprise Architecture Modeling (EAM) methodologies become more and
more popular, an EAM methodology tailored to the needs of virtual organizations
(VO) is still to be developed. Among the most popular EAM methodologies, TOGAF
has been chosen as the basis for a new EAM methodology taking into account
characteristics of VOs presented in this paper. In this new methodology,
referred as Virtual Organization Breeding Methodology (VOBM), concepts
developed within the ECOLEAD project, e.g. the concept of Virtual Breeding
Environment (VBE) or the VO creation schema, serve as fundamental elements for
development of VOBM. VOBM is a generic methodology that should be adapted to a
given VBE. VOBM defines the structure of VBE and VO architectures in a
service-oriented environment, as well as an architecture development method for
virtual organizations (ADM4VO). Finally, a preliminary set of tools and methods
for VOBM is given in this paper.
"
650,"Enhancing Information Systems Security in Educational Organizations in
  KSA through proposing security model","  It is well known that technology utilization is not restricted for one sector
than the other anymore, Educational organizations share many parts of their
information systems with commercial organizations. In this paper we will try to
identify the main characteristics of information systems in educational
organizations, then we will propose a model of two parts to enhance the
information systems security, the first part of the model will handle the
policy and laws of the information system, the second part will provide a
technical approach on how to audit and subsequently maintain the security of
information system.
"
651,Multimedia-based Medicinal Plants Sustainability Management System,"  Medicinal plants are increasingly recognized worldwide as an alternative
source of efficacious and inexpensive medications to synthetic
chemo-therapeutic compound. Rapid declining wild stocks of medicinal plants
accompanied by adulteration and species substitutions reduce their efficacy,
quality and safety. Consequently, the low accessibility to and
non-affordability of orthodox medicine costs by rural dwellers to be healthy
and economically productive further threaten their life expectancy. Finding
comprehensive information on medicinal plants of conservation concern at a
global level has been difficult. This has created a gap between computing
technologies' promises and expectations in the healing process under
complementary and alternative medicine. This paper presents the design and
implementation of a Multimedia-based Medicinal Plants Sustainability Management
System addressing these concerns. Medicinal plants' details for designing the
system were collected through semi-structured interviews and databases. Unified
Modelling Language, Microsoft-Visual-Studio.Net, C#3.0,
Microsoft-Jet-Engine4.0, MySQL, Loquendo Multilingual Text-to-Speech Software,
YouTube, and VLC Media Player were used. Keywords: Complementary and
Alternative Medicine, conservation, extinction, medicinal plant, multimedia,
phytoconstituents, rural dwellers
"
652,Two Squares of Opposition: for Analytic and Synthetic Propositions,"  In the paper I prove that there are two squares of opposition. The
unconventional one is built up for synthetic propositions. There a, i are
contrary, a, o (resp. e, i) are contradictory, e, o are subcontrary, a, e
(resp. i, o) are said to stand in the subalternation.
"
653,"On an Approach to the Design of a Logical Model of Innovation Project
  Data","  Questions concerning the development of a logical model of innovation project
data, as well as those concerning the design of information systems for
decision-making support in the management of innovation projects, are
discussed.
"
654,"Being, space and time in the Web","  The Web initially emerged as an ""antidote"" to accumulated scientific
knowledge since it enables global representation and communication with minimum
costs. Its gigantic scale and interdependence incommode our ability to find
relevant information and develop trustworthy contexts. It is time for science
to compensate by providing an epistemological ""antidote"" to Web issues.
Philosophy should be in the front line by forming the salient questions and
analysis. The scope of our research is to provide a theory about the Web being
that will bridge philosophical thinking and engineering. We analyze existence
and spatiotemporality in the Web and how it transforms the traditional
actualities. The Web space is specified by incoming and outgoing links. The
primordial role of visiting durations in Web's existence is approximated by
Bergsonian time. The physical space becomes more discoverable. The human
activity can be asynchronous, synchronous and continuous. Networked individuals
operate in a flexible and spatially dispersed environment. The resulting issues
concern the self-determination of a being and the way in which the Web could be
a free and open platform for innovation and participation.
"
655,Employees Adoption of E-Procurement System: An Empirical Study,"  Today, organizations are investing a lot in their IT infrastructure and
reengineering their business processes by digitizing firms. If organizational
employees will not optimum utilize its IT infrastructure, the productivity gain
reduced enormously. In Uttarakhand e-procurement system implemented by public
sector under e-governance integrated mission mode projects. So, there is need
to find the determinants which influence employee's adoption and uses of
e-procurement systems. This research study assesses the organizational and
individual determinants that influence the use of e-procurement system in
Uttarakhand public sector. This study provides managers with the valuable
information to take intervention programs to achieve greater acceptance and
usage of e-procurement system. Data collected for this study by the means of a
survey conducted in Uttarakhand state in 2011. A total 1200 questionnaire forms
were distributed personally and online to employees using e-procurement system
in Uttarakhand.
"
656,Partial order approach to compute shortest paths in multimodal networks,"  Many networked systems involve multiple modes of transport. Such systems are
called multimodal, and examples include logistic networks, biomedical
phenomena, manufacturing process and telecommunication networks. Existing
techniques for determining optimal paths in multimodal networks have either
required heuristics or else application-specific constraints to obtain
tractable problems, removing the multimodal traits of the network during
analysis. In this paper weighted coloured--edge graphs are introduced to model
multimodal networks, where colours represent the modes of transportation.
Optimal paths are selected using a partial order that compares the weights in
each colour, resulting in a Pareto optimal set of shortest paths. This approach
is shown to be tractable through experimental analyses for random and real
multimodal networks without the need to apply heuristics or constraints.
"
657,The Expectation Monad in Quantum Foundations,"  The expectation monad is introduced abstractly via two composable
adjunctions, but concretely captures measures. It turns out to sit in between
known monads: on the one hand the distribution and ultrafilter monad, and on
the other hand the continuation monad. This expectation monad is used in two
probabilistic analogues of fundamental results of Manes and Gelfand for the
ultrafilter monad: algebras of the expectation monad are convex compact
Hausdorff spaces, and are dually equivalent to so-called Banach effect
algebras. These structures capture states and effects in quantum foundations,
and also the duality between them. Moreover, the approach leads to a new
re-formulation of Gleason's theorem, expressing that effects on a Hilbert space
are free effect modules on projections, obtained via tensoring with the unit
interval.
"
658,Multi databases in Health Care Networks,"  E-Health is a relatively recent term for healthcare practice supported by
electronic processes and communication, dating back to at least 1999. E-Health
is greatly impacting on information distribution and availability within the
health services, hospitals and to the public. E-health was introduced as the
death of telemedicine, because - in the context of a broad availability of
medical information systems that can interconnect and communicate -
telemedicine will no longer exist as a specific field. The same could also be
said for any other traditional field in medical informatics, including
information systems and electronic patient records. E-health presents itself as
a common name for all such technological fields. In this paper we focuses in
multi database by determined some sites and distributed it in Homogenous way.
This will be followed by an illustrative example as related works. Finally, the
paper concludes with general remarks and a statement of further work.
"
659,"Informatics Perspectives on Decision Taking, a Case Study on Resolving
  Process Product Ambiguity","  A decision is an act or event of decision taking. Decision making always
includes decision taking, the latter not involving significant exchanges with
non-deciding agents. A decision outcome is a piece of storable information
constituting the result of a decision. Decision outcomes are typed, for
instance: plan, command, assertion, or boolean reply to a question. Decision
outcomes are seen by an audience and autonomous actions from the audience is
supposed to realize the putting into effect of a decision outcome, thus leading
to so-called decision effects. Decision outcomes are supposedly expected by the
decider. Using a model or a theory concerning the causal chain leading from a
decision outcome to one or more decision effects may support a decision taker
decision taker in predicting plausible decision effects for candidate decision
outcomes. Decision taking is positioned amidst many related notions including:
decision making, decision process, decision making process, decision process
making, decision engineering, decision progression, and decision progression
production.
"
660,Simulating City-level Airborne Infectious Diseases,"  With the exponential growth in the world population and the constant increase
in human mobility, the danger of outbreaks of epidemics is rising. Especially
in high density urban areas such as public transport and transfer points, where
people come in close proximity of each other, we observe a dramatic increase in
the transmission of airborne viruses and related pathogens. It is essential to
have a good understanding of the `transmission highways' in such areas, in
order to prevent or to predict the spreading of infectious diseases. The
approach we take is to combine as much information as is possible, from all
relevant sources and integrate this in a simulation environment that allows for
scenario testing and decision support. In this paper we lay out a novel
approach to study Urban Airborne Disease spreading by combining traffic
information, with geo-spatial data, infection dynamics and spreading
characteristics.
"
661,Methods and Techniques of Quality Management for ICT Audit Processes,"  In modern organizations, Information and Communication Technologies are used
to support the organizations' activities. To manage the quality of the
organization processes, audit processes are implemented. Also, the audit
processes can aim the quality of ICT systems themselves because their
involvement in organization processes. The paper investigates the ways in which
a quality management can be applied for audit processes in order to obtain a
high level of quality for the audit recommendations.
"
662,Compressed Beamforming Applied to B-Mode Ultrasound Imaging,"  Emerging sonography techniques often imply increasing in the number of
transducer elements involved in the imaging process. Consequently, larger
amounts of data must be acquired and processed by the beamformer. The
significant growth in the amounts of data effects both machinery size and power
consumption. Within the classical sampling framework, state of the art systems
reduce processing rates by exploiting the bandpass bandwidth of the detected
signals. It has been recently shown, that a much more significant sample-rate
reduction may be obtained, by treating ultrasound signals within the Finite
Rate of Innovation framework. These ideas follow the spirit of Xampling, which
combines classic methods from sampling theory with recent developments in
Compressed Sensing. Applying such low-rate sampling schemes to individual
transducer elements, which detect energy reflected from biological tissues, is
limited by the noisy nature of the signals. This often results in erroneous
parameter extraction, bringing forward the need to enhance the SNR of the
low-rate samples. In our work, we manage to achieve such SNR enhancement, by
beamforming the sub-Nyquist samples obtained from multiple elements. We refer
to this process as ""compressed beamforming"". Applying it to cardiac ultrasound
data, we successfully image macroscopic perturbations, while achieving a nearly
eight-fold reduction in sample-rate, compared to standard techniques.
"
663,"Transparent caching of virtual stubs for improved performance in
  ubiquitous environments","  Context-awareness is an essential requirement for pervasive computing
applications, which enables them to adapt and perform tasks based on context.
One of the adaptive features of context-awareness is contextual
reconfiguration. Contextual reconfiguration involves discovering remote
service(s) based on context and binding them to the application components to
realize new behaviors, which may be needed to satisfy user needs or to enrich
user experience. One of the steps in the reconfiguration process involves a
remote lookup to discover the service(s) based on context. This remote lookup
process provides the largest contribution to reconfiguration time and this is
due to fact that the remote calls are much slower than local calls.
Consequently, it affects system performance. In pervasive computing
applications, this may turn out to be undesirable in terms of user experience.
Moreover, other distributed applications using the network may be affected as
every remote method call decreases the amount of bandwidth available on the
network. Various systems provide reconfiguration support and offer high-level
reconfiguration directives to develop adaptive context-aware applications, but
do not address this performance bottleneck. We address this issue and implement
seamless caching of virtual stubs within our PCRA1 for improved performance. In
this paper we present and describe our transparent caching support and also
provide its performance evaluation.
"
664,An Example for the Use of Bitwise Operations in Programming,"  This piece of work presents a meaningful example for the advantages of using
bitwise operations for creating effective algorithms in programming. A task
connected with mathematical modeling in weaving industry is examined and
computed.
"
665,"Single bit full adder design using 8 transistors with novel 3
  transistors XNOR gate","  In present work a new XNOR gate using three transistors has been presented,
which shows power dissipation of 550.7272$\mu$W in 0.35$\mu$m technology with
supply voltage of 3.3V. Minimum level for high output of 2.05V and maximum
level for low output of 0.084V have been obtained. A single bit full adder
using eight transistors has been designed using proposed XNOR cell, which shows
power dissipation of 581.542$\mu$W. Minimum level for high output of 1.97V and
maximum level for low output of 0.24V is obtained for sum output signal. For
carry signal maximum level for low output of 0.32V and minimum level for high
output of 3.2V have been achieved. Simulations have been performed by using
SPICE based on TSMC 0.35$\mu$m CMOS technology. Power consumption of proposed
XNOR gate and full adder has been compared with earlier reported circuits and
proposed circuit's shows better performance in terms of power consumption and
transistor count.
"
666,Low Power Low Voltage Bulk Driven Balanced OTA,"  The last few decades, a great deal of attention has been paid to low-voltage
(LV) low-power (LP) integrated circuits design since the power consumption has
become a critical issue. Among many techniques used for the design of LV LP
analog circuits, the Bulk-driven principle offers a promising route towards
this design for many aspects mainly the simplicity and using the conventional
MOS technology to implement these designs. This paper is devoted to the
Bulk-driven (BD) principle and utilizing this principle to design LV LP
building block of Operational Transconductance Amplifier (OTA) in standard CMOS
processes and supply voltage 0.9V. The simulation results have been carried out
by the Spice simulator using the 130nm CMOS technology from TSMC.
"
667,"Improvement of Anomoly Detection Algorithms in Hyperspectral Images
  using Discrete Wavelet Transform","  Recently anomaly detection (AD) has become an important application for
target detection in hyperspectral remotely sensed images. In many applications,
in addition to high accuracy of detection we need a fast and reliable algorithm
as well. This paper presents a novel method to improve the performance of
current AD algorithms. The proposed method first calculates Discrete Wavelet
Transform (DWT) of every pixel vector of image using Daubechies4 wavelet. Then,
AD algorithm performs on four bands of ""Wavelet transform"" matrix which are the
approximation of main image. In this research some benchmark AD algorithms
including Local RX, DWRX and DWEST have been implemented on Airborne
Visible/Infrared Imaging Spectrometer (AVIRIS) hyperspectral datasets.
Experimental results demonstrate significant improvement of runtime in proposed
method. In addition, this method improves the accuracy of AD algorithms because
of DWT's power in extracting approximation coefficients of signal, which
contain the main behaviour of signal, and abandon the redundant information in
hyperspectral image data.
"
668,"Fostering continuous innovation in design with an integrated knowledge
  management approach","  In the global competition, companies are propelled by an immense pressure to
innovate. The trend to produce more new knowledge-intensive products or
services and the rapid progress of information technologies arouse huge
interest on knowledge management for innovation. However the strategy of
knowledge management is not widely adopted for innovation in industries due to
a lack of an effective approach of their integration. This study aims to help
the designers to innovate more efficiently based on an integrated approach of
knowledge management. Based on this integrated approach, a prototype of
distributed knowledge management system for innovation is developed. An
industrial application is presented and its initial results indicate the
applicability of the approach and the prototype in practice.
"
669,"A New Design Technique of Reversible BCD Adder Based on NMOS With Pass
  Transistor Gates","  In this paper, we have proposed a new design technique of BCD Adder using
newly constructed reversible gates are based on NMOS with pass transistor
gates, where the conventional reversible gates are based on CMOS with
transmission gates. We also compare the proposed reversible gates with the
conventional CMOS reversible gates which show that the required number of
Transistors is significantly reduced.
"
670,"Improved Strategies for Enhanced Business Performance in Cloud based IT
  Industries","  Emergence of sophisticated technologies in IT industries has posed several
challenges such as production of products using advanced technical process for
instance Result Orientation Approach, Deployment, Assessment and Refinement
(RADAR) in a dynamic and competitive environment. The key challenge for any
engineer is therefore to develop process and products which ultimately lead
towards total customer satisfaction. Recent development in technology has
driven most of the IT industries to operate in the cloud environment due to
reduced infrastructure investment and maintenance overheads. However, existing
process in cloud lacks efficient multiple service paradigms that can provide
improved business gain. Thus, it is the responsibility of every engineer to
contribute towards effective and efficient techniques and models that can
enhance the business performance. The position of this paper is to present
several major issues prevailing in the IT industries such as delay time,
response time, performance etc., which call for immediate attention in order to
position themselves in the market. Further, this paper provides improved
strategies through efficient job scheduling and modified resource allocation
techniques for aforementioned issues in order to enhance the business
performance in cloud-based IT sectors. The simulated results provided in this
paper indicate the impact of enhanced solutions incorporated in the job
processing strategies. They further enable better performance of the cloud with
reduced delay and response time resulting towards improved throughput.
Subsequently, it increases the job acceptance ratio with respect to time and
thereby leading the industry to accomplish total customer satisfaction in
addition to the continued sustainability in the competitive business market.
"
671,A Genetic Algorithm for the Calibration of a Micro-Simulation Model,"  This paper describes the process followed to calibrate a micro-simulation
model for the Altmark region in Germany and a Derbyshire region in the UK. The
calibration process is performed in three main steps: first, a subset of input
and output variables to use for the calibration process is selected from the
complete parameter space in the model; second, the calibration process is
performed using a genetic algorithm calibration approach; finally, a comparison
between the real data and the data obtained from the best fit model is done to
verify the accuracy of the model.
"
672,"On Some Entertaining Applications of the Concept of Set in Computer
  Science Course","  Some aspects of programming education are examined in this work. It is
emphasised, based on the entertainment value, the most appropriate examples are
chosen to demonstrate the different language constructions and data structures.
Such an example is the demonstrated algorithm for solving the widespread
nowadays ""Sudoku"" puzzle. This is made, because of the connection with the term
set and putting it into practice in the programming. Using the so built program
there are solved some combinatorial problems, connected to the Sudoku matrices.
  Key words: Education in programming, programming languages, data structures,
set, Sudoku matrix, Sudoku puzzle.
"
673,"An Entertaining Example for the Usage of Bitwise Operations in
  Programming","  The present study is meant to fill in some information gaps occurring in the
most widespread and well-known educational and reference literature about
programming. The stress is laid on a very useful instrument - the bitwise
operations, topic which is, unfortunately, seldom dealt with in most of the
well-known books on programming. In addition, the research is very useful as
regards the topic of overloading operators in any Object-oriented programming
course. Given some appropriate examples, with the emphasis being laid on some
particular and data structures language constructions, the results are quite
interesting. The algorithm of solving the popular Sudoku puzzle is one such
entertaining example.
"
674,"Construction of Learning Path Using Ant Colony Optimization from a
  Frequent Pattern Graph","  In an e-Learning system a learner may come across multiple unknown terms,
which are generally hyperlinked, while reading a text definition or theory on
any topic. It becomes even harder when one tries to understand those unknown
terms through further such links and they again find some new terms that have
new links. As a consequence they get confused where to initiate from and what
are the prerequisites. So it is very obvious for the learner to make a choice
of what should be learnt before what. In this paper we have taken the data
mining based frequent pattern graph model to define the association and
sequencing between the words and then adopted the Ant Colony Optimization, an
artificial intelligence approach, to derive a searching technique to obtain an
efficient and optimized learning path to reach to a unknown term.
"
675,Learners' Quanta based Design of a Learning Management System,"  In this paper IEEE Learning Technology System Architecture (LTSA) for LMS
software has been analyzed. It has been observed that LTSA is too abstract to
be adapted in a uniform way by LMS developers. A Learners' Quanta based high
level design that satisfies the IEEE LTSA standard has been proposed for future
development of efficient LMS software. A hybrid model of learning fitting into
LTSA model has also been proposed while designing.
"
676,"A comparison algorithm to check LTSA Layer 1 and SCORM compliance in
  e-Learning sites","  The success of e-Learning is largely dependent on the impact of its
multimedia aided learning content on the learner over the hyper media. The
e-Learning portals with different proportion of multimedia elements have
different impact on the learner, as there is lack of standardization. The
Learning Technology System Architecture (LTSA) Layer 1 deals with the effect of
environment on the learner. From an information technology perspective it
specifies learner interaction from the environment to the learner via
multimedia content. Sharable Content Object Reference Model (SCROM) is a
collection of standards and specifications for content of web-based e-learning
and specifies how JavaScript API can be used to integrate content development.
In this paper an examination is made on the design features of interactive
multimedia components of the learning packages by creating an algorithm which
will give a comparative study of multimedia component used by different
learning packages. The resultant graph as output helps us to analysis to what
extent any LMS compliance LTSA layer 1 and SCORM specification.
"
677,The milling process monitoring using 3D envelope method,"  This paper proposes a method to vibration analysis in order to on-line
monitoring of milling process quality. Adapting envelope analysis to
characterize the milling tool materials is an important contribution to the
qualitative and quantitative characterization of milling capacity and a step by
modeling the three-dimensional cutting process. An experimental protocol was
designed and developed for the acquisition, processing and analyzing
three-dimensional signal. The vibration envelope analysis is proposed to detect
the cutting capacity of the tool with the optimization application of cutting
parameters. The research is focused on Hilbert transform optimization to
evaluate the dynamic behavior of the machine/ tool/workpiece.
"
678,New Approach of Envelope Dynamic Analysis for Milling Process,"  This paper proposes a method to vibration analysis in order to on-line
monitoring of milling process quality. Adapting envelope analysis to
characterize the milling tool materials is an important contribution to the
qualitative and quantitative characterization of milling capacity and a step by
modeling the three-dimensional cutting process. An experimental protocol was
designed and developed for the acquisition, processing and analyzing
three-dimensional signal. The vibration envelope analysis is proposed to detect
the cutting capacity of the tool with the optimization application of cutting
parameters. The research is focused on FFT Fourier transform optimization of
vibration analysis and vibration envelope to evaluate the dynamic behavior of
the machine/ tool/workpiece
"
679,The Knowledge-Based Economy and the Triple Helix Model,"  1. Introduction - the metaphor of a ""knowledge-based economy""; 2. The Triple
Helix as a model of the knowledge-based economy; 3. Knowledge as a social
coordination mechanism; 4. Neo-evolutionary dynamics in a Triple Helix of
coordination mechanism; 5. The operation of the knowledge base; 6. The
restructuring of knowledge production in a KBE; 7. The KBE and the
systems-of-innovation approach; 8. The KBE and neo-evolutionary theories of
innovation; 8.1 The construction of the evolving unit; 8.2 User-producer
relations in systems of innovation; 8.3 'Mode-2' and the production of
scientific knowledge; 8.4 A Triple Helix model of innovations; 9. Empirical
studies and simulations using the TH model; 10. The KBE and the measurement;
10.1 The communication of meaning and information; 10.2 The expectation of
social structure; 10.3 Configurations in a knowledge-based economy
"
680,Logical operations with Localized Structures,"  We show how to exploit excitable regimes mediated by localized structures
(LS) to perform AND, OR, and NOT logical operations providing full logical
functionality. Our scheme is general and can be implemented in any physical
system displaying LS. In particular, LS in nonlinear photonic devices can be
used for all-optical computing applications where several reconfigurable logic
gates can be implemented in the transverse plane of a single device, allowing
for parallel computing.
"
681,A Knowledge Engineering Method for New Product Development,"  Engineering activities involve large groups of people from different domains
and disciplines. They often generate important information flows that are
difficult to manage. To face these difficulties, a knowledge engineering
process is necessary to structure the information and its use. This paper
presents a deployment of a knowledge capitalization process based on the
enrichment of MOKA methodology to support the integration of Process Planning
knowledge in a CAD System. Our goal is to help different actors to work
collaboratively by proposing one referential view of the domain, the context
and the objectives assuming that it will help them in better decision-making.
"
682,"Channel Estimation Study for Block - Pilot Insertion in OFDM Systems
  under Slowly Time Varying Conditions","  In this paper, we propose a study of performance of the channel estimation
using LS, MMSE, LMMSE and Lr-LMMSE algorithms in OFDM (Orthogonal Frequency
Division Multiplexing) system which, as known suffers from the time variation
of the channel under high mobility conditions, using block pilot insertion. The
loss of sub channel orthogonality leads to inter-carrier interference (ICI).
Using many algorithms for channel estimation, we will show that, for a 16- QAM
modulation, the LMMSE algorithm performs well to achieve this estimation but
when the SNR (Signal Noise Rate) is high, the four algorithms (LS, MMSE, LMMSE
and Lr-LMMSE) perform similarly, this is not always the case for another scheme
of modulation. We will improve also the mean squared error for these
algorithms. It will be illustrious in this paper that the LMMSE algorithm
performs well with the block- pilot insertion as well as its low rank version
which behave very good even when the size of FFT is very high.
"
683,Design of wireless electronic stethoscope based on zigbee,"  Heart sound stethoscope is primary stage to access diseases. In this paper
design of an electronic stethoscope with the functions of wireless transmission
is discussed. This electronic stethoscope based on embedded processor. The data
can be transmitted through wireless transmission using Zigbee module. A
microphone is used to pick up the sound of the heart beat. Acoustic stethoscope
can be changed into a digital stethoscope by inserting an electric capacity
microphone into its head. The signal is processed and amplified to play with or
without earphone. Heart sounds are processed, sampled and sent wirelessly using
Zigbee module so that multiple doctors can do auscultation. PC connectivity is
provided through serial port where from audio and video can be made available
through LAN and internet for telemedicine consultation. Heart beat signals are
sensed, sent, displayed, monitored, stored, reviewed, and analysed with ease.
"
684,Design and Fabrication of Micromachined Resonators,"  Microelectromechanical system (MEMS) based on-chip resonators offer great
potential for sensing and high frequency signal processing applications due to
their exceptional features like small size, large frequency-quality factor
product, integrability with CMOS ICs, low power consumption etc. This work is
mainly aimed at the design, modeling, simulation, and fabrication of
micromachined polysilicon disk resonators exhibiting radial-contour mode
vibrations. A few other bulk mode modified resonator geometries are also being
explored. The resonator structures have been designed and simulated in
CoventorWare finite-element platform and fabricated by the PolyMUMPs surface
micromachining process.
"
685,"Towards Maximum Spanning Tree Model in Web 3.0 Design and Development
  for Students using Discriminant Analysis","  Web 3.0 is an evolving extension of the web 2.0 scenario. The perceptions
regarding web 3.0 is different from person to person . Web 3.0 Architecture
supports ubiquitous connectivity, network computing, open identity, intelligent
web, distributed databases and intelligent applications. Some of the
technologies which lead to the design and development of web 3.0 applications
are Artificial intelligence, Automated reasoning, Cognitive architecture,
Semantic web . An attempt is made to capture the requirements of Students
inline with web 3.0 so as to bridge the gap between the design and development
of web 3.0 applications and requirements among Students. Maximum Spanning Tree
modeling of the requirements facilitate the identification of key areas and key
attributes in the design and development of software products for Students in
Web 3.0 using Discriminant analysis. Keywords : Web 3.0, Discriminant analysis,
Design and Development, Model, Maximum Spanning Tree 1.
"
686,Innovative SQA Service Maturity Model using CMMI and ITIL,"  This Journal details a maturity model for SQA services which has been
developed during QMS implementation in the IT division of a large multinational
organization. The scope of the engagement was to establish a standard set of
processes based on CMMI\textregistered and ITIL\textregistered Framework across
four business verticals scattered in Europe, United States and Asia. The
services of Software Quality Analyst (SQA) from different vendors were
leveraged to facilitate implementation of processes which was referred to as
the Quality Management System (QMS). To co-ordinate and support QMS
implementation, a Software Quality Assurance Group (SQAG) was established at
the organizational level. Considering the large number of applications, the
business verticals proposed that process implementation should be owned and
managed by practitioners themselves so that the mass deployment of QMS can be
achieved at a faster rate with the same SQA capacity. This called for a need to
devise an innovative implementation solution before moving to a process
implementation model which proposed Project Managers implementing processes
themself. While there are process models and frameworks available in the market
for establishing processes in an organization, there is no model that
elaborates activities to be performed by the SQA for effective implementation
of processes. SQA service maturity model was proposed as a solution based on
CMMI\textregistered and developed to eventually proceed towards a 'Process
Implementation Model proposing Project Managers implementing processes
themself'.
  SQA Service Maturity Model is a Software Quality Assurance implementation
framework that enables organisations to increase Efficiencies in Software
Quality Assurance, reduce the Cost of Defects and ultimately Increasing Return
on Investment in IT.
"
687,Computer applications in clinical psychology,"  The computer-assisted analysis is not currently a novelty, but a necessity in
all areas of psychology. A number of studies that examine the limits of the
computer assisted and analyzed interpretations, also its advantages. A series
of studies aim to assess how the computer assisting programs are able to
establish a diagnosis referring to the presence of certain mental disorders. We
will present the results of one computer application in clinical psychology
regarding the assessment of Theory of Mind capacity by animation.
"
688,Verification and Diagnosis Infrastructure of SoC HDL-model,"  This article describes technology for diagnosing SoC HDL-models, based on
transactional graph. Diagnosis method is focused to considerable decrease the
time of fault detection and memory for storage of diagnosis matrix by means of
forming ternary relations in the form of test, monitor, and functional
component. The following problems are solved: creation of digital system model
in the form of transaction graph and multi-tree of fault detection tables, as
well as ternary matrices for activating functional components in tests,
relative to the selected set of monitors; development of a method for analyzing
the activation matrix to detect the faults with given depth and synthesizing
logic functions for subsequent embedded hardware fault diagnosing.
"
689,Performance Evaluation of Biometric Template Update,"  Template update allows to modify the biometric reference of a user while he
uses the biometric system. With such kind of mechanism we expect the biometric
system uses always an up to date representation of the user, by capturing his
intra-class (temporary or permanent) variability. Although several studies
exist in the literature, there is no commonly adopted evaluation scheme. This
does not ease the comparison of the different systems of the literature. In
this paper, we show that using different evaluation procedures can lead in
different, and contradictory, interpretations of the results. We use a
keystroke dynamics (which is a modality suffering of template ageing quickly)
template update system on a dataset consisting of height different sessions to
illustrate this point. Even if we do not answer to this problematic, it shows
that it is necessary to normalize the template update evaluation procedures.
"
690,"Analysis of neighbour and isolated node of intersection area based
  geocasting protocol (IBGP) in VANET","  Geocasting is a special variant of multicasting, where data packet or message
is transmitted to a predefined geographical location i.e., known as geocast
region. The applications of geocasting in VANET are to disseminate information
like, collision warning, advertising, alerts message, etc. In this paper, we
have proposed a model for highway scenario where the highway is divided into
number of cells. The intersection area between two successive cells is computed
to find the number of common nodes. Therefore, probabilistic analysis of the
nodes present and void occurrence in the intersection area is carried out.
Further, we have defined different forwarding zones to restrict the number of
participated nodes for data delivery. Number of nodes present and void
occurrence in the different forwarding zones have also been analysed based on
various node density in the network to determine the successful delivery of
data. Our analytical results show that in a densely populated network, data can
be transmitted with low radio transmission range. In a densely populated
network smaller forwarding zones will be selected for data delivery.
"
691,Spread spectrum magnetic resonance imaging,"  We propose a novel compressed sensing technique to accelerate the magnetic
resonance imaging (MRI) acquisition process. The method, coined spread spectrum
MRI or simply s2MRI, consists of pre-modulating the signal of interest by a
linear chirp before random k-space under-sampling, and then reconstructing the
signal with non-linear algorithms that promote sparsity. The effectiveness of
the procedure is theoretically underpinned by the optimization of the coherence
between the sparsity and sensing bases. The proposed technique is thoroughly
studied by means of numerical simulations, as well as phantom and in vivo
experiments on a 7T scanner. Our results suggest that s2MRI performs better
than state-of-the-art variable density k-space under-sampling approaches
"
692,"An Optimum Time Quantum Using Linguistic Synthesis for Round Robin
  Scheduling Algorithm","  In Round Robin CPU scheduling algorithm the main concern is with the size of
time quantum and the increased waiting and turnaround time. Decision for these
is usually based on parameters which are assumed to be precise. However, in
many cases the values of these parameters are vague and imprecise. The
performance of fuzzy logic depends upon the ability to deal with Linguistic
variables. With this intent, this paper attempts to generate an Optimal Time
Quantum dynamically based on the parameters which are treated as Linguistic
variables. This paper also includes Mamdani Fuzzy Inference System using
Trapezoidal membership function, results in LRRTQ Fuzzy Inference System. In
this paper, we present an algorithm to improve the performance of round robin
scheduling algorithm. Numerical analysis based on LRRTQ results on proposed
algorithm show the improvement in the performance of the system by reducing
unnecessary context switches and also by providing reasonable turnaround time.
"
693,System on Programable Chip for Performance Estimation of Loom Machine,"  System on programmable chip for the performance estimation of loom machine,
which calculates the efficiency and meter count for weaved cloth automatically.
Also it calculates the efficiency of loom machine. Previously the same was done
using manual process which was not efficient. This article is intended for loom
machines which are not modern.
"
694,Modified Quine-McCluskey Method,"  The digital gates are basic electronic component of any digital circuit.
Digital circuit should be simplified in order to reduce its cost by reducing
number of digital gates required to implement it. To achieve this, we use
Boolean expression that helps in obtaining minimum number of terms and does not
contain any redundant pair. Karnaugh map(K-map) and Quine-McCluskey(QM) methods
are well known methods to simplify Boolean expression. K-map method becomes
complex beyond five variable Boolean expression. Quine-McCluskey method is
computer based technique for minimization of Boolean function and it is faster
than K-map method. This paper proposes E-sum based optimization to
Quine-McCluskey Method to increase its performance by reducing number of
comparisons between mintermlist in determination of prime implicants. Modified
Quine-McCluskey method(MQM) can be implemented to any number of variable.
"
695,"Institutional repository `eKMAIR': establishing and populating a
  research repository for the National University ""Kyiv Mohyla Academy""","  University libraries have an increasingly important role to play in
supporting open access publishing and dissemination of research outputs.1 In
particular, many libraries are playing a leading role in establishing and
managing institutional repositories. Institutional repositories are, most
often, Open Access Initiative (OAI)-compliant databases of a university or
other research institution's intellectual output, most typically research
papers, although many other forms of digital media can also be stored and
disseminated. Their main function is to provide improved access to the full
text of research articles and improve retrieval of relevant research.
  The National University ""Kyiv Mohyla Academy"" is a small-sized institution
with approximately 3,000 students and 500 academic staff. Although it is a
teaching-intensive university, developing research and knowledge-transfer
capacity is a strategic priority and four research institutes have been
established, with further research activity going on in the academic schools
and research centres.
"
696,"High Speed, Low Power Current Comparators with Hysteresis","  This paper, presents a novel idea for analog current comparison which
compares input signal current and reference currents with high speed, low power
and well controlled hysteresis. Proposed circuit is based on current mirror and
voltage latching techniques which produces rail to rail output voltage as a
result of current comparison. The same design can be extended to a simple
current comparator without hysteresis (or very less hysteresis), where
comparator gives high accuracy (less than 50nA) and speed at the cost of
moderate power consumption. The comparators are designed optimally and studied
at 180nm CMOS process technology for a supply voltage of 3V.
"
697,A Semantic Without Syntax 1,"  Here, by introducing a version of ""Unexpected hanging paradox"" we try to open
a new way and a new explanation for paradoxes, similar to liar paradox. Also,
we will show that we have a semantic situation which no syntactical logical
system could support that. In the end, we propose a claim as a question. Based
on this claim, having an axiomatic system for computability theory is not
possible. In fact we will show that the method applied here could yields us as
a generalized result, some Theories like Physic is not axiomatizable.
"
698,"On the Impact of Information Technologies on Society: an Historical
  Perspective through the Game of Chess","  The game of chess as always been viewed as an iconic representation of
intellectual prowess. Since the very beginning of computer science, the
challenge of being able to program a computer capable of playing chess and
beating humans has been alive and used both as a mark to measure
hardware/software progresses and as an ongoing programming challenge leading to
numerous discoveries. In the early days of computer science it was a topic for
specialists. But as computers were democratized, and the strength of chess
engines began to increase, chess players started to appropriate to themselves
these new tools. We show how these interactions between the world of chess and
information technologies have been herald of broader social impacts of
information technologies. The game of chess, and more broadly the world of
chess (chess players, literature, computer softwares and websites dedicated to
chess, etc.), turns out to be a surprisingly and particularly sharp indicator
of the changes induced in our everyday life by the information technologies.
Moreover, in the same way that chess is a modelization of war that captures the
raw features of strategic thinking, chess world can be seen as small society
making the study of the information technologies impact easier to analyze and
to grasp.
"
699,"Designing of RF Single Balanced Mixer with a 65nm CMOS Technology
  Dedicated to Low Power Consumption Wireless Applications","  The present work consists of designing a Single Balanced Mixer(SBM) with the
65 nm CMOS technology, this for a 1.9 GHz RF channel, dedicated to wireless
applications. This paper shows; the polarization chosen for this structure,
models of evaluating parameters of the mixer, then simulation of the circuit in
65nm CMOS technology and comparison with previously treated. Keywords: SBM
Mixer, Radio Frequency, 65 nm CMOS Technology, Non-Linearity, Power
Consumption.
"
700,"An Open Question about Dependency of Life Time of Hardware Components
  and Dynamic Voltage Scaling","  Open question about Dependency of Life Time of Hardware Components and
Dynamic Voltage Scaling (A primary idea)
"
701,"Building Healthcare - Patient Relationship with CRM 2.0: Lesson Learnt
  from Prita Mulyasari's Case","  Healthcare is implementing CRM as a strategy for managing interactions and
communication with patients which involves using Information and Communication
Technology (ICT) to organize, automate, and coordinate business processes. CRM
with the Web technology provides healthcare the ability to broaden service
beyond its usual practices, and thus provides a particular advantageous
environment for them that want to use ICT to achieve complex healthcare goal.
This paper we will discuss and demonstrate how a new approach in CRM will help
the healthcare increasing their customer support, and promoting better health
to patient. The patients benefited from the customized personal service so that
they have full information access to perform self managed their own health and
the healthcare provider will have a loyal and retains the right customer. A
conceptual framework of approach will be highlighted. Customer centric paradigm
in social network's era and value creation of healthcare's business process
will be taken into consideration.
"
702,Health Information Systems (HIS): Concept and Technology,"  A health information system (HIS) is the intersection of between healthcare's
business process, and information systems to deliver better healthcare
services. The nature of healthcare industry, which is highly influenced by
economic, social, politic, and technological factors, has changed over time.
This paper will address some important concepts of healthcare and related
terminologies to provide a holistic view for HIS. Related technological
milestones and major events are briefly summarized. The trends and rapid
development of health information technologies are also discussed.
"
703,"CRM 2.0 within E-Health Systems: Towards Achieving Health Literacy &
  Customer Satisfaction","  Customer Relationship Management (CRM) within healthcare organization can be
viewed as a strategy to attract new customers and retaining them throughout
their entire lifetime of relationships. At the same time, the advancement of
Web technology known as Web 2.0 plays a significant part in the CRM transition
which drives social change that impacts all institutions including business and
healthcare organizations. This new paradigm has been named as Social CRM or CRM
2.0 because it is based on Web 2.0. We conducted survey to examine the features
of CRM 2.0 in healthcare scenario to the customer in Brunei Darussalam. We draw
the conclusion that the CRM 2.0 in healthcare technologies has brought a
possibility to extend the services of e-health by enabling patients, patient's
families, and community at large to participate more actively in the process of
health education; it helps improve health literacy through empowerment, social
networking process, and online health educator. This paper is based on our
works presented at ICID 2011.
"
704,A tour about Isaac Newton's life,"  Here we propose a tour about the life of Isaac Newton, using a georeferenced
method, based on the free satellite maps. Our tour is modelled on the time-line
of the great scientist's life, as an ancient ""itinerarium"" was modelled on the
Roman roads, providing a listing of places and intervening distances, sometimes
with short description or symbols concerning the places. KML language and
Google Earth, with its Street View and 3D images are powerful tools to create
this virtual tour.
"
705,Autonomic Model for Self-Configuring C#.NET Applications,"  With the advances in computational technologies over the last decade, large
organizations have been investing in Information Technology to automate their
internal processes to cut costs and efficiently support their business
projects. However, this comes to a price. Business requirements always change.
Likewise, IT systems constantly evolves as developers make new versions of
them, which require endless administrative manual work to customize and
configure them, especially if they are being used in different contexts, by
different types of users, and for different requirements. Autonomic computing
was conceived to provide an answer to these ever-changing requirements.
Essentially, autonomic systems are self-configuring, self-healing,
self-optimizing, and self-protecting; hence, they can automate all complex IT
processes without human intervention. This paper proposes an autonomic model
based on Venn diagram and set theory for self-configuring C#.NET applications,
namely the self-customization of their GUI, event-handlers, and security
permissions. The proposed model does not require altering the source-code of
the original application; rather, it uses an XML-based customization file to
turn on and off the internal attributes of the application. Experiments
conducted on the proposed model, showed a successful automatic customization
for C# applications and an effective self-adaption based on dynamic business
requirements. As future work, other programming languages such as Java and C++
are to be supported, in addition to other operating systems such as Linux and
Mac so as to provide a standard platform-independent autonomic self-configuring
model.
"
706,"A 100 mA Low Voltage Linear Regulators for Systems on Chip Applications
  Using 0.18 {\mu}m CMOS Technology","  A novel design for a low dropout (LDO) voltage regulator is presented and
dedicated to power many sections of a typical cellular handset. However, these
baseband, RF, and audio sections have different requirements that influence
which LDO is most appropriate. After discussion of the specific requirements,
different LDOs are recommended. Also, some LDO design techniques are briefly
discussed to demonstrate how an LDO may be optimized for a specific level of
performance. Cellular phone designs require linear regulators with lowdropout,
low-noise, high PSRR, low quiescent current (Iq), and low-cost. They need to
deliver a stable output and use smallvalue output capacitors. Ideally, one
device would have all these characteristics and one low-dropout linear
regulator (LDO) could be used anywhere in the phone without worry. But in
practice, the various cell phone blocks are best powered by LDOs with different
performance characteristics. This paper provides a new design methodology to
choosing the right LDO to power each cell phone and especially for the Voltage
Phase-Locked loops (VPLLs) blocks. Fabricated in a 0.18 {\mu}m CMOS process,
the measured results show the adopted topology achieves a better phase noise
than the conventional saturation current source. and the spread of the current
limitation (without matching) is 100mA, the VPLLs system demonstrates a phase
noise of 782 nv/sqrtHz at 100-kHz, and 33 nv/sqrtHz at 1 MHz, while quiescent
current 33 {\mu}A from a 2.6 V supply voltage.
"
707,"A Scheme for Automation of Telecom Data Processing for Business
  Application","  As the telecom industry is witnessing a large scale growth, one of the major
challenges faced in the domain deals with the analysis and processing of
telecom transactional data which are generated in large volumes by embedded
system communication controllers having various functions. This paper deals
with the analysis of such raw data files which are made up of the sequences of
the tokens. It also depicts the method in which the files are parsed for
extracting the information leading to the final storage in predefined data base
tables. The parser is capable of reading the file in a line structured way and
store the tokens into the predefined tables of data bases. The whole process is
automated using the SSIS tools available in the SQL server. The log table is
maintained in each step of the process which will enable tracking of the file
for any risk mitigation. It can extract, transform and load data resulting in
the processing.
"
708,"Critical Task Re-assignment under Hybrid Scheduling Approach in
  Multiprocessor Real-Time Systems","  Embedded hard real time systems require substantial amount of emergency
processing power for the management of large scale systems like a nuclear power
plant under the threat of an earth quake or a future transport systems under a
peril. In order to meet a fully coordinated supervisory control of multiple
domains of a large scale system, it requires the scenario of engaging
multiprocessor real time design. There are various types of scheduling schemes
existing for meeting the critical task assignment in multiple processor
environments and it requires the tracking of faulty conditions of the subsystem
to avoid system underperformance from failure patterns. Hybrid scheduling
usually engages a combined scheduling philosophy comprising of a static
scheduling of a set of tasks and a highly pre-emptive scheduling for another
set of tasks in different situations of process control. There are instances
where highly critical tasks need to be introduced at a least expected
catastrophe and it cannot be ensured to meet all deadline in selected
processors because of the arrival pattern of such tasks and they bear low
tolerance of time to meet the required target. In such circumstances an
effective switching of processors for this set of task is feasible and we
describe a method to achieve this effectively.
"
709,"A Simulation Approach Paradigm: An Optimization and Inventory Challenge
  Case Study","  The paper presents a simulation on automotive inventory and stock issue,
followed by evaluated performance of automotif Sector Company, focused on
getting optimum profit from supply and demand balancing. Starting by evaluating
and verification of customer's document until car delivered to customer.
Simulation method of performance is used to evaluate company activity. excess
demand of car by customer, not eligible customer to rented a car, number of
customer who served and number of customer who served including the driver, the
last result is number of optimum demand that match with the stock or supply of
car by the company. Finally, board of management should be making decision; the
first decision is buy the new car for meet with the demand or second decision
is recruit new staff for increasing customer service or customer care.
"
710,A procedural framework and mathematical analysis for solid sweeps,"  Sweeping is a powerful and versatile method of designing objects. Boundary of
volumes (henceforth envelope) obtained by sweeping solids have been extensively
investigated in the past, though, obtaining an accurate parametrization of the
envelope remained computationally hard. The present work reports our approach
to this problem as well as the important problem of identifying
self-intersections within the envelope. Parametrization of the envelope is, of
course, necessary for its use in most current CAD systems. We take the more
interesting case when the solid is composed of several faces meeting smoothly.
We show that the face structure of the envelope mimics locally that of the
solid. We adopt the procedural approach at defining the geometry in this work
which has the advantage of being accurate as well as computationally efficient.
The problem of detecting local self-intersections is central to a robust
implementation of the solid sweep. This has been addressed by computing a
subtle mathematical invariant which detects self-intersections, and which is
computationally benign and requires only point queries.
"
711,Exploring Application Logs,"  This paper deals with the problem of analyzing application event logs in
relevance to dependability evaluation. We present the significance of
application logs as a valuable source of information on operational profiles,
anomalies and errors. They can enhance classical approaches based on monitoring
system logs and performance variables. Keywords; event monitoring, operational
profiles, anomalies
"
712,Towards Quranic reader controlled by speech,"  In this paper we describe the process of designing a task-oriented continuous
speech recognition system for Arabic, based on CMU Sphinx4, to be used in the
voice interface of Quranic reader. The concept of the Quranic reader controlled
by speech is presented, the collection of the corpus and creation of acoustic
model are described in detail taking into account a specificities of Arabic
language and the desired application.
"
713,Mobile Web - Strategy for Enterprise Success,"  Today, enterprises are faced with increased global competition in an
environment where customers are demanding faster delivery, better service and
also want to gain significant and immediate business value by increasing
productivity and reducing operational cost. Spurred by unprecedented customer
demand, each Industry cluster has developed its own source of comparative
advantage. Even within a single organization, the business value chain is
geographically fragmented. Such diversification and fragmentation of value
chain drives the need for cross-platform Web applications over mobile channel.
Mobile Web is the next logical transition in this evolutionary process and
Mobile Web applications will continue to gain more prominence in the
enterprises not just to improve the return on investment in their existing
system landscape, but also to expand global reach and improve operational
efficiency of their mobile workforce. This paper outlines the critical business
needs to rapidly create flexible Mobile web solutions across all lines of
business. The paper enlightens the benefits offered by enabling web
applications on Mobile devices and also addresses the current business
challenges in developing Mobile Web applications. This paper is intended for
all business domains irrespective of application portfolios.
"
714,"Design and Implementation of BCM Rule Based on Spike-Timing Dependent
  Plasticity","  The Bienenstock-Cooper-Munro (BCM) and Spike Timing-Dependent Plasticity
(STDP) rules are two experimentally verified form of synaptic plasticity where
the alteration of synaptic weight depends upon the rate and the timing of pre-
and post-synaptic firing of action potentials, respectively. Previous studies
have reported that under specific conditions, i.e. when a random train of
Poissonian distributed spikes are used as inputs, and weight changes occur
according to STDP, it has been shown that the BCM rule is an emergent property.
Here, the applied STDP rule can be either classical pair-based STDP rule, or
the more powerful triplet-based STDP rule. In this paper, we demonstrate the
use of two distinct VLSI circuit implementations of STDP to examine whether BCM
learning is an emergent property of STDP. These circuits are stimulated with
random Poissonian spike trains. The first circuit implements the classical
pair-based STDP, while the second circuit realizes a previously described
triplet-based STDP rule. These two circuits are simulated using 0.35 um CMOS
standard model in HSpice simulator. Simulation results demonstrate that the
proposed triplet-based STDP circuit significantly produces the threshold-based
behaviour of the BCM. Also, the results testify to similar behaviour for the
VLSI circuit for pair-based STDP in generating the BCM.
"
715,Construction of Community Web Directories based on Web usage Data,"  This paper support the concept of a community Web directory, as a Web
directory that is constructed according to the needs and interests of
particular user communities. Furthermore, it presents the complete method for
the construction of such directories by using web usage data. User community
models take the form of thematic hierarchies and are constructed by employing
clustering approach. We applied our methodology to the ODP directory and also
to an artificial Web directory, which was generated by clustering Web pages
that appear in the access log of an Internet Service Provider. For the
discovery of the community models, we introduced a new criterion that combines
a priori thematic informativeness of the Web directory categories with the
level of interest observed in the usage data. In this context, we introduced
and evaluated new clustering method. We have tested the methodology using
access log files which are collected from the proxy servers of an Internet
Service Provider and provided results that indicates the usability of the
community Web directories. The proposed clustering methodology is evaluated
both on a specialized artificial and a community Web directory, indicating its
value to the user of the web.
"
716,Introducing convex layers to the Traveling Salesman Problem,"  In this paper, we will propose convex layers to the Traveling Salesman
Problem (TSP). Firstly, we will focus on human performance on the TSP.
Experimental data shows that untrained humans appear to have the ability to
perform well in the TSP. On the other hand, experimental data also supports the
hypothesis of convex hull i.e. human relies on convex hull to search for the
optimal tour for the TSP. Secondly, from the paper published by Bonabeau,
Dorigo and Theraulaz, social insect behavior would be able to help in some of
the optimizing problems, especially the TSP. Thus, we propose convex layers to
the TSP based on the argument that, by the analogy to the social insect
behavior, untrained humans' cognition should be able to help in the TSP.
Lastly, we will use Tour Improvement algorithms on convex layers to search for
an optimal tour for a 13-cities problem to demonstrate the idea.
"
717,"Applying convex layers, nearest neighbor and triangle inequality to the
  Traveling Salesman Problem (TSP)","  The author would like to propose a simple but yet effective method, convex
layers, nearest neighbor and triangle inequality, to approach the Traveling
Salesman Problem (TSP). No computer is needed in this method. This method is
designed for plain folks who faced the TSP everyday but do not have the
sophisticated knowledge of computer science, programming language or applied
mathematics. The author also hopes that it would give some insights to
researchers who are interested in the TSP.
"
718,"Optimal tree for Genetic Algorithms in the Traveling Salesman Problem
  (TSP)","  In this paper, the author proposes optimal tree as a ""gauge"" for the
generation of the initial population at random in the Genetic Algorithms (GA)
to benchmark against the good and the bad parent tours. Thus, without having
the so-called bad parent tours in the initiate population, it will speed up the
GA. The characteristics of the gauge (algorithm, complexity time, trade-off,
etc.) will be discussed in this paper as well.
"
719,"Noise based logic: why noise? A comparative study of the necessity of
  randomness out of orthogonality","  Although noise-based logic shows potential advantages of reduced power
dissipation and the ability of large parallel operations with low hardware and
time complexity the question still persist: is randomness really needed out of
orthogonality? In this Letter, after some general thermodynamical
considerations, we show relevant examples where we compare the computational
complexity of logic systems based on orthogonal noise and sinusoidal signals,
respectively. The conclusion is that in certain special-purpose applications
noise-based logic is exponentially better than its sinusoidal version: its
computational complexity can be exponentially smaller to perform the same task.
"
720,Cloud Computing For Microfinances,"  Evolution of Science and Engineering has led to the growth of several
commercial applications. The wide spread implementation of commercial based
applications has in turn directed the emergence of advanced technologies such
as cloud computing. India has well proven itself as a potential hub for
advanced technologies including cloud based industrial market. Microfinance
system has emerged out as a panacea to Indian economy since the population
encompasses of people who come under poverty and below poverty index. However,
one of the key challenges in successful operation of microfinance system in
India has given rise to integration of financial services using sophisticated
cloud computing model. This paper, therefore propose a fundamental cloud-based
microfinance model in order to reduce high transaction risks involved during
microfinance operations in an inexpensive and efficient manner.
"
721,"Teaching Chemistry in a Social Learning Environment: Facing Drivers and
  Barriers","  The Portuguese Technological Plan for Education (TPE) was established to
modernize schools and to consolidate the role of Information and Communication
Technologies (ICT) in order to promote the academic success of students and
allow schools to be transformed into technological enriched environments
through a significant learning and knowledge building in a participatory,
collaborative and sharing logic. With this work we aimed to establish dynamical
interactions students-content- teacher in order to overcome a diagnosed
students' lack of effort towards studying curriculum chemistry content. Our
methodology design is a theoretical and descriptive one, carried out in a
secondary school during the 2009/2010 school year, in order to answer the
question ""How to improve the engagement of K-12 students in chemistry
classes?"". Students, gathered in small groups were asked to create digital
learning resources (DLR) during classes. The teacher assumed the role of the
supervisor, coacher and facilitator of every task that had to be taken or
chosen by the students. To enhance interaction student-student and
student-teacher, a Twitter account and a Ning site were created for the class.
Both supported the Social Learning Environment (SLE) that was intended to be
created. The data collected led us to satisfactory results in what concerns the
goals of the study. The affordances and constraints of SLE as an open
architecture that has potential to facilitate collaborative learning are
delineated. Future work should focus on mechanisms that allow assessment both
of the methodology used and the students' generated content in order to improve
students' learning in this environment.
"
722,Towards Fuzzy-Hard Clustering Mapping Processes,"  Although the validation step can appear crucial in the case of clustering
adopting fuzzy approaches, the problem of the partition validity obtained by
those adopting the hard ones was not tackled. To cure this problem, we propose
in this paper fuzzy-hard mapping processes of clustering while benefitting from
those adopting the fuzzy case. These mapping processes concern: (1) local and
global clustering evaluation measures: the first for the detection of the
""worst"" clusters to merging or splitting them. The second relates to the
evaluation of the obtained partition for each iteration, (2) merging and
splitting processes taking into account the proposed measures, and (3)
automatic clustering algorithms implementing these new concepts.
"
723,SaVi: satellite constellation visualization,"  SaVi, a program for visualizing satellite orbits, movement, and coverage, is
maintained at the University of Surrey. This tool has been used for research in
academic papers, and by industry companies designing and intending to deploy
satellite constellations. It has also proven useful for demonstrating aspects
of satellite constellations and their geometry, coverage and movement for
educational and teaching purposes. SaVi is introduced and described briefly
here.
"
724,Universal Programmable Quantum Circuit Schemes to Emulate an Operator,"  Unlike fixed designs, programmable circuit designs support an infinite number
of operators. The functionality of a programmable circuit can be altered by
simply changing the angle values of the rotation gates in the circuit. Here, we
present a new quantum circuit design technique resulting in two general
programmable circuit schemes. The circuit schemes can be used to simulate any
given operator by setting the angle values in the circuit. This provides a
fixed circuit design whose angles are determined from the elements of the given
matrix-which can be non-unitary-in an efficient way. We also give both the
classical and quantum complexity analysis for these circuits and show that the
circuits require a few classical computations. They have almost the same
quantum complexities as non-general circuits. Since the presented circuit
designs are independent from the matrix decomposition techniques and the global
optimization processes used to find quantum circuits for a given operator, high
accuracy simulations can be done for the unitary propagators of molecular
Hamiltonians on quantum computers. As an example, we show how to build the
circuit design for the hydrogen molecule.
"
725,Improving Customer Service in Healthcare with CRM 2.0,"  The Healthcare industry is undergoing a paradigm shift from healthcare
institution-centred care to a citizen-centred care that emphasises on
continuity of care from prevention to rehabilitation. The recent development of
Information and Communication Technology (ICT), especially the Internet and its
related technologies has become the main driver of the paradigm shift. Managing
relationship with customers (patients) is becoming more important in the new
paradigm. The paper discusses Customer Relationship Management (CRM) in
healthcare and proposes a Social CRM or CRM 2.0 model to take advantage of the
multi-way relationships created by Web 2.0 and its widespread use in improving
customer services for mutual benefits between healthcare providers and their
customers.
"
726,E-Health Initiative and Customer's Expectation: Case Brunei,"  This paper is to determine the dimension of e-health services in Brunei
Darussalam (Brunei) from customers' perspective. It is to identify, understand,
analyze and evaluate public's expectation on e-health in Brunei. A
questionnaire was designed to gather quantitative and qualitative data to
survey patients, patient's family, and health practitioners at hospitals,
clinics, or home care centers in Brunei starting from February to March, 2011.
A 25-item Likert-type survey instrument was specifically developed for this
study and administered to a sample of 366 patients. The data were analyzed to
provide initial ideas and recommendation to policy makers on how to move
forward with the e-health initiative as a mean to improve healthcare services.
The survey revealed that there exists a high demand and expectation from people
in Brunei to have better healthcare services accessible through an e-health
system in order to improve health literacy as well as quality and efficiency of
healthcare. Regardless of the limitations of the survey, the general public has
responded with a great support for the capabilities of an e-health system
listed from the questionnaires. The results of the survey provide a solid
foundation for our on going research project to proceed further to develop a
model of e-health and subsequently develop a system prototype that incorporate
expectations from the people.
"
727,"Stimulus and correlation matching measurement technique in computer
  based characterization testing","  Constructive theory of characterization test is considered. The theory is
applicable to a nano devices characterization: current-voltage, Auger current
dependence. Generally small response of device under test on an applied
stimulus is masked by an unknown deterministic background and a random noise.
Characterization test in this signal corruption scenario should be based on
correlation measurement technique of device response on applied optimal
stimulus with optimal reference signal. Co-synthesis solution of stimulus and
reference signal is proposed.
"
728,A Cost- Effective Design of Reversible Programmable Logic Array,"  In the recent era, Reversible computing is a growing field having
applications in nanotechnology, optical information processing, quantum
networks etc. In this paper, the authors show the design of a cost effective
reversible programmable logic array using VHDL. It is simulated on xilinx ISE
8.2i and results are shown. The proposed reversible Programming logic array
called RPLA is designed by MUX gate [10] & Feynman gate for 3- inputs, which is
able to perform any reversible 3- input logic function or Boolean function.
Furthermore the quantized analysis with camparitive finding is shown for the
realized RPLA against the existing one. The result shows improvement in the
quantum cost and total logical caculation in proposed RPLA.
"
729,"Towards the Solution of Power Dissipation in Electronics Systems through
  Thermodynamics","  Power loss in the electronic system is a very crucial limiting factor that
can be reduced or minimized with the help of using the reversible logics ""a
concept came from Thermodynamics"". In this paper the authors shows the concept
of reversible logics for the Electronics system. The logical and physical
designing approach is given in the paper in detail. The contradiction of
logical and physical reversibility with the conventional CMOS designing is also
shows and the solution of that contradiction is also proposed by the authors
using adiabatic logic. This Paper gives a complete and clear idea if the
thermodynamical concept for the electronics industries for power reduction.
"
730,"Specification and Verification of Uplink Framework for Application of
  Software Engineering using RM-ODP","  This paper present a survey and discussion of the Reference Model for Open
Distributed Processing (RM-ODP) viewpoints; oriented approaches to requirements
engineering viewpoint and a presentation of new work in the application
wireless mobile phone, this area which has been designed with practical
application using the Unified Modelling Language (UML)/VHDL_AMS (VHSIC Hardware
Description Language Analog and Mixed-Signal). We mainly focus on rising and
fulling time, action, uplink behaviour constraints (sequentiality, non
determinism and concurrency constraints).We discuss the practical problems of
introducing viewpoint; oriented requirements engineering into industrial
software engineering practice and why these have prevented the widespread use
of existing approaches. The goal of this article is to check the uplink path
using the MIC (Microphone amplifier) with all analog inputs, and check the
amplifier gain. This paper provides an example of using the Uplink Framework to
build a comprehensive, good solution for Application Wireless Mobile Phone.
Finally, we discuss how well this approach addresses some outstanding problems
in requirements engineering (RE) and the practical industrial problems of
introducing new requirements engineering methods.
"
731,"A Novel Window Function Yielding Suppressed Mainlobe Width and Minimum
  Sidelobe Peak","  In many applications like FIR filters, FFT, signal processing and
measurements, we are required (~45 dB) or less side lobes amplitudes. However,
the problem is usual window based FIR filter design lies in its side lobes
amplitudes that are higher than the requirement of application. We propose a
window function, which has better performance like narrower main lobe width,
minimum side lobe peak compared to the several commonly used windows. The
proposed window has slightly larger main lobe width of the commonly used
Hamming window, while featuring 6.2\ sim 22.62 dB smaller side lobe peak. The
proposed window maintains its maximum side lobe peak about -58.4 \sim -52.6 dB
compared to -35.8 \sim -38.8 dB of Hamming window for M=10~14, while offering
roughly equal main lobe width. Our simulated results also show significant
performance upgrading of the proposed window compared to the Kaiser, Gaussian,
and Lanczos windows. The proposed window also shows better performance than
Dolph-Chebyshev window. Finally, the example of designed low pass FIR filter
confirms the efficiency of the proposed window.
"
732,"Visitor schedule management system- an intelligent decision support
  system","  Travelling salesman problem is a problem which is of high interest for
researchers, industry professionals, and academicians. Visitor or salesman used
to face lot of problems with respect to scheduling based on meeting top ranked
clients. Even excel sheet made the work tedious. So these flaws propelled us to
design an intelligent decision support system. This paper reports the problem
definition we tried to address and possible solution to this problem. We even
explained the project design and implementation of our visitor schedule
management system.. Our system made a major contribution in terms of valuable
resources such as time and satisfying high ranked clients efficiently. We used
optimization via mathematical programming to solve these issues.
"
733,Traductor Writing System Web,"  A compilator is a program which is development in a programming language that
read a file known as source. After this file have to translate and have to
convert in other program known as object or to generate a exit. The best way
for to know any programming language is analizing a compilation process which
is same in all programming paradigm existents. To like to generate a tool that
permit a learning in university course. This course could explain in any
plataform such as Linux o Windows. This goal is posible through development a
Web aplication which is unite with a compilator, it is Traductor Writing System
(Sistema de Escritura de Traductores). This system is complete and permit
extend and modify the compilator. The system is a module in Moodle which is a
Course Management System (CMS) that help teachers for to create comunities of
learning in line. This software is in free software license (GPL).
"
734,"The necessities for building a model to evaluate Business Intelligence
  projects- Literature Review","  In recent years Business Intelligence (BI) systems have consistently been
rated as one of the highest priorities of Information Systems (IS) and business
leaders. BI allows firms to apply information for supporting their processes
and decisions by combining its capabilities in both of organizational and
technical issues. Many of companies are being spent a significant portion of
its IT budgets on business intelligence and related technology. Evaluation of
BI readiness is vital because it serves two important goals. First, it shows
gaps areas where company is not ready to proceed with its BI efforts. By
identifying BI readiness gaps, we can avoid wasting time and resources. Second,
the evaluation guides us what we need to close the gaps and implement BI with a
high probability of success. This paper proposes to present an overview of BI
and necessities for evaluation of readiness. Key words: Business intelligence,
Evaluation, Success, Readiness
"
735,The impact of pharmacybernetic in reducing medication error,"  Doctors and Pharmacists play a foremost role in safe, effective use of
medication in health care. Still, there is no database available through which
Doctor can communicate with all field of pharmacy such as hospital Pharmacy,
Clinical Pharmacy, Community Pharmacy, Nutrition Pharmacy and Drug research
center so that they would like to cooperate with pharmacists in Medication
error prevention, Drug-Disease management, Nutrition management, and
pharmacotherapy. The authors examined the comprehensive project of implementing
Electronic Drug Information Record (EDIR), introduce the new term
Pharmacybernetic and how to reduce the medication error by integrated
management system (IMS). This paper presented EDIR conceptual model and the
flow sheet of the Pharmacybernetic system, which describes the integration of
different Pharmaceutical related aspect in the field of Cybernetic.
"
736,"Factors affecting acceptance of web-based training system: Using
  extended UTAUT and structural equation modeling","  Advancement in information system leads organizations to apply e-learning
system to train their employees in order to enhance its performance. In this
respect, applying web based training will enable the organization to train
their employees quickly, efficiently and effectively anywhere at any time. This
research aims to extend Unified Theory of Acceptance and Use Technology (UTAUT)
using some factors such flexibility of web based training system, system
interactivity and system enjoyment, in order to explain the employees'
intention to use web based training system. A total of 290 employees have
participated in this study. The findings of the study revealed that performance
expectancy, facilitating conditions, social influence and system flexibility
have direct effect on the employees' intention to use web based training
system, while effort expectancy, system enjoyment and system interactivity have
indirect effect on employees' intention to use the system.
"
737,"Dynamic Grouping of Web Users Based on Their Web Access Patterns using
  ART1 Neural Network Clustering Algorithm","  In this paper, we propose ART1 neural network clustering algorithm to group
users according to their Web access patterns. We compare the quality of
clustering of our ART1 based clustering technique with that of the K-Means and
SOM clustering algorithms in terms of inter-cluster and intra-cluster
distances. The results show the average inter-cluster distance of ART1 is high
compared to K-Means and SOM when there are fewer clusters. As the number of
clusters increases, average inter-cluster distance of ART1 is low compared to
K-Means and SOM which indicates the high quality of clusters formed by our
approach.
"
738,Analysis of WiMAX Physical Layer Using Spatial Multiplexing,"  Broadband Wireless Access (BWA) has emerged as a promising solution for
providing last mile internet access technology to provide high speed internet
access to the users in the residential as well as in the small and medium sized
enterprise sectors. IEEE 802.16e is one of the most promising and attractive
candidate among the emerging technologies for broadband wireless access. The
emergence of WiMAX protocol has attracted various interests from almost all the
fields of wireless communications. MIMO systems which are created according to
the IEEE 802.16-2005 standard (WiMAX) under different fading channels can be
implemented to get the benefits of both the MIMO and WiMAX technologies. In
this paper analysis of higher level of modulations (i.e. M-PSK and M-QAM for
different values of M) with different code rates and on WiMAX-MIMO system is
presented for Rayleigh channel by focusing on spatial multiplexing MIMO
technique. Signal-to Noise Ratio (SNR) vs Bit Error Rate (BER) analysis has
been done.
"
739,A Greedy Double Swap Heuristic for Nurse Scheduling,"  One of the key challenges of nurse scheduling problem (NSP) is the number of
constraints placed on preparing the timetable, both from the regulatory
requirements as well as the patients' demand for the appropriate nursing care
specialists. In addition, the preferences of the nursing staffs related to
their work schedules add another dimension of complexity. Most solutions
proposed for solving nurse scheduling involve the use of mathematical
programming and generally considers only the hard constraints. However, the
psychological needs of the nurses are ignored and this resulted in subsequent
interventions by the nursing staffs to remedy any deficiency and often results
in last minute changes to the schedule. In this paper, we present a staff
preference optimization framework which is solved with a greedy double swap
heuristic. The heuristic yields good performance in speed at solving the
problem. The heuristic is simple and we will demonstrate its performance by
implementing it on open source spreadsheet software.
"
740,"Maximum Spanning Tree Model on Personalized Web Based Collaborative
  Learning in Web 3.0","  Web 3.0 is an evolving extension of the current web environme bnt.
Information in web 3.0 can be collaborated and communicated when queried. Web
3.0 architecture provides an excellent learning experience to the students. Web
3.0 is 3D, media centric and semantic. Web based learning has been on high in
recent days. Web 3.0 has intelligent agents as tutors to collect and
disseminate the answers to the queries by the students. Completely Interactive
learner's query determine the customization of the intelligent tutor. This
paper analyses the Web 3.0 learning environment attributes. A Maximum spanning
tree model for the personalized web based collaborative learning is designed.
"
741,Juppix: a Linux Live-CD for Undergraduate Students,"  Juppix is a Linux Live-CD with a comfortable programming environment for the
Java, C and O'Caml programming languages that has been distributed to hundreds
of undergaduate students at the University of Paris 7 over the last few years.
We describe the lessons we learnt while compiling and distributing Juppix, and
outline our future plans.
"
742,"A Full Performance Analysis of Channel Estimation Methods for Time
  Varying OFDM Systems","  In this paper, we have evaluated various methods of time-frequency-selective
fading channels estimation in OFDM system and some of them improved under time
varying conditions. So, these different techniques will be studied through
different algorithms and for different schemes of modulations (16 QAM, BPSK,
QPSK, ...). Channel estimation gathers different schemes and algorithms, some
of them are dedicated for slowly time varying (such as block type arrangement
insertion, Bayesian Cramer-Rao Bound, Kalman estimator, Subspace estimator,
...) whereas the others concern highly time varying channels (comb type
insertion, ...). There are others methods that are just suitable for stationary
channels like blind or semi blind estimators. For this aim, diverse algorithms
were used for these schemes such as Least Squares estimator LS, Least Minimum
Squares LMS, Minimum Mean-Square-Error MMSE, Linear Minimum Mean-Square-Error
LMMSE, Maximum Likelihood ML, ... to refine estimators shown previously.
"
743,"Finite State Machine based Vending Machine Controller with Auto-Billing
  Features","  Nowadays, Vending Machines are well known among Japan, Malaysia and
Singapore. The quantity of machines in these countries is on the top worldwide.
This is due to the modern lifestyles which require fast food processing with
high quality. This paper describes the designing of multi select machine using
Finite State Machine Model with Auto-Billing Features. Finite State Machine
(FSM) modelling is the most crucial part in developing proposed model as this
reduces the hardware. In this paper the process of four state (user Selection,
Waiting for money insertion, product delivery and servicing) has been modelled
using MEALY Machine Model. The proposed model is tested using Spartan 3
development board and its performance is compared with CMOS based machine.
"
744,"Adaptive Reduced-Rank LCMV Beamforming Algorithms Based on Joint
  Iterative Optimization of Filters: Design and Analysis","  This paper presents reduced-rank linearly constrained minimum variance (LCMV)
beamforming algorithms based on joint iterative optimization of filters. The
proposed reduced-rank scheme is based on a constrained joint iterative
optimization of filters according to the minimum variance criterion. The
proposed optimization procedure adjusts the parameters of a projection matrix
and an adaptive reducedrank filter that operates at the output of the bank of
filters. We describe LCMV expressions for the design of the projection matrix
and the reduced-rank filter. We then describe stochastic gradient and develop
recursive least-squares adaptive algorithms for their efficient implementation
along with automatic rank selection techniques. An analysis of the stability
and the convergence properties of the proposed algorithms is presented and
semi-analytical expressions are derived for predicting their mean squared error
(MSE) performance. Simulations for a beamforming application show that the
proposed scheme and algorithms outperform in convergence and tracking the
existing full-rank and reduced-rank algorithms while requiring comparable
complexity
"
745,"CloudPass - a passport system based on Cloud Computing and Near Field
  Communication","  Wireless communication has penetrated into all fields of technology,
especially in mobility, where wireless transactions are gaining importance with
improvements in standards like 3G and 4G. There are many technologies that
support the wireless forms of interactions between devices. One among them is
NFC - Near Field Communication. In addition to NFC, other external technologies
like Quick Response (QR) Codes assist in establishing interactions among
participating devices. In this paper, we examine an approach that will involve
standards and technologies like NFC, QR Codes and Cloud Infrastructure to
design a mobile application which will perform desired functionalities. Cloud
Storage is used as a reservoir to store the artifacts used by the application.
Development and testing of the application is initially carried out on
emulators or simulators followed by testing on real handsets/devices.
"
746,Determination of RF source power in WPSN using modulated backscattering,"  A wireless sensor network (WSN) is a wireless network consisting of spatially
distributed autonomous devices using sensors to cooperatively monitor physical
or environmental conditions, such as temperature, sound, vibration, pressure,
motion or pollutants, at different locations. During RF transmission energy
consumed by critically energy-constrained sensor nodes in a WSN is related to
the life time system, but the life time of the system is inversely proportional
to the energy consumed by sensor nodes. In that regard, modulated
backscattering (MB) is a promising design choice, in which sensor nodes send
their data just by switching their antenna impedance and reflecting the
incident signal coming from an RF source. Hence wireless passive sensor
networks (WPSN) designed to operate using MB do not have the lifetime
constraints. In this we are going to investigate the system analytically. To
obtain interference-free communication connectivity with the WPSN nodes number
of RF sources is determined and analyzed in terms of output power and the
transmission frequency of RF sources, network size, RF source and WPSN node
characteristics. The results of this paper reveal that communication coverage
and RF Source Power can be practically maintained in WPSN through careful
selection of design parameters
"
747,"Designing the Mode solving of the photonic crystal fiber via BPM and
  Exploring the Single-Mode Properties","  Microstructured optical fibers (MOFs) are one of the most exciting recent
developments in fiber optics. A MOF usually consists of a hexagonal arrangement
of air holes running down the length of a silica fiber surrounding a central
core of solid silica or, in some cases air. MOFs can exhibit a number of unique
properties, including zero dispersion at visible wavelengths and low or high
effective nonlinearity [3]-[17], By varying the size of the holes and their
number and position, one can also design MOFs with carefully controlled
dispersive and modal properties. In this paper, we analyze and modeling the
behavior of the photonic crystal fiber (PCF) by using in the first step a
propagator method based on the BPM method. With our BPM software, the electric
field contour of the fundamental mode of PCF was demonstrated. We also used it
to see the variation of the effective index; an effective index model confirms
that such a fiber can be single mode for any wavelength. It would make a study
of photonic crystal fibers, and a study of the numerical simulation methods
allow the simulation of optical properties and has modeled the propagation of
light in this fiber type. After that we use the V-parameter because it offers a
simple way to design a photonic crystal fiber (PCF), by basing in a recent
formulation of this parameter of a PCF, we provide numerically based empirical
expression for this quantity only dependent on the two structural parameters,
the air hole diameter and the hole-to-hole center spacing.
"
748,MIPS code compression,"  MIPS machine code is very structured: registers used before are likely to be
used again, some instructions and registers are used more heavily than others,
some instructions often follow each other and so on. Standard file compression
utilities, such as gzip and bzip2, does not take full advantage of the
structure because they work on byte-boundaries and don't see the underlying
instruction fields. My idea is to filter opcodes, registers and immediates from
MIPS binary code into distinct streams and compress them individually to
achieve better compression ratios. Several different ways to split MIPS code
into streams are considered. The results presented in this paper shows that a
simple filter can reduce final compressed size by up to 10 % with gzip and
bzip2.
"
749,Open source based cadastral information system : ANCFCC-MOROCCO,"  This present project is developing a geographic information system to support
the cadastral business. This system based on open source solutions which
developed within the National Agency of Land Registry, Cadastre and Cartography
(ANCFCC) enabling monitoring and analysis of cadastral procedures as well as
offering consumable services by other information systems: consultation and
querying spatial data. The project will also assist the various user profiles
in the completion of production tasks and the possibility to eliminate the
deficiencies identified to ensure an optimum level of productivity
"
750,"DSTN (Distributed Sleep Transistor Network) for Low Power Programmable
  Logic array Design","  With the high demand of the portable electronic products, Low- power design
of VLSI circuits & Power dissipation has been recognized as a challenging
technology in the recent years. PLA (Programming logic array) is one of the
important off shelf part in the industrial application. This paper describes
the new design of PLA using power gating structure sleep transistor at circuit
level implementation for the low power applications. The important part of the
power gating design i.e. header and footer switch selection is also describes
in the paper. The simulating results of the proposed architecture of the new
PLA is shown and compared with the conventional PLA. This paper clearly shows
the optimization in the reduction of power dissipation in the new design
implementation of the PLA. The transient response of the power gates structure
of PLA is also illustrate in the paper by using TINA-PRO software.
"
751,An Architecture for Context-Aware Knowledge Flow Management Systems,"  The organizational knowledge is one of the most important and valuable assets
of organizations. In such environment, organizations with broad, specialized
and up-to-date knowledge, adequately using knowledge resources, will be more
successful than their competitors. For effective use of knowledge, dynamic
knowledge flow from the sources to destinations is essential. In this regard, a
novel complex concept in knowledge management is the analysis, design and
implementation of knowledge flow management systems. One of the major
challenges in such systems is to explore the knowledge flow from the source to
the recipient and control the flow for quality improvements concerning the
users' needs as possible. Therefore, the purpose of this paper is to provide an
architecture in order to solve this challenge. For this purpose, in addition to
the architecture for knowledge flow management systems, a new node selection
strategy is provided with higher success rate compared to previous strategies.
"
752,Dynamic Threshold Optimization - A New Approach?,"  Dynamic Threshold Optimization (DTO) adaptively ""compresses"" the decision
space (DS) in a global search and optimization problem by bounding the
objective function from below. This approach is different from ""shrinking"" DS
by reducing bounds on the decision variables. DTO is applied to Schwefel's
Problem 2.26 in 2 and 30 dimensions with good results. DTO is universally
applicable, and the author believes it may be a novel approach to global search
and optimization.
"
753,"Transformation of Traditional Marketing Communications in to Paradigms
  of Social Media Networking","  Effective Communication for marketing is a vital field in business
organizations, which is used to convey the details about their products and
services to the market segments and subsequently to build long lasting customer
relationships. This paper focuses on an emerging component of the integrated
marketing communication, ie. social media networking, as it is increasingly
becoming the trend. In 21st century, the marketing communication platforms show
a tendency to shift towards innovative technology bound people networking which
is becoming an acceptable domain of interaction. Though the traditional
channels like TV, print media etc. are still active and prominent in marketing
communication, the presences of the Internet and more specifically the Social
Media Networking, has started influencing the way individuals and business
enterprises communicate. It has become evident that more individuals and
business enterprises are engaging the social media networking sites either to
accelerate the sales of their products and services or to provide post-purchase
feedbacks. This shift in scenario has motivated this research which took six
months (June 2011 - December 2011), using empirical analysis which is carried
out based on several primary and secondary evidences. The research paper also
analyzes the factors that govern the social media networking sites to influence
consumers and subsequently enable their purchase decisions. The secondary data
presented for this research were those pertaining to the period between the
year 2005 and year 2011. The study revealed promising facts like the transition
to marketing through SMN gives visible advantages like bidirectional
communication, interactive product presentation, and a firm influence on
customer who has a rudimentary interest...
"
754,Implementation of a Real Time Passenger Information System,"  Intelligent Transportation Systems (ITS) are gaining recognition in
developing countries like India. This paper describes the various components of
our prototype implementation of a Real-time Passenger Information System
(RTPIS) for a public transport system like a fleet of buses. Vehicle-mounted
units, bus station units and a server located at the transport company premises
comprise the system. The vehicle unit reports the current position of the
vehicle to a central server periodically via General Packet Radio Service
(GPRS). An Estimated Time of Arrival (ETA) algorithm running on the server
predicts the arrival times of buses at their stops based on real-time
observations of the buses' current Global Positioning System (GPS) coordinates.
This information is displayed and announced to passengers at stops using
station units, which periodically fetch the required ETA from the server via
GPRS. Novel features of our prototype include: (a) a route creator utility
which automatically creates new routes from scratch when a bus is driven along
the new route, and (b) voice tagging of stops and points of interest along any
route. Besides, the prototype provides: (i) web-based applications for
passengers, providing useful information like a snapshot of present bus
locations on the streets, and (ii) web-based analysis tools for the transport
authority, providing information useful for fleet management, like number of
trips undertaken by a specific bus. The prototype has been demonstrated in a
campus environment, with four-wheelers and two-wheelers emulating buses. The
automatic real-time passenger information system has the potential of making
the public transport system an attractive alternative for city-dwellers,
thereby contributing to fewer private vehicles on the road, leading to lower
congestion levels and less pollution.
"
755,ICT's role in e-Governance in India and Malaysia: A Review,"  Information and Communication Technologies (ICTs) play a key role in
Development & Economic growth of the Developing countries of the World.
Political, Cultural, Socio-economic Developmental & Behavioral decisions today
rests on the ability to access, gather, analyze and utilize Information and
Knowledge. Government of India is having an ambitious objective of transforming
the citizen-government interaction at all levels to by the electronic mode by
2020.Similarly according to the Vision 2020-The Way Forward presented by His
Excellency YAB Dato' Seri Dr Mahathir Mohamad at the Malaysian Business Council
""By the year 2020, Malaysia can be a united nation, with a confident Malaysian
society, infused by strong moral and ethical values, living in a society that
is democratic, liberal and tolerant, caring, economically just and equitable,
progressive and prosperous, and in full possession of an economy that is
competitive, dynamic, robust and resilient"". This paper presents a comparative
study and review relating to e-Governance and application of ICT development
between India & Malaysia.
"
756,"Soft Computing in Product Recovery: A Survey Focusing on Remanufacturing
  System","  This paper focuses on the application of soft computing in remanufacturing
system, in which end-of-life products are disassembled into basic components
and then remanufactured for both economic and environmental reasons. The
disassembly activities include disassembly sequencing and planning, while the
remanufacturing process is composed of product design, production planning &
scheduling, and inventory management. This paper presents a review of the
related articles and suggests the corresponding further research directions.
"
757,Iterated tabu search for the circular open dimension problem,"  This paper mainly investigates the circular open dimension problem (CODP),
which consists of packing a set of circles of known radii into a strip of fixed
width and unlimited length without overlapping. The objective is to minimize
the length of the strip. An iterated tabu search approach, named ITS, is
proposed. ITS starts from a randomly generated solution and attempts to gain
improvements by a tabu search procedure. After that, if the obtained solution
is not feasible, a perturbation operator is subsequently employed to
reconstruct the incumbent solution and an acceptance criterion is implemented
to determine whether or not accept the perturbed solution. This process is
repeated until a feasible solution has been found or the allowed computation
time has been elapsed. Computational experiments based on well-known benchmark
instances show that ITS produces quite competitive results with respect to the
best known results. For 18 representative CODP instances taken from the
literature, ITS succeeds in improving 13 best known results within reasonable
time. In addition, for another challenging related variant: the problem of
packing arbitrary sized circles into a circular container, ITS also succeeds in
improving many best known results. Supplementary experiments are also provided
to analyze the influence of the perturbation operator, as well as the
acceptance criterion.
"
758,"Reciprocally induced coevolution: A computational metaphor in
  Mathematics","  Natural phenomenon of coevolution is the reciprocally induced evolutionary
change between two or more species or population. Though this biological
occurrence is a natural fact, there are only few attempts to use this as a
simile in computation. This paper is an attempt to introduce reciprocally
induced coevolution as a mechanism to counter problems faced by a typical
genetic algorithm applied as an optimization technique. The domain selected for
testing the efficacy of the procedure is the process of finding numerical
solutions of Diophantine equations. Diophantine equations are polynomial
equations in Mathematics where only integer solutions are sought. Such
equations and its solutions are significant in three aspects-(i) historically
they are important as Hilbert's tenth problem with a background of more than
twenty six centuries; (ii) there are many modern application areas of
Diophantine equations like public key cryptography and data dependency in super
computers (iii) it has been proved that there does not exist any general method
to find solutions of such equations. The proposed procedure has been tested
with Diophantine equations with different powers and different number of
variables.
"
759,"Analytical Study for Seeking Relation Between Customer Relationship
  Management and Enterprise Resource Planning","  Enterprise Resource Planning (ERP) is a integration of various resources of
any organization. It is computer software. All kinds of organization data that
is relating to each and every function of the organization are available in
ERP. So most of the big business organizations are implementing ERP and some of
the medium, small scale companies are also using ERP system. CRM in an
organization helps to retain their existing customers as well as capturing new
customers for their products. So it makes the organization to produce those
goods required by their consumers. This paper focuses mainly on the merging of
CRM and ERP through Neural Networks.
"
760,C-Band VSAT Data Communication System and RF Impairments,"  This paper is concerned with modelling and simulation of VSAT (very small
aperture terminal) data messaging network operating in India at Karnataka with
extended C-band. VSATs in Karnataka of KPTCL use VSATS 6.875-6.9465G Hz uplinks
and 4.650- 4.7215 GHz downlinks. These frequencies are dedicated to fixed
services. The Satellite is Intelsat -3A, the hub has a 7.2 m diameter antenna
and uses 350W or 600W TWTA (Travelling wave Tube Amplifier). The VSAT's are 1.2
m with RF power of 1W or 2W depending on their position in the uplink beam with
data rate of 64 or 128 K bit/s. The performance of the system is analysed by
the error probability called BER (Bit Error Rate) and results are derived from
Earth station to hub and hub to Earth station using satellite Transponder as
the media of communication channel. The Link budgets are developed for a single
one-way satellite link.
"
761,"The Use of Fuzzy Cognitive Maps in Analyzing and Implementation of ITIL
  Processes","  Information Technology Infrastructure Library (ITIL) is series of best
practices that helps Information technology Organizations to provide
Information technology (IT) services for their customers with better
performances and quality. This article is looking for a way to implement ITIL
in an organization and also using Fuzzy Cognitive Maps (FCM) to model the
problem for better understanding of environment. ITIL helps to improve the
performance of IT services in order to gain business objectives and Fuzzy
Cognitive Maps will help to model the problem of needing ITIL processes for
those objectives. First, it defines the concept of FCM and ITIL in two separate
sections and then, it will describe the relationship and the way that FCM helps
to implement ITIL. The paper will measure the cost of service support that is
depended on the metrics like changes Authorization Degree, Process Oriented
activities degree, Response time and Interrupt time. This paper will be used as
a part of gap analyzes step in implementing ITIL in each organizations.
"
762,"Combining configuration and recommendation to define an interactive
  product line configuration approach","  This paper is interested in e-commerce for complex configurable
products/systems. In e-commerce, satisfying the customer needs is a vital
concern. One particular way to achieve this is to offer customers a panel of
options among which they can select their preferred ones. While solution
exists, they are not adapted for highly complex configurable systems such as
product lines. This paper proposes an approach that combines two complementary
forms of guidance: configuration and recommendation, to help customers define
their own products out of a product line specification. The proposed approach,
called interactive configuration supports the combination by organizing the
configuration process in a series of partial configurations where decisions are
made by the recommendation.
"
763,Information Security Awareness Within Business Environment: An IT Review,"  The beauty of Information Technology (IT) is with its multifunction nature;
it is a support system, a networking system, a storage system, as well as an
information facilitator. Aided with their broad line of services, an IT system
aims to support or even drive organizations towards desired paths. Trends of IT
and information security awareness (ISA) in society today, particularly within
the business environment is quite interesting phenomenon. The overviews of the
role of IT in the modern world as well as the perception towards ISA are
initially introduced. A series of scope are outlined, and also further
examination on matter of IT and ISA in the business environment-emphasis on
revolution of business with ISA, security threats such as identity thefts,
hacking and web harassment, and the different mode of protections that are
applied in different business environments. Unfortunately, the advancement of
IT is not followed by the awareness of its security issues properly, especially
in the context of the business settings and functions. This research and review
is expected to influence the awareness of information security issues in
business processes.
"
764,"A Novel Low Power UWB Cascode SiGe BiCMOS LNA with Current Reuse and
  Zero-Pole Cancellation","  A low power cascode SiGe BiCMOS low noise amplifier (LNA) with current reuse
and zero-pole cancellation is presented for ultra-wideband (UWB) application.
The LNA is composed of cascode input stage and common emitter (CE) output stage
with dual loop feedbacks. The novel cascode-CE current reuse topology replaces
the traditional two stages topology so as to obtain low power consumption. The
emitter degenerative inductor in input stage is adopted to achieve good input
impedance matching and noise performance. The two poles are introduced by the
emitter inductor, which will degrade the gain performance, are cancelled by the
dual loop feedbacks of the resistance-inductor (RL) shunt-shunt feedback and
resistance-capacitor (RC) series-series feedback in the output stage.
Meanwhile, output impedance matching is also achieved. Based on TSMC 0.35{\mu}m
SiGe BiCMOS process, the topology and chip layout of the proposed LNA are
designed and post-simulated. The LNA achieves the noise figure of 2.3~4.1dB,
gain of 18.9~20.2dB, gain flatness of \pm0.65dB, input third order intercept
point (IIP3) of -7dBm at 6GHz, exhibits less than 16ps of group delay
variation, good input and output impedances matching, and unconditionally
stable over the whole band. The power consuming is only 18mW.
"
765,Pricing of insurance policies against cloud storage price rises,"  When a company migrates to cloud storage, the way back is neither easy nor
cheap. The company is then locked up in the storage contract and exposed to
upward market prices, which reduce the company's profit and may even bring it
below zero. We propose a protection means based on an insurance contract, by
which the cloud purchaser is indemnified when the current storage price exceeds
a pre-defined threshold. By applying the financial options theory, we provide a
formula for the insurance price (the premium). By using historical data on
market prices for disks, we apply the formula in realistic scenarios. We show
that the premium grows nearly quadratically with the length of the coverage
period as long as this is below one year, but grows more slowly, though faster
than linearly, over longer coverage periods.
"
766,"Defining the symmetry of the universal semi-regular autonomous
  asynchronous systems","  The regular autonomous asynchronous systems are the non-deterministic Boolean
dynamical systems and universality means the greatest in the sense of the
inclusion. The paper gives four definitions of symmetry of these systems in a
slightly more general framework, called semi-regularity and also many examples.
"
767,"On the basins of attraction of the regular autonomous asynchronous
  systems","  The Boolean autonomous dynamical systems, also called regular autonomous
asynchronous systems are systems whose 'vector field' is a function
{\Phi}:{0,1}^{n}{\to}{0,1}^{n} and time is discrete or continuous. While the
synchronous systems have their coordinate functions {\Phi}_{1},...,{\Phi}_{n}
computed at the same time:
{\Phi},{\Phi}{\circ}{\Phi},{\Phi}{\circ}{\Phi}{\circ}{\Phi},... the
asynchronous systems have {\Phi}_{1},...,{\Phi}_{n} computed independently on
each other. The purpose of the paper is that of studying the basins of
attraction of the fixed points, of the orbits and of the {\omega}-limit sets of
the regular autonomous asynchronous systems. The bibliography consists in
analogies.
"
768,"Universal Regular Autonomous Asynchronous Systems: Fixed Points,
  Equivalencies and Dynamic Bifurcations","  The asynchronous systems are the non-deterministic models of the asynchronous
circuits from the digital electrical engineering. In the autonomous version,
such a system is a set of functions x:R{\to}{0,1}^{n} called states (R is the
time set). If an asynchronous system is defined by making use of a so called
generator function {\Phi}:{0,1}^{n}{\to}{0,1}^{n}, then it is called regular.
The property of universality means the greatest in the sense of the inclusion.
The purpose of the paper is that of defining and of characterizing the fixed
points, the equivalencies and the dynamical bifurcations of the universal
regular autonomous asynchronous systems. We use analogies with the dynamical
systems theory.
"
769,"The decomposition of the regular asynchronous systems as parallel
  connection of regular asynchronous systems","  The asynchronous systems are the non-deterministic models of the asynchronous
circuits from the digital electrical engineering, where non-determinism is a
consequence of the fact that modelling is made in the presence of unknown and
variable parameters. Such a system is a multi-valued function f that assigns to
an (admissible) input u:R{\to}{0,1}^{m} a set f(u) of (possible) states
x:R{\to}{0,1}^{n}. When this assignment is defined by making use of a so-called
generator function {\Phi}:{0,1}^{n}{\times}{0,1}^{m}{\to}{0,1}^{n}, then the
asynchronous system f is called regular. The generator function {\Phi} acts in
this asynchronous framework similarly with the next state function from a
synchronous framework. The parallel connection of the asynchronous systems f'
and f"" is the asynchronous system (f'||f"")(u)=f'(u){\times}f""(u). The purpose
of the paper is to give the circumstances under which a regular asynchronous
system f may be written as a parallel connection of regular asynchronous
systems.
"
770,Usage and Impact of ICT in Education Sector; A Study of Pakistan,"  In many countries, information and communication technology (ICT) has a lucid
impact on the development of educational curriculum. This is the era of
Information Communication Technology, so to perk up educational planning it is
indispensable to implement the ICT in Education sector. Student can perform
well throughout the usage of ICT. ICT helps the students to augment their
knowledge skills as well as to improve their learning skills. To know with
reference to the usage and Impact of ICT in Education sector of Pakistan, we
accumulate data from 429 respondents from 5 colleges and universities, we use
convenient sampling to accumulate the data from district Rawalpindi of
Pakistan. The consequences show that Availability and Usage of ICT improves the
knowledge and learning skills of students. This indicates that existence of ICT
is improving the educational efficiency as well as obliging for making policies
regarding education sector.
"
771,"It takes two to tango. A Review of the Empirical Literature on
  Information Technology Outsourcing Relationship Satisfaction","  There is growing recognition that the overall client-vendor relationship, and
not only the contract, plays a critical role in Information Technology
Outsourcing (ITO) success. However, our understanding of how ITO relationships
function is limited. This paper contributes to this understanding by reviewing
empirical literature on ITO success in terms of relationship satisfaction. A
key finding is that the majority of reviewed studies concentrates on client
satisfaction, thus neglecting the vendor perspective. We argue that this raises
questions about the construct validity of these studies. Consequently, concerns
exist about the validity and reliability of their empirical findings. Some
scholars have acknowledged the problem and use a dyadic perspective. However, a
review of these studies reveals that the authors have underestimated their
contributions and do not explain why there is a problem. Therefore, the purpose
of this paper is to highlight their contributions by comparing the findings of
the dyadic perspective studies with those of the ""client perspective"" research.
In doing so, we assess whether the dyadic studies produce better explanations
for ITO success than the client-oriented studies. We argue that this is indeed
the case, by producing a better view on how underlying mechanisms of ITO
relationships work.
"
772,"A Multi-State Power Model for Adequacy Assessment of Distributed
  Generation via Universal Generating Function","  The current and future developments of electric power systems are pushing the
boundaries of reliability assessment to consider distribution networks with
renewable generators. Given the stochastic features of these elements, most
modeling approaches rely on Monte Carlo simulation. The computational costs
associated to the simulation approach force to treating mostly small-sized
systems, i.e. with a limited number of lumped components of a given renewable
technology (e.g. wind or solar, etc.) whose behavior is described by a binary
state, working or failed. In this paper, we propose an analytical multi-state
modeling approach for the reliability assessment of distributed generation
(DG). The approach allows looking to a number of diverse energy generation
technologies distributed on the system. Multiple states are used to describe
the randomness in the generation units, due to the stochastic nature of the
generation sources and of the mechanical degradation/failure behavior of the
generation systems. The universal generating function (UGF) technique is used
for the individual component multi-state modeling. A multiplication-type
composition operator is introduced to combine the UGFs for the mechanical
degradation and renewable generation source states into the UGF of the
renewable generator power output. The overall multi-state DG system UGF is then
constructed and classical reliability indices (e.g. loss of load expectation
(LOLE), expected energy not supplied (EENS)) are computed from the DG system
generation and load UGFs. An application of the model is shown on a DG system
adapted from the IEEE 34 nodes distribution test feeder.
"
773,Topological model for machining of parts with complex shapes,"  Complex shapes are widely used to design products in several industries such
as aeronautics, automotive and domestic appliances. Several variations of their
curvatures and orientations generate difficulties during their manufacturing or
the machining of dies used in moulding, injection and forging. Analysis of
several parts highlights two levels of difficulties between three types of
shapes: prismatic parts with simple geometrical shapes, aeronautic structure
parts composed of several shallow pockets and forging dies composed of several
deep cavities which often contain protrusions. This paper mainly concerns High
Speed Machining (HSM) of these dies which represent the highest complexity
level because of the shapes' geometry and their topology. Five axes HSM is
generally required for such complex shaped parts but 3 axes machining can be
sufficient for dies. Evolutions in HSM CAM software and machine tools lead to
an important increase in time for machining preparation. Analysis stages of the
CAD model particularly induce this time increase which is required for a wise
choice of cutting tools and machining strategies. Assistance modules for
prismatic parts machining features identification in CAD models are widely
implemented in CAM software. In spite of the last CAM evolutions, these kinds
of CAM modules are undeveloped for aeronautical structure parts and forging
dies. Development of new CAM modules for the extraction of relevant machining
areas as well as the definition of the topological relations between these
areas must make it possible for the machining assistant to reduce the machining
preparation time. In this paper, a model developed for the description of
complex shape parts topology is presented. It is based on machining areas
extracted for the construction of geometrical features starting from CAD models
of the parts. As topology is described in order to assist machining assistant
during machining process generation, the difficulties associated with tasks he
carried out are analyzed at first. The topological model presented after is
based on the basic geometrical features extracted. Topological relations which
represent the framework of the model are defined between the basic geometrical
features which are gathered afterwards in macro-features. Approach used for the
identification of these macro-features is also presented in this paper.
Detailed application on the construction of the topological model of forging
dies is presented in the last part of the paper.
"
774,Prediction of under pickling defects on steel strip surface,"  An extremely important part of the finishing line is the pickling process, in
which oxides formed during the hot rolling stage are removed from the surface
of the steel sheets. The efficiency of the pickling process is mainly dependent
on the nature of the oxide present at the surface of the steel, but, also, on
process parameters such as bath composition and time duration are relevant.
When acid concentration, solution temperatures and line speed are not properly
balanced, in fact, sheet defects like under pickling or over pickling may
happen and their occurrence does have a very serious effect on cold-reduction
performance and surface appearance of the finished product. Furthermore,
product damage from handling or improper equipment adjustment can render the
steel unsuitable for further processing. This is the reason why it is important
that process significant parameters are controlled and maintained as accurately
as possible in order to avoid these undesired phenomena. In the present work, a
control algorithm, composed by two different modules, i.e. decision tree and
rectangular Basis Function Network, has been implemented to aim of predicting
pickling defects and suggesting the optimal speed or the admissible speed range
of the steel strip in the process line. In this way the most suitable line
speed value can be set in an automatic way or by the technical personnel.
"
775,"Design and Performance Analysis Of Ultra Low Power 6T SRAM Using
  Adiabatic Technique","  Power consumption has become a critical concern in both high performance and
portable applications. Methods for power reduction based on the application of
adiabatic techniques to CMOS circuits have recently come under renewed
investigation. In thermodynamics, an adiabatic energy transfer through a
dissipative medium is one in which losses are made arbitrarily small by causing
the transfer to occur sufficiently slowly. In this work adiabatic technique is
used for reduction of average power dissipation. Simulation of 6T SRAM cell has
been done for 180nm CMOS technology. It shows that average power dissipation is
reduced up to 75% using adiabatic technique and also shows the effect on static
noise margin.
"
776,"Design and Development of Low Cost PC Based Real Time Temperature and
  Humidity Monitoring System","  This paper presents the design and development of a low cost Data Acquisition
System (DAS) using PIC12F675 microcontroller for real time temperature and
humidity monitoring. The designed DAS has 4 analog input channels having 10-bit
resolution and was interfaced through the serial port of the PC. A precision
integrated temperature sensor and an instrumentation-quality RH (Relative
Humidity) sensor were used for sensing the temperature and humidity
respectively. The firmware was written in Basic and compiled using Oshonsoft
PIC IDE and downloaded to the microcontroller by using PICkit2 programmer. An
application program was also developed using Visual Basic 6, which allows
displaying the waveform of the signal(s) in real time and the data can be saved
into the hard disk of the computer for future use and analysis. It can also be
interfaced to the USB port of the PC or laptop using USB to serial adapter BAFO
BF-810. Thus, the designed low cost device works with the legacy hardware as
well as the modern USB interface.
"
777,Statistical Simulation Models for Cascaded Rayleigh Fading Channels,"  In this paper, we present statistical simulators for cascaded Rayleigh fading
channels with and without line-of-sight (LOS). These simulators contain two
individual summations and are therefore easy to implement with lower
complexity. Detailed statistical properties, including auto- and
cross-correlations of the in-phase, quadrature components of the channels,
envelopes, and squared envelopes, are derived. The time-average statistical
properties and the corresponding variance are also investigated to justify that
the proposed simulators achieve good convergence performance.
  Extensive Monte Carlo simulations are performed for various statistical
properties to validate the proposed simulators. Results show that the
simulators provide fast convergence to all desired statistical properties,
including the probability density function (PDF), various auto- and
cross-correlations, level crossing rate (LCR), and average fading duration
(AFD).
  While various tests and measurements in dense scattering urban and forest
environments indicate that mobile-to-mobile channels may experience cascaded
Rayleigh fading, the proposed statistical models can be applied to simulate the
underlying channels.
"
778,Acquiring IT Solutions through Open Source Software,"  Open source software is free software that provides user freedom to use,
replicate, modify, and distribute for any purpose. The quality of well-known
open source software is very high and they are used by big companies such as
IBM, Google and Amazon.com. Recently the number of open source software project
growing very fast, which indicates that adoption of open source software is
growing although still limited. Businesses should consider open source software
as alternative solutions to their business problems or opportunities. An
example of a very good open source software for office suite is discussed and
compared with the well-known proprietary counterpart.
"
779,"Developing an Activity-Based Costing Approach to Maximize the Efficiency
  of Customer Relationship Management Projects","  In today's competitive environment, profitability analysis is not just about
looking at the profit and loss statement. It is more about knowing which of
your customers are making you money and which are losing you money. This paper
considers how activity-based costing approach may complement a customer
relationship management effort. The model presented in this paper combines the
principles of activity-based costing with performance measurement. Applying
this model helps managers understand the true costs of providing products and
services, and the factors that drive these costs, while addressing other
concerns such as customer satisfaction. This approach has the potential to
integrate all business processes around the requirements of significant
profitable customers, a fact that most of the previous researches fail to
acknowledge.
"
780,Clown: a Microprocessor Simulator for Operating System Studies,"  In this paper, I present the design and implementation of Clown--a simulator
of a microprocessor-based computer system specifically optimized for teaching
operating system courses at undergraduate or graduate levels. The package
includes the simulator itself, as well as a collection of basic I/O devices, an
assembler, a linker, and a disk formatter. The simulator architecturally
resembles mainstream microprocessors from the Intel 80386 family, but is much
easier to learn and program. The simulator is fast enough to be used as an
emulator--in the direct user interaction mode.
"
781,"The Effectiveness of Virtual R&D Teams in SMEs: Experiences of Malaysian
  SMEs","  The number of small and medium enterprises (SMEs), especially those involved
with research and development (R&D) programs and employed virtual teams to
create the greatest competitive advantage from limited labor are increasing.
Global and localized virtual R&D teams are believed to have high potential for
the growth of SMEs. Due to the fast-growing complexity of new products coupled
with new emerging opportunities of virtual teams, a collaborative approach is
believed to be the future trend. This research explores the effectiveness of
virtuality in SMEs' virtual R&D teams. Online questionnaires were emailed to
Malaysian manufacturing SMEs and 74 usable questionnaires were received,
representing a 20.8 percent return rate. In order to avoid biases which may
result from pre-suggested answers, a series of open-ended questions were
retrieved from the experts. This study was focused on analyzing an open-ended
question, whereby four main themes were extracted from the experts'
recommendations regarding the effectiveness of virtual teams for the growth and
performance of SMEs. The findings of this study would be useful to product
design managers of SMEs in order to realize the key advantages and significance
of virtual R&D teams during the new product development (NPD) process. This in
turn, leads to increased effectiveness in new product development's procedure.
"
782,"Universal Numeric Segment Display for Indian Scheduled Languages: an
  Architectural View","  India is country of several hundred different languages. Though twenty two
languages have only been devised as scheduled to the Eighth Schedule of Indian
Constitution in 2007. But as there is yet no proposed compact display
architecture to display all the scheduled language numerals at a time, this
paper proposes a uniform display architecture to display all twenty two
different language digits with higher accuracy and simplicity by using a
17-segment display, which is an improvement over the 16-segment display.
"
783,"Portals and Task Innovation: A Theoretical Framework Founded on Business
  Intelligence Thinking","  The main aim of this study is to develop a theoretical framework for the
success of Web portals in promoting task innovation. This is deemed significant
as yet little research has tackled this important domain from the business
intelligence perspective. The D&M IS Success Model was used as a foundational
theory and then was refined to match the context of the current research.
Importantly, in this study, system quality and information quality constructs
were defined on the basis of portals' characteristics since a mapping was
conducted between the most significant functions and features of Web portals
and quality constructs. The developed framework is deemed useful for theory and
practice. From theoretical perspective, the dimensions that affect the
perceived quality of Web portals are identified, and the measures that affect
each quality dimension are also defined. On the practical level, contributions
gained by this study can be observed in terms of the benefits decision makers,
strategists, operational employees and IT developers can gain. Assessing
portals success in improving task innovation is important to help managers
(i.e. decision makers) in making appropriate decisions concerning the adoption
of portals' technology, by weighing its benefits against the costs needed to
establish and run such a technology. Moreover, assessing Web portals' success
gives some insight to IT developers and designers concerning what aspects
should be taken when designing and establishing high quality portals, and what
functions and features should be contained that would affect the perceived
quality of portals and therefore users' intention to use portals.
"
784,Establishing Virtual R&D Teams: Obliged Policy,"  In a global and technology oriented world the requirements that products and
services have to fulfill are increasing and are getting more complicated.
Research and development (R&D) is becoming increasingly important in creating
the knowledge that makes research and business more competitive. Companies are
obliged to produce more rapidly, more effectively and more efficiently. In
order to meet these requirements and to secure the viability of business
processes, services and products R&D teams need to access and retrieve
information from as many sources as possible. From the other perspective
virtual teams are important mechanisms for organizations seeking to leverage
scarce resources across geographic and other boundaries moreover; virtual
collaboration has become vital for most organizations. This is particularly
true in the context of designing new product and service innovation. Such
collaboration often involves a network of partners located around the world.
However at the R&D project level, dealing with such distributed teams
challenges both managers and specialists. In new product development, it is
necessary to put together the growing different capabilities and services with
the goal, through cooperation between suppliers and customers, service
providers and scientific institutions to achieve innovations of high quality.
In this paper based on comprehensive literature review of recent articles, at
the first step provides an primary definition and characterization of virtual
R&D team; next, the potential value created by virtual R&D teams for new
product development is explored and lastly along with a guide line for future
study, it is argued that the establishing of virtual R&D teams should be given
consideration in the management of R&D projects.
"
785,Performance of MIMO Relay DCSK-CD Systems over Nakagami Fading Channels,"  A multi-access multiple-input multiple-output (MIMO) relay differential chaos
shift keying cooperative diversity (DCSK-CD) system is proposed in this paper
as a comprehensive cooperation scheme, in which the relay and destination both
employ multiple antennas to strengthen the robustness against signal fading in
a wireless network. It is shown that, with spatial diversity gains, the bit
error rate (BER) performance of the proposed system is remarkably better than
the conventional DCSK non-cooperation (DCSK-NC) and DCSK cooperative
communication (DCSK-CC) systems. Moreover, the exact BER and close-form
expressions of the proposed system are derived over Nakagami fading channels
through the moment generating function (MGF), which is shown to be highly
consistent with the simulation results. Meanwhile, this paper illustrates a
trade-off between the performance and the complexity, and provides a threshold
for the number of relay antennas keeping the user consumed energy constant. Due
to the above-mentioned advantages, the proposed system stands out as a good
candidate or alternative for energy-constrained wireless communications based
on chaotic modulation, especially for low-power and low-cost wireless personal
area networks (WPANs).
"
786,"Compressed Sensing based Protocol for Efficient Reconstruction of Sparse
  Superimposed Data in a Multi-Hop Wireless Sensor Network","  We consider a multi-hop wireless sensor network that measures sparse events
and propose a simple forwarding protocol based on Compressed Sensing (CS) which
does not need any sophisticated Media Access Control (MAC) scheduling, neither
a routing protocol, thereby making significant overhead and energy savings. By
means of flooding, multiple packets with different superimposed measurements
are received simultaneously at any node. Thanks to our protocol, each node is
able to recover each measurement and forward it while avoiding cycles.
Numerical results show that our protocol achieves close to zero reconstruction
errors at the sink, while greatly reducing overhead. This initial research
reveals a new and promising approach to protocol design through CS for wireless
mesh and sensor networks.
"
787,"Deploying Health Monitoring ECU Towards Enhancing the Performance of
  In-Vehicle Network","  Electronic Control Units (ECUs) are the fundamental electronic building
blocks of any automotive system. They are multi-purpose, multi-chip and
multicore computer systems where more functionality is delivered in software
rather than hardware. ECUs are valuable assets for the vehicles as critical
time bounded messages are communicated through. Looking into the safety
criticality, already developed mission critical systems such as ABS, ESP etc,
rely fully on electronic components leading to increasing requirements of more
reliable and dependable electronic systems in vehicles. Hence it is inevitable
to maintain and monitor the health of an ECU which will enable the ECUs to be
followed, assessed and improved throughout their life-cycle starting from their
inception into the vehicle. In this paper, we propose a Health monitoring ECU
that enables the early trouble shooting and servicing of the vehicle prior to
any catastrophic failure.
"
788,"Design For Change: Information-Centric Architecture to Support Agile
  Disaster Response","  This paper presents a case for the adoption of an information-centric
architecture for a global disaster management system. Drawing from a case study
of the 2010/2011 Queensland floods, we describe the challenges in providing
every participant with relevant and actionable information. We use various
examples to argue for a more flexible information dissemination framework which
is designed from the ground up to minimise the effort needed to fix the
unexpected and unavoidable information acquisition, quality, and dissemination
challenges posed by any real disaster.
"
789,"Analysis of Trim Commands on Overprovisioning and Write Amplification in
  Solid State Drives","  This paper presents a performance model of the ATA/ATAPI SSD Trim command
under various types of user workloads, including a uniform random workload, a
workload with hot and cold data, and a workload with N temperatures of data. We
first examine the Trim-modified uniform random workload to predict utilization,
then use this result to compute the resultant level of effective
overprovisioning. This allows modification of models previously suggested to
predict write amplification of a non-Trim uniform random workload under greedy
garbage collection. Finally, we expand the theory to cover a workload
consisting of hot and cold data (and also N temperatures of data), providing
formulas to predict write amplification in these scenarios.
"
790,`CodeAliker' - Plagiarism Detection on the Cloud,"  Plagiarism is a burning problem that academics have been facing in all of the
varied levels of the educational system. With the advent of digital content,
the challenge to ensure the integrity of academic work has been amplified. This
paper discusses on defining a precise definition of plagiarized computer code,
various solutions available for detecting plagiarism and building a cloud
platform for plagiarism disclosure. 'CodeAliker', our application thus
developed automates the submission of assignments and the review process
associated for essay text as well as computer code. It has been made available
under the GNU's General Public License as a Free and Open Source Software.
"
791,Empowered Customers in E-Health Business Process,"  E-health innovations support empowered customers. It offers the ability for
customers to have greater control and ready access applications of health
information, clinical information, and social interaction between interested
groups. However, providing empowerment in any state of interaction levels to
customers (patients) in a healthcare organization is challenging tasks.
Customers are empowered in the sense of controlling the process of interaction
between a firm with its customers, and among customers themselves. This paper
discusses dimension of customers' empowerment in e-health business process. We
propose reference model of Personal Health Cycle (PHC) as a holistic view of
healthcare business process. The PHC is used to define and distinct electronic
health record (EHR) from electronic medical record (EMR) and customers
empowerment.
"
792,Personal Safety Triggering System on Android Mobile Platform,"  Introduction of Smart phones redefined the usage of mobile phones in the
communication world. Smart phones are equipped with various sophisticated
features such as Wi-Fi, GPS navigation, high resolution camera, touch screen
with broadband access which helps the mobile phone users to keep in touch with
the modern world. Many of these features are primarily integrated with the
mobile operating system which is out of reach to public, by which the users
can't manipulate those features. Google came up with an innovative operation
system termed as ANDROID, which is open system architecture with customizable
third party development and debugging environment which helps the user's to
manipulate the features and to create their own customizable applications. In
this paper, 'Emergency Based Remote Collateral Tracking System' application
using Google's Android Mobile Platform is addressed. Emergency is divided into
three categories: heart beat based emergency, security threats like personal
safety and road accidents. This application is targeted to a person who is
driving a vehicle. Heart rate monitoring device is integrated with our
application to sense the heart beat of a person driving the vehicle and if
there is any abnormalities in the heart beat, then our application performs a
dual role. One in which, application uses a GPS to track the location
information of the user and send those location information as a message via
SMS, email and post it on Facebook wall Simultaneously, an emergency signal is
sent to Arduino Microcontroller.This application is written in JAVA programming
language which runs on Eclipse Integrated Development Kit.
"
793,Adding Methodological Testing to Naur's Anti-formalism,"  Peter Naur is the leading critic of formalist computing because of his
extensive writings that disprove the now dominate characterization of human
thought as cognitive information processing. Naur criticizes the ideological
position that only discourse that adopts computer inspired forms are
acceptable. Lakatosian philosophy of the methodology of scientific research
programmes (MSRP) is added to Naur's studies to allow testing of computing
theories. After discussing Naur's criticism of mechanical cognitive information
processing, I show how to add MSRP competition to Naur's descriptive
philosophy. Next, Naur's claim that computing can not become scientific until
organizational issues involving ideological suppression of discussions of
computing and human thinking are solved is corroborated by institutional
suppression of my 1970s attempts to criticize structured programming (SP).
  Various problems in computing related philosophy are discussed. First, I
argue that my MSRP based degenerating research programme disproof of SP is
better than Naur's programming as a human activity, Demillo's social processes
and Fetzer's unprovable causal nature. Three areas for post ideologically based
computing study are discussed: computing as a path to rediscovering 19th
century conceptions of infinity, axiom of choice testing facilitated by
computing and relation to physical theory, and testing concrete complexity
methods based on efficiency proof analysis.
"
794,Business Intelligence: A Rapidly Growing Option through Web Mining,"  The World Wide Web is a popular and interactive medium to distribute
information in this scenario. The web is huge, diverse, ever changing, widely
disseminated global information service center. We are familiar with terms like
e-commerce, e-governance, e-market, e-finance, e-learning, e-banking etc. for
an organization it is new challenge to maintain direct contact with customers
because of the rapid growth in e-commerce, e-publishing and electronic service
delivery. To deal with this there is need of intelligent marketing strategies
and CRM (customer relationship management) i.e. the effective way of
integrating enterprise applications in real time. Web mining is the vast field
that helps to understand various concepts of different fields. Web usage mining
techniques are attempted to reason about different materialized issues of
Business Intelligence which include marketing expertise as domain knowledge and
are specifically designed for electronic commerce purposes. To this end, the
chapter provides an introduction to the field of Web mining and examines
existing as well as potential Web mining applications applicable for different
business function, like marketing, human resources, and fiscal administration.
Suggestions for improving information technology infrastructure are made, which
can help businesses interested in Web mining hit the ground running.
"
795,Resonant Clocking Circuits for Reversible Computation,"  A mechanism for the reduction of dynamic energy dissipation in the computing
circuit is described. The resonant circuit with controlled switches conserves
the stored energy by recovering upto 90% of energy that would be otherwise lost
during logic state transitions. This energy-conserving approach preserves
thermodynamic entropy, ideally preventing heat generation in the system. This
approach is used in a proposed resonant clocking and logic application without
dynamic energy dissipation.
"
796,Survey of Multiscale and Multiphysics Applications and Communities,"  Multiscale and multiphysics applications are now commonplace, and many
researchers focus on combining existing models to construct combined multiscale
models. Here we present a concise review of multiscale applications and their
source communities. We investigate the prevalence of multiscale projects in the
EU and the US, review a range of coupling toolkits they use to construct
multiscale models and identify areas where collaboration between disciplines
could be particularly beneficial. We conclude that multiscale computing has
become increasingly popular in recent years, that different communities adopt
very different approaches to constructing multiscale simulations, and that
simulations on a length scale of a few metres and a time scale of a few hours
can be found in many of the multiscale research domains. Communities may
receive additional benefit from sharing methods that are geared towards these
scales.
"
797,Simulating Lattice Spin Models on Graphics Processing Units,"  Lattice spin models are useful for studying critical phenomena and allow the
extraction of equilibrium and dynamical properties. Simulations of such systems
are usually based on Monte Carlo (MC) techniques, and the main difficulty is
often the large computational effort needed when approaching critical points.
In this work, it is shown how such simulations can be accelerated with the use
of NVIDIA graphics processing units (GPUs) using the CUDA programming
architecture. We have developed two different algorithms for lattice spin
models, the first useful for equilibrium properties near a second-order phase
transition point and the second for dynamical slowing down near a glass
transition. The algorithms are based on parallel MC techniques, and speedups
from 70- to 150-fold over conventional single-threaded computer codes are
obtained using consumer-grade hardware.
"
798,Sociotechnical Management Model for Governance of an Ecosystem,"  This is an opinion paper regarding a proposal of a model for a Ecosystemm
Governance. In the globalized world the importance of Information Systems (IS)
and Information Technology (IT) become increasingly relevant regarding the
requirements imposed by competition. Both the knowledge of the business as the
rapid flow of information are fundamental for a enterprise decision making.
Whereas the basic definition of IT = hardware + software, i.e., tools that has
been used to create, store and disseminate data and information in the creation
of knowledge, and IS = IT + People + procedures that collect, process and
disseminate the information to support decision making, coordination, control,
analysis and visualization in the organization [01], it makes implicit the
understanding of IS is essential to create competitive companies, to manage
global corporations and provide customers with products and services of value.
In this work we are correlating IS with the governance of management of an
ecosystem. Yet, as IT is redefining the foundations of business, then the
customer service, operations, strategies of product marketing and its
distribution and even the knowledge management (KM) depends very much, or
sometimes even completely, on the IS. The IT and its costs have become a part
of day-to-day business [02]. In order to meet this complexity of business
needs, today is not possible to disregard the IT and its available resources,
which makes very dificult to draw up IS. Therefore, the perspective view of the
Sociotehcnical Aspects of an IS are directly concerned with governance and the
model proposed regarding an ecosystem. Finally, whereas the summary above, the
main objective of this opinion paper is to propose the guidelines for a
Sociotechnical Management Model of Governance for an Ecosystem.
"
799,CrowdInside: Automatic Construction of Indoor Floorplans,"  The existence of a worldwide indoor floorplans database can lead to
significant growth in location-based applications, especially for indoor
environments. In this paper, we present CrowdInside: a crowdsourcing-based
system for the automatic construction of buildings floorplans. CrowdInside
leverages the smart phones sensors that are ubiquitously available with humans
who use a building to automatically and transparently construct accurate motion
traces. These accurate traces are generated based on a novel technique for
reducing the errors in the inertial motion traces by using the points of
interest in the indoor environment, such as elevators and stairs, for error
resetting. The collected traces are then processed to detect the overall
floorplan shape as well as higher level semantics such as detecting rooms and
corridors shapes along with a variety of points of interest in the environment.
Implementation of the system in two testbeds, using different Android phones,
shows that CrowdInside can detect the points of interest accurately with 0.2%
false positive rate and 1.3% false negative rate. In addition, the proposed
error resetting technique leads to more than 12 times enhancement in the median
distance error compared to the state-of-the-art. Moreover, the detailed
floorplan can be accurately estimated with a a relatively small number of
traces. This number is amortized over the number of users of the building. We
also discuss possible extensions to CrowdInside for inferring even higher level
semantics about the discovered floorplans.
"
800,"Performance Analysis of MIMO Radar Waveform using Accelerated Particle
  Swarm Optimization Algorithm","  The Accelerated Particle Swarm Optimization Algorithm is promoted to
numerically design orthogonal Discrete Frequency Waveforms and Modified
Discrete Frequency Waveforms (DFCWs) with good correlation properties for MIMO
radar. We employ Accelerated Particle Swarm Optimization algorithm (ACC_PSO),
Particles of a swarm communicate good positions, velocity and accelerations to
each other as well as dynamically adjust their own position, velocity and
acceleration derived from the best of all particles. The simulation results
show that the proposed algorithm is effective for the design of DFCWs signal
used in MIMO radar.
"
801,"AutoAmp : An Open-Source Analog Amplifier Design Tool - For Classroom
  and Lab Purposes","  This correspondence presents an open-source tool AutoAmp developed at the
Indian Institute of Technology, Guwahati. It is available at
http://sourceforge.net/projects/autoamp-iitg/ This tool helps the user to
design different types of electronic amplifiers, using solid state devices, for
a given specification. It can handle several types of designs namely
common-emitter BJT amplifier (single and two-stage), operational amplifiers
(inverting and non-inverting) and power amplifier. Not only does it design the
amplifier, it also simulates the designed amplifier using SPICE simulator and
displays the performance curves. This tool is deemed to prove invaluable in
undergraduate teaching and labs. Especially in electronics-design related
laboratories, the student need not design the amplifiers which are mostly the
heart of many electronic designs.
"
802,"A Connected Enterprise - Transformation through Mobility and Social
  Networks","  Due to rapid changes in business dynamics, there is a growing demand to
encourage social conversations/exchanges and the ability to connect and
communicate with peers, partners, customers and other stakeholders anytime,
anywhere which drives the need of mobile-enable, the existing enterprise
applications. This paper highlights a distinct set of needs and key customer
challenges that must be considered and addressed for deployment of Social
Collaboration applications and Mobility services in enterprises. It not only
addresses the Critical Success Factors for enterprise mobility enablement but
also outlines the unique business requirements to rapidly create social
collaboration culture and the discipline of turning social data into meaningful
insights to drive business decisions in real-time. Moreover, the paper
emphasizes on developing composite offerings on social enterprise and Mobile
networks that not only offer the value proposition in terms of financially
oriented results, but also help customer to maximize return on investment
(ROI).
"
803,"Controlling and securing a Digital Home using Multiple Sensor Based
  Perception system Integrated with Mobile and Voice technology","  Fully controlled digital home had always been considered as a luxury of rich
people because of excessive cost to install the system. It is now within the
reach of mass people with lots of inexpensive cool features. In this paper we
have designed and developed a very low cost, efficient and reliable Digital
home system. Fully Controlled Digital Home is no more a Luxury. Our proposed
system made it affordable. We built a low-cost feature-rich Digital Home System
(DHS). Digital Home System is combination of automated services i.e. Electronic
Device Controller, IR Security System, Web Desktop, Remote Video Surveillance
System and Virtual Mobile by which we can control our home by avoiding old
manual processes e.g. our physical presence at home is optional. The System
provides some of the modern luxury & security features to us. Now we can
control Light, fan, AC or any electronic devices by voice command, Blue-tooth,
GPRS or Website. To control the system remotely, GPRS connectivity is added. We
can also monitor our home from remote area by using Remote Video Surveillance
System. This enables live video into mobile device of the digital home.
Moreover, we can also access our PC and do the necessary tasks from any
internet enabled computer in the world by using Web Desktop which is specially
built for this purpose. Furthermore, access of unauthorized person in the home
will be notified by SMS & store the image of the person and also generate a
voice alarm. So that ensures the security of our valuable things. Also we can
identify and monitor the location of our valuable assets e.g. precious metals
remotely. Finally, Virtual mobile application is a Universal mobile Driver by
which we can exactly perform some same task e.g. Remote call, Phone book
access, SMS read-write of our mobile device from our new invented computer's
virtual mobile.
"
804,"Automatic Electric Meter Reading System: A Cost-Feasible Alternative
  Approach In Meter Reading For Bangladesh Perspective Using Low-Cost Digital
  Wattmeter And Wimax Technology","  Energy meter reading is a monotonous and an expensive task. Now the meter
reader people goes to each meter and take the meter reading manually to issue
the bill which will later be entered in the billing software for billing and
payment automation. If the manual meter reading and bill data entry process can
be automated then it would reduced the laborious task and financial wastage.
""Automatic Electric Meter Reading (AMR) System"" is a metering system that is to
be used for data collecting from the meter and processing the collected data
for billing and other decision purposes. In this paper we have proposed an
automatic meter reading system which is low cost, high performance, highest
data rate, highest coverage area and most appropriate for Bangladesh
perspective. In this AMR system there are four basic units. They are reading
unit, communication unit, data receiving and processing unit and billing
system. For reading unit we identified the disk rotation of the energy meter
and stored the data in microcontroller. So it is not required to change the
current analog energy meter. An external module will be added with the current
energy meter. In the communication unit Wimax transceiver was used for wireless
communication between meter end and the server end because of its wide coverage
area. In the data receiving and processing unit meter reading will be collected
from the transceiver which is controlled by another microcontroller. There will
be a computer application that will take the data from the microcontroller.
This will also help to avoid any tampering or break down of energy meter. There
are various AMR system exists all over the world. Those systems were analyzed
and we found they are not feasible for Bangladesh.
"
805,Secure electronic lock using pic 16f628a microcontroller,"  The proposed system implements the electronic embedded lock, its provides a
great benefit over traditional lock, which use the manual key, so if the key
lost or theft then anybody could open the lock, while thieving or losing the
long and complex password is harder compare to traditional key, furthermore
combining both manual key with computerized password make the system more
secure. Long password will reduce the possibilities of breaking the code and
opening the lock. The system comprised keypad, and HD44780 20x2 LCD Along with
PIC16f628a microcontroller. The firmware control these components such that
interaction with keypad is very is ver easy and smoothly, the LCD provide user
with messages and notification to be informed about what is the system state.
User can performing opening and closing the lock, changing the current password
in the microcontroller EEPROM and clearing single digit while entering the
password when wrong digit entered (back space). The proposed system firmware
developed using assembly language with MPLAB development environment. It tested
and implemented in real hardware with proper functioning and bug free.
"
806,A Tutorial for Creating and Publishing Open Source Lisp Software,"  The proliferation and accessability of the Internet have made it simple to
view, download, and publish source code. This paper gives a short tutorial on
how to create a new Common Lisp project and publish it.
"
807,Smart Charging Technologies for Portable Electronic Devices,"  In this article we describe our efforts of extending demand-side control
concepts to the application in portable electronic devices, such as laptop
computers, mobile phones and tablet computers. As these devices feature
built-in energy storage (in the form of batteries) and the ability to run
complex control routines, they are ideal for the implementation of smart
charging concepts. We developed a prototype of a smart laptop charger that
controls the charging process depending on the locally measured frequency of
the electricity grid. If this technique is incorporated into millions of
devices in UK households, this will contribute significantly to the stability
of the electricity grid, help to mitigate the power production fluctuations
from renewable energy sources and avoid the high cost of building and
maintaining conventional power plants as standby reserve.
"
808,Vulnerability Management for an Enterprise Resource Planning System,"  Enterprise resource planning (ERP) systems are commonly used in technical
educational institutions(TEIs). ERP systems should continue providing services
to its users irrespective of the level of failure. There could be many types of
failures in the ERP systems. There are different types of measures or
characteristics that can be defined for ERP systems to handle the levels of
failure. Here in this paper, various types of failure levels are identified
along with various characteristics which are concerned with those failures. The
relation between all these is summarized. The disruptions causing
vulnerabilities in TEIs are identified .A vulnerability management cycle has
been suggested along with many commercial and open source vulnerability
management tools. The paper also highlights the importance of resiliency in ERP
systems in TEIs.
"
809,PCNM: A New Platform for Cellular Networks Measurements and Optimization,"  In this paper, we present PCNM, a new mobile platform for cellular networks
measurements. PCNM is based on a set of techniques that tailors theoretical
calculations and simulations to the real cellular network environment. It
includes: (a) modules that measure different parameters of a base station (BS)
such as localization, cells identification, time advance information, reception
level and quality, (b) a new protocol that optimizes the task of network
measurement by monitoring a set of mobile nodes and finally (c) the ability to
extend an existing cellular network by adding new base stations. We evaluate
our genetic algorithm used to reduce the nodes mobility and optimize the
measurement extraction of N base stations using k mobile sensors (k >= 1). We
show how connecting real measurements (using mobile sensors in a collaborative
way) to theoretical and prediction methods is of high benefits for cellular
networks maintenance, extension and performances evaluation.
"
810,A Review Paper on Microprocessor Based Controller Programming,"  Designing of microprocessor based controllers requires specific hardware as
well as software programming. Programming depends upon type of the software
whether operating software or application software. Programming requires
knowledge of system configuration and controller specific programming. Programs
are always in digital form so microprocessor can control directly at digital
level called Direct Digital Control (DDC).
"
811,Integration of CAD and rapid manufacturing for sand casting optimisation,"  In order to reduce the time and costs of the products development in the sand
casting process, the SMC Colombier Fontaine company has carried out a study
based on tooling manufacturing with a new rapid prototyping process. This
evolution allowed the adequacy of the geometry used for the simulation to the
tooling employed physically in the production. This allowed a reduction of the
wall thickness to 4mm and retained reliable manufacturing process.
"
812,Customised high-value document generation,"  Contributions of different experts to innovation projects improve enterprise
value, captured in documents. A subset of them is the centre of expert
constraint convergence. Their production needs to be tailored case by case.
Documents are often considered as knowledge transcription. As the base of a
structured knowledge-based information environment, this paper presents a
global approach that helps knowledge-integration tool deployment. An example,
based on process plan in aircraft manufacturing, indicates how fundamental
understanding of domain infrastructure contributes to a more coherent
architecture of knowledge-based information environments. A comparison with an
experiment in insurance services generalised the application of presented
principles.
"
813,"VCS: Value Chains Simulator, a Tool for Value Analysis of Manufacturing
  Enterprise Processes (A Value-Based Decision Support Tool)","  Manufacturing enterprises are facing a competitive challenge. This paper
proposes the use of a value chain based approach to support the modelling and
simulation of manufacturing enterprise processes. The aim is to help experts to
make relevant decisions on product design and/or product manufacturing process
planning. This decision tool is based on the value chain modelling, by
considering the product requirements. In order to evaluate several performance
indicators, a simulation of various potential value chains adapted to market
demand was conducted through a Value Chains Simulator (VCS). A discrete event
simulator is used to perform the simulation of these scenarios and to evaluate
the value as a global performance criterion (balancing cost, quality, delivery
time, services, etc.). An Analytical Hierarchy Process module supports the
analysis process. The value chain model is based on activities and uses the
concepts of resource consumption, while integrating the benefiting entities
view point. A case study in the microelectronic field is carried out to
corroborate the validity of the proposed VCS.
"
814,"Minimum Component Based First-Order Inverting and Non-inverting Outputs
  of All-Pass Filter at the Same Circuit","  In this paper, a new voltage-mode first order all-pass filter using minimum
active and passive components is presented. The proposed circuit employs one
fully differential second generation current conveyor (FDCCII), one grounded
capacitor, one resistor and offers the following advantages: the use of only
grounded capacitor which is attractive for integrated circuit implementation,
low active and passive sensitivities, providing inverting and non-inverting
voltage-mode all-pass responses simultaneously from the single circuit and no
requirement for component matching conditions. The theory is validated through
PSPICE simulation using TSMC 0.35micrometer CMOS process parameters.
"
815,"Minimum Grounded Component Based Voltage-Mode Quadrature Oscillator
  using DVCC","  In this paper, a new voltage-mode quadrature oscillator using minimum number
of active and passive component is proposed. The proposed circuit employs
single modified DVCC, two grounded capacitor and two grounded resistors, which
is ideal for IC implementation. The active and passive sensitivity are no more
than unity. The proposed circuit is verified through PSPICE simulation results.
"
816,"Performance Evaluation of Mobile U-Navigation based on GPS/WLAN
  Hybridization","  This paper present our mobile u-navigation system. This approach utilizes
hybridization of wireless local area network and Global Positioning System
internal sensor which to receive signal strength from access point and the same
time retrieve Global Navigation System Satellite signal. This positioning
information will be switched based on type of environment in order to ensure
the ubiquity of positioning system. Finally we present our results to
illustrate the performance of the localization system for an indoor/ outdoor
environment set-up.
"
817,Microelectromechanical system cantilever-based frequency doublers,"  Microelectromechanical system (MEMS) based on-chip resonators offer great
potential for high frequency signal processing circuits like reference
oscillators and filters. This is due to their exceptional features like small
size, large frequency-quality factor product, integrability with CMOS ICs, low
power consumption, low cost batch fabrication etc. A capacitively transduced
cantilever beam resonator is one such popular MEMS resonator topology. In this
letter, the inherent square-law nonlinearity of the voltage-to-force transfer
function of a cantilever resonator's capacitive transducer has been employed
for the realization of frequency doubling effect. Using this concept, frequency
doubling of input signals of 500 kHz to 1 MHz, and 227.5 kHz to 455 kHz has
been experimentally demonstrated for two cantilever beams of length 51.75 and
76.75 micrometer respectively. The MEMS cantilevers have been fabricated with
polysilicon using the PolyMUMPs surface micromachining process, and their
testing has been performed using Laser Doppler Vibrometry. The test results
obtained are in reasonable compliance with the analytical and CoventorWare
finite-element simulation results. The high efficiency demonstrated by the
cantilever frequency doubler makes it a promising choice for signal generation
at high frequencies.
"
818,Varactor-Based Dynamic Load Modulation of High Power Amplifiers,"  In this work, dynamic load modulation of high power amplifiers using a
varactor-based tunable matching network is presented. The feasibility of
dynamic tuning and efficiency enhancement of this technique is demonstrated
using a modular design approach for two existing high efficiency power
amplifiers (PA), a 7-W class-E, and a 10-W class-J power amplifier PA at 1 GHz.
For this purpose and for each of the PAs, a simple quasi-static inverse model
is developed allowing an efficiency-optimized control of the PA and the
varactor-based tunable matching network. Modulated measurements using a single
carrier WCDMA signal with 11.3 dB peak-to-average ratio (PAR) indicate about 10
to 14 percentage units improvements in the average power-added efficiency (PAE)
for the complete architecture.
"
819,\'Economie des biens immat\'eriels - Economics of Intangible Goods,"  We introduce a new economic system suited for Intangible Goods ({\sc ig}). We
argue that such system can now be implemented in the real world using advance
technics in distributed network computing and cryptography. The specification
of the so called \net{} is presented. To Limit the number of financial
transactions, the system is forced to define its own currency, with many
benefits. The new ""cup"" currency, extended worldwide, is dedicated to {\sc ig},
available only for person-to-person trading, protected from speculation and
adapted for tax recovery with no additional computation. Those nices features
makes the \net{} a new democratic tool, fixing specific issues in {\sc ig}
trading and reviving a whole domain activity. We emphasis on the fact that all
proposed documentation, algorithm, program in any language related to this
proposal shall be open-source without any possibility to post any patent of any
sort on the system or subsystem. This new trading model should be considered as
a pure intellectual construction, like parts of Mathematics and then belongs to
nobody or everybody, like $1+1=2$. Next step will be to test, validate the
security of various implementations details, and to ask for legal rules
adaptations. The first draft paper is written in French language and posted to
arXiv.org and hal.archive-ouverte.fr . We expect to provide an English
translation before Christmas.
"
820,Dimensions and issues of mobile agent technology,"  Mobile Agent is a type of software system which acts ""intelligently"" on one's
behalf with the feature of autonomy, learning ability and most importantly
mobility. Now mobile agents are gaining interest in the research community. In
this article mobile agents will be addressed as tools for mobile computing.
Mobile agents have been used in applications ranging from network management to
information management. We present mobile agent concept, characteristics,
classification, need, applications and technical constraints in the mobile
technology. We also provide a brief case study about how mobile agent is used
for information retrieval.
"
821,A Robust Lot Sizing Problem with Ill-known Demands,"  The paper deals with a lot sizing problem with ill-known demands modeled by
fuzzy intervals whose membership functions are possibility distributions for
the values of the uncertain demands. Optimization criteria, in the setting of
possibility theory, that lead to choose robust production plans under fuzzy
demands are given. Some algorithms for determining optimal robust production
plans with respect to the proposed criteria, and for evaluating production
plans are provided. Some computational experiments are presented.
"
822,"Propuesta de sistema GeoInform\'atico con representaci\'on de escenarios
  para auxiliar en la nueva metodolog\'ia propuesta por INETER y la UNI para el
  estudio a gran escala de la vulnerabilidad y da\~nos debido a sismos en las
  edificaciones","  A GIS based software is presented which permits the estimation of seismic
vulnerability and the presentation of results in digital maps for single
houses, groups of buildings, parts of settlements or even complete towns.
Nicaragua is a country with a high seismic activity. The assessment of seismic
vulnerability requires the execution of distinct tasks, e.g. recollection of
field data, integration of data from the municipal cadastre, reprocessing or
screening to test the reliability of the data, definition of calculation of
vulnerability functions, calculation of vulnerability for single objects as
houses or buildings, calculation of mean vulnerability for certain areas as
barrios or squares. In order to reduce time and effort to be spent with several
unspecialized tools and procedures, an integrated software system was created,
the user of which has not to care about separate software tools for each part
of the process. The main advantage of the software is the combination of
Geographical Information System (GIS) with the logics that surrounds the
specific methodologies of seismic vulnerability index, index of damages and
presentation of results. The new software uses a connection with an external
centralized Enterprise Data Base which stores all the input information and
calculation results and which is automatically synchronized for the
presentation of results using GIS. The cadastral information contains data on
the constructive type of the house, dimensions, year of construction, type of
walls, roof, number of inhabitants, etc.. The system also allows to present
damage scenarios for specific seismic events with given hypocenter and
magnitude. The documentation of the software serves as a guide for students
working on object oriented software engineering by using unified modeling
language (UML) and software logic architecture (3-tiers).
"
823,"Designing a High Efficiency Pulse Width Modulation Step-Down DC/DC
  Converter for Mobile Phone Applications","  This paper presents the design and analysis of a high efficiency, PWM
(Pulse-Width-Modulation) Buck converters for mobile phone applications. The
steady-state and average-value models for the proposed converter are developed
and simulated. A practical design approach which aims at systematizing the
procedure for the selection of the control parameters is introduced. The
switching losses are reduced by using soft switching, additionally, a simple
analog and digital form of the controller for practical realization is
provided. It is found that this controller adopts a structure similar to the
conventional PWM voltage mode controller. The proposed circuit uses a
current-mode control and a voltage-to-pulse converter for the PWM. The circuit,
fabricated using a 0.18-{\mu}m CMOS technology, reaches a peak load regulation
of 20 mV/V and line regulation of 0.5 mV/V at Current load equal 300 mA. The
used 10{\mu}H inductance and 22{\mu}F capacitor and requires clock and
Vref/Vramp input of 1,23V.
"
824,"Modified Stage-Gate: A Conceptual Model of Virtual Product Development
  Process","  In today s dynamic marketplace, manufacturing companies are under strong
pressure to introduce new products for long-term survival with their
competitors. Nevertheless, every company cannot cope up progressively or
immediately with the market requirements due to knowledge dynamics being
experienced in the competitive milieu. Increased competition and reduced
product life cycles put force upon companies to develop new products faster. In
response to these pressing needs, there should be some new approach compatible
in flexible circumstances. This paper presents a solution based on the popular
Stage-Gate system, which is closely linked with virtual team approach. Virtual
teams can provide a platform to advance the knowledge-base in a company and
thus to reduce time-to-market. This article introduces conceptual product
development architecture under a virtual team umbrella. The paper describes all
the major aspects of new product development (NPD), NPD process and its
relationship with virtual teams, Stage-Gate system finally presents a modified
Stage-Gate system to cope up with the changing needs. It also provides the
guidelines for the successful implementation of virtual teams in new product
development.
"
825,Plagiarism Detection: Keeping Check on Misuse of Intellectual Property,"  Today, Plagiarism has become a menace. Every journal editor or conference
organizers has to deal with this problem. Simply Copying or rephrasing of text
without giving due credit to the original author has become more common. This
is considered to be an Intellectual Property Theft. We are developing a
Plagiarism Detection Tool which would deal with this problem. In this paper we
discuss the common tools available to detect plagiarism and their short comings
and the advantages of our tool over these tools.
"
826,Virtual Collaborative R&D Teams in Malaysia Manufacturing SMEs,"  This paper presents the results of empirical research conducted during March
to September 2009. The study focused on the influence of virtual research and
development teams within Malaysian manufacturing small and medium sized
enterprises (SMEs). The specific objective of the study is better understanding
of the application of collaborative technologies in business, to find the
effective factors to assist SMEs to remain competitive in the future. The paper
stresses to find an answer for a question Is there any relationship between
company size, Internet connection facility and virtuality?. The survey data
shows SMEs are now technologically capable of performing the virtual
collaborative team, but the infrastructure usage is less. SMEs now have the
necessary technology to begin the implementation process of collaboration tools
to reduce research and development time, costs and increase productivity. So,
the manager of R and D should take the potentials of virtual teams into
account.
"
827,Bayesian inference and the world mind,"  Knowledge is a central concept within both Bayesian inference and the
mathematical and philosophical program of logic and semiotics initiated by
Charles Sanders Peirce and further developed by George Spencer-Brown and Louis
Kauffman. The latter school is more philosophical than is usual with the
practitioners of Bayesian inference and claims the existence of a world mind.
When these two disciplines inform each other semiotics is provided with
mathematical mechanism and Bayesian inference is seen to be closely related to
the act of distinction, the fundamental basis of logic in the work of
Spencer-Brown. This hybridization also suggests a definition for knowledge
within Bayesian inference; a definition that has been curiously lacking. Given
that Darwinian processes are physical implementations of Bayesian inference and
are utilized within numerous scientific theories across a wide range of
disciplines as mechanisms for the creation and evolution of knowledge we may
view the conjunction of these theories, within universal Darwinism, as
descriptive of a world mind. Placing the world mind in this context provides
detailed support from the scientific literature and goes some way to refute the
charges of mysticism which have been leveled at the semiotic approach.
"
828,A Synthesis Method for Quaternary Quantum Logic Circuits,"  Synthesis of quaternary quantum circuits involves basic quaternary gates and
logic operations in the quaternary quantum domain. In this paper, we propose
new projection operations and quaternary logic gates for synthesizing
quaternary logic functions. We also demonstrate the realization of the proposed
gates using basic quantum quaternary operations. We then employ our synthesis
method to design of quaternary adder and some benchmark circuits. Our results
in terms of circuit cost, are better than the existing works.
"
829,A model for quantum queue,"  We consider an extension of Discrete Time Markov Chain queueing model to the
quantum domain by use of Discrete Time Quantum Markov Chain. We introduce
methods for numerical analysis of such models. Using this tools we show that
quantum model behaves fundamentally differently from the classical one.
"
830,"Project G.N.O.S.I.S.: Geographical Network Of Synoptic Information
  System","  Everybody knows how much synoptic maps are useful today. An excellent example
above all is Google Earth: its simplicity and friendly interface allows every
user to have the Earth maps ready in just one simple layout; nevertheless a
crucial dimension is missing in Google Earth: the time. This doesn't mean we
simply aim to add history to Google Earth (though it could be already a nice
goal): the main idea behind GNOSIS project is to produce applications to ""dress
up"" the Globe with a set of skin-maps representing the most various different
kind of histories like the evolution of geology, genetics, agriculture,
ethnology, linguistics, musicology, metallurgy and so forth, in time. It may be
interesting in the near future to have such a possibility to watch on the map
the positions and movements of the armies during the battles of Waterloo or
Thermopylae, the spreading of the cultivation of corn in time, the rise and
fall of Roman Empire or the diffusion of Smallpox together with the spread of a
religion, a specific dialect, the early pottery techniques or the natural
resources available to pre-Columbian civilizations on a Google-Earth-map-like,
that is to say to have at one's hand the ultimate didactic-enciclopedic tool.
To do so we foresee the use of a general-purpose intermediate/high level
programming language, possibly object-oriented such C++ or Java.
"
831,An Evaluation of Arabic Language Learning Websites,"  As a result of ICT development and the increasingly growing use of the
Internet in particular, practices of language teaching and learning are about
to evolve significantly. Our study focuses on the Arabic language, and aims to
explore and evaluate Arabic language learning websites. To reach these goals,
we propose in a first step, to define an evaluation model, based on a set of
criteria for assessing the quality of websites dedicated to teaching and
learning Arabic. We subsequently apply our model on a set of Arabic sites
available on the web and give an assessment of these web sites. We finally
discuss their strengths and limitations.
"
832,Information and Communication Technology in Combating Counterfeit Drugs,"  Pharma frauds are on the rise, counterfeit drugs are giving sleepless nights
to patients, pharmaceutical companies and governments. The laws prohibiting the
sales of counterfeit drugs cannot succeed without technological interventions.
Several analytical techniques and tools including spectroscopy, holograms,
barcoding, differentiated packing, radio frequency identification,
fingerprints, hyperspectral imaging etc. have been employed over the years in
combating this menace; however this challenge is becoming more sophisticated
with the evolution of the World Wide Web and online pharmacies. This paper
presents a review on the contribution of Information and Communication
Technology (ICT) as a drug counterfeit countermeasure.
"
833,A Semi-Structured Tailoring-Driven Approach for ERP Selection,"  It has been widely reported that selecting an inappropriate system is a major
reason for ERP implementation failures. The selection of an ERP system is
therefore critical. While the number of papers related to ERP implementation is
substantial, ERP evaluation and selection approaches have received few
attention. Motivated by the adaptation concept of the ERP systems, we propose
in this paper a semi-structured approach for ERP system selection that differs
from existing models in that it has a more holistic focus by simultaneously 1)
considering the anticipated fitness of ERP solutions after the optimal
resolution, within limited resources, of a set of the identified mismatches and
2) evaluating candidate products according to both functional and
non-functional requirements. The approach consists of an iterative selection
process model and an evaluation methodology based on 0-1 linear programming and
MACBETH technique to elaborate multi-criteria performance expressions.
"
834,Performance Evaluation of DOA Estimation using MATLAB,"  This paper presents the performance analysis of directions of arrival
estimation techniques, Subspace and the Non-Subspace methods. In this paper,
exploring the analysis category of high resolution and super resolution
algorithms, presentation of description, comparison and the performance and
resolution analyses of these algorithms are made. Sensitivity to various
perturbations and the effect of parameters related to the design of the sensor
array itself such as the number of array elements and their spacing are also
investigated.
"
835,"Online Energy Generation Scheduling for Microgrids with Intermittent
  Energy Sources and Co-Generation","  Microgrids represent an emerging paradigm of future electric power systems
that can utilize both distributed and centralized generations. Two recent
trends in microgrids are the integration of local renewable energy sources
(such as wind farms) and the use of co-generation (i.e., to supply both
electricity and heat). However, these trends also bring unprecedented
challenges to the design of intelligent control strategies for microgrids.
Traditional generation scheduling paradigms rely on perfect prediction of
future electricity supply and demand. They are no longer applicable to
microgrids with unpredictable renewable energy supply and with co-generation
(that needs to consider both electricity and heat demand). In this paper, we
study online algorithms for the microgrid generation scheduling problem with
intermittent renewable energy sources and co-generation, with the goal of
maximizing the cost-savings with local generation. Based on the insights from
the structure of the offline optimal solution, we propose a class of
competitive online algorithms, called CHASE (Competitive Heuristic Algorithm
for Scheduling Energy-generation), that track the offline optimal in an online
fashion. Under typical settings, we show that CHASE achieves the best
competitive ratio among all deterministic online algorithms, and the ratio is
no larger than a small constant 3.
"
836,"From the Closed Classical Algorithmic Universe to an Open World of
  Algorithmic Constellations","  In this paper we analyze methodological and philosophical implications of
algorithmic aspects of unconventional computation. At first, we describe how
the classical algorithmic universe developed and analyze why it became closed
in the conventional approach to computation. Then we explain how new models of
algorithms turned the classical closed algorithmic universe into the open world
of algorithmic constellations, allowing higher flexibility and expressive
power, supporting constructivism and creativity in mathematical modeling. As
Goedels undecidability theorems demonstrate, the closed algorithmic universe
restricts essential forms of mathematical cognition. In contrast, the open
algorithmic universe, and even more the open world of algorithmic
constellations, remove such restrictions and enable new, richer understanding
of computation.
"
837,Secured Ontology Mapping,"  Todays market evolution and high volatility of business requirements put an
increasing emphasis on the ability for systems to accommodate the changes
required by new organizational needs while maintaining security objectives
satisfiability. This is all the more true in case of collaboration and
interoperability between different organizations and thus between their
information systems. Ontology mapping has been used for interoperability and
several mapping systems have evolved to support the same. Usual solutions do
not take care of security. That is almost all systems do a mapping of
ontologies which are unsecured.We have developed a system for mapping secured
ontologies using graph similarity concept. Here we give no importance to the
strings that describe ontology concepts, properties etc. Because these strings
may be encrypted in the secured ontology. Instead we use the pure graphical
structure to determine mapping between various concepts of given two secured
ontologies. The paper also gives the measure of accuracy of experiment in a
tabular form in terms of precision, recall and F-measure.
"
838,Memoization technique for optimizing functions with stochastic input,"  In this paper we present a strategy for optimization functions with
stochastic input. The main idea is to take advantage of decomposition in
combination with a look-up table. Deciding what input values should be used for
memoization is determined based on the underlying probability distribution of
input variables. Special attention is given to difficulties caused by
combinatorial explosion.
"
839,"Understanding Complex Service Systems Through Different Lenses: An
  Overview","  The 2011 Grand Challenge in Service conference aimed to explore, analyse and
evaluate complex service systems, utilising a case scenario of delivering on
improved perception of safety in the London Borough of Sutton, which provided a
common context to link the contributions. The key themes that emerged included
value co-creation, systems and networks, ICT and complexity, for which we
summarise the contributions. Contributions on value co-creation are based
mainly on empirical research and provide a variety of insights including the
importance of better understanding collaboration within value co-creation.
Contributions on the systems perspective, considered to arise from networks of
value co-creation, include efforts to understand the implications of the
interactions within service systems, as well as their interactions with social
systems, to co-create value. Contributions within the technological sphere,
providing ever greater connectivity between entities, focus on the creation of
new value constellations and new demand being fulfilled through hybrid
offerings of physical assets, information and people. Contributions on
complexity, arising from the value co- creation networks of technology enabled
services systems, focus on the challenges in understanding, managing and
analysing these complex service systems. The theory and applications all show
the importance of understanding service for the future.
"
840,"Value, Variety and Viability: New Business Models for Co-Creation in
  Outcome-based Contracts","  We propose that designing a manufacturer's equipment-based service value
proposition in outcome-based contracts is the design of a new business model
capable of managing threats to the firm's viability that can arise from the
contextual variety of use that customers may subject the firm's value
propositions. Furthermore, manufacturers need to understand these emerging
business models as the capability of managing both asset and service provision
to achieve use outcomes with customers, including emotional outcomes such as
customer experience. Service-Dominant Logic proposes that all ""goods are a
distribution mechanism for service provision"", upon which we propose a
value-centric approach to understanding the interactions between the asset and
service provision, and suggest a viable systems approach towards reorganising
the firm to achieve such a business model. Three case studies of B2B
equipment-based service systems were analysed to understand customers'
co-creation activities in achieving outcomes, in which we found that the
co-creation of complex multi-dimensional value could be delivered through the
different value propositions of the firm catering to different aspects
(dimensions) of the value to be co-created. The study provides a way for
managers to understand the effectiveness (rather than efficiency) of firms in
adopting emerging business models that design for value co-creation in what are
ultimately complex socio- technical systems.
"
841,A new class of SETI beacons that contain information (22-aug-2010),"  In the cm-wavelength range, an extraterrestrial electromagnetic narrow band
(sine wave) beacon is an excellent choice to get alien attention across
interstellar distances because 1) it is not strongly affected by interstellar /
interplanetary dispersion or scattering, and 2) searching for narrowband
signals is computationally efficient (scales as Ns log(Ns) where Ns = number of
voltage samples). Here we consider a special case wideband signal where two or
more delayed copies of the same signal are transmitted over the same frequency
and bandwidth, with the result that ISM dispersion and scattering cancel out
during the detection stage. Such a signal is both a good beacon (easy to find)
and carries arbitrarily large information rate (limited only by the atmospheric
transparency to about 10 GHz). The discovery process uses an autocorrelation
algorithm, and we outline a compute scheme where the beacon discovery search
can be accomplished with only 2x the processing of a conventional sine wave
search, and discuss signal to background response for sighting the beacon. Once
the beacon is discovered, the focus turns to information extraction.
Information extraction requires similar processing as for generic wideband
signal searches, but since we have already identified the beacon, the
efficiency of information extraction is negligible.
"
842,"Performance Improvement by Changing Modulation Methods for Software
  Defined Radios","  This paper describes an automatic switching of modulation method to
reconfigure transceivers of Software Defined Radio (SDR) based wireless
communication system. The programmable architecture of Software Radio promotes
a flexible implementation of modulation methods. This flexibility also
translates into adaptively, which is used here to optimize the throughput of a
wireless network, operating under varying channel conditions. It is robust and
efficient with processing time overhead that still allows the SDR to maintain
its real-time operating objectives. This technique is studied for digital
wireless communication systems. Tests and simulations using an AWGN channel
show that the SNR threshold is 5dB for the case study.
"
843,Centralized Integrated Spectrum Sensing for Cognitive Radios,"  Spectrum sensing is the challenge for cognitive radio design and
implementation, which allows the secondary user to access the primary bands
without interference with primary users. Cognitive radios should decide on the
best spectrum band to meet the Quality of service requirements over all
available spectrum bands. This paper investigates the integrated centralized
spectrum sensing techniques in multipath fading environment and the performance
was analyzed with energy detection and wavelet based sensing techniques for
unknown signal. Keywords: Cognitive Radio, Spectrum Sensing, Signal Detection,
Primary User, Secondary User
"
844,Review of Knowledge Management Systems As Socio-Technical System,"  Knowledge Management Systems as socio-technical systemperspectives has
recognized for decades. Practitioners and scholars belief Knowledge Management
is best carried out throught the optimization both technological and
social-aspect.Lacking of understand and consider both aspects could lead
organizations in misinterpretation while developing andimplementing Knowledge
Management System. There is a need for practical guidance how Knowledge
Management System should implement in organizations. We propose a framework
that could use by practitioner and manager as guidance in developing and
implementing Knowledge Management System as Socio-Technical Systems. The
framework developed base on Pan and Scarborough view of Knowledge Management as
Socio-Technical system. Our framework consists of: Infrastructure(technology),
Info structure (organizational structure) and Info culture (organizational
culture). This concept would lead practitioners get clear understand aspect
contribute to Knowledge Management System success as Socio-Technical System.
"
845,"BigFoot: Analysis, monitoring, tracking and sharing of bio-medical
  features of human appendages using consumer-grade home and office based
  imaging devices","  Here we describe a system for personal and professional management and
analysis of bio-medical images captured using off-the-shelf, consumer-grade
imaging devices such as scanners, digital cameras, cellphones, webcams and
tablet PCs. Specifically, we describe an implementation of this system for the
analysis, monitoring and tracking of conditions and features of human feet
using a flatbed scanner as the image capture device and a custom-designed set
of algorithms and software to manage and analyze the acquired data.
"
846,"Hidden Markov Estimation of Bistatic Range From Cluttered Ultra-wideband
  Impulse Responses","  Ultra-wideband (UWB) multistatic radar can be used for target detection and
tracking in buildings and rooms. Target detection and tracking relies on
accurate knowledge of the bistatic delay. Noise, measurement error, and the
problem of dense, overlapping multipath signals in the measured UWB channel
impulse response (CIR) all contribute to make bistatic delay estimation
challenging. It is often assumed that a calibration CIR, that is, a measurement
from when no person is present, is easily subtracted from a newly captured CIR.
We show this is often not the case. We propose modeling the difference between
a current set of CIRs and a set of calibration CIRs as a hidden Markov model
(HMM). Multiple experimental deployments are performed to collect CIR data and
test the performance of this model and compare its performance to existing
methods. Our experimental results show an RMSE of 2.85 ns and 2.76 ns for our
HMM-based approach, compared to a thresholding method which, if the ideal
threshold is known a priori, achieves 3.28 ns and 4.58 ns. By using the
Baum-Welch algorithm, the HMM-based estimator is shown to be very robust to
initial parameter settings. Localization performance is also improved using the
HMM-based bistatic delay estimates.
"
847,Foundations of scientific research (Foundations of Research Activities),"  During years 2008 to 2011 author gives several courses on Foundations of
Scientific Research at Computer Science Faculty of the National Aviation
University in Kiev. This text presents material to lectures of the courses. It
consists of 18 sections and some ideas of the manual can be seen from their
titles. These include: General notions about scientific research. Ontologies
and upper ontologies. Ontologies of object domains. Examples of Research
Activity. Some Notions of the Theory of Finite and Discrete Sets. Algebraic
Operations and Algebraic Structures. Elements of the Theory of Graphs and Nets.
Scientific activity on the example of Information and its investigation.
Scientific research in Artificial Intelligence. Compilers and compilation.
Objective, Concepts and History of Computer security. Methodological and
categorical apparatus of scientific research. Methodology and methods of
scientific research. Scientific idea and significance of scientific research.
Forms of scientific knowledge organization and principles of scientific
research. Theoretical study, applied study and creativity. Types of scientific
research: theoretical study, applied study. Types of scientific research: forms
of representation of material. Some sections of the text contain enough
material to lectures, but in some cases these are sketchs without references to
Foundations of Research Activities. Really this is the first version of the
manual and author plans to edit, modify and extend the version. Some reasons
impose the author to post it as e-print. . Author compiled material from many
sources and hope that it gives various points of view on Foundations of
Research Activities.
"
848,Controlling Home Appliances Remotely through Voice Command,"  Controlling appliances is a main part of automation. The main object of Home
automation is to provide a wireless communication link of home appliances to
the remote user. The main objective of this work is to make such a system which
controls the home appliances remotely. This paper discusses two methods of
controlling home appliances one is via voice to text SMS and other is to use
the mobile as a remote control, this system will provide a benefit to the
elderly and disable people and also to those who are unaware of typing an SMS.
"
849,Generating Strategic IS: Towards the Winning Strategy,"  In modern era, the role of information system in organization has been taken
many discussions. The models of information system are constantly updated.
However, most of them can not face the changing world. This paper discusses an
approach to generating of strategic information system based on features in
organization. We proposed an approach by using disadvantages in some tools of
analysis whereby the lack of analysis appear as behaviour of relation between
organisation and the world.
"
850,Enhanced Image Analysis Using Cached Mobile Robots,"  In the field of Artificial intelligence Image processing plays a vital role
in Decision making. Nowadays Mobile robots work as a Network sharing
Centralized Database. All Image inputs are compared against this database and
decision is made. In some cases the Centralized database is in other side of
the globe and Mobile robots compare Input image through satellite link this
sometime results in delays in decision making which may result in castrophe.
This Research paper is about how to make image processing in mobile robots less
time consuming and fast decision making. This research paper compares search
techniques employed currently and optimum search method which we are going to
state. Nowadays Mobile robots are extensively used in environments which are
dangerous to human beings. In this dangerous situations quick Decision making
makes the difference between Hit and Miss this can also results in Day to day
tasks performed by Mobile robots Successful or Failure.
"
851,Brain Connectivity Analysis Methods for Better Understanding of Coupling,"  Action, cognition, emotion and perception can be mapped in the brain by using
set of techniques. Translating unimodal concepts from one modality to another
is an important step towards understanding the neural mechanisms. This paper
provides a comprehensive survey of multimodal analysis of brain signals such as
fMRI, EEG, MEG, NIRS and motivations, assumptions and pitfalls associated with
it. All these non-invasive brain modalities complement and restrain each other
and hence improve our understating of functional and neuronal organization. By
combining the various modalities together, we can exploit the strengths and
flaws of individual brain imaging methods. Integrated anatomical analysis and
functional measurements of human brain offer a powerful paradigm for the brain
mapping. Here we provide the brief review on non invasive brain modalities,
describe the future of co-analysis of these brain signals.
"
852,Voltage Temperature Monitoring System (VTMS) for a BTS Room,"  Although Cellular communication is getting more and more popular in our
country present days, but its network improvement is hampered by the crysis of
electricity. The recent decision of present Government is that they will not
provide any electricity from the grid to any new BTS rooms of any Celluler
operator companies like Grammen Phone, Robi, Airtel etc. These companies have
to develop their own power stations either by using generators or by developing
solar plants. Now a days most of the BTS rooms, that the cellular operators are
installing with a generator and 48 volt battery backup. So for the
synchronisation of the operation of PDB, Generator and battery, they require a
device called Voltage Temperature Monitoring System or VTMS. It is a
Microcontroller based controlling unit which controlls the operation of
generator and battery when PDB in not available in the BTS room.
"
853,Binary Sequences with Minimum Peak Sidelobe Level up to Length 68,"  Results of an exhaustive search for minimum peak sidelobe level binary
sequences are presented. Several techniques for efficiency implementation of
search algorithm are described. A table of number of non-equivalent optimal
binary sequences with minimum peak sidelobe (MPS) level up to length 68 is
given. This number can be used in prediction of the longest length for a given
sidelobe level of binary sequences. The examples of optimal binary MPS
sequences having high merit factor are shown.
"
854,Guadalupe: a browser design for heterogeneous hardware,"  Mobile systems are embracing heterogeneous architectures by getting more
types of cores and more specialized cores, which allows applications to be
faster and more efficient. We aim at exploiting the hardware heterogeneity from
the browser without requiring any changes to either the OS or the web
applications. Our design, Guadalupe, can use hardware processing units with
different degrees of capability for matched browser services. It starts with a
weak hardware unit, determines if and when a strong unit is needed, and
seamlessly migrates to the strong one when necessary. Guadalupe not only makes
more computing resources available to mobile web browsing but also improves its
energy proportionality. Based on Chrome for Android and TI OMAP4, We provide a
prototype browser implementation for resource loading and rendering. Compared
to Chrome for Android, we show that Guadalupe browser for rendering can
increase other 3D application's frame rate by up to 767% and save 4.7% of the
entire system's energy consumption. More importantly, by using the two cases,
we demonstrate that Guadalupe creates the great opportunity for many browser
services to get better resource utilization and energy proportionality by
exploiting hardware heterogeneity.
"
855,Green WSUS,"  The new era of information and communication technology (ICT) calls for a
greater understanding of the environmental impacts of recent technology. With
increasing energy cost and growing environmental concerns, green IT is
receiving more and more attention. Network and system design play a crucial
role in both computing and telecommunication systems. Significant part of this
energy cost goes to system update by downloading regularly patches and bug
fixes to solve security problems and to assure that the operating system and
other systems function properly. This paper describes a new design of Windows
Server Update Services (WSUS), system responsible of downloads of the mentioned
patches and updates from Microsoft Update website and then distributes them to
computers on a network. The general idea behind our proposed design is simple.
Instead of the periodical check done by the WSUS servers to ensure update form
Microsoft main servers, we rather propose to reverse the scenario in order to
reduce energy consumption. In the proposed design, the Microsoft main server(s)
sends signal to all WSUS servers to inform them about new updates. Once the
signal received, WSUS can contact the main server to start downloading.
"
856,"Design and Performance Study of Smart Antenna Systems for WIMAX
  Applications","  In this paper we propose an approach that uses homodyne receivers to design
smart antenna systems. The receivers functions are to detect angles of arrivals
of seven incoming RF signals using MUSIC or ESPRIT algorithms. The
characteristics of each algorithm are critical for the systems precision as
well as receivers types. Results are deduced from the simulation of each
system, using the Advanced Design System (ADS) and MATLAB. These are compared
to results deduced from real systems in the WIMAX (3.5GHz) domains.
"
857,Beamforming Techniques for Multichannel audio Signal Separation,"  Beamforming is a signal processing technique. It has been studied in many
areas such as radar, sonar, seismology and wireless communications, to name but
a few. It can be used for a myriad of purposes, such as detecting the presence
of a signal, estimating the direction of arrival, and enhancing a desired
signal from its measurements corrupted by noise, competing sources and
reverberation. Actually, Beamforming has been adopted by the audio research
society, mostly to separate or extract speech for noisy environment.
Beamforming techniques basically approach the problem from a spatial point of
view. A microphone array is used to form a spatial filter which can extract a
signal from a specific direction and reduce the contamination of signals from
other directions. In this paper we survey some Beamforming techniques used for
multichannel audio signal separation.
"
858,Development of Low Cost Private Office Access Control System (OACS),"  Over the years, access control systems have become more and more
sophisticated and several security measures have been employed to combat the
menace of insecurity of lives and property. This is done by preventing
unauthorized entrance into buildings through entrance doors using conventional
and electronic locks, discrete access code, and biometric methods such as the
finger prints, thumb prints, the iris and facial recognition. We have designed
a flexible and low cost modular system based on integration of keypad, magnetic
lock and a controller. PIC 16F876A which is an 8-bit Microcontroller, is used
here as a main controller. An advanced simulation based compiler Flowcode V4 is
used to develop the software part in this project.
"
859,"Building design in tropical climates. Elaboration of the ECODOM standard
  in the french tropical islands","  This paper deals with the elaboration of global quality standards for natural
and low energy cooling in french tropical island buildings. Electric load
profiles of tropical islands in developed countries are characterised by
morning, midday and evening peaks arising from all year round high power demand
in the commercial and residential sectors, mostly due to air conditioning
appliances and bad thermal conception of the building. In early 1995, a DSM
pilot initiative has been launched in the french islands of Guadeloupe and
Reunion through a partnership between the French Public Utility EDF,
institutions involved in energy conservation, environment preservation (ADEME)
and construction quality improvment, the University of Reunion Island and
several other public and private partners (low cost housing institutions,
architects, energy consultant, etc...) to set up a standard in the thermal
conception of buildings in tropical climates. This has led to definition of
optimized bioclimatic urban planning and architectural design, the use of
passive cooling architectural components, natural ventilation and energy
efficient systems. The impact of each technical solution on the thermal comfort
within the building was evaluated with an airflow and thermal building
simulation software (CODYRUN). These technical solutions have been edited in a
pedagogical reference document and have been implemented in 300 new pilot
dwelling projects through the year 1996 in Reunion Island and in Guadeloupe
island. An experimental follow up is still in process in the first ECODOM
dwellings for an experimental validation of the impact of the passive cooling
solutions on the comfort of the occupants and to modify them if necessary.
"
860,YAGI Antenna Design for Signal Phone Jammer,"  Mobile phone is one of the most widely used today in mobile communications.
This technology is very useful for communication but this raises several
problems in a situation where silence is required such as in libraries, places
of worship, classrooms and others. Mobile phone jammer is a device that used to
block the incoming signal to a mobile phone from the base station. If the
mobile phone jammer is turned on then it can not form the incoming or outgoing
calls even sms. In this research, we designed a Yagi antenna (900MHz) to expand
the range of jamming because Yagi has a great gain. Results of impedance by
gamma match are 50.16 Om. Obtained the value of VSWR Yagi is 1.46:1 and jamming
distance that can be taken approximately 16 meters, It is different from the
jamming distance of helical antenna on a mobile phone jammer itself is about 4
meters.
"
861,URI Identity and Web Architecture Revisited,"  This document reexamined the URI's identity issue and the debate regarding
the nature of ""information resource"". By making emphasis on the abstract nature
of resource and the role of URI as an interface to the web, this article
presented an alternative viewpoint about the architecture of the web that would
allow us to objectively and consistently treat all kinds of resources.
"
862,On Neighborhood Tree Search,"  We consider the neighborhood tree induced by alternating the use of different
neighborhood structures within a local search descent. We investigate the issue
of designing a search strategy operating at the neighborhood tree level by
exploring different paths of the tree in a heuristic way. We show that allowing
the search to 'backtrack' to a previously visited solution and resuming the
iterative variable neighborhood descent by 'pruning' the already explored
neighborhood branches leads to the design of effective and efficient search
heuristics. We describe this idea by discussing its basic design components
within a generic algorithmic scheme and we propose some simple and intuitive
strategies to guide the search when traversing the neighborhood tree. We
conduct a thorough experimental analysis of this approach by considering two
different problem domains, namely, the Total Weighted Tardiness Problem
(SMTWTP), and the more sophisticated Location Routing Problem (LRP). We show
that independently of the considered domain, the approach is highly
competitive. In particular, we show that using different branching and
backtracking strategies when exploring the neighborhood tree allows us to
achieve different trade-offs in terms of solution quality and computing cost.
"
863,"Application of polynomial vector (pv) processing to improve the
  estimation performance of bio diesel in variable compression ratio diesel
  engine","  This paper presents the implementation of polynomial vector back propagation
algorithm (PVBPA) for estimating the power, torque, specific fuel consumption
and presence of carbon monoxide, hydrocarbons in the emission of a direct
injection diesel engine. Experimental readings were obtained using the
biodiesel prepared form the waste low quality cooking oil collected from the
canteen of Sri Sairam Engineering College, India.. This waste cooking oil was
due to the preparation of varieties of food (vegetables fried and non
vegetarian). Over more than a week, trans esterification was done in chemical
lab and the biodiesel was obtained. The biodiesel was mixed in proportions of
10%, 20%, 30%, 40%, 50% with remaining combinations of the diesel supplied by
the Indian government. Variable compression ratio (VCR) diesel engine with
single cylinder, four stroke diesel type was used. The outputs of the engine as
power, torque and specific fuel consumption were obtained from the
computational facility attached to the engine. The data collected for different
input conditions of the engine was further used to train (PVBPA). The trained
PVBPA network was further used to predict the power, torque and brake specific
fuel consumption (SFC) for different speed, biodiesel and diesel combinations
and full load condition. The estimation performance of the PVBPA network is
discussed.
"
864,"Microelectromechanical Resonators for Radio Frequency Communication
  Applications","  Over the past few years, microelectromechanical system (MEMS) based on-chip
resonators have shown significant potential for sensing and high frequency
signal processing applications. This is due to their excellent features like
small size, large frequency-quality factor product, low power consumption, low
cost batch fabrication, and integrability with CMOS IC technology. Radio
frequency communication circuits like reference oscillators, filters, and
mixers based on such MEMS resonators can be utilized for meeting the increasing
count of RF components likely to be demanded by the next generation
multi-band/multi-mode wireless devices. MEMS resonators can provide a feasible
alternative to the present day well established quartz crystal technology that
is riddled with major drawbacks like relatively large size, high cost, and low
compatibility with IC chips. This article presents a survey of the developments
in this field of resonant MEMS structures with detailed enumeration on the
various micromechanical resonator types, modes of vibration, equivalent
mechanical and electrical models, materials and technologies used for
fabrication, and the application of the resonators for implementing oscillators
and filters. These are followed by a discussion on the challenges for RF MEMS
technology in comparison to quartz crystal technology; like high precision,
stability, reliability, need for hermetic packaging etc. which remain to be
addressed for enabling the inclusion of micromechanical resonators into
tomorrow's highly integrated communication systems.
"
865,"Generic System Verilog Universal Verification Methodology based Reusable
  Verification Environment for Efficient Verification of Image Signal
  Processing IPs/SoCs","  In this paper,we present Generic System Verilog Universal Verification
Methodology based Reusable Verification Environment for efficient verification
of Image Signal Processing IP's/SoC's. With the tight schedules on all projects
it is important to have a strong verification methodology which contributes to
First Silicon Success. Deploy methodologies which enforce full functional
coverage and verification of corner cases through pseudo random test scenarios
is required. Also, standardization of verification flow is needed. Previously,
inside imaging group of ST, Specman (e)/Verilog based Verification Environment
for IP/Subsystem level verification and C/C++/Verilog based Directed
Verification Environment for SoC Level Verification was used for Functional
Verification. Different Verification Environments were used at IP level and SoC
level. Different Verification/Validation Methodologies were used for SoC
Verification across multiple sites. Verification teams were also looking for
the ways how to catch bugs early in the design cycle? Thus, Generic System
Verilog Universal Verification Methodology (UVM) based Reusable Verification
Environment is required to avoid the problem of having so many methodologies
and provides a standard unified solution which compiles on all tools. The main
aim of development of this Generic and automatic verification environment is to
develop an efficient and unified verification environment (at IP/Subsystem/SoC
Level) which reuses the already developed Verification components and also
sequences written at IP/Subsystem level can be reused at SoC Level both with
Host BFM and actual Core using Incisive Software Extension (ISX) and Virtual
Register Interface (VRI)/Verification Abstraction Layer (VAL) approaches.
IP-XACT based tools are used for automatically configuring the environment for
various imaging IPs/SoCs.
"
866,Aplikasi belajar membaca iqro' berbasis mobile,"  IPTEK and IMTAQ should be followed by knowledge of the ability in reading the
hijaiyah letters as Al Qur-an base. Current people are so busy with their
activities thats way authors develop this mobile application using pocket pc.
The development of this research using waterfall model. Authors use the
programming language of Microsoft Visual BASIC.Net. Authors also use Photoshop
to prepare the image of every letter. In Indonesia, there six level in reading
Al Qur-an, but for the purpose of thi research authors only use Iqro-1 until
Iqro-4. This mobile application also enriched with the voice for every letter
image.
"
867,MFLP: Most Frequent Least Power Encoding,"  This paper has been withdrawn by the authors. In this paper, we propose a new
low power coding technique by decreasing the number of switching activities on
the buses which use transition signaling to transmit data. This approach
dedicates the symbols with less ones to high probability data. MFLP unlike the
most low power encoding does not rely on spatial redundancy. Due to this
superiority, MFLP is unique in power decreasing in the Network on Chip (NoC).
Not only does this algorithm reduce the power consumption, but also it can
compress the data. It offers a tradeoff to designers to choose between
compression and power; that is, the more power consumption decrease we need,
the less compression we earn. This coding uses tree based infrastructure in
order to decrease the number of ones to reduce the switching activities, and
the power consumption consequently. The proposed algorithm constructs the tree
with this contribution that code words with less ones are allocated to more
frequent data. The experimental results for the outside and inside of the NoC
indicate that the proposed coding algorithm reduces the switching activity up
to 30 and 45%, the link power consumption up to 35 and 46% and the total power
dissipation up to 34.9 and 16% for the outside and inside of the NoC,
respectively.
"
868,Data Analysis on the High-Frequency Pollution Data Collected in India,"  Fine grained 1Hz Carbon Monoxide pollution data were collected on a busy road
in Hyderabad, India. In this paper we report the findings from analysing the
experimental data, in which it was found that the data were log-normally
distributed and nonlinear. The dominant frequencies at peak hours were caused
by the pattern of traffic flow.
"
869,"Analisis laporan tugas akhir mahasiswa Diploma I dari sudut pandang
  kaidah ilmiah dan penggunaan teknologi informasi","  The purposes of this research are: 1) to analyze final report from scientific
role, 2) the use of information technology (IT), and 3) to conduct academic
athmosphere in research area. This research gives contributions to study
program of MI-DI, such as: 1) to know the pattern of student final report from
scientific role and the use of IT, 2) give input to study program for next
final report scheme, and 3) can be used for next research reference. If we look
to the quality of final report, there are several focuses to be prepared on
tittle, literature review, methodology, results of report, discussion and also
conclusion. But for the use of IT is already good but the varian is decrease.
"
870,Transfers of entanglement qudit states in quantum networks,"  The issue of quantum states' transfer -- in particular, for so-called Perfect
State Transfer (PST) -- in the networks represented by the spin chains seems to
be one of the major concerns in quantum computing. Especially, in the context
of future communication methods that can be used in broadly defined computer
science. The chapter presents a definition of Hamiltonian describing the
dynamics of quantum data transfer in one-dimensional spin chain, which is able
to transfer the state of unknown qudits. The main part of the chapter is the
discussion about possibility of entangled states' perfect transfer, in
particular, for the generalized Bell states for qudits. One of the sections
also contains the results of numerical experiments for the transmission of
quantum entangled state in a noisy quantum channel.
"
871,"A CMOS Tailed Tent Map for the Generation of Uniformly Distributed
  Chaotic Sequences","  This paper describes the design of a modified tent map characterized by a
uniform probability density function. The use of this map is proposed as an
alternative to the tent map and the Bernoulli shift. It is shown that practical
circuits implementing the latter two maps may possess parasitic stable
equilibria, fact which would prevent the desired chaotic behavior of the
system. On the other hand, commonly used strategies to avoid the parasitic
equilibria onset also affect the uniformity of the probability density
function. Conversely, the use of the proposed tailed tent map allows to assure
a certain degree of parameter deviation robustness, without compromising on the
statistical properties of the system.
"
872,"Circuit proposition for copying the value of a resistor into a
  memristive device supported by HSPICE simulation","  Memristor is the fourth fundamental passive circuit element with potential
applications in development of analog memories, artificial brains (with the
capacity of hardware training) and neuro-science. In most of these applications
the memristance of the device should be set to the desired value, which is
currently performed by trial and error. The aim of this paper is to propose a
circuit for copying the value of the given resistor into a memristive device.
HSPICE simulations are also presented to confirm the efficiency of the proposed
circuit.
"
873,"Improving Mixed-Criticality System Consistency and Behavior on
  Multiprocessor Platforms by Means of Multi-Moded Approaches","  Recent research in the domain of real-time scheduling theory has tackled the
problem of scheduling mixed-criticality systems upon uniprocessor or
multiprocessor platforms, with the main objective being to respect the
timeliness of the most critical tasks, at the expense of the requirements of
the less critical ones. In particular, the less critical tasks are carelessly
discarded when the computation demand of (some of) the high critical tasks
increases. This might nevertheless result in system failure, as these less
critical tasks could be accessing data, the consistency of which should be
preserved. In this paper, we address this problem and propose a method to
cautiously handle task suspension. Furthermore, it is usually assumed that the
less critical tasks will never be re-enabled once discarded. In this paper, we
also address this concern by proposing an approach to re-enable the less
critical tasks, without jeopardizing the timeliness of the high critical ones.
The suggested approaches apply to systems having two or more criticality
levels.
"
874,"An Approach to Select Cost-Effective Risk Countermeasures Exemplified in
  CORAS","  Risk is unavoidable in business and risk management is needed amongst others
to set up good security policies. Once the risks are evaluated, the next step
is to decide how they should be treated. This involves managers making
decisions on proper countermeasures to be implemented to mitigate the risks.
The countermeasure expenditure, together with its ability to mitigate risks, is
factors that affect the selection. While many approaches have been proposed to
perform risk analysis, there has been less focus on delivering the prescriptive
and specific information that managers require to select cost-effective
countermeasures. This paper proposes a generic approach to integrate the cost
assessment into risk analysis to aid such decision making. The approach makes
use of a risk model which has been annotated with potential countermeasures,
estimates for their cost and effect. A calculus is then employed to reason
about this model in order to support decision in terms of decision diagrams. We
exemplify the instantiation of the generic approach in the CORAS method for
security risk analysis.
"
875,"Perangkat lunak bantu mengenal huruf arab melayu ke bentuk huruf latin
  bahasa Indonesia","  The development of computer science has contributed greatly for increasing of
efficiency and effectively. Many areas are covered by computer science,
included education. The purpose of this research is to introduce jawi a type of
Indonesian letters. Jawis letter is one of the most popular letter in the past.
But right now few people can read and understand it. Many documents in the past
was written in Jawi. The writer develop or build the software using Pressman
method, and tools such as Microsoft Visual Basic, and Microsoft Access. This
software can introduce Jawi then people can learn it easily.
"
876,Embedding of Deterministic Test Data for In-Field Testing,"  This paper presents a new feedback shift register-based method for embedding
deterministic test patterns on-chip suitable for complementing conventional
BIST techniques for in-field testing. Our experimental results on 8 real
designs show that the presented approach outperforms the bit-flipping approach
by 24.7% on average. We also show that it is possible to exploit the uneven
distribution of don't care bits in test patterns in order to reduce the area
required for storing deterministic test patterns more than 3 times with less
than 2% fault coverage drop.
"
877,"Adaptive Modulation (QPSK, QAM)","  In this paper, introduced below are the concepts of digital modulation used
in many communication systems today. Techniques described include quadrature
phase shift keying (QPSK) and quadrature amplitude modulation (QAM) and how
these techniques can be used to increase the capacity and speed of a wireless
network. These modulation techniques are the basis of communications for
systems like cable modems, DSL modems, CDMA, 3G, Wi-Fi* (IEEE 802.11) and
WiMAX* (IEEE 802.16).
"
878,"Energy Aware Task Scheduling for Soft Real Time Systems using an
  Analytical Approach for Energy Estimation","  Embedded systems have pervaded all walks of our life. With the increasing
importance of mobile embedded systems and flexible applications, considerable
progress in research has been made for power management. Power constraints are
increasingly becoming the critical component of the design specifications of
these systems. It helps in pre-determining the suitable hardware architecture
for the target application. The aim of this paper is to present a technique to
estimate 'pre-run time' and 'power' of a software mapped onto a hardware
system; guaranteeing the compliance of temporal constraints while generating a
schedule of tasks of software. Real time systems must handle several
independent macro-tasks, each represented by a task graph, which includes
communications and precedence constraints. We propose a novel approach for
power estimation of embedded software using the Control Data Flow Graph (CDFG)
or task graph model. This methodology uses an existing Hierarchical Concurrent
Flow Graph (HCFG) technique for the power analysis of the CDFGs. We have
evaluated our technique for energy efficient scheduling over various task graph
benchmarks. The results obtained prove the utility and efficacy of our proposed
approach for power analysis of embedded software. We also present a methodology
to obtain an energy optimal voltage assignment and perform scheduling by taking
advantage of the relaxation in execution time of tasks.
"
879,"Joint Ultra-wideband and Signal Strength-based Through-building Tracking
  for Tactical Operations","  Accurate device free localization (DFL) based on received signal strength
(RSS) measurements requires placement of radio transceivers on all sides of the
target area. Accuracy degrades dramatically if sensors do not surround the
area. However, law enforcement officers sometimes face situations where it is
not possible or practical to place sensors on all sides of the target room or
building. For example, for an armed subject barricaded in a motel room, police
may be able to place sensors in adjacent rooms, but not in front of the room,
where the subject would see them. In this paper, we show that using two
ultra-wideband (UWB) impulse radios, in addition to multiple RSS sensors,
improves the localization accuracy, particularly on the axis where no sensors
are placed (which we call the x-axis). We introduce three methods for combining
the RSS and UWB data. By using UWB radios together with RSS sensors, it is
still possible to localize a person through walls even when the devices are
placed only on two sides of the target area. Including the data from the UWB
radios can reduce the localization area of uncertainty by more than 60%.
"
880,"A Multi-objective Perspective for Operator Scheduling using Fine-grained
  DVS Architecture","  The stringent power budget of fine grained power managed digital integrated
circuits have driven chip designers to optimize power at the cost of area and
delay, which were the traditional cost criteria for circuit optimization. The
emerging scenario motivates us to revisit the classical operator scheduling
problem under the availability of DVFS enabled functional units that can
trade-off cycles with power. We study the design space defined due to this
trade-off and present a branch-and-bound(B/B) algorithm to explore this state
space and report the pareto-optimal front with respect to area and power. The
scheduling also aims at maximum resource sharing and is able to attain
sufficient area and power gains for complex benchmarks when timing constraints
are relaxed by sufficient amount. Experimental results show that the algorithm
that operates without any user constraint(area/power) is able to solve the
problem for most available benchmarks, and the use of power budget or area
budget constraints leads to significant performance gain.
"
881,A Digital Automatic Sliding Door with a Room Light Control System,"  Automatic door is an automated movable barrier installed in the entry of a
room or building to restrict access, provide ease of opening a door or provide
visual privacy. As a result of enhanced civilization and modernization, the
human nature demands more comfort to his life. The man seeks ways to do things
easily and which saves time. So thus, the automatic gates are one of the
examples that human nature invent to bring comfort and ease in its daily life.
To this end, we model and design an automatic sliding door with a room light
control system to provide the mentioned needs. This was achieved by considering
some factors such as economy, availability of components and research
materials, efficiency, compatibility and portability and also durability in the
design process. The performance of the system after test met design
specifications. This system works on the principle of breaking an infrared beam
of light, sensed by a photodiode. It consists of two transmitting infrared
diodes and two receiving photo-diodes. The first one is for someone coming in
and the second one is for someone going out of the room. The photodiodes are
connected to comparators, which give a lower output when the beam is broken and
high output when transmitting normally. The general operation of the work and
performance is dependent on the presence of an intruder entering through the
door and how close he/she is in closer to the door. The door is meant to open
automatically but in a case where there is no power supply trying to force the
door open would damage the mechanical control system of the unit. The overall
work was implemented with a constructed work, tested working and perfectly
functional.
"
882,Design and Development of an Ultrasonic Motion Detector,"  The ultrasonic motion detector devices emit ultrasonic sound energy into an
area of interest (monitored area), and this further reacts to a change in the
reflected energy pattern. The system uses a technique that is based on a
frequency shift in reflected energy to detect a movement or change in position
(motion). In this system, ultrasonic sound is transmitted from the transmitting
device which is normally in the form of energy. The transmitted sound utilizes
air as its medium and this travel in a wave type motion. The wave is reflected
back from the surroundings in the room/hallway and the device hears a pitch
characteristic of the protected environment. In this system, the wave pattern
is disturbed and reflected back more quickly, thus increasing the pitch and
signaling an alarm whenever motion is detected. The main contribution of this
work is the design of a circuit that can sense motion through movement of
anything, a low cost and portable motion detector, and the design of a circuit
that can be used to trigger another circuit whether to ON or OFF depending on
the circuit attached to it. Generally, the design is made to detect movement or
moving object in a an enclosed area. In this work, a transmitter transducer
generates a signal at a frequency of 40khz, and when the signal is blocked by
any moving object, and this in turn, triggers a buzzer via a timing circuit.
This system works on the principle of the signal interference by a moving body,
and the system is dependent on the presence of an intruder or moving object
within a monitored area. The system after design and construction was tested
and found to work in accordance with specifications.
"
883,"An electronic digital combination lock: A precise and reliable security
  system","  The increasing rate of crime, attacks by thieves, intruders and vandals,
despite all forms of security gadgets and locks still need the attention of
researchers to find a permanent solution to the well being of lives and
properties of individuals. To this end, we design a cheap and effective
security system for buildings, cars, safes, doors and gates, so as to prevent
unauthorized person from having access to ones properties through the use of
codes, we therefore experiment the application of electronic devices as locks.
However, a modular approach was employed in the design in which the combination
lock was divided into units and each unit designed separately before being
coupled to form a whole functional system. During the design, we conducted
Twenty tests with the first eight combinations being four in number, the next
seven tests being five and the last five combinations being six. This was done
because of the incorporation of 2 dummy switches in the combinations. From the
result obtained, combinations 8, 11, 13 gave the correct output combination.
However, 8 being the actual combination gave the required output. The general
operation of the system and performance is dependent on the key combinations.
The overall system was constructed and tested and it works perfectly.
"
884,"Mixed Maps for Kolmogoroff-Nagumo-Type Averaging on the Compact Stiefel
  Manifold","  The present research work proposes a new fast fixed-point averaging algorithm
on the compact Stiefel manifold based on a mixed retraction/lifting pair.
Numerical comparisons between fixed-point algorithms based on the proposed
non-associated retraction/lifting map pair and two associated
retraction/lifting pairs confirm that the averaging algorithm based on a
combination of mixed maps is remarkably less computationally demanding than the
same averaging algorithm based on any of the constituent associated
retraction/lifting pairs.
"
885,Speaking Plant Approach for Automatic Fertigation System in Greenhouse,"  Nowadays, many vegetables are grown insidegreenhouses in which environment is
controlled and nutrition can be supplied through water supply using electrical
pump, namely fertigation. Dosage of nutrition in water for many vegetable
plants are also known so that by controllingwater supply all the needsfor the
plants to grow are available. Furthermore, water supply can be controlled using
electrical pump which is activated according to theplants conditionin relation
with water supply. In order to supply water and nutrition in the right amount
and time, plants condition can be observed using a CCD camera attached to image
processing facilitiesto develop a speaking plant approach. In this study,
plants development during their growing periodare observedusing image
processing. Three populationsof tomato plants, with less, enough, and exceeded
nutrition in water,are captured using a CCD camera every three days, and the
images were analyzed using a developed computer program for the heightof
plants. The results showed that the development of the plants can be monitored
using this method. After that, the responseof plant growth in the same
condition was monitored, and the responsewas used as input for the fertigation
system to turn electrical pump automatically on and off, so the fertigation
system could maintain the growth of the plants.
"
886,Proposition d'une technique de gestion de projet dans les startups,"  This project is part of the development of mobile CRM. It aims to develop a
management application client named NOMALYS. This application allows the
commercial and business leaders to see their CRM Mobile. We have focused in
this project on the techniques of projects management, this study allowed to
classify different techniques for managing software projects and proposed the
most closely technique that match the needs of the studied company.
"
887,"Multi-User Multi-Carrier Differential Chaos Shift Keying Communication
  System","  In this paper, a multi user Multi-Carrier Differential Chaos Shift Keying
(MC-DCSK) modulation is presented. The system endeavors to provide a good
trade-off between robustness, energy efficiency and high data rate, while still
being simple. In this architecture of MC-DCSK system, for each user, chaotic
reference sequence is transmitted over a predefined subcarrier frequency.
Multiple modulated data streams are transmitted over the remaining subcarriers
allocated for each user. This transmitter structure saves energy and increases
the spectral efficiency of the conventional DCSK system.
"
888,"Independent Component Analysis for Filtering Airwaves in Seabed Logging
  Application","  Marine controlled source electromagnetic (CSEM) sensing method used for the
detection of hydrocarbons based reservoirs in seabed logging application does
not perform well due to the presence of the airwaves (or sea-surface). These
airwaves interfere with the signal that comes from the subsurface seafloor and
also tend to dominate in the receiver response at larger offsets. The task is
to identify these air waves and the way they interact, and to filter them out.
In this paper, a popular method for counteracting with the above stated problem
scenario is Independent Component Analysis (ICA). Independent component
analysis (ICA) is a statistical method for transforming an observed
multidimensional or multivariate dataset into its constituent components
(sources) that are statistically as independent from each other as possible.
ICA-type de-convolution algorithm that is FASTICA is considered for mixed
signals de-convolution and considered convenient depending upon the nature of
the source and noise model. The results from the FASTICA algorithm are shown
and evaluated. In this paper, we present the FASTICA algorithm for the seabed
logging application.
"
889,Adaptive Transmission Techniques for Mobile Satellite Links,"  Adapting the transmission rate in an LMS channel is a challenging task
because of the relatively fast time variations, of the long delays involved,
and of the difficulty in mapping the parameters of a time-varying channel into
communication performance. In this paper, we propose two strategies for dealing
with these impairments, namely, multi-layer coding (MLC) in the forward link,
and open-loop adaptation in the return link. Both strategies rely on
physical-layer abstraction tools for predicting the link performance. We will
show that, in both cases, it is possible to increase the average spectral
efficiency while at the same time keeping the outage probability under a given
threshold. To do so, the forward link strategy will rely on introducing some
latency in the data stream by using retransmissions. The return link, on the
other hand, will rely on a statistical characterization of a physical-layer
abstraction measure.
"
890,"Design and Analysis of a Multi-Carrier Differential Chaos Shift Keying
  Communication System","  A new Multi-Carrier Differential Chaos Shift Keying (MC-DCSK) modulation is
presented in this paper. The system endeavors to provide a good trade-off
between robustness, energy efficiency and high data rate, while still being
simple compared to conventional multi-carrier spread spectrum systems. This
system can be seen as a parallel extension of the DCSK modulation where one
chaotic reference sequence is transmitted over a predefined subcarrier
frequency. Multiple modulated data streams are transmitted over the remaining
subcarriers. This transmitter structure increases the spectral efficiency of
the conventional DCSK system and uses less energy. The receiver design makes
this system easy to implement where no radio frequency (RF) delay circuit is
needed to demodulate received data. Various system design parameters are
discussed throughout the paper, including the number of subcarriers, the
spreading factor, and the transmitted energy. Once the design is explained, the
bit error rate performance of the MC-DCSK system is computed and compared to
the conventional DCSK system under an additive white Gaussian noise (AWGN) and
Rayleigh channels. Simulation results confirm the advantages of this new hybrid
design.
"
891,"Performance Analysis of LMS Filter for SSPA Linearization in Different
  Modulation Conditions","  The SSPA has wide application in Communication system, but its high output
power varies due to its non linear gain. Pre-distortion method plays major role
in power amplifier linearization. Polynomial is one of the methods used. The
error estimation in Polynomial method is carried out by LMS Filter. Our main
work is to analysis the error estimation performance of the LMS Filter for the
Solid state power amplifiers (SSPA) in different modulation conditions. Here we
are calculating the ACP and analyzing how effectively the memoryless non
linearity has been reduced for all digital modulation techniques. All the
analysis and results are taken using Matlab software.
"
892,Device-Free Person Detection and Ranging in UWB Networks,"  We present a novel device-free stationary person detection and ranging
method, that is applicable to ultra-wide bandwidth (UWB) networks. The method
utilizes a fixed UWB infrastructure and does not require a training database of
template waveforms. Instead, the method capitalizes on the fact that a human
presence induces small low-frequency variations that stand out against the
background signal, which is mainly affected by wideband noise. We analyze the
detection probability, and validate our findings with numerical simulations and
experiments with off-the-shelf UWB transceivers in an indoor environment.
"
893,Medical Process Modeling: an Artifact-Centric Approach,"  In this position paper we argue that just as traditional business process
modeling has been adopted to deal with clinical pathways, also the
artifact-centric process modeling technique may be successfully used to model
various kinds of medical processes: physiological processes, disease behavior
and treatment processes. We also discuss how a proposed approach may be used to
deal with an interplay of all the processes a patient is subject to and what
are the queries that might be imposed over an overall patient model.
"
894,Using UWB for Human Trajectory Extraction,"  In this paper we report on a methodology to model pedestrian behaviours
whilst aggregate variables are concerned, with potential applications to
different situations, such as evacuating a building in emergency events. The
approach consists of using UWB (ultra-wide band) based data collection to
characterise behaviour in specific scenarios. From a number of experiments
carried out, we detail the single-file scenario to demonstrate the ability of
this approach to represent macroscopic characteristics of the pedestrian flow.
Results are discussed and we can conclude that UWB-based data collection shows
great potential and suitability for human trajectory extraction, when compared
to other traditional approaches.
"
895,"Universal Numerical Encoder and Profiler Reduces Computing's Memory Wall
  with Software, FPGA, and SoC Implementations","  In the multicore era, the time to computational results is increasingly
determined by how quickly operands are accessed by cores, rather than by the
speed of computation per operand. From high-performance computing (HPC) to
mobile application processors, low multicore utilization rates result from the
slowness of accessing off-chip operands, i.e. the memory wall. The APplication
AXcelerator (APAX) universal numerical encoder reduces computing's memory wall
by compressing numerical operands (integers and floats), thereby decreasing CPU
access time by 3:1 to 10:1 as operands stream between memory and cores. APAX
encodes numbers using a low-complexity algorithm designed both for time series
sensor data and for multi-dimensional data, including images. APAX encoding
parameters are determined by a profiler that quantifies the uncertainty
inherent in numerical datasets and recommends encoding parameters reflecting
this uncertainty. Compatible software, FPGA, and systemon-chip (SoC)
implementations efficiently support encoding rates between 150 MByte/sec and
1.5 GByte/sec at low power. On 25 integer and floating-point datasets, we
achieved encoding rates between 3:1 and 10:1, with average correlation of
0.999959, while accelerating computational ""time to results.""
"
896,"Principle ""synthesis"" for the solution of tasks of class NP","  Initial contours of the non-standard approach to reception of the answer of
any task on discrete structures are considered: the algorithm independently
creates such answer from separate fragments.
"
897,"Development of a Device for Remote Monitoring of Heart Rate and Body
  Temperature","  We present a new integrated, portable device to provide a convenient solution
for remote monitoring heart rate at the fingertip and body temperature using
Ethernet technology and widely spreading internet. Now a days, heart related
disease is rising. Most of the times in these cases, patients may not realize
their actual conditions and even it is a common fact that there are no doctors
by their side, especially in rural areas, but now a days most of the diseases
are curable if detected in time.
  We have tried to make a system which may give information about one's
physical condition and help him or her to detect these deadly but curable
diseases. The system gives information of heart rate and body temperature
simultaneously acquired on the portable side in real time and transmits results
to web. In this system, the condition of heart and body temperature can be
monitored from remote places. Eventually, this device provides a low cost,
easily accessible human health monitor solution bridging the gaps between
patients and doctors.
"
898,Hubs and Authorities of the English Premier League for 2010-2011,"  In this work author applies well known web search algorithm Hyperlink -
Induced Topic Search (HITS) to problem of ranking football teams in English
Premier League (EPL). The algorithm allows the ranking of the teams using the
notions of hubs and authorities well known for ranking pages in the World Wide
Web. Results of the games introduced as a graph where losing team 'gives a
link' to a winning team and, if draw registered both team give links to each
other. In case of a win link is weighted as three points in adjacent matrix and
in case of draw as one point. Author uses notion of authority in order to
define team which win a game and hub as a team which lose a game, the winner of
the competition defined as the 'worst' hub, team that didn't reinforced any
other team. Using this ranking system, the champion's team, which is a 'worst
hub' must not lose, or draw games to other 'good authorities' teams. If by the
end of the competition there are teams with an equal number of wins and losses
then the team which has beaten more teams with higher authority ranks, wins.
"
899,A Simulation and Modeling of Access Points with Definition Language,"  This submission has been withdrawn by arXiv administrators because it
contains fictitious content and was submitted under a pseudonym, which is
against arXiv policy.
"
900,Scheduling Cutting Process for Large Paper Rolls,"  Paper cutting is a simple process of slicing large rolls of paper,
jumbo-reels, into various sub-rolls with variable widths based on demands risen
by customers. Since the variability is high due to collected various orders
into a pool, the process turns to be production scheduling problem, which
requires optimisation so as to minimise the final remaining amount of paper
wasted. The problem holds characteristics similar one-dimensional bin-packing
problem to some extends and differs with some respects. This paper introduces a
modelling attempt as a scheduling problem with an integer programming approach
for optimisation purposes. Then, a constructive heuristic algorithm revising
one of well-known approaches, called Best-fit algorithm, is introduced to solve
the problem. The illustrative examples provided shows the near optimum solution
provided with very low complexity .
"
901,Design and Development of a Heart Rate Measuring Device using Fingertip,"  In this paper, we presented the design and development of a new integrated
device for measuring heart rate using fingertip to improve estimating the heart
rate. As heart related diseases are increasing day by day, the need for an
accurate and affordable heart rate measuring device or heart monitor is
essential to ensure quality of health. However, most heart rate measuring tools
and environments are expensive and do not follow ergonomics. Our proposed Heart
Rate Measuring (HRM) device is economical and user friendly and uses optical
technology to detect the flow of blood through index finger. Three phases are
used to detect pulses on the fingertip that include pulse detection, signal
extraction, and pulse amplification. Qualitative and quantitative performance
evaluation of the device on real signals shows accuracy in heart rate
estimation, even under intense of physical activity. We compared the
performance of HRM device with Electrocardiogram reports and manual pulse
measurement of heartbeat of 90 human subjects of different ages. The results
showed that the error rate of the device is negligible.
"
902,"The design of high-speed data transmission method for a small nuclear
  physics DAQ system","  A large number of data need to be transmitted in high-speed between Field
Programmable Gate Array (FPGA) and Advanced RISC Machines 11 micro-controller
(ARM11) when we design a small data acquisition (DAQ) system for nuclear
experiments. However, it is a complex problem to beat the target. In this
paper, we will introduce a method which can realize the high-speed data
transmission. By this way, FPGA is designed to acquire massive data from
Front-end electronics (FEE) and send it to ARM11, which will transmit the data
to other computer through the TCP/IP protocol. This paper mainly introduces the
interface design of the high-speed transmission between FPGA and ARM11, the
transmission logic of FPGA and the driver program of ARM11. The research shows
that the maximal transmission speed between FPGA and ARM11 by this way can
reach 50MB/s theoretically, while in nuclear physics experiment, the system can
acquire data with the speed of 2.2MB/s.
"
903,"Hardware Acceleration of the Gipps Model for Real-Time Traffic
  Simulation","  Traffic simulation software is becoming increasingly popular as more cities
worldwide use it to better manage their crowded traffic networks. An important
requirement for such software is the ability to produce accurate results in
real time, requiring great computation resources. This work proposes an
ASIC-based hardware accelerated approach for the AIMSUN traffic simulator,
taking advantage of repetitive tasks in the algorithm. Different system
configurations using this accelerator are also discussed. Compared with the
traditional software simulator, it has been found to improve the performance by
as much as 9x when using a single processing element approach, or more
depending on the chosen hardware configuration.
"
904,"Graphical Methods for Defense Against False-data Injection Attacks on
  Power System State Estimation","  The normal operation of power system relies on accurate state estimation that
faithfully reflects the physical aspects of the electrical power grids.
However, recent research shows that carefully synthesized false-data injection
attacks can bypass the security system and introduce arbitrary errors to state
estimates. In this paper, we use graphical methods to study defending
mechanisms against false-data injection attacks on power system state
estimation. By securing carefully selected meter measurements, no false data
injection attack can be launched to compromise any set of state variables. We
characterize the optimal protection problem, which protects the state variables
with minimum number of measurements, as a variant Steiner tree problem in a
graph. Based on the graphical characterization, we propose both exact and
reduced-complexity approximation algorithms. In particular, we show that the
proposed tree-pruning based approximation algorithm significantly reduces
computational complexity, while yielding negligible performance degradation
compared with the optimal algorithms. The advantageous performance of the
proposed defending mechanisms is verified in IEEE standard power system
testcases.
"
905,"Marketplaces for Energy Demand-Side Management based on Future-Internet
  Technology","  Renewable energies become more important, and they contribute to the EU's
goals for greenhouse-gas reduction. However, their fluctuating nature calls for
demand-side-management techniques, which balance energy generation and
consumption. Such techniques are currently not broadly deployed. This paper
describes the latest results from the FINSENY project on how Future-Internet
enablers and market mechanisms can be used to realise such systems.
"
906,Patterns to analyze requirements of a Decisional Information System,"  The domain of analysis and conception of Decisional Information System (DIS)
is, highly, applying new techniques and methods to succeed the process of the
decision and minimizing the time of conception. Our objective in this paper is
to define a group of patterns to ensure a systematic reuse of our approach to
analyse a DIS s business requirements. We seek, through this work, to guide the
discovery of an organizations business requirements, expressed as goals by
introducing the notion of context, to promote good processes design for a DIS,
to capitalize the process and models proposed in our approach and systematize
reuse steps of this approach to analyze similar projects or adapt them as
needed. The patterns are at the same time the process s patterns and product s
patterns as they capitalize models and their associated processes. These
patterns are represented according to the PSIGMA formalism.
"
907,Universality in symbolic dynamics constrained by Medvedev degrees,"  We define a weak notion of universality in symbolic dynamics and, by
generalizing a proof of Mike Hochman, we prove that this yields necessary
conditions on the forbidden patterns defining a universal subshift: These
forbidden patterns are necessarily in a Medvedev degree greater or equal than
the degree of the set of subshifts for which it is universal.
  We also show that this necessary condition is optimal by giving constructions
of universal subshifts in the same Medvedev degree as the subshifts they
simulate and prove that this universality can be achieved by the sofic
projective subdynamics of such a subshift as soon as the necessary conditions
are verified.
  This could be summarized as: There are obstructions for the existence of
universal subshifts due to the theory of computability and they are the only
ones.
"
908,Policy Aware Geospatial Data,"  Digital Rights Management (DRM) prevents end-users from using content in a
manner inconsistent with its creator's wishes. The license describing these
use-conditions typically accompanies the content as its metadata. A resulting
problem is that the license and the content can get separated and lose track of
each other. The best metadata have two distinct qualities--they are created
automatically without user intervention, and they are embedded within the data
that they describe. If licenses are also created and transported this way, data
will always have licenses, and the licenses will be readily examinable. When
two or more datasets are combined, a new dataset, and with it a new license,
are created. This new license is a function of the licenses of the component
datasets and any additional conditions that the person combining the datasets
might want to impose. Following the notion of a data-purpose algebra, we model
this phenomenon by interpreting the transfer and conjunction of data as
inducing an algebraic operation on the corresponding licenses. When a dataset
passes from one source to the next its license is transformed in a
deterministic way, and similarly when datasets are combined the associated
licenses are combined in a non-trivial algebraic manner. Modern,
computer-savvy, licensing regimes such as Creative Commons allow writing the
license in a special kind of language called Creative Commons Rights Expression
Language (ccREL). ccREL allows creating and embedding the license using RDFa
utilizing XHTML. This is preferred over DRM which includes the rights in a
binary file completely opaque to nearly all users. The colocation of metadata
with human-visible XHTML makes the license more transparent. In this paper we
describe a methodology for creating and embedding licenses in geographic data
utilizing ccREL, and programmatically examining embedded licenses in component
data...
"
909,Priority Based Pre-emptive Task Scheduling for Android Operating System,"  Android mobile operating system which is based on Linux Kernel 2.6, has open
source license and adaptability to user driven applications. As all other
operating systems it has all the basic features like process scheduling, memory
management, process management etc associated with it. Any mobile platform
works smoothly when the process scheduling is performed in a proper way. Ideal
platform is that in which no resource conflict occurs. Thus scheduling in every
manner is essential for the operating system to adapt itself with the
requirement of a particular application. In this paper, priority based
pre-emptive task scheduling is proposed for the SMS application. The idea is to
define High priority to required contacts, for ex. Contact numbers of parents
or teachers will be given High priority. If in case, any SMS from these High
priority contacts is received, the application would flash the SMS on the
active screen and redirect this High priority SMS to the Priority Inbox.
"
910,Object Oriented Model for Evaluation of On-Chip Networks,"  The Network on Chip (NoC) paradigm is rapidly replacing bus based System on
Chip (SoC) designs due to their inherent disadvantages such as non-scalability,
saturation and congestion. Currently very few tools are available for the
simulation and evaluation of on-chip architectures. This study proposes a
generic object oriented model for performance evaluation of on-chip
interconnect architectures and algorithms. The generic nature of the proposed
model can help the researchers in evaluation of any kind of on-chip switching
networks. The model was applied on 2D-Mesh and 2D-Diagonal-Mesh on-chip
switching networks for verification and selection of best out of both the
analyzed architectures. The results show the superiority of 2D-Diagonal-Mesh
over 2D-Mesh in terms of average packet delay.
"
911,"GUI Based Automatic Remote Control of Gas Reduction System using PIC
  Microcontroller","  The GRS is a one of the important units in Erbil Power Station EPS, which is
responsible on controlling gas pressure and gas temperature this unit
previously works manually. The local control panel for GRS system contains two
types of digital signals the first one indicated by Light Emitting Diodes LED
to point normal operations, fault and alarm, and event of operations while the
second indicated by ON-OFF switches, which consists of two types the push
buttons switch and mode selector switch. To overcome human in manual control
faults in controlling GRS systems, automation system becomes the best solution.
The purpose of this research is to design and implement embedded automation
system that can be used to control a GRS automatically through a GUI and from
remote location by using programmable interface controller (PIC16F877A). In
this research the (PIC) software which is based on (C language), developed by
Microchip (MPLAB) is used in programming a PIC microcontroller, then Visual
Basic is used in the construction of GUI, the RS-232 serial cable is used as a
connector between PIC and PC. Implement the proposed design and test it as a
first system shows all operations of GRS successful were converted into full
computerize controlling (with the ability of full automatic control) from
remote location through proposed GUI. Keywords-Peripheral Interface Controller
(PIC); Microcontroller; Graphical User Interface (GUI); Remote; Control.
"
912,BiEntropy - The Approximate Entropy of a Finite Binary String,"  We design, implement and test a simple algorithm which computes the
approximate entropy of a finite binary string of arbitrary length. The
algorithm uses a weighted average of the Shannon Entropies of the string and
all but the last binary derivative of the string. We successfully test the
algorithm in the fields of Prime Number Theory (where we prove explicitly that
the sequence of prime numbers is not periodic), Human Vision, Cryptography,
Random Number Generation and Quantitative Finance.
"
913,Augmented Reality in ICT for Minimum Knowledge Loss,"  Informatics world digitizes the human beings, with the contribution made by
all the industrial people. In the recent survey it is proved that people are
not accustomed or they are not able to access the electronic devices to its
extreme usage. Also people are more dependent to the technologies and their
day-to-day activities are ruled by the same. In this paper we discuss on one of
the advanced technology which will soon rule the world and make the people are
more creative and at the same time hassle-free. This concept is introduced as
6th sense technology by an IIT, Mumbai student who is presently Ph.D., scholar
in MIT, USA. Similar to this research there is one more research going on under
the title Augmented Reality. This research makes a new association with the
real world to digital world and allows us to share and manipulate the
information directly with our mental thoughts. A college which implements state
of the art technology for teaching and learning, Higher College of Technology,
Muscat, (HCT) tries to identify the opportunities and limitations of
implementing this augmented reality for teaching and learning. The research
team of HCT, here, tries to give two scenarios in which augmented reality can
fit in. Since this research is in the conceptual level we are trying to
illustrate the history of this technology and how it can be adopted in the
teaching environment
"
914,"Field Programmable DSP Arrays - A Novel Reconfigurable Architecture for
  Efficient Realization of Digital Signal Processing Functions","  Digital Signal Processing functions are widely used in real time high speed
applications. Those functions are generally implemented either on ASICs with
inflexibility, or on FPGAs with bottlenecks of relatively smaller utilization
factor or lower speed compared to ASIC. The proposed reconfigurable DSP
processor is redolent to FPGA, but with basic fixed Common Modules (CMs) (like
adders, subtractors, multipliers, scaling units, shifters) instead of CLBs.
This paper introduces the development of a reconfigurable DSP processor that
integrates different filter and transform functions. The switching between DSP
functions is occurred by reconfiguring the interconnection between CMs.
Validation of the proposed reconfigurable architecture has been achieved on
Virtex5 FPGA. The architecture provides sufficient amount of flexibility,
parallelism and scalability.
"
915,"An UHF RFID Energy-Harvesting System Enhanced by a DC-DC Charge Pump in
  Silicon-on-Insulator Technology","  An RF-DC converter enhanced by a DC-DC voltage booster in
silicon-on-insulator technology for UHF radio frequency identification (RFID)
energy harvesting is presented in this letter. When the received RF power level
is -14 dBm or higher, the system, fabricated on an FR4 substrate using
off-the-shelf low-cost discrete components and connected to a flexible dipole
antenna, is able to produce 2.4-V DC voltage to power general-purpose
electronic devices. As a simple proof of concept,a device comprising
microcontroller, temperature sensor, and EEPROM is considered in this work. The
experimental results demonstrate the capability of the system to autonomously
perform temperature data logging up to a distance of 5 m from a conventional
UHF RFID reader used as an RF energy source.
"
916,"Enabling Self-Powered Autonomous Wireless Sensors with New-Generation
  I2C-RFID Chips","  A self-powered autonomous RFID device with sensing and computing capabilities
is presented in this paper. Powered by an RF energy-harvesting circuit enhanced
by a DC-DC voltage booster in silicon-on-insulator (SOI) technology, the device
relies on a microcontroller and a new generation I2C-RFID chip to wirelessly
deliver sensor data to standard RFID EPC Class-1 Generation-2 (Gen2) readers.
When the RF power received from the interrogating reader is -14 dBm or higher,
the device, fabricated on an FR4 substrate using low-cost discrete components,
is able to produce 2.4-V DC voltage to power its circuitry. The experimental
results demonstrate the effectiveness of the device to perform reliable sensor
data transmissions up to 5 meters in fully-passive mode. To the best of our
knowledge, this represents the longest read range ever reported for passive UHF
RFID sensors compliant with the EPC Gen2 standard.
"
917,"Comparative Study of ERP Implementation Methodology Case Study:
  Accelerated SAP VS Dantes & Hasibuan Methodology","  Enterprise Resource Planning (ERP) system is a concept of enterprise system
that describe the integration of the whole process in the organization. Study
in this field mostly about external development paradigm on information system
development. So, issue in ERP is all about how to adopt it in the organization,
not about the application development. This paper reviews two methodologies on
ERP system implementation, one is vendor perspective methodology and new
generic perspective methodology. Comparation of both methodology is done in
this study using certain metric measurements. Result is the vendor perspective
slightly superior than new generic perspective methodology.
"
918,Information System as a Service: Issues and Challenges,"  Information system evolved as the evolution of information technology. The
current state of information technology, placed the internet as a main
resources of computing. Cloud technology as the backbone of internet has been
utilized as a powerful computing resources. Therefore, cloud introduced new
term of service oriented technology, popular with ""as a service"" kind of name.
In this paper, the service oriented paradigm will be used to address future
trend of information system. Thus, this paper try to introduce the term
""information system as a service"", holistic view of infrastructure as a
service, platform as a service, software as a service, and data as a service.
"
919,"Calculation of geometric characteristics of land cover and urban canyon
  for multi-scale parameterization of megalopolis meteorological models","  The results of studies on the development of computational techniques for
geometric and thematic characteristics of the underlying surface and urban
canyon are presented. These characteristics are intended for parameterization
of the local model of energy-mass exchange between the surface layer of the
atmosphere and the surface of the active layer of the underlying urban areas
(cities). Multiscale database of parameters of the underlying surface with a
resolution of 200, 500 and 1000 meters is obtained. The use of
micro-meteorology models that take into account the specificity of the urban
environment, coupled with mesoscale prognostic models will significantly
improve the sound quality of the meteorological fields and local operational
weather forecasting in the metropolitan areas where considering the
hydrometeorological situation is particularly important.
"
920,"A Compact Dual Band Dielectric Resonator Antenna For Wireless
  Applications","  This paper presents the design of a dual band rectangular Dielectric
Resonator Antenna (DRA) coupled to narrow slot aperture that is fed by
microstrip line. The fundamental TE111 mode and higher-order TE113 mode are
excited with their resonant frequencies respectively. These frequencies can be
controlled by changing the DRA dimensions. A dielectric resonator with high
permittivity is used to miniaturize the global structure. The proposed antenna
is designed to have dual band operation suitable for both DCS (1710 - 1880 MHz)
and WLAN (2400 - 2484 MHz) applications. The return loss, radiation pattern and
gain of the proposed antenna are evaluated. Reasonable agreement between
simulation and experimental results is obtained.
"
921,Stable equilibrium study cascaded one bit sigma-delta modulator,"  In the paper defines a boundary of stability zone for sigma-delta modulator.
The boundary depends from inner sigma-delta modulator coefficients. For
designing purposes such result could be used to find or compare some
appropriate schemes with each other. It is proved some statements and showed
that boundary could be found theoretically for any order of sigma-delta
modulator, but practically till 5-th order.
"
922,An Optimized Design of Reversible Sequential Digital Circuits,"  In the today's era, reversible logics are the promising technology for the
designing of low power digital logic system having major application in the
field of nanotechnology, quantum computation, DNA and other low power digital
circuits. Reversible logics provide zero power dissipation (Ideally) in the
digital operations. There are numbers of circuit designed by the reversible
logics and sequential circuits have their own importance in the digital
systems. In this paper authors provides a optimized approach and optimized
design for the sequential circuit (counter as an example) by using the MUX gate
(a reversible gate) which provides the better results against the previous
designs discussed in the literature. The proposed design has lower quantum
cost, garbage output, constant input and total number of logical calculations
performing by the design.
"
923,"Valuating Surface Surveillance Technology for Collaborative
  Multiple-Spot Control of Airport Departure Operations","  Airport departure operations are a source of airline delays and passenger
frustration. Excessive surface traffic is a cause of increased controller and
pilot workload. It is also a source of increased emissions and delays, and does
not yield improved runway throughput. Leveraging the extensive past research on
airport departure management, this paper explores the environmental and safety
benefits that improved surveillance technologies can bring in the context of
gate- or spot-release strategies. The paper shows that improved surveillance
technologies can yield 4% to 6% reduction of aircraft on taxiway, and therefore
emissions, in addition to the savings currently observed by implementing
threshold starategies under evaluation at Boston Logan Airport and other busy
airports during congested periods. These calculated benefits contrast sharply
with our previous work, which relied on simplified airport ramp areas with a
single departure spot, and where fewer environmental and economic benefits of
advanced surface surveillance systems could be established. Our work is
illustrated by its application to New-York LaGuardia and Seattle Tacoma
airports.
"
924,Impact of Gate Assignment on Gate-Holding Departure Control Strategies,"  Gate holding reduces congestion by reducing the number of aircraft present on
the airport surface at any time, while not starving the runway. Because some
departing flights are held at gates, there is a possibility that arriving
flights cannot access the gates and have to wait until the gates are cleared.
This is called a gate conflict. Robust gate assignment is an assignment that
minimizes gate conflicts by assigning gates to aircraft to maximize the time
gap between two consecutive flights at the same gate; it makes gate assignment
robust, but passengers may walk longer to transfer flights. In order to
simulate the airport departure process, a queuing model is introduced. The
model is calibrated and validated with actual data from New York La Guardia
Airport (LGA) and a U.S. hub airport. Then, the model simulates the airport
departure process with the current gate assignment and a robust gate assignment
to assess the impact of gate assignment on gate-holding departure control. The
results show that the robust gate assignment reduces the number of gate
conflicts caused by gate holding compared to the current gate assignment.
Therefore, robust gate assignment can be combined with gate-holding departure
control to improve operations at congested airports with limited gate
resources.
"
925,"Roughening Methods to Prevent Sample Impoverishment in the Particle PHD
  Filter","  Mahler's PHD (Probability Hypothesis Density) filter and its particle
implementation (as called the particle PHD filter) have gained popularity to
solve general MTT (Multi-target Tracking) problems. However, the resampling
procedure used in the particle PHD filter can cause sample impoverishment. To
rejuvenate the diversity of particles, two easy-to-implement roughening
approaches are presented to enhance the particle PHD filter. One termed as
""separate-roughening"" is inspired by Gordon's roughening procedure that is
applied on the resampled particles. Another termed as ""direct-roughening"" is
implemented by increasing the simulation noise of the state propagation of
particles. Four proposals are presented to customize the roughening approach.
Simulations are presented showing that the roughening approach can benefit the
particle PHD filter, especially when the sample size is small.
"
926,"A Comparative study of Analog and digital Controller On DC/DC Buck-Boost
  Converter Four Switch for Mobile Device Applications","  This paper presents comparative performance between Analog and digital
controller on DC/DC buck-boost converter four switch. The design of power
electronic converter circuit with the use of closed loop scheme needs modeling
and then simulating the converter using the modeled equations. This can easily
be done with the help of state equations and MATLAB/SIMULINK as a tool for
simulation of those state equations. DC/DC Buckboost converter in this study is
operated in buck (step-down) and boost (step-up) modes.
"
927,"Dipole-Loaded Monopole Optimized Using VSO, v.3","  A dipole-loaded monopole antenna is optimized for uniform hemispherical
coverage using VSO, a new global search design and optimization algorithm. The
antenna's performance is compared to genetic algorithm and hill-climber
optimized loaded monopoles, and VSO is tested against two suites of benchmark
functions and several other algorithms.
"
928,How to Build an RSS Feed using ASP,"  RSS is a XML based format. The Current popular version of RSS is RSS version
2.0. The purpose of adding an RSS feed to your site is to show if anything new
is added to the site. For example, if a new article or blog or news item is
added to your site that should automatically appear in the RSS feed so that the
visitors/ RSS readers will automatically get updated about this new addition.
The RSS feed is also called RSS channel.
  There are two main elements of the RSS XML file, one is the header or channel
element that describes the details about the site/feeder and other is the body
or item element that describes the consists of individual articles/entries
updated in the site. As the format of the RSS feed file is pretty simple, it
can be coded in any language, ASP, PHP or anything of that sort. We will build
an RSS feeder using classical ASP (Active Server Pages) code in this article.
"
929,Wireless sensor network technology for moisture monitoring of wood,"  Leaks represent a very important hazard for the buildings and they can affect
all sorts of building materials and specially wood due to its hygroscopic
properties. Excessive moisture content can affect in a negative way building
processes such as the installation of wooden floors or the use of wood as a
structural material. Moisture meters can provide prompt and non-destructive
determination of wood moisture, and as such are among the most useful tools
available to wood products manufacturers and scientists. However, a continuous
monitoring system is needed in order to avoid excessive moisture content which
can damage wooden floors as well as structural wood. Data and procedures are
presented in order to develop a suitable monitoring tool based on wireless
sensor networks to provide an electronic tool of active security both for the
installation of wooden floors and for the proper maintenance of existent
buildings which have a timber structure.
"
930,Machining of complex-shaped parts with guidance curves,"  Nowadays, high-speed machining is usually used for production of hardened
material parts with complex shapes such as dies and molds. In such parts, tool
paths generated for bottom machining feature with the conventional parallel
plane strategy induced many feed rate reductions, especially when boundaries of
the feature have a lot of curvatures and are not parallel. Several machining
experiments on hardened material lead to the conclusion that a tool path
implying stable cutting conditions might guarantee a better part surface
integrity. To ensure this stability, the shape machined must be decomposed when
conventional strategies are not suitable. In this paper, an experimental
approach based on high-speed performance simulation is conducted on a master
bottom machining feature in order to highlight the influence of the curvatures
towards a suitable decomposition of machining area. The decomposition is
achieved through the construction of intermediate curves between the closed
boundaries of the feature. These intermediate curves are used as guidance curve
for the tool paths generation with an alternative machining strategy called
""guidance curve strategy"". For the construction of intermediate curves, key
parameters reflecting the influence of their proximity with each closed
boundary and the influence of the curvatures of this latter are introduced.
Based on the results, a method for defining guidance curves in four steps is
proposed.
"
931,"Design of One-Dimensional Linear Phase Digital IIR Filters Using
  Orthogonal Polynomials","  In the present paper, we discuss a method to design a linear phase
1-dimensional Infinite Impulse Response (IIR) filter using orthogonal
polynomials. The filter is designed using a set of object functions. These
object functions are realized using a set of orthogonal polynomials. The method
includes placement of zeros and poles in such a way that the amplitude
characteristics are not changed while we change the phase characteristics of
the resulting IIR filter.
"
932,CMOS Low Power Cell Library For Digital Design,"  Historically, VLSI designers have focused on increasing the speed and
reducing the area of digital systems. However, the evolution of portable
systems and advanced Deep Sub-Micron fabrication technologies have brought
power dissipation as another critical design factor. Low power design reduces
cooling cost and increases reliability especially for high density systems.
Moreover, it reduces the weight and size of portable devices. The power
dissipation in CMOS circuits consists of static and dynamic components. Since
dynamic power is proportional to V2 dd and static power is proportional to Vdd,
lowering the supply voltage and device dimensions, the transistor threshold
voltage also has to be scaled down to achieve the required performance. In case
of static power, the power is consumed during the steady state condition i.e
when there are no input/output transitions. Static power has two sources: DC
power and Leakage power. Consecutively to facilitate voltage scaling without
disturbing the performance, threshold voltage has to be minimized. Furthermore
it leads to better noise margins and helps to avoid the hot carrier effects in
short channel devices. In this paper we have been proposed the new CMOS library
for the complex digital design using scaling the supply voltage and device
dimensions and also suggest the methods to control the leakage current to
obtain the minimum power dissipation at optimum value of supply voltage and
transistor threshold. In this paper CMOS Cell library has been implemented
using TSMC (0.18um) and TSMC (90nm) technology using HEP2 tool of IC designing
from Mentor Graphics for various analysis and simulations.
"
933,Design and Implementation of Car Parking System on FPGA,"  As, the number of vehicles are increased day by day in rapid manner. It
causes the problem of traffic congestion, pollution (noise and air). To
overcome this problem A FPGA based parking system has been proposed. In this
paper, parking system is implemented using Finite State Machine modelling. The
system has two main modules i.e. identification module and slot checking
module. Identification module identifies the visitor. Slot checking module
checks the slot status. These modules are modeled in HDL and implemented on
FPGA. A prototype of parking system is designed with various interfaces like
sensor interfacing, stepper motor and LCD.
"
934,Low Power Dual Edge-Triggered Static D Flip-Flop,"  This paper enumerates new architecture of low power dual-edge triggered
Flip-Flop (DETFF) designed at 180nm CMOS technology. In DETFF same data
throughput can be achieved with half of the clock frequency as compared to
single edge triggered Flip-Flop (SETFF). In this paper conventional and
proposed DETFF are presented and compared at same simulation conditions. The
post layout experimental results comparison shows that the average power
dissipation is improved by 48.17%, 41.29% and 36.84% when compared with SCDFF,
DEPFF and SEDNIFF respectively and improvement in PDP is 42.44%, 33.88% and
24.69% as compared to SCDFF, DEPFF and SEDNIFF respectively. Therefore the
proposed DETFF design is suitable for low power and small area applications.
"
935,"Solving the Parity Problem with Rule 60 in Array Size of the Power of
  Two","  In the parity problem, a given cellular automaton has to classify any initial
configuration into two classes according to its parity. Elementary cellular
automaton rule 60 can solve the parity problem in periodic boundary conditions
with array size of the power of two. The spectral analysis of the
configurations of rule 60 at each time step in the evolution reveals that
spatial periodicity emerges as the evolution proceeds and the patterns with
longer period split into the ones with shorter period. This phenomenon is
analogous to the cascade process in which large scale eddies split into smaller
ones in turbulence. By measuring the Lempel-Ziv complexity of configuration, we
found the stepping decrease of the complexity during the evolution. This result
might imply that a decision problem solving process is accompanied with the
decline of complexity of configuration.
"
936,Computational Universality and 1/f Noise in Elementary Cellular Automata,"  It is speculated that there is a relationship between 1/f noise and
computational universality in cellular automata. We use genetic algorithms to
search for one-dimensional and two-state, five-neighbor cellular automata which
have 1/f-type spectrum. A power spectrum is calculated from the evolution
starting from a random initial configuration. The fitness is estimated from the
power spectrum in consideration of the similarity to 1/f-type spectrum. The
result shows that the rule with the highest average fitness has a propagating
structure like other computationally universal cellular automata, although
computational universality of the rule has not been proved yet.
"
937,Forecasting Intermittent Demand by Hyperbolic-Exponential Smoothing,"  Croston's method is generally viewed as superior to exponential smoothing
when demand is intermittent, but it has the drawbacks of bias and an inability
to deal with obsolescence, in which an item's demand ceases altogether. Several
variants have been reported, some of which are unbiased on certain types of
demand, but only one recent variant addresses the problem of obsolescence. We
describe a new hybrid of Croston's method and Bayesian inference called
Hyperbolic-Exponential Smoothing, which is unbiased on non-intermittent and
stochastic intermittent demand, decays hyperbolically when obsolescence occurs
and performs well in experiments.
"
938,"Abstract Geometrical Computation 8: Small Machines, Accumulations and
  Rationality","  In the context of abstract geometrical computation, computing with colored
line segments, we study the possibility of having an accumulation with small
signal machines, ie, signal machines having only a very limited number of
distinct speeds. The cases of 2 and 4 speeds are trivial: we provide a proof
that no machine can produce an accumulation in the case of 2 speeds and exhibit
an accumulation with 4 speeds. The main result is the twofold case of 3 speeds.
On the one hand, we prove that accumulations cannot happen when all ratios
between speeds and all ratios between initial distances are rational. On the
other hand, we provide examples of an accumulation in the case of an irrational
ratio between 2 speeds and in the case of an irrational ratio between two
distances in the initial configuration. This dichotomy is explained by the
presence of a phenomenon computing Euclid's algorithm (gcd): it stops if and
only if its input is commensurate (ie, of rational ratio).
"
939,"An Integrated Geographic Information System and Marketing Information
  System Model","  Maintaining competitive advantage is significant in this present day of
globalization, knowledge management and enormous economic activities. An
organization's future developments are influenced by its managements'
decisions. Businesses today are facing a lot of challenges in terms of
competition and they have to be in the lead by strengthening their research and
development strategies with the aid of cutting edge technologies. Hence
marketing intelligence is now a key to the success of any business in today's
rapidly changing business environment. With all the technologies available in
marketing research, businesses still struggle with how to gather information
and make decisions in a short time and real-time about their customers' needs
and purchasing patterns in various geographical areas.
  This paper is set out to contribute to the body of knowledge in the area of
the application of Geographic Information Systems technology solutions to
businesses by developing a model for integrating Geographic Information Systems
into existing Marketing Information Systems for effective marketing research.
This model will interconnect organizations at the highest levels, providing
reassurance to enable broad scope of checks and balances as well as benefiting
many business activities including operational, tactical and strategic decision
making due to its analytical and solution driven functions.
"
940,Complexity Analysis in Cyclic Tag System Emulated by Rule 110,"  It is known that elementary cellular automaton rule 110 is capable of
supporting universal computation by emulating cyclic tag system. Since the
whole information necessary to perform computation is stored in the
configuration, it is reasonable to investigate the complexity of configuration
for the analysis of computing process. In this research we employed Lempel-Ziv
complexity as a measure of complexity and calculated it during the evolution of
emulating cyclic tag system by rule 110. As a result, we observed the stepwise
decline of complexity during the evolution. That is caused by the
transformation from table data to moving data and the elimination of table data
by a rejector.
"
941,Improved Median Polish Kriging for Simulation Metamodeling,"  In simulation, Median Polish Kriging is a technique used to predict
unobserved data points in two-dimensional space. The linear behavior of the
traditional Median Polish Kriging in the estimation of the mean function in a
high grid makes the interpolation of O(1) which has a low order in the
prediction and that leads to a high prediction error. Therefore, an improvement
in the estimation of the mean function has been introduced using Biharmonic
spline interpolation and the new technique has been called Improved Median
Polish Kriging (IMPK). The IMPK has been applied to the standard coal-ash data
in two-dimension. The novel method gave much better results according to the
cross validation results that were obtained when compared with the traditional
Median Polish Kriging.
"
942,"Sistem Informasi Penjualan Dan Perbaikan Komputer (Studi Kasus: CV
  Computer Plus Palembang)","  The purpose of this research is to develop an Information System of Selling
and Services using Microsoft Visual Basic and Microsoft Access for it database.
The benefits of this research is to help CV Computer Plus in selling and
services data processing everyday. To develop this IS is used 5 (five) steps:
1) Planning, 2) Analysis, 3) Design, 4) Implementation, and 5) Evaluation. The
Information System can record the selling and services data, it also prepared
usefull reports. By using this IS, CV Computer Plus can operate their selling
and services efficiency and effectively. In the future it can be upgraded for
network application.
"
943,On Two Conversion Methods of Decimal-to-Binary,"  Decimal-to-binary conversion is important to modern binary computers. The
classical method to solve this problem is based on division operation. In this
paper, we investigate a decimal-to-binary conversion method based on addition
operation. The method is very easily implemented by software. The cost analysis
shows that the latter is more preferable than the classical method. Thus the
current Input/Output translation hardware to convert between the internal digit
pairs and the external standard BCD codes can be reasonably removed.
"
944,Introduction to Management Information system,"  A Management Information System (MIS) is a systematic organization and
presentation of information that is generally required by the management of an
organization for taking better decisions for the organization. The MIS data may
be derived from various units of the organization or from other sources.
However it is very difficult to say the exact structure of MIS as the structure
and goals of different types of organizations are different. Hence both the
data and structure of MIS is dependent on the type of organization and often
customized to the specific requirement of the management.
"
945,"A New Mattress Development Based on Pressure Sensors for Body-contouring
  Uniform Support","  For getting good sleep quality, an improved approach of new mattress
development based on the pressure sensors for body-contouring uniform support
is proposed in this paper. This method solved the problems of innerspring
mattresses that cannot allow body-contouring uniform support, and foam
mattresses that cannot provide everybody equal comfort from the same mattress.
By the buried pressure sensor array and actuator array in foam layer of a
mattress, both are connected to a controller to generate the pressure
distribution mapping of a human body on the mattress, then from the data of
this mapping, some of the actuators are driven up or down by the controller to
generate a body-contouring uniform support. By the aid of mathematical
morphology algorithms, user can also choose a different support mode by another
wireless controller with touch-screen to accommodate personal favorite firmness
of the mattress and to take his tensed mood and pressure off with good sleep
until daylight. Moreover, some other homecare functions, such as temperature
measurement, sleep on posture correction and fall down prevention, can approach
by additional hardware and software as user requirement in the future.
"
946,3D Printing for Math Professors and Their Students,"  In this primer, we will describe a number of projects that can be completed
with a 3D printer, particularly by mathematics professors and their students.
For many of the projects, we will utilize Mathematica to design objects that
mathematicians may be interested in printing. Included in the projects that are
described is a method to acquire data from an XBox Kinect.
"
947,Services in Android can Share Your Personal Information in Background,"  Mobile phones have traveled a very long journey in a very short span of time
since its inception in 1973.This wonderful toy of 20th century has started
playing significant role in daily life.More than 5 billion mobile users are
there around the world and almost 90 percent of the entire earth is under the
mobile coverage now.These days smart phones are equipped with numerous
features,faster processors and high storage capacity.Android is a latest trend
in this series whose popularity is growing by leaps and bounds.Android has a
number of components which helps Application developers to embed distinguish
features in applications.This paper explains how the Service component of
Android can share your personal information to others without users
interaction.
"
948,How to implement Marketing 2.0 Successfully,"  The purpose of this research is to develop a model that would close the gap
between marketing plans and strategies from one side and the advanced online
collaboration applications platforms known as WEB 2.0 in order to implement
marketing 2.0 smoothly without disrupting the working environment. We started
by examining published articles related to marketing, Web 2.0, Customer
Relationship Management Systems, CRM, and social media in a step to conduct an
extensive review of the available literature. Then, we presented critique of
the articles we have examined. After that, we have been able to develop the
model we are proposing in this research. As this paper shows, the proposed
model will help in transforming marketing plans and strategies from its
traditional approach into, what we would like to call, marketing 2.0 approach
smoothly. There are some unavoidable limitations due to the given time and
scope constrains. The factors included in the proposed model does not cover
every related aspect, however, they cover the most important ones.
"
949,"Numerical Analysis of Gate Conflict Duration and Passenger Transit Time
  in Airport","  Robustness is as important as efficiency in air transportation. All
components in the air traffic system are connected to form an interactive
network. So, a disturbance that occurs in one component, for example, a severe
delay at an airport, can influence the entire network. Delays are easily
propagated between flights through gates, but the propagation can be reduced if
gate assignments are robust against stochastic delays. In this paper, we
analyze gate delays and suggest an approach that involves assigning gates while
making them robust against stochastic delays. We extract an example flight
schedule from data source and generate schedules with increased traffic to
analyze how the compact flight schedules impact the robustness of gate
assignment. Simulation results show that our approach improves the robustness
of gate assignment. Particularly, the robust gate assignment reduces average
duration of gate conflicts by 96.3% and the number of gate conflicts by 96.7%
compared to the baseline assignment. However, the robust gate assignment
results in longer transit time for passengers, and a trade-off between the
robustness of gate assignment and passenger transit time is presented.
"
950,"Implementation and optimization of Wavelet modulation in Additive
  Gaussian channels","  In this paper, we investigate the implementation of wavelet modulation (WM)
in a digital communication system and propose novel methods to improve its
performance. We will put particular focus on the structure of an optimal
detector in AWGN channels and address two main methods for inserting the
samples of the message signal in different frequency layers. Finally, computer
based algorithms are described in order to implement and optimize receivers and
transmitters.
"
951,"Multiparameter Monitoring and Fault Indication Using Inductive Power
  Transfer System","  The paper aims at demonstrating communication capabilities of IPT. For this
data communication is performed between two modules using the concept of IPT.
IPT was deemed to be the best solution to the system houses a multi parameter
acquisition module such as temperature, speed, voltage, current and data
transfer from the motor. The receiver side is another microcontroller coupled
to an inductive coil that gets the data and displays in the LCD. A brief
background to IPT Inductive Power Transfer technology and its applications is
given and the design criteria for the paper are defined in detail. To be
accurate, IPT data communication helps to reduce unnecessary wire connections
and data is transmitted without any touch. Further the paper can be enhanced by
looking for fault analysis inside the motor. This can be done by analyzing
various parameters of the motor. A novel two-way IPT communication system was
designed, which worked on the concept of pulsing the system on and off to send
data serially. The paper involves transmission of data through inductive flux
without any contact between the two modules. Further as no frequency tunings or
any calibration is required between different modules a single system can be
used with multiple clients. This reduces a lot of hazards such as interference
with other modules and RF transmitters in the vicinity.
"
952,Survey of Insurance Fraud Detection Using Data Mining Techniques,"  With an increase in financial accounting fraud in the current economic
scenario experienced, financial accounting fraud detection has become an
emerging topics of great importance for academics, research and industries.
Financial fraud is a deliberate act that is contrary to law, rule or policy
with intent to obtain unauthorized financial benefit and intentional
misstatements or omission of amounts by deceiving users of financial
statements, especially investors and creditors. Data mining techniques are
providing great aid in financial accounting fraud detection, since dealing with
the large data volumes and complexities of financial data are big challenges
for forensic accounting. Financial fraud can be classified into four: bank
fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but
wrongful or criminal trick planned to result in financial or personal gains.
This paper describes the more details on insurance sector related frauds and
related solutions. In finance, insurance sector is doing important role and
also it is unavoidable sector of every human being.
"
953,"Design and Implementation of Wireless Energy Meter System for Monitoring
  the Single Phase Supply","  Wireless energy meter is a system developed to serve as a basic single-phase
energy meter with advanced functionalities such as Peak hour setting, Peak load
setting Wireless reading transmission; further the system eliminates the role
of a Meter Reader.
"
954,Condition-Based Maintenance using Sensor Arrays and Telematics,"  Emergence of uniquely addressable embeddable devices has raised the bar on
Telematics capabilities. Though the technology itself is not new, its
application has been quite limited until now. Sensor based telematics
technologies generate volumes of data that are orders of magnitude larger than
what operators have dealt with previously. Real-time big data computation
capabilities have opened the flood gates for creating new predictive analytics
capabilities into an otherwise simple data log systems, enabling real-time
control and monitoring to take preventive action in case of any anomalies.
Condition-based-maintenance, usage-based-insurance, smart metering and
demand-based load generation etc. are some of the predictive analytics use
cases for Telematics. This paper presents the approach of condition-based
maintenance using real-time sensor monitoring, Telematics and predictive data
analytics.
"
955,"DyPS: Dynamic Processor Switching for Energy-Aware Video Decoding on
  Multi-core SoCs","  In addition to General Purpose Processors (GPP), Multicore SoCs equipping
modern mobile devices contain specialized Digital Signal Processor designed
with the aim to provide better performance and low energy consumption
properties. However, the experimental measurements we have achieved revealed
that system overhead, in case of DSP video decoding, causes drastic
performances drop and energy efficiency as compared to the GPP decoding. This
paper describes DyPS, a new approach for energy-aware processor switching (GPP
or DSP) according to the video quality . We show the pertinence of our solution
in the context of adaptive video decoding and describe an implementation on an
embedded Linux operating system with the help of the GStreamer framework. A
simple case study showed that DyPS achieves 30% energy saving while sustaining
the decoding performance
"
956,"Analytical and experimental stability investigation of a
  hardware-in-the-loop satellite docking simulator","  The European Proximity Operation Simulator (EPOS) of the DLR-German Aerospace
Center is a robotics-based simulator that aims at validating and verifying a
satellite docking phase. The generic concept features a robotics tracking
system working in closed loop with a force/torque feedback signal. Inherent
delays in the tracking system combined with typical high stiffness at contact
challenge the stability of the closed-loop system. The proposed concept of
operations is hybrid: the feedback signal is a superposition of a measured
value and of a virtual value that can be tuned in order to guarantee a desired
behavior. This paper is concerned with an analytical study of the system's
closed-loop stability, and with an experimental validation of the hybrid
concept of operations in one dimension (1D). The robotics simulator is modeled
as a second-order loop-delay system and closed-form expressions for the
critical delay and associated frequency are derived as a function of the
satellites' mass and the contact dynamics stiffness and damping parameters. A
numerical illustration sheds light on the impact of the parameters on the
stability regions. A first-order Pade approximation provides additional means
of stability investigation. Experiments were performed and tests results are
described for varying values of the mass and the damping coefficients. The
empirical determination of instability is based on the coefficient of
restitution and on the observed energy. There is a very good agreement between
the critical damping values predicted by the analysis and observed during the
tests...
"
957,Inadmissible Class of Boolean Functions under Stuck-at Faults,"  Many underlying structural and functional factors that determine the fault
behavior of a combinational network, are not yet fully understood. In this
paper, we show that there exists a large class of Boolean functions, called
root functions, which can never appear as faulty response in irredundant
two-level circuits even when any arbitrary multiple stuck-at faults are
injected. Conversely, we show that any other Boolean function can appear as a
faulty response from an irredundant realization of some root function under
certain stuck-at faults. We characterize this new class of functions and show
that for n variables, their number is exactly equal to the number of
independent dominating sets (Harary and Livingston, Appl. Math. Lett., 1993) in
a Boolean n-cube. We report some bounds and enumerate the total number of root
functions up to 6 variables. Finally, we point out several open problems and
possible applications of root functions in logic design and testing.
"
958,"A Simple Solution To The Uncertain Delay Problem in USRP Based SDR-Radar
  Systems","  We propose a simple solution to the uncertain delay problem in USRP
(Universal Software Radio Peripheral)-based SDR (Software-Defined Radio)-radar
systems. Instead of time-synchronization as employed in (pseudo-) passive radar
configurations, which require at least two synchronized receivers, we use
direct reception signal in a single receiver system as a reference to the exact
location of the target echoes. After finding the reference position, reordering
of the echoes is conducted by circular shift so that the reference moved to the
origin. We demonstrate the effectiveness of the proposed method by simulating
the problem on Matlab and implementing a 128 length random code radar on a
USRP. The random code is constructed from zero padded Barker sequence product.
Experiments on measuring multiple echoes of the targets at precise range bins
confirm the applicability of the proposed method.
"
959,Context-dependent Trust Decisions with Subjective Logic,"  A decision procedure implemented over a computational trust mechanism aims to
allow for decisions to be made regarding whether some entity or information
should be trusted. As recognised in the literature, trust is contextual, and we
describe how such a context often translates into a confidence level which
should be used to modify an underlying trust value. J{\o}sang's Subjective
Logic has long been used in the trust domain, and we show that its operators
are insufficient to address this problem. We therefore provide a
decision-making approach about trust which also considers the notion of
confidence (based on context) through the introduction of a new operator. In
particular, we introduce general requirements that must be respected when
combining trustworthiness and confidence degree, and demonstrate the soundness
of our new operator with respect to these properties.
"
960,"A Calibration Algorithm for Microelectromechanical Systems
  Accelerometers in Inertial Navigation Sensors","  In the present work we develop an algorithm for calibrating MEMS sensors,
which accounts for the nonorthogonality of the accelerometers' axis, as well as
for the constant bias and scaling errors. We derive an explicit formula for
computing the calibrated acceleration, given data from the sensors. We also
study the error, that is caused by the nonorthogonality of the axis.
"
961,"Formal Contexts, Formal Concept Analysis, and Galois Connections","  Formal concept analysis (FCA) is built on a special type of Galois
connections called polarities. We present new results in formal concept
analysis and in Galois connections by presenting new Galois connection results
and then applying these to formal concept analysis. We also approach FCA from
the perspective of collections of formal contexts. Usually, when doing FCA, a
formal context is fixed. We are interested in comparing formal contexts and
asking what criteria should be used when determining when one formal context is
better than another formal context. Interestingly, we address this issue by
studying sets of polarities.
"
962,Human Resource Management System,"  The paper titled HUMAN RESOURCE MANAGEMENT SYSTEM is basically concerned with
managing the Administrator of HUMAN RESOURCE Department in a company. A Human
Resource Management System, refers to the systems and processes at the
intersection between human resource management and information technology. It
merges HRM as a discipline and in particular its basic HR activities and
processes with the information technology field, whereas the programming of
data processing systems evolved into standardized routines and packages of
enterprise resource planning software. The main objective of this paper is to
reduce the effort of Administrator to keep the daily events such as attendance,
projects, works, appointments, etc. This paper deals with the process of
identifying the employees, recording their attendance hourly and calculating
their effective payable hours or days. This paper should maintain the records
of each and every employee and their time spend in to company, which can be
used for performance appraisal. Based on that transfer, removal, promotion can
be done.
"
963,"Demodulation of Sparse PPM Signals with Low Samples Using Trained RIP
  Matrix","  Compressed sensing (CS) theory considers the restricted isometry property
(RIP) as a sufficient condition for measurement matrix which guarantees the
recovery of any sparse signal from its compressed measurements. The RIP
condition also preserves enough information for classification of sparse
symbols, even with fewer measurements. In this work, we utilize RIP bound as
the cost function for training a simple neural network in order to exploit the
near optimal measurements or equivalently near optimal features for
classification of a known set of sparse symbols. As an example, we consider
demodulation of pulse position modulation (PPM) signals. The results indicate
that the proposed method has much better performance than the random
measurements and requires less samples than the optimum matched filter
demodulator, at the expense of some performance loss. Further, the proposed
approach does not need equalizer for multipath channels in contrast to the
conventional receiver.
"
964,"Evolution of choices over time: The U.S. Presidential election 2012 and
  the NY City Mayoral Election, 2013","  We conducted surveys before and after the 2012 U.S. Presidential election and
prior to the NY City Mayoral election in 2013. The surveys were done using
Amazon Turk. This poster describes the results of our analysis of the surveys
and predicts the winner of the NY City Mayoral Election.
"
965,Optical Disk with Blu-Ray Technology,"  Blu-ray is the name of a next-generation optical disc format jointly
developed by the Blu-ray Disc Association a group of the world's leading
consumer electronics, personal computer and media manufacturers. The format was
developed to enable recording, rewriting and playback of high-definition video,
as well as storing large amounts of data. This extra capacity combined with the
use of advanced video and audio codec will offer consumers an unprecedented HD
experience. While current optical disc technologies such as DVD and DVDRAM rely
on a red laser to read and write data, the new format uses a blue-violet laser
instead, hence the name Blu-ray. Blu ray also promises some added security,
making ways for copyright protections. Blu-ray discs can have a unique ID
written on them to have copyright protection inside the recorded streams. Blu
.ray disc takes the DVD technology one step further, just by using a laser with
a nice color.
"
966,Emergency and Normal Navigation in Confined Spaces,"  Emergency navigation algorithms direct evacuees to exits when disastrous
events such as fire take place. Due to the spread of hazards, latency in
information updating and unstable flows of civilians, emergency evacuation is
absolutely a complex transshipment problem involving numerous sources and
multiple destinations. Previous algorithms which commonly need either a full
graph search or a convergence process suffer from high computational and
communication overheads. This research report surveys the current emergency
navigation algorithms and adapts the concept of Cognitive Packet Network (CPN)
to the context of emergency evacuation. By using random neural networks, the
CPN based algorithm can explore optimal routes rapidly and adaptively in a
highly dynamic emergency environment with low expense. Simultaneously, in
emergency situations there are typically different categories of evacuees such
as people of different age groups. However, current algorithms only consider
""normal"" evacuees and do not meet the specific requirements of diverse
evacuees. Our algorithms make use of the flexibility of CPN which can operate
with different user-defined goals to customize appropriate paths for each
category. The CPN algorithm is simulated in a graph based discrete-event
simulator and Dijkstra's shortest path algorithm is taken as reference. The
results show that the CPN algorithm reaches the performance of ideal
path-finding algorithm and quality of service is improved by using specific
goal functions for diverse categories of evacuees. Finally, we present a future
plan for further research.
"
967,"Implementation of the Cluster Based Tunable Sleep Transistor Cell Power
  Gating Technique for a 4x4 Multiplier Circuit","  A modular, programmable, and high performance Power Gating strategy, called
cluster based tunable sleep transistor cell Power Gating, has been introduced
in the present paper with a few modifications. Furthermore, a detailed
comparison of its performance with some of the other conventional Power Gating
schemes; such as Cluster Based Sleep Transistor Design (CBSTD), Distributed
Sleep Transistor Network (DSTN) etc.; has also been presented here. Considering
the constraints of power consumption, performance, and the area overhead, while
doing the actual implementation of any Power Gating scheme, it becomes
important to deal with the various design issues like the proper sizing of the
sleep transistors (STs), controlling the voltage drop (IR drop) across the STs,
and obviously maintaining a desired performance with lower amount of delay
degradation. With this notion, we tried to find out an efficient Power Gating
strategy which can reduce the overall power consumption of any CMOS circuit by
virtue of reducing the standby mode leakage current. Taking the different
performance parameters into account, for an example circuit, which is actually
the conventional 4x4 multiplier design, we found that the modified tunable
sleep transistor cell Power Gating gives very much promising results. The
reported architecture of the 4x4 multiplier with the tunable sleep transistor
cell Power Gating, is designed using 45 nm technology and it consumes
1.3638x10-5 Watt of Average Power while being operated with the nominal case of
the bit configuration word, that is, 1000. ...........
"
968,"Value-chain oriented identification of indicators to establish a
  comprehensive process improvement framework","  The process development and optimization potential needs to be driven by the
individial coporate value chain. The identification of this specific value
chain and the related indicators is essential to limit the scope of any
analysis and optimization to the core business The process framework consisting
of clearly defined value chain, the related processes and the corresponding
indicators is a pre-requisite for a meaningful and efficient process analysis
and continuous process optimization.
"
969,"Optimal Energy Consumption Model for Smart Grid Households with Energy
  Storage","  In this paper, we propose to model the energy consumption of smart grid
households with energy storage systems as an intertemporal trading economy.
Intertemporal trade refers to transaction of goods across time when an agent,
at any time, is faced with the option of consuming or saving with the aim of
using the savings in the future or spending the savings from the past. Smart
homes define optimal consumption as either balancing/leveling consumption such
that the utility company is presented with a uniform demand or as minimizing
consumption costs by storing energy during off-peak time periods when prices
are lower and use the stored energy during peak time periods when prices are
higher. Due to the varying nature of energy requirements of household and
market energy prices over different time periods in a day, households face a
trade-off between consuming to meet their current energy requirements and/or
storing energy for future consumption and/or spending energy stored in the
past. These trade-offs or consumption preferences of the household are modeled
as utility functions using consumer theory. We introduce two different utility
functions, one for cost minimization and another for consumption
balancing/leveling, that are maximized subject to respective budget,
consumption, storage and savings constraints to solve for the optimum
consumption profile. The optimization problem of a household with energy
storage is formulated as a geometric program for consumption
balancing/leveling, while cost minimization is formulated as a linear
programming problem. Simulation results show that the proposed model achieves
extremely low peak to average ratio in the consumption balancing/leveling
scheme with about 8% reduction in consumption costs and the least possible
amount for electricity bill with about 12% reduction in consumption costs in
the cost minimization scheme.
"
970,Ear-Phone: A Context-Aware Noise Mapping using Smart Phones,"  A noise map facilitates the monitoring of environmental noise pollution in
urban areas. However, state-of-the-art techniques for rendering noise maps in
urban areas are expensive and rarely updated, as they rely on population and
traffic models rather than on real data. Smart phone based urban sensing can be
leveraged to create an open and inexpensive platform for rendering up-to- date
noise maps. In this paper, we present the design, implementation and
performance evaluation of an end-to-end, context-aware, noise mapping system
called Ear-Phone. Ear-Phone investigates the use of different interpolation and
regularization methods to address the fundamental problem of recovering the
noise map from incomplete and random samples obtained by crowdsourcing data
collection. Ear-Phone, implemented on Nokia N95, N97 and HP iPAQ, HTC One
mobile devices, also addresses the challenge of collecting accurate noise
pollution readings at a mobile device. A major challenge of using smart phones
as sensors is that even at the same location, the sensor reading may vary
depending on the phone orientation and user context (for example, whether the
user is carrying the phone in a bag or holding it in her palm). To address this
problem, Ear-Phone leverages context-aware sensing. We develop classifiers to
accurately determine the phone sensing context. Upon context discovery,
Ear-Phone automatically decides whether to sense or not. Ear-phone also
implements in-situ calibration which performs simple calibration that can be
carried out without any technical skills whatsoever required on the user's
part. Extensive simulations and outdoor experiments demonstrate that Ear-Phone
is a feasible platform to assess noise pollution, incurring reasonable system
resource consumption at mobile devices and providing high reconstruction
accuracy of the noise map.
"
971,"Gait Velocity Estimation using time interleaved between Consecutive
  Passive IR Sensor Activations","  Gait velocity has been consistently shown to be an important indicator and
predictor of health status, especially in older adults. It is often assessed
clinically, but the assessments occur infrequently and do not allow optimal
detection of key health changes when they occur. In this paper, we show that
the time gap between activations of a pair of Passive Infrared (PIR) motion
sensors installed in the consecutively visited room pair carry rich latent
information about a person's gait velocity. We name this time gap transition
time and show that despite a six second refractory period of the PIR sensors,
transition time can be used to obtain an accurate representation of gait
velocity.
  Using a Support Vector Regression (SVR) approach to model the relationship
between transition time and gait velocity, we show that gait velocity can be
estimated with an average error less than 2.5 cm/sec. This is demonstrated with
data collected over a 5 year period from 74 older adults monitored in their own
homes.
  This method is simple and cost effective and has advantages over competing
approaches such as: obtaining 20 to 100x more gait velocity measurements per
day and offering the fusion of location-specific information with time stamped
gait estimates. These advantages allow stable estimates of gait parameters
(maximum or average speed, variability) at shorter time scales than current
approaches. This also provides a pervasive in-home method for context-aware
gait velocity sensing that allows for monitoring of gait trajectories in space
and time.
"
972,Flickers Forecasting In CRT Using Stochastic Analysis,"  Videos are composed of sequence of interrelated frames. There is a minute
difference among frames. Flicker is an error which is found in every video. It
is like a checker box in a video, there are several reasons behind flickers
generation, one of the main reasons is refresh rate of the monitor and second
reason is number of frames per second in a video. The main objective of this
study is to propose and develop a framework that identifies flicker location
and minimizes the flickers rate. Analysis shows that flickers can be minimize
by adjusting the persistence of pixel and higher refresh rate of CRT monitor.
Further we have compared different isotopes of phosphorous pixels and generate
its graphs. This paper highlighted the cause of flicker and its avoidance
.Statistical research proves that proposed algorithm improves the video quality
and reduce flickers ratio up to 90%.
"
973,"Using CamiTK for rapid prototyping of interactive Computer Assisted
  Medical Intervention applications","  Computer Assisted Medical Intervention (CAMI hereafter) is a complex
multi-disciplinary field. CAMI research requires the collaboration of experts
in several fields as diverse as medicine, computer science, mathematics,
instrumentation, signal processing, mechanics, modeling, automatics, optics,
etc.
"
974,The Energetic Reasoning Checker Revisited,"  Energetic Reasoning (ER) is a powerful filtering algorithm for the Cumulative
constraint. Unfortunately, ER is generally too costly to be used in practice.
One reason of its bad behavior is that many intervals are considered as
relevant by the checker of ER, although most of them should be ignored. In this
paper, we provide a sharp characterization that allows to reduce the number of
intervals by a factor seven. Our experiments show that associating this checker
with a Time-Table filtering algorithm leads to promising results.
"
975,Multivalued Logic Circuit Design for Binary Logic Interface,"  Binary logic and devices have been in used since inception with advancement
and technology and millennium gate design era. The development in binary logic
has become tedious and cumbersome. Multivalued logic enables significant more
information to be packed within a single digit. The design and development of
logic circuit becomes very compact and easier. Attempts are being made to
fabricate multivalued logic based devices. Since present devices can be
implemented only in binary system,it is necessary to evolve a system that can
built the circuit in multivalued logic system and convert in binary logic
system. In multivalued logic system logic gates differ in different logic
system, a quaternary has become mature in terms of logic algebra and gates.
Hence logic design based on above system can be done using standard procedure.
In this dissertation a logic circuit design entry based on multivalued logic
system has been taken up that can provide the ease of circuit design in
multivalued system and output as binary valued circuit. The named ""MVL-DEV""
offers editing, storage and conversion into binary facility.
"
976,Stacked Patch Antenna With Cross Slot Electronic Band Gap Structure,"  A cross slotted electronic band gap (EBG) with stacked rectangular patches
shorted with a shorting pin is proposed in this paper. The study is being done
on how the various parameters are varied by changing the probe feed location.
The design is constructed by using stacking of patches, shorting pin and cross
slotted EBG to form an optimized antenna design with antenna efficiency of
approximately 99.06%. The radiation patterns are given at 2.586 GHz which can
be used for wireless communications.
"
977,E-Business Implications for Productivity and Competitiveness,"  Information and Communication Technology (ICT) affects to a great extent the
output and productivity growth. Evidence suggests that investment growth in ICT
has rapidly accelerated the TFP (total factor productivity) growth within the
European Union. Such progress is particularly essential for the sectors which
themselves produce new technology, but it is dispersing to other sectors, as
well. Nevertheless, decrease in ICT investment does not necessarily decline the
ICT contribution to output and productivity growth. These variations come out
from the problems related to the particular phenomenon proper assessment, but
predominantly from the companies' special requirements, as well as the
necessary adjustments of labour employed. Hence, this paper aims at estimating
the huge distinction in terms of ICT and TFB contributions to labour
productivity growth among some of the European member states, as well as the
factors which might stand behind the particular findings.
"
978,"On the Optimum Energy Efficiency for Flat-fading Channels with
  Rate-dependent Circuit Power","  This paper investigates the optimum energy efficiency (EE) and the
corresponding spectral efficiency (SE) for a communication link operating over
a flat-fading channel. The EE is evaluated by the total energy consumption for
transmitting per message bit. Three channel cases are considered, namely static
channel with channel state information available at transmitter (CSIT),
fast-varying (FV) channel with channel distribution information available at
transmitter (CDIT), and FV channel with CSIT. A general circuit power model is
considered. For all the three channel cases, the tradeoff between the EE and SE
is studied. It is shown that the EE improves strictly as the SE increases from
0 to the optimum SE, and then strictly degrades as the SE increases beyond the
optimum SE. The impact of {\kappa}, {\rho} and other system parameters on the
optimum EE and corresponding SE is investigated to obtain insight.Some of the
important and interesting results for all the channel cases include: (1) when
{\kappa} increases the SE corresponding to the optimum EE should keep unchanged
if {\phi}(R) = R, but reduced if {\phi}(R) is strictly convex of R; (2) when
the rate-independent circuit power {\rho} increases, the SE corresponding to
the optimum EE has to be increased. A polynomial-complexity algorithm is
developed with the bisection method to find the optimum SE. The insight is
corroborated and the optimum EE for the three cases are compared by simulation
results.
"
979,Automatic Airspace Sectorisation: A Survey,"  Airspace sectorisation provides a partition of a given airspace into sectors,
subject to geometric constraints and workload constraints, so that some cost
metric is minimised. We survey the algorithmic aspects of methods for automatic
airspace sectorisation, for an intended readership of experts on air traffic
management.
"
980,"SolarStat: Modeling Photovoltaic Sources through Stochastic Markov
  Processes","  In this paper, we present a methodology and a tool to derive simple but yet
accurate stochastic Markov processes for the description of the energy
scavenged by outdoor solar sources. In particular, we target photovoltaic
panels with small form factors, as those exploited by embedded communication
devices such as wireless sensor nodes or, concerning modern cellular system
technology, by small-cells. Our models are especially useful for the
theoretical investigation and the simulation of energetically self-sufficient
communication systems including these devices. The Markov models that we derive
in this paper are obtained from extensive solar radiation databases, that are
widely available online. Basically, from hourly radiance patterns, we derive
the corresponding amount of energy (current and voltage) that is accumulated
over time, and we finally use it to represent the scavenged energy in terms of
its relevant statistics. Toward this end, two clustering approaches for the raw
radiance data are described and the resulting Markov models are compared
against the empirical distributions. Our results indicate that Markov models
with just two states provide a rough characterization of the real data traces.
While these could be sufficiently accurate for certain applications, slightly
increasing the number of states to, e.g., eight, allows the representation of
the real energy inflow process with an excellent level of accuracy in terms of
first and second order statistics. Our tool has been developed using Matlab(TM)
and is available under the GPL license at[1].
"
981,"Road Accident Prevention Unit: An prototyping approach towards
  mitigating an omnipresent threat","  An intelligent multisensor front end based on the ARM Cortex M3. It deduces a
driver's configuration, ascertains his ability to drive safely and contacts
near ones with location data for urgent disaster mitigation. Prevention
measures are undertaken through external display modules and provision for
being vehicle-powered through external voltage regulated supplies. The proof of
concept for this paper is an ALL INDIA THIRD PRIZE WINNER at the Texas
Instruments Analog Design Contest 2012-13 National Finals and this paper is due
for digital publication in IEEE Xplore. All documentation is property of Texas
Instruments, the Texas Instruments Analog Design Contest 2012-13 and IEEE
Xplore.
"
982,"ProMC: Input-output data format for HEP applications using varint
  encoding","  A new data format for Monte Carlo (MC) events, or any structural data,
including experimental data, is discussed. The format is designed to store data
in a compact binary form using variable-size integer encoding as implemented in
the Google's Protocol Buffers package. This approach is implemented in the
ProMC library which produces smaller file sizes for MC records compared to the
existing input-output libraries used in high-energy physics (HEP). Other
important features of the proposed format are a separation of abstract data
layouts from concrete programming implementations, self-description and random
access. Data stored in ProMC files can be written, read and manipulated in a
number of programming languages, such C++, JAVA, FORTRAN and PYTHON.
"
983,Unfaithful Glitch Propagation in Existing Binary Circuit Models,"  We show that no existing continuous-time, binary value-domain model for
digital circuits is able to correctly capture glitch propagation. Prominent
examples of such models are based on pure delay channels (P), inertial delay
channels (I), or the elaborate PID channels proposed by Bellido-D\'iaz et al.
We accomplish our goal by considering the solvability/non-solvability border of
a simple problem called Short-Pulse Filtration (SPF), which is closely related
to arbitration and synchronization. On one hand, we prove that SPF is solvable
in bounded time in any such model that provides channels with non-constant
delay, like I and PID. This is in opposition to the impossibility of solving
bounded SPF in real (physical) circuit models. On the other hand, for binary
circuit models with constant-delay channels, we prove that SPF cannot be solved
even in unbounded time; again in opposition to physical circuit models.
Consequently, indeed none of the binary value-domain models proposed so far
(and that we are aware of) faithfully captures glitch propagation of real
circuits. We finally show that these modeling mismatches do not hold for the
weaker eventual SPF problem.
"
984,"Tasks and architecture of documentation subsystem in multi-level
  modeling environment MARS","  The article describes the automated documentation system designed to generate
reports on research conducted by computer complex technical objects and systems
in multi-level modeling environment {\guillemotleft}MARS{\guillemotright}. We
defined the purposes, tasks and abilities of documentation system and examined
the types and structure of documents, and gave an example of its practical use
"
985,Une repr\'esentation en graphe pour l'enseignement de XML,"  Currently, XML is a format widely used. In the context of computer science
teaching, it is necessary to introduce students to this format and, especially,
at its eco-system. We have developed a model to support the teaching of XML. We
propose to represent an XML schema as a graph highlighting the structural
characteristics of the valide documents. We present in this report different
graphic elements of the model and the improvements it brings to data modeling
in XML.---XML est un format actuellement tr\`es utilis\'e. Dans le cadre des
formations en informatique, il est indispensable d'initier les \'etudiants \`a
ce format et, surtout, \`a tout son \'eco-syst\`eme. Nous avons donc mis au
point un mod\`ele permettant d'appuyer l'enseignement de XML. Ce mod\`ele
propose de repr\'esenter un sch\'ema XML sous la forme d'un graphe mettant en
valeur les caract\'eristiques structurelles des documents valides. Nous
pr\'esentons dans ce rapport les diff\'erents \'el\'ements graphique du
mod\`ele et les am\'eliorations qu'il apporte \`a la mod\'elisation de
donn\'ees en XML.
"
986,"The structure and functions of an automated project management system
  for the centers of scientific and technical creativity of students","  This article discusses the possibility of automating of the student's
projecting through the use of automated project management system. There are
described the purpose, structure and formalism of automated workplace of
student-designer (AWSD), and shown its structural-functional diagram.
"
987,Smart: Semantically mashup rest web services,"  A mashup is a combination of information from more than one source, mixed up
in a way to create something new, or at least useful. Anyone can find mashups
on the internet, but these are always specifically designed for a predefined
purpose. To change that fact, we implemented a new platform we called the SMART
platform. SMART enables the user to make his own choices as for the REST web
services he needs to call in order to build an intelligent personalized mashup,
from a Google-like simple search interface, without needing any programming
skills. In order to achieve this goal, we defined an ontology that can hold
REST web services descriptions. These descriptions encapsulate mainly, the
input type needed for a service, its output type, and the kind of relation that
ties the input to the output. Then, by matching the user input query keywords,
with the REST web services definitions in our ontology, we can find registered
services individuals in this ontology, and construct the raw REST query for
each service found. The wrap up from the keywords, into semantic definitions,
in order to find the matching service individual, then the wrap down from the
semantic service description of the found individual, to the raw REST call, and
finally the wrap up of the result again into semantic individuals, is done for
two main purposes: the first to let the user use simple keywords in order to
build complex mashups, and the second to benefit from the ontology inference
engine in a way, where services instances can be tied together into an
intelligent mashup, simply by making each service output individuals, stand as
the next service input.
"
988,Current Services In Cloud Computing: A Survey,"  Due to the fast development of the Cloud Computing technologies, the rapid
increase of cloud services are became very remarkable. The fact of integration
of these services with many of the modern enterprises cannot be ignored.
Microsoft, Google, Amazon, SalesForce.com and the other leading IT companies
are entered the field of developing these services. This paper presents a
comprehensive survey of current cloud services, which are divided into eleven
categories. Also the most famous providers for these services are listed.
Finally, the Deployment Models of Cloud Computing are mentioned and briefly
discussed.
"
989,Wireless Computing and IT Ecosystems,"  We have evolved an IT system that is ubiquitous and pervasive and integrated
into most aspects of our lives. Many of us are working on 4th and 5th level
refinements in efficiency and functionality. But, we stand on the shoulders of
those who came before and this restricts our freedom of action. The prior work
has left us with an ecosystem which is the living embodiment of our
state-of-the-art. While we work on integration, refinement, broader application
and efficiency, the results must move seamlessly into the ecosystem.
Fundamental concepts are being researched in the lab and may rebuild the world
we all live in, until that happens, we must work within the ecosystem.
"
990,Intuitionistic Neutrosophic Soft Set,"  In this paper we study the concept of intuitionistic neutrosophic set of
Bhowmik and Pal. We have introduced this concept in soft sets and defined
intuitionistic neutrosophic soft set. Some definitions and operations have been
introduced on intuitionistic neutrosophic soft set. Some properties of this
concept have been established.
"
991,Routing Diverse Evacuees with Cognitive Packets,"  This paper explores the idea of smart building evacuation when evacuees can
belong to different categories with respect to their ability to move and their
health conditions. This leads to new algorithms that use the Cognitive Packet
Network concept to tailor different quality of service needs to different
evacuees. These ideas are implemented in a simulated environment and evaluated
with regard to their effectiveness.
"
992,"An Improved Variable Step-size Affine Projection Sign Algorithm for Echo
  Cancellation","  This paper proposes an improved variable step-size (VSS) algorithm for the
recently introduced affine projection sign algorithm (APSA) based on the
recovery of the near-end signal energy in the error signal. Simulation results
demonstrate that, compared to the previous VSS for APSA, the proposed approach
provides both more robustness to impulse interference and better tracking
ability of echo path change.
"
993,"Classification of ST and Q Type MI variant using thresholding and
  neighbourhood estimation method after cross wavelet based analysis","  This paper proposes a cross wavelet transform based method for
Electrocardiogram signal analysis where parameters are identified from wavelet
cross spectrum and wavelet cross coherence of ECG patterns. Most of the ECG
analysing systems use explicit time plane features for cardiac pattern
classification. Application of this proposed technique for classification
eliminates the need for extraction of various explicit time plane features and
hence reduces the complexity of the system. The cross-correlation is the
measure of similarity between two waveforms or two time series and the cross
examination reveals localized similarities in time and scale. Parameters
extracted from Wavelet Cross Spectrum (WCS) and Wavelet Coherence (WCOH) is
used for classification. A pathologically varying pattern in QT zone of
inferior lead III shows the presence of Inferior Myocardial Infarction (IMI).
The Cross Wavelet Transform and Wavelet Coherence is used for the cross
examination of single normal and abnormal (IMI) beats. A normal template beat
is selected as the absolute normal pattern. Computation of the WCS and WCOH of
the selected normal template and various other normal and abnormal beats
reveals the existence of variation among patterns under study. The Wavelet
cross spectrum and Wavelet coherence of various ECG patterns shows
distinguishing characteristics over two specific regions R1 and R2, where R1 is
the QRS complex location and R2 is the T wave region. Parameters are identified
for classification of Type 1 IMI (non Q type, with ST elevation and attenuated
QRS complex) and Type 2 IMI (Q type MI with deep Q and inverted T) and normal
subjects. Accuracy of the proposed classification method is obtained as 99.43%
for normal and abnormal class and 88.5% and 87.02% for Type I and Type II
respectively.
"
994,"Criticality estimation of IT business functions with the Business
  Continuity Testing Points method for implementing effective recovery
  exercises of crisis scenarios","  The primary goal of the present paper is the introduction of a new approach
of defining IT unit business functions exact criticality levels and
respectively categorize them to the appropriate recovery tests, prior to their
thorough documentation which includes actual desired recovery time frames. The
method is entitled as Business Continuity Testing Points and it is based on the
concept of Use Case Points, a fundamental project estimation tool utilized for
sizing of object-oriented system development. The aim of the contribution is to
ameliorate the existing manual way of determining recovery time of IT business
functions that is based exclusively on experience of IT personnel, by
introducing a calculation method of multiple factors that can negatively affect
the recovery process. The elimination of damage as a result of tested immediate
response action in a crisis situation that disrupts core IT operations
constitutes the aimed advantage of the proposed contribution
"
995,Simulation Model Of Functional Stability Of Business Processes,"  Functioning of business processes of high-tech enterprise is in a constant
interaction with the environment. Herewith a wide range of such interaction
represents a variety of conflicts affecting the achievement of the goals of
business processes. All these things lead to the disruption of functioning of
business processes. That's why modern enterprises should have mechanisms to
provide a new property of business processes - ability to maintain and/or
restore functions in various adverse effects. This property is called
functional stability of business processes (FSBP). In this article we offer,
showcase and test the new approach to assessing the results of business process
re-engineering by simulating their functional stability before and after
re-engineering.
"
996,"Spreading huge free software without internet connection, via
  self-replicating USB keys","  We describe and discuss an affordable way to spread huge software without
relying on internet connection, via the use of self-replicating live USB keys.
"
997,"Distributed Power Loss Minimization in Residential Micro Grids: a
  Communications Perspective","  The constantly increasing number of power generation devices based on
renewables is calling for a transition from the centralized control of
electrical distribution grids to a distributed control scenario. In this
context, distributed generators (DGs) are exploited to achieve other objectives
beyond supporting loads, such as the minimization of the power losses along the
distribution lines. The aim of this work is that of designing a full-fledged
system that extends existing state of the art algorithms for the distributed
minimization of power losses. We take into account practical aspects such as
the design of a communication and coordination protocol that is resilient to
link failures and manages channel access, message delivery and DG coordination.
Thus, we analyze the performance of the resulting optimization and
communication scheme in terms of power loss reduction, reduction of aggregate
power demand, convergence rate and resilience to communication link failures.
After that, we discuss the results of a thorough simulation campaign, obtained
using topologies generated through a statistical approach that has been
validated in previous research, by also assessing the performance deviation
with respect to localized schemes, where the DGs are operated independently.
Our results reveal that the convergence and stability performance of the
selected algorithms vary greatly. However, configurations exist for which
convergence is possible within five to ten communication steps and, when just
30% of the nodes are DGs, the aggregate power demand is roughly halved. Also,
some of the considered approaches are quite robust against link failures as
they still provide gains with respect to the localized solutions for failure
rates as high as 50%.
"
998,"A customized flocking algorithm for swarms of sensors tracking a swarm
  of targets","  Wireless mobile sensor networks (WMSNs) are groups of mobile sensing agents
with multi-modal sensing capabilities that communicate over wireless networks.
WMSNs have more flexibility in terms of deployment and exploration abilities
over static sensor networks. Sensor networks have a wide range of applications
in security and surveillance systems, environmental monitoring, data gathering
for network-centric healthcare systems, monitoring seismic activities and
atmospheric events, tracking traffic congestion and air pollution levels,
localization of autonomous vehicles in intelligent transportation systems, and
detecting failures of sensing, storage, and switching components of smart
grids. The above applications require target tracking for processes and events
of interest occurring in an environment. Various methods and approaches have
been proposed in order to track one or more targets in a pre-defined area.
Usually, this turns out to be a complicated job involving higher order
mathematics coupled with artificial intelligence due to the dynamic nature of
the targets. To optimize the resources we need to have an approach that works
in a more straightforward manner while resulting in fairly satisfactory data.
In this paper we have discussed the various cases that might arise while
flocking a group of sensors to track targets in a given environment. The
approach has been developed from scratch although some basic assumptions have
been made keeping in mind some previous theories. This paper outlines a
customized approach for feasibly tracking swarms of targets in a specific area
so as to minimize the resources and optimize tracking efficiency.
"
999,"A Fraud Detection Visualization System Utilizing Radial Drawings and
  Heat-maps","  We present a prototype system developed in cooperation with a business
organization that combines information visualization and pattern-matching
techniques to detect fraudulent activity by employees. The system is built upon
common fraud patterns searched while trying to detect occupational fraud
suggested by internal auditors of a business company. The main visualization of
the system consists of a multi-layer radial drawing that represents the
activity of the employees and clients. Each layer represents a different
examined pattern whereas heat-maps indicating suspicious activity are
incorporated in the visualization. The data are first preprocessed based on a
decision tree generated by the examined patterns and each employee is assigned
a value indicating whether or not there exist indications of fraud. The
visualization is presented as an animation and the employees are visualized one
by one according to their severity values together with their related clients.
"
1000,"Theoretical Foundation for Research in Communication using Information
  and Communication Technology Devices in Healthcare: An Interdisciplinary
  Scoping Review","  Faulty communication between team members is one of the most important
factors preventing substantial improvement in patient safety. Aviation, nuclear
power and defense have been able to improve their safety record by adopting
theory and model based solutions. In contrast, healthcare's thrust towards
modern communication devices is largely devoid of theoretical foundation. The
objective of this scoping review is to compile communication theories,
frameworks, and models used by high risk organizations outside healthcare to
study and resolve workplace communication issues. The healthcare databases
searched included Medline, CINAHL, EMBASE, and PsycInfo. In addition, we
searched engineering and science literature to include articles in the fields
of information sciences, computer sciences, nuclear power generation, aviation,
the military and other domains such as sociology that address the science and
theory of communication. Comprehensive searching was also done in the
communication studies literature. We also reviewed conference proceedings and
grey literature and conducted citation tracking. Our initial systematic search
yielded 15,365 articles. Hand searching and reviewing references resulted in a
set of 181 articles. 144 full text articles were read and 40 of them were
selected to be included in the review. We were able to identify 14 theories and
12 models which could be applied in hospital communication research. However,
it must be noted that most of them have not yet been applied in biomedical
research in hospital communication and as such their applicability can only be
suggested-a gap which future research may be able to address. Formulation of a
custom model representing the unique features and complexities of communication
within hospitals is recommended.
"
1001,Unconventional research in USSR and Russia: short overview,"  This work briefly surveys unconventional research in Russia from the end of
the 19th until the beginning of the 21th centuries in areas related to
generation and detection of a 'high-penetrating' emission of non-biological
origin. The overview is based on open scientific and journalistic materials.
The unique character of this research and its history, originating from
governmental programs of the USSR, is shown. Relations to modern studies on
biological effects of weak electromagnetic emission, several areas of
bioinformatics and theories of physical vacuum are discussed.
"
1002,"FELFCNCA: Fast & Efficient Log File Compression Using Non Linear
  Cellular Automata Classifier","  Log Files are created for Traffic Analysis, Maintenance, Software debugging,
customer management at multiple places like System Services, User Monitoring
Applications, Network servers, database management systems which must be kept
for long periods of time. These Log files may grow to huge sizes in this
complex systems and environments. For storage and convenience log files must be
compressed. Most of the existing algorithms do not take temporal redundancy
specific Log Files into consideration. We propose a Non Linear based Classifier
which introduces a multidimensional log file compression scheme described in
eight variants, differing in complexity and attained compression ratios. The
FELFCNCA scheme introduces a transformation for log file whose compressible
output is far better than general purpose algorithms. This proposed method was
found lossless and fully automatic. It does not impose any constraint on the
size of log file
"
1003,"Solve of problems of mathematical theory of learning with using computer
  modeling methods","  Analyzed models of learning, which take into account that: 1) the rate of
increase of student's knowledge is proportional to the difference between
levels of teacher's requirements and prior knowledge; 2) if the requirements
are too high, then student motivation decreases and he stops learning. Was
proposed: 1) a one component model, coming from the fact that the training
information consists of equal elements; 2) a two component model that takes
into account that knowledge is assimilated with varying strength, 'trustworthy'
knowledge forgotten much slower then 'weak'; 3) two component model, which
takes into account the transition of 'weak' knowledge in 'trustworthy'
knowledge. The solution of the five predictors and optimization problems of
learning theory are represented.
"
1004,A New Variable Step-size Zero-point Attracting Projection Algorithm,"  This paper proposes a new variable step-size (VSS) scheme for the recently
introduced zero-point attracting projection (ZAP) algorithm. The proposed
variable step-size ZAPs are based on the gradient of the estimated filter
coefficients sparseness that is approximated by the difference between the
sparseness measure of current filter coefficients and an averaged sparseness
measure. Simulation results demonstrate that the proposed approach provides
both faster convergence rate and better tracking ability than previous ones.
"
1005,Personalized real time weather forecasting,"  Temperature forecasting and rain forecasting in today's environment is
playing a major role in many fields like transportation, tour planning and
agriculture. The purpose of this paper is to provide a real time forecasting to
the user according to their current position and requirement. The simplest
method of forecasting the weather, persistence, relies upon today's conditions
to forecast the conditions tomorrow i.e. analyzing historical data for
predicting future weather conditions. The weather data used for the DM research
include daily temperature, daily pressure and monthly rainfall.
"
1006,Abridged Petri Nets,"  A new graphical framework, Abridged Petri Nets (APNs) is introduced for
bottom-up modeling of complex stochastic systems. APNs are similar to
Stochastic Petri Nets (SPNs) in as much as they both rely on component-based
representation of system state space, in contrast to Markov chains that
explicitly model the states of an entire system. In both frameworks, so-called
tokens (denoted as small circles) represent individual entities comprising the
system; however, SPN graphs contain two distinct types of nodes (called places
and transitions) with transitions serving the purpose of routing tokens among
places. As a result, a pair of place nodes in SPNs can be linked to each other
only via a transient stop, a transition node. In contrast, APN graphs link
place nodes directly by arcs (transitions), similar to state space diagrams for
Markov chains, and separate transition nodes are not needed.
  Tokens in APN are distinct and have labels that can assume both discrete
values (""colors"") and continuous values (""ages""), both of which can change
during simulation. Component interactions are modeled in APNs using triggers,
which are either inhibitors or enablers (the inhibitors' opposites).
Hierarchical construction of APNs rely on using stacks (layers) of submodels
with automatically matching color policies. As a result, APNs provide at least
the same modeling power as SPNs, but, as demonstrated by means of several
examples, the resulting models are often more compact and transparent,
therefore facilitating more efficient performance evaluation of complex
systems.
"
1007,"Maturity Model for IT Service Outsourcing in Higher Education
  Institutions","  The current success of organizations depends on the successful implementation
of Information and Comunication Technologies (ICTs). Good governance and ICT
management are essential for delivering value, managing technological risks,
managing resources and performance measurement. In addition, outsourcing is a
strategic option which complements IT services provided internally in
organizations. This paper proposes the design of a new holistic maturity model
based on standards ISO/IEC 20000 and ISO/IEC 38500, the frameworks and best
practices of ITIL and COBIT, with a specific focus on IT outsourcing. This
model is validated by practices in the field of higher education, using a
questionnaire and a metrics table among other measurement tools. Models,
standards and guidelines are proposed in the model for facilitating adaptation
to universities and achieving excellence in the outsourcing of IT services. The
applicability of the model allows an effective transition to a model of good
governance and management of outsourced IT services which, aligned with the
core business of universities (teaching, research and innovation), affect the
effectiveness and efficiency of its management, optimizes its value and
minimizes risks.
"
1008,"Various models of process of the learning, based on the numerical
  solution of the differential equations","  The principles on which can be based computer model of process of training
are formulated. Are considered: 1) the unicomponent model, which is recognizing
that educational information consists of equal elements; 2) the multicomponent
model, which is considering that knowledge is assimilate with a various
strength, and on lesson weak knowledge becomes strong; 3) the generalized
multicomponent model which considers change of working capacity of the pupil
and various complexity of studied elements of a training material. Typical
results of imitating modeling of learning process are presented in article.
"
1009,"Exact Reconstruction of Spatially Undersampled Signals in Evolutionary
  Systems","  We consider the problem of spatiotemporal sampling in which an initial state
$f$ of an evolution process $f_t=A_tf$ is to be recovered from a combined set
of coarse samples from varying time levels $\{t_1,\dots,t_N\}$. This new way of
sampling, which we call dynamical sampling, differs from standard sampling
since at any fixed time $t_i$ there are not enough samples to recover the
function $f$ or the state $f_{t_i}$. Although dynamical sampling is an inverse
problem, it differs from the typical inverse problems in which $f$ is to be
recovered from $A_Tf$ for a single time $T$. In this paper, we consider signals
that are modeled by $\ell^2(\mathbb Z)$ or a shift invariant space $V\subset
L^2(\mathbb R)$.
"
1010,Vulnerability of LTE to Hostile Interference,"  LTE is well on its way to becoming the primary cellular standard, due to its
performance and low cost. Over the next decade we will become dependent on LTE,
which is why we must ensure it is secure and available when we need it.
Unfortunately, like any wireless technology, disruption through radio jamming
is possible. This paper investigates the extent to which LTE is vulnerable to
intentional jamming, by analyzing the components of the LTE downlink and uplink
signals. The LTE physical layer consists of several physical channels and
signals, most of which are vital to the operation of the link. By taking into
account the density of these physical channels and signals with respect to the
entire frame, as well as the modulation and coding schemes involved, we come up
with a series of vulnerability metrics in the form of jammer to signal ratios.
The ``weakest links'' of the LTE signals are then identified, and used to
establish the overall vulnerability of LTE to hostile interference.
"
1011,"The solution of complex problems on calculation of the electrostatic
  fields on lessons on computer modeling","  In article the following tasks on computer modeling of electric fields are
analyzed: 1) calculation of distribution of potential for the field created by
two parallel plates and charged bodies in the non-uniform environment; 2)
calculation of distribution of potential and force lines of electric field in
which are brought the cylinder, a pipe, a plate, a rectangular parallelepiped
from dielectric, and also the metal cylinder; 3) calculation of distribution of
potential in the one-dimensional nonuniform environment; 4) the solution of the
equation of Poisson in spherical coordinates; 5) calculation of distribution of
potential in cylindrical coordinates with the subsequent creation of
equipotential surfaces and force lines.
"
1012,"Technical Report: A New Multi-Device Wireless Power Transfer Scheme
  Using an Intermediate Energy Storage Circuit","  A new multi-device wireless power transfer scheme that reduces the overall
charging time is presented. The proposed scheme employs the intermediated
energy storage (IES) circuit which consists of a constant power driving circuit
and a super-capacitor. By utilizing the characteristic of high power density of
the super-capacitor, the receiver can receive and store the energy in short
duration and supply to the battery for long time. This enables the overlap of
charging duration between all receivers. As a result, the overall charging time
can be reduced.
"
1013,Cyclostationary Spectrum Sensing in Cognitive Radios Using FRESH Filters,"  This paper deals with spectrum sensing in Cognitive Radios to enable
unlicensed secondary users to opportunistically access a licensed band. The
ability to detect the presence of a primary user at a low signal to noise ratio
(SNR) is a challenging prerequisite to spectrum sensing and earlier proposed
techniques like energy detection and cyclostationary detection have only been
partially successful. This paper proposes the use of FRESH (FREquency SHift)
filters [1] to enable spectrum sensing at low SNR by optimally estimating a
cyclostationary signal using its spectral coherence properties. We establish
the mean square error convergence of the adaptive FRESH filter through
simulation. Subsequently, we formulate a cyclostationarity based binary
hypothesis test on the filtered signal and observe the resultant detection
performance. Simulation results show that the proposed approach performs better
than energy detection and cyclostationary detection techniques for spectrum
sensing.
"
1014,"Heuristics for Routing and Spiral Run-time Task Mapping in NoC-based
  Heterogeneous MPSOCs","  This paper describes a new Spiral Dynamic Task Mapping heuristic for mapping
applications onto NoC-based Heterogeneous MPSoC. The heuristic proposed in this
paper attempts to map the tasks of an applications that are most related to
each other in spiral manner and to find the best possible path load that
minimizes the communication overhead. In this context, we have realized a
simulation environment for experimental evaluations to map applications with
varying number of tasks onto an 8x8 NoC-based Heterogeneous MPSoCs platform, we
demonstrate that the new mapping heuristics with the new modified dijkstra
routing algorithm proposed are capable of reducing the total execution time and
energy consumption of applications when compared to state-of the-art run-time
mapping heuristics reported in the literature.
"
1015,Advanced Data Processing in the Business Network System,"  The discovery, representation and reconstruction of Business Networks (BN)
from Network Mining (NM) raw data is a difficult problem for enterprises. This
is due to huge amounts of e.g. complex business processes within and across
enterprise boundaries, heterogeneous technology stacks, and fragmented data. To
remain competitive, visibility into the enterprise and partner networks on
different, interrelated abstraction levels is desirable.
  We show the query and data processing capabilities of a novel data discovery,
mining and network inference system, called Business Network System (BNS) that
reconstructs the BN--integration and business process networks - from raw data,
hidden in the enterprises' landscapes. The paper covers both the foundation and
the key data processing characteristics features of BNS, including its
underlying technologies, its overall system architecture, and data provenance
approach.
"
1016,"A Novel Carrier Waveform Inter-Displacement Modulation Method in
  Underwater Communication Channel","  As the main way of underwater wireless communication, underwater acoustic
communication is one of the focuses of ocean research. Compared with the free
space wireless communication channel, the underwater acoustic channel suffers
from more severe multipath effect, the less available bandwidth and the even
complex noise. The underwater acoustic channel is one of the most complicated
wireless communication channels. To achieve a reliable underwater acoustic
communication, Phase Shift Keying (PSK) modulation and Passive Time Reversal
Mirror (PTRM) equalization are considered to be a suitable scheme. However, due
to the serious distortion of the received signal caused by the channel, this
scheme suffers from a high Bit Error Rate (BER) under the condition of the low
Signal to Noise Ratio (SNR). To solve this problem, we proposes a Carrier
Waveform Inter-Displacement (CWID) modulation method based on the Linear
Frequency Modulation (LFM) PSK and PTRM scheme. The new communication scheme
reduces BER by increasing the difference from the carrier waveform for
different symbols. Simulation results show the effectiveness and superiority of
the proposed method.
"
1017,Effect of Carouseling on Angular Rate Sensor Error Processes,"  Carouseling is an efficient method to mitigate the measurement errors of
inertial sensors, particularly MEMS gyroscopes. In this article, the effect of
carouseling on the most significant stochastic error processes of a MEMS
gyroscope, i.e., additive bias, white noise, 1/f noise, and rate random walk,
is investigated. Variance propagation equations for these processes under
averaging and carouseling are defined. Furthermore, a novel approach to
generating 1/f noise is presented. The experimental results show that
carouseling reduces the contributions of additive bias, 1/f noise, and rate
random walk significantly in comparison with plain averaging, which can be
utilized to improve the accuracy of dead reckoning systems.
"
1018,"Design and implementation of the Customer Experience Data Mart in the
  Telecommunication Industry: Application Order-To-Payment end to end process","  Facing the new market challenges, service providers are looking for solutions
to improve three major business areas namely the Customer Experience, The
Operational Efficiency and Revenue and Margin. To meet the business requiements
related to these areas, service providers are going through three major
transformation programs namely the Business Support Systems transformation
program for Customer related aspects, the Operations Support System
transformation program for mainly service Fulfillment and Assurance and
Resource, Fulfillment and Assurance, and Time To Market Transformation program
for Products ans Services development and management. These transformations are
about making a transition from a current situation with all its views to a
desired one. The information view transformation is about reorganizing and
reengineering the existing information to be used for the day to day activities
and reporting to support decision making. For reporting purpose, service
providers have to invest in Business Intelligence solutions. For which the main
purpose is to provide the right information in a timely manner to efficiently
support the decision making. One of the key BI challenges is to model an
information structure where to host all the information coming from multiple
sources. The purpose of this paper is to suggest a step by step methodology to
design a Telco Data Mart, one of the fundamental BI components.
Order-To-Payment, an end to end customer process, will be used as an
application for this methodology. Our methodology consists on bringing together
the concepts of business intelligence and the telecom business frameworks
developed by the TM Forum: the Business Process Framework, the Information
Framework, and the Business Metrics. The advantage of this solution is its
ability to adapt to any telecom enterprise architecture since it's built around
the business standards.
"
1019,Smart Grid Demand Monitoring Model,"  This paper is in related to the demand genrated by the consumer for a time
for the power which is being viewed by taking some measures to solve the demand
need.
"
1020,"Pi Fractions for Generating Uniformly Distributed Sampling Points in
  Global Search and Optimization Algorithms","  Pi Fractions are used to create deterministic uniformly distributed
pseudorandom decision space sample points for a global search and optimization
algorithm. These fractions appear to be uniformly distributed on [0,1] and can
be used in any stochastic algorithm rendering it effectively deterministic
without compromising its ability to explore the decision space. Pi Fractions
are generated using the BBP Pi digit extraction algorithm. The Pi Fraction
approach is tested using genetic algorithm Pi-GASR with very good results. A Pi
Fraction data file is available upon request.
"
1021,"Comprehensive Analysis and Measurement of Frequency-Tuned and
  Impedance-Tuned Wireless Non-Radiative Power Transfer Systems","  This paper theoretically and experimentally investigates frequency-tuned and
impedance-tuned wireless non-radiative power transfer (WNPT) systems.
Closed-form expressions for the efficiencies of both systems, as a function of
frequency and system (circuit) parameters, are presented. In the
frequency-tuned system, the operating frequency is adjusted to compensate for
changes in mutual inductance that occur for variations of transmitter and
receiver loop positions. Frequency-tuning is employed for a range of distances
over which the loops are strongly coupled. In contrast, the impedance-tuned
system employs varactor-based matching networks to compensate for changes in
mutual inductance and achieve a simultaneous conjugate impedance match over a
range of distances. The frequency-tuned system is simpler to implement, while
the impedance-tuned system is more complex but can achieve higher efficiencies.
Both of the experimental WNPT systems studied employ resonant shielded loops as
transmitting and receiving devices.
"
1022,"Advanced Self-interference Cancellation and Multiantenna Techniques for
  Full-Duplex Radios","  In an in-band full-duplex system, radios transmit and receive simultaneously
in the same frequency band at the same time, providing a radical improvement in
spectral efficiency over a half-duplex system. However, in order to design such
a system, it is necessary to mitigate the self-interference due to simultaneous
transmission and reception, which seriously limits the maximum transmit power
of the full-duplex device. Especially, large differences in power levels in the
receiver front-end sets stringent requirements for the linearity of the
transceiver electronics. We present an advanced architecture for a compact
full-duplex multiantenna transceiver combining antenna design with analog and
digital cancellation, including both linear and nonlinear signal processing.
"
1023,"Accelerating SystemVerilog UVM Based VIP to Improve Methodology for
  Verification of Image Signal Processing Designs Using HW Emulator","  In this paper we present the development of Acceleratable UVCs from standard
UVCs in SystemVerilog and their usage in UVM based Verification Environment of
Image Signal Processing designs to increase run time performance. This paper
covers development of Acceleratable UVCs from standard UVCs for internal
control and data buses of ST imaging group by partitioning of transaction-level
components and cycle-accurate signal-level components between the software
simulator and hardware accelerator respectively. Standard Co-Emulation API:
Modeling Interface (SCE-MI) compliant, transaction-level communications link
between test benches running on a host system and Emulation machine is
established. Accelerated Verification IPs are used at UVM based Verification
Environment of Image Signal Processing designs both with simulator and emulator
as UVM acceleration is an extension of the standard simulation-only UVM and is
fully backward compatible with it. Acceleratable UVCs significantly reduces
development schedule risks while leveraging transaction models used during
simulation.
  In this paper, we discuss our experiences on UVM based methodology adoption
on TestBench-Xpress(TBX) based technology step by step. We are also doing
comparison between the run time performance results from earlier simulator-only
environment and the new, hardware-accelerated environment. Although this paper
focuses on Acceleratable UVCs development and their usage for image signal
processing designs, Same concept can be extended for non-image signal
processing designs.
  KEYWORDS- SystemVerilog, Universal Verification Methodology (UVM),
TestBench-Xpress (TBX), Universal Verification Component (UVC), Standard
Co-Emulation API: Modelling Interface (SCE-MI), Acceleratable UVC, Emulator,
XRTL Tasks/Functions (xtf), Transactor interface (tif), Verification IP (VIP).
"
1024,"Transport Information System using Query Centric Cyber Physical Systems
  (QCPS)","  To incorporate the computation and communication with the physical world,
next generation architecture i.e. CPS is viewed as a new technology. To improve
the better interaction with the physical world or to perk up the electricity
delivery usage, various CPS based approaches have been introduced. Recently
several GPS equipped smart phones and sensor based frameworks have been
proposed which provide various services i.e. environment estimation, road
safety improvement but encounter certain limitations like elevated energy
consumption and high computation cost. To meet the high reliability and safety
requirements, this paper introduces a novel approach based on QCPS model which
provides several users services (discussed in this paper). Further, this paper
proposed a Transport Information System (TIS), which provide the communication
with lower cost overhead by arranging the similar sensors in the form of grids.
Each grid has a coordinator which interacts with cloud to process the user
query. In order to evaluate the performance of proposed approach we have
implemented a test bed of 16 wireless sensor nodes and have shown the
performance in terms of computation and communication cost.
"
1025,"The Energy/Frequency Convexity Rule: Modeling and Experimental
  Validation on Mobile Devices","  This paper provides both theoretical and experimental evidence for the
existence of an Energy/Frequency Convexity Rule, which relates energy
consumption and CPU frequency on mobile devices. We monitored a typical
smartphone running a specific computing-intensive kernel of multiple nested
loops written in C using a high-resolution power gauge. Data gathered during a
week-long acquisition campaign suggest that energy consumed per input element
is strongly correlated with CPU frequency, and, more interestingly, the curve
exhibits a clear minimum over a 0.2 GHz to 1.6 GHz window. We provide and
motivate an analytical model for this behavior, which fits well with the data.
Our work should be of clear interest to researchers focusing on energy usage
and minimization for mobile devices, and provide new insights for optimization
opportunities.
"
1026,"Antifragility = Elasticity + Resilience + Machine Learning: Models and
  Algorithms for Open System Fidelity","  We introduce a model of the fidelity of open systems - fidelity being
interpreted here as the compliance between corresponding figures of interest in
two separate but communicating domains. A special case of fidelity is given by
real-timeliness and synchrony, in which the figure of interest is the physical
and the system's notion of time. Our model covers two orthogonal aspects of
fidelity, the first one focusing on a system's steady state and the second one
capturing that system's dynamic and behavioural characteristics. We discuss how
the two aspects correspond respectively to elasticity and resilience and we
highlight each aspect's qualities and limitations. Finally we sketch the
elements of a new model coupling both of the first model's aspects and
complementing them with machine learning. Finally, a conjecture is put forward
that the new model may represent a first step towards compositional criteria
for antifragile systems.
"
1027,"Beyond Reuse Distance Analysis: Dynamic Analysis for Characterization of
  Data Locality Potential","  Emerging computer architectures will feature drastically decreased flops/byte
(ratio of peak processing rate to memory bandwidth) as highlighted by recent
studies on Exascale architectural trends. Further, flops are getting cheaper
while the energy cost of data movement is increasingly dominant. The
understanding and characterization of data locality properties of computations
is critical in order to guide efforts to enhance data locality. Reuse distance
analysis of memory address traces is a valuable tool to perform data locality
characterization of programs. A single reuse distance analysis can be used to
estimate the number of cache misses in a fully associative LRU cache of any
size, thereby providing estimates on the minimum bandwidth requirements at
different levels of the memory hierarchy to avoid being bandwidth bound.
However, such an analysis only holds for the particular execution order that
produced the trace. It cannot estimate potential improvement in data locality
through dependence preserving transformations that change the execution
schedule of the operations in the computation. In this article, we develop a
novel dynamic analysis approach to characterize the inherent locality
properties of a computation and thereby assess the potential for data locality
enhancement via dependence preserving transformations. The execution trace of a
code is analyzed to extract a computational directed acyclic graph (CDAG) of
the data dependences. The CDAG is then partitioned into convex subsets, and the
convex partitioning is used to reorder the operations in the execution trace to
enhance data locality. The approach enables us to go beyond reuse distance
analysis of a single specific order of execution of the operations of a
computation in characterization of its data locality properties. It can serve a
valuable role in identifying promising code regions for manual transformation,
as well as assessing the effectiveness of compiler transformations for data
locality enhancement. We demonstrate the effectiveness of the approach using a
number of benchmarks, including case studies where the potential shown by the
analysis is exploited to achieve lower data movement costs and better
performance.
"
1028,Simulating Behaviours to face up an Emergency Evacuation,"  Computer based models describing pedestrian behavior in an emergency
evacuation play a vital role in the development of active strategies that
minimize the evacuation time when a closed area must be evacuated. The
reference model has a hybrid structure where the dynamics of fire and smoke
propagation are modeled by means of Cellular Automata and for simulating
people's behavior we are using Intelligent Agents. The model consists of two
sub-models, called environmental and pedestrian ones. As part of the pedestrian
model, this paper concentrates in a methodology that is able to model some of
the frequently observed human's behaviors in evacuation exercises. Each agent
will perceive what is happening around, select the options that exist in that
context and then it makes a decision that will reflect its ability to cope with
an emergency evacuation, called in this work, behavior. We also developed
simple exercises where the model is applied to the simulation of an evacuation
due to a potential hazard, such as fire, smoke or some kind of collapse.
"
1029,"Intelligent Product: Mobile Agent Architecture Integrating the End of
  Life Cycle (EOL) For minimizing the lunch phase PLM","  To improve the increasingly demands products that are customized, all
business activities performed along the product life cycle must be coordinated
and efficiently managed along the extended enterprise. For this, enterprise had
wanted to retain control over the whole product lifecycle especially when the
product is in use/recycling (End Of Life phase). Although there have been many
previous research works about product lifecycle management in the beginning of
life (BOL) and middle of life (MOL) phases, few addressed the end of life (EOL)
phase, in particular, when the product is at the customers. In this paper,
based on product embedded device identification (PEID) and mobile agent
technologies, and with the advent of the development of the ""intelligent
products"", we will try to improve innovation: (a) by minimize the lunch phase,
(b) and the involvement of the customer in product lifecycle.
"
1030,GreenMail: Reducing Email Service's Carbon Emission with Minimum Cost,"  Internet services contribute a large fraction of worldwide carbon emission
nowadays, in a context of increasing number of companies tending to provide and
more and more developers use Internet services. Noticeably, a trend is those
service providers are trying to reduce their carbon emissions by utilizing
on-site or off-site renewable energy in their datacenters in order to attract
more customers. With such efforts have been paid, there are still some users
who are aggressively calling for even cleaner Internet services. For example,
over 500,000 Facebook users petitioned the social networking site to use
renewable energy to power its datacenter. However, it seems impossible for such
demand to be satisfied merely from the inside of those production datacenters,
considering the transition cost and stability. Outside the existing Internet
services, on the other hand, may easily set up a proxy service to attract those
renewable-energy-sensitive users, by 1) using carbon neutral or even
over-offsetting cloud instances to bridge the end user and traditional Internet
services; and 2) estimating and offsetting the carbon emissions from the
traditional Internet services. In our paper, we proposed GreenMail, which is a
general IMAP proxy caching system that connects email users and traditional
email services. GreenMail runs on green web hosts to cache users' emails on
green cloud instances. Besides, it offsets the carbon emitted by traditional
backend email services. With GreenMail, users could set a carbon emission
constraint and use traditional email service without breaking any code
modification of user side and email server side.
"
1031,"Specification of the State Lifetime in the DEVS Formalism by Fuzzy
  Controller","  This paper aims to develop a new approach to assess the duration of state in
the DEVS formalism by fuzzy controller. The idea is to define a set of fuzzy
rules obtained from observers or expert knowledge and to specify a fuzzy model
which computes this duration, this latter is fed into the simulator to specify
the new value in the model. In conventional model, each state is defined by a
mean lifetime value whereas our method, calculates for each state the new
lifetime according to inputs values. A wildfire case study is presented at the
end of the paper. It is a challenging task due to its complex behavior,
dynamical weather condition, and various variables involved. A global
specification of the fuzzy controller and the forest fire model are presented
in the DEVS formalism and comparison between conventional and fuzzy method is
illustrated.
"
1032,"Software Architecture and Subclassing Technique for Semiconductor
  Manufacturing Machines","  This paper proposed software architecture for operating an automatic
semiconductor manufacturing machine. Recent machines for semiconductor process
are required for high level of automation which are composed of motion control,
machine vision, data acquisition and networking. These functions are executed
through industrial equipments that are generally installed in a computer. The
equipments occupy a great part of system resource and generate a lot of
computation, so the software structure should be designed for efficiency. The
proposed architecture is consisted of four layers and virtual equipments(VEs).
The VEs are made by subclassing the physical equipments(PEs) and the layers are
coded into thread which updates the status of VEs. Subroutines in a program
refer to the pointer of VEs, and direct access to physical equipment are
prohibited. The number of access (NOA) to PEs in typical industrial application
was simulated for the unlimited access structure and the presented structure.
The result showed that the proposed structure was more efficient than typical
ones and irrespective of subroutines. This architecture was also applied to
design a machine operating software and performed automatic wafer dicing.
"
1033,A Software Design through Electrical System for a Building,"  Computer aided design of lighting systems made new installations of lighting
dimensioning and verification of existing lighting systems for both indoor and
outdoor lighting systems.The design of the building light system was in a
dedicated software, named DiaLux, version 4.11.
"
1034,Modeling Life as Cognitive Info-Computation,"  This article presents a naturalist approach to cognition understood as a
network of info-computational, autopoietic processes in living systems. It
provides a conceptual framework for the unified view of cognition as evolved
from the simplest to the most complex organisms, based on new empirical and
theoretical results. It addresses three fundamental questions: what cognition
is, how cognition works and what cognition does at different levels of
complexity of living organisms. By explicating the info-computational character
of cognition, its evolution, agent-dependency and generative mechanisms we can
better understand its life-sustaining and life-propagating role. The
info-computational approach contributes to rethinking cognition as a process of
natural computation in living beings that can be applied for cognitive
computation in artificial systems.
"
1035,ICT technologies for the refurbishment of wooden structure buildings,"  Nowadays, one would think that after years of massive concrete and steel
construction in Spain, there are not many wood structure buildings left to be
refurbished except for some palaces or cathedrals. However, if we go for a walk
and have a look at the old part of any city, we will realize that still most of
the buildings have a wood structure. In spite of the fact that the majority of
urban regulations forbid their demolition, other bad practices such as casting
and overloading the wood structure are very common. Considering that we want to
reach a standard of sustainable construction, the economical and environmental
costs, which are implied by the deficient refurbishment makes it well worth a
previous study of the structure, which in most cases represents less than a 1%
of the total budget. The main goal of this paper is to present most relevant
parts of the whole process of diagnosis of a wood structure building by means
of Non-Destructive Testing Techniques. Among the ones to be considered, we
could mention the analysis of the building and its surroundings, on-site
inspection of the building, structural diagnosis, definition of the corrective
actions to be taken, definition of treatments, quality control and a
maintenance plan. For the on-site inspection of the building, in the paper we
will highlight the use of Non-Destructive methods such as resistograph
drilling, X-ray imaging, ultrasound-based testing or moisture measurement. We
will provide practical examples of all this. The aim of this paper is to give
the audience an overall idea on how a pre-assessment work can enhance the
refurbishment of a wood structure building while reducing costs and
environmental impact.
"
1036,Introducing E-maintenance 2.0,"  While research literature is still debating e-maintenance definition, a new
reality is emerging in business world confirming the enterprise 2.0 model.
Executives are more and more forced to stop running against current trend
towards social media and instead envisage harnessing its power within the
enterprise. Maintenance can't be an exception for long and has to take
advantage of new opportunities created by social technological innovations. In
this paper, a combination of pure E perspective and 2.0 perspective is proposed
to avoid a lock-in and allow continous evolution of e-maintenance within the
new context of business: A combination of data centric models and people
oriented applications to form a collaborative environment in order to conceive
and achieve global goals of maintenance.New challenges are also to be expected
as to the effecient integration of enterprise 2.0 tools within current
e-maintenance platforms and futher research work is still to be done in this
area.
"
1037,Planar Shielded-Loop Resonators,"  The design and analysis of planar shielded-loop resonators for use in
wireless non-radiative power transfer systems is presented. The difficulties
associated with coaxial shielded-loop resonators for wireless power transfer
are discussed and planar alternatives are proposed. The currents along these
planar structures are analyzed and first-order design equations are presented
in the form of a circuit model. In addition, the planar structures are
simulated and fabricated. Planar shielded-loop resonators are compact and
simple to fabricate. Moreover, they are well-suited for printed circuit board
designs or integrated circuits
"
1038,Deployment of an Innovative Resource Choice Method for Process Planning,"  Designers, process planners and manufacturers naturally consider different
concepts for a same object. The stiffness of production means and the design
specification requirements mark out process planners as responsible of the
coherent integration of all constraints. First, this paper details an
innovative solution of resource choice, applied for aircraft manufacturing
parts. In a second part, key concepts are instanced for the considered
industrial domain. Finally, a digital mock up validates the solution viability
and demonstrates the possibility of an in-process knowledge capitalisation and
use. Formalising the link between Design and Manufacturing allows to hope
enhancements of simultaneous Product / Process developments.
"
1039,Real Time Industrial Monitoring System,"  Industries are the biggest workplace all over the world, also there are large
number of peoples involves as a worker and most of them are work as a machine
operator. There are many systems developed for industrial work place, some of
them, monitors machine processes and some do monitoring and control of machine
parameters. Such as speed, temperature, production batch count etc. However
there is no such system available that provides monitoring of operator during
their work is in progress at workplace. This paper proposes the monitoring of
the operators and the machines, by Real time Operator -Machine Allocation and
monitoring system (Omams). Omams allocates a work machine to worker at entry
point itself. It uses automation with RFID and one of the standards of wireless
communication method. The system can be industry specific. Through this
research paper our approach is to make fair allocation of machine to the
operator in industry and reduce hassle for efficiency calculations.
"
1040,dRTI: Directional Radio Tomographic Imaging,"  Radio tomographic imaging (RTI) enables device free localisation of people
and objects in many challenging environments and situations. Its basic
principle is to detect the changes in the statistics of some radio quality
measurements in order to infer the presence of people and objects in the radio
path. However, the localisation accuracy of RTI suffers from complicated radio
propagation behaviours such as multipath fading and shadowing. In order to
improve RTI localisation accuracy, we propose to use inexpensive and energy
efficient electronically switched directional (ESD) antennas to improve the
quality of radio link behaviour observations, and therefore, the localisation
accuracy of RTI. We implement a directional RTI (dRTI) system to understand how
directional antennas can be used to improve RTI localisation accuracy. We also
study the impact of the choice of antenna directions on the localisation
accuracy of dRTI and propose methods to effectively choose informative antenna
directions to improve localisation accuracy while reducing overhead. We
evaluate the performance of dRTI in diverse indoor environments and show that
dRTI significantly outperforms the existing RTI localisation methods based on
omni-directional antennas.
"
1041,Tasks for Temporal Graph Visualisation,"  In [1], we describe the design and development of a task taxonomy for
temporal graph visualisation. This paper details the full instantiation of that
task taxonomy. Our task taxonomy is based on the Andrienko framework [2], which
uses a systematic approach to develop a formal task framework for visual tasks
specifically associated with Exploratory Data Analysis. The Andrienko framework
is intended to be applicable to all types of data, however, it does not
consider relational (graph) data. We therefore extended both their data model
and task framework for temporal graph data, and instantiated the extended
version to produce a comprehensive list of tasks of interest during exploratory
analysis of temporal graph data. As expected, our instantiation of the
framework resulted in a very large task list; with more than 144 variations of
attribute based tasks alone, it is too large to fit in a standard journal
paper, hence we provide the detailed listing in this document.
"
1042,Evaluating ECG Capturing Using Sound-Card of PC/Laptop,"  The purpose of the Evaluating ECG capturing using sound-card of PC/Laptop is
provided portable and low cost ECG monitoring system using laptop and mobile
phones. There is no need to interface micro controller or any other device to
transmit ECG data. This research is based on hardware design, implementation,
signal capturing and Evaluation of an ECG processing and analyzing system which
attend the physicians in heart disease diagnosis. Some important modification
is given in design part to avoid all definitive ECG instrument problems faced
in previous designs. Moreover, attenuate power frequency noise and noise that
produces from patient's body have required additional developments. The
hardware design has basically three units: transduction and conditioning Unit,
interfacing unit and data processing unit.The most focusing factor is the ECG
signal/data transmits in laptop/PC via microphone pin. The live simulation is
possible using SOUNDSCOPE software in PC/Laptop. The software program that is
written in MATLAB and LAB-View performs data acquisition (record, stored,
filtration) and several tasks such as QRS detection, calculate heart rate.
"
1043,QTC3D: Extending the Qualitative Trajectory Calculus to Three Dimensions,"  Spatial interactions between agents (humans, animals, or machines) carry
information of high value to human or electronic observers. However, not all
the information contained in a pair of continuous trajectories is important and
thus the need for qualitative descriptions of interaction trajectories arises.
The Qualitative Trajectory Calculus (QTC) (Van de Weghe, 2004) is a promising
development towards this goal. Numerous variants of QTC have been proposed in
the past and QTC has been applied towards analyzing various interaction
domains. However, an inherent limitation of those QTC variations that deal with
lateral movements is that they are limited to two-dimensional motion;
therefore, complex three-dimensional interactions, such as those occurring
between flying planes or birds, cannot be captured. Towards that purpose, in
this paper QTC3Dis presented: a novel qualitative trajectory calculus that can
deal with full three-dimensional interactions. QTC3D is based on
transformations of the Frenet-Serret frames accompanying the trajectories of
the moving objects. Apart from the theoretical exposition, including definition
and properties, as well as computational aspects, we also present an
application of QTC3D towards modeling bird flight. Thus, the power of QTC is
now extended to the full dimensionality of physical space, enabling succinct
yet rich representations of spatial interactions between agents.
"
1044,"Control Loop Feedback Mechanism for Generic Array Logic Chip
  Multiprocessor","  Control Loop Feedback Mechanism for Generic Array Logic Chip Multiprocessor
is presented. The approach is based on control-loop feedback mechanism to
maximize the efficiency on exploiting available resources such as CPU time,
operating frequency, etc. Each Processing Element (PE) in the architecture is
equipped with a frequency scaling module responsible for tuning the frequency
of processors at run-time according to the application requirements. We show
that generic array logic Chip Multiprocessors with large inter-processor First
In First Outputs (First In First Outs) buffers can inherently hide much of the
Generic Array Logic performance penalty while executing applications that have
been mapped with few communication loops. In fact, the penalty can be driven to
zero with sufficiently large First In First Outs and the removal of
multiple-loop communication links. We present an example mesh-connected Generic
Array Logic chip multiprocessor and show it has a less than 1% performance
(throughput) reduction on average compared to the corresponding synchronous
system for many DSP workloads. Furthermore, adaptive clock and voltage scaling
for each processor provides an approximately 40% power savings without any
performance reduction.
"
1045,"A Hybrid Estimation of Distribution Algorithm with Random Walk local
  Search for Multi-mode Resource-Constrained Project Scheduling problems","  Multi-mode resource-constrained project scheduling problems (MRCPSPs) are
classified as NP-hard problems, in which a task has different execution modes
characterized by different resource requirements. Estimation of distribution
algorithm (EDA) has shown an effective performance for solving such real-world
optimization problems but it fails to find the desired optima. This paper
integrates a novel hybrid local search technique with EDA to enhance their
local search ability. The new local search is based on delete-then-insert
operator and a random walk (DIRW) to enhance exploitation abilities of EDA in
the neighborhoods of the search space. The proposed algorithm is capable to
explore and exploit the search mechanism in the search space through its outer
and inner loops. The proposed algorithm is tested and evaluated using benchmark
test problems of the project scheduling problem library PSPLIB. Simulation
results of the proposed algorithm are compared with the classical EDA
algorithm. The obtained results showed that the effectiveness of the proposed
algorithm and outperformed the compared EDA algorithm.
"
1046,Recommendation System for Outfit Selection (RSOS),"  We propose a system which will be able to recommend the user to choose
appropriate outfits suits to their personality. The necessity of this system is
to reduce the outfit selection and purchasing time; this will also help to
create tailor made outfits as per the personality traits. The guidelines for
selection of their respective outfits are based upon various bodily parameters
that evolve with the learning of available labeled and unlabeled data. The
system is based on two modules of processes; first one is to recognize the
features for usage of outfits like traditional, western, functional, daytime or
night etc, second is to calculate the body measurement parameters. The proposed
system will have image capturing by using HAAR feature or input device for
getting body parameters. We intend to classify and extract the best possible
outfits from the system by using HIGEN MINER algorithm. The applications of
outfit selection will be ranging from manual gender selection, image processing
with body feature extractions, Value comparison with database by using
different statistical techniques and data mining algorithms. After that it will
recommend best outfits as per body parameters, inputs and availability
"
1047,Complex Beauty,"  Complex systems and their underlying convoluted networks are ubiquitous, all
we need is an eye for them. They pose problems of organized complexity which
cannot be approached with a reductionist method. Complexity science and its
emergent sister network science both come to grips with the inherent complexity
of complex systems with an holistic strategy. The relevance of complexity,
however, transcends the sciences. Complex systems and networks are the focal
point of a philosophical, cultural and artistic turn of our tightly
interrelated and interdependent postmodern society. Here I take a different,
aesthetic perspective on complexity. I argue that complex systems can be
beautiful and can the object of artification - the neologism refers to
processes in which something that is not regarded as art in the traditional
sense of the word is changed into art. Complex systems and networks are
powerful sources of inspiration for the generative designer, for the artful
data visualizer, as well as for the traditional artist. I finally discuss the
benefits of a cross-fertilization between science and art.
"
1048,Three Experiments to Analyze the Nature of the Heat Spreader,"  In this paper, we describe ongoing work to investigate the properties of the
heat spreader, and its implication on architecture research. In specific, we
conduct two experiments to quantify the heat distribution across the surface of
a spreader during normal operation. The first experiment uses T-type
thermocouples, to find the temperature difference across different points on
the spreader. We observe about a 6 degree celsius temperature difference on
average. In the second experiment, we try to capture the temperature gradients
using an infrared camera. However, this experiment was inconclusive because of
some practical constraints such as the low emissivity of the spreader. We
conclude that to properly model the spreader, it is necessary to conduct
detailed finite element simulations. We describe a method to accurately measure
the thermal conductivity of the heat spreader such that it can be used to
compute the steady state temperature distribution across the spreader.
"
1049,"A Three-State Received Signal Strength Model for Device-free
  Localization","  The indoor radio propagation channel is typically modeled as a two-state
time-variant process where one of the states represents the channel when the
environment is static, whereas the other state characterizes the medium when it
is altered by people. In this paper, the aforementioned process is augmented
with an additional state. It is shown that the changes in received signal
strength are dictated by: i) electronic noise, when a person is not present in
the monitored area; ii) reflection, when a person is moving in the close
vicinity of the line-of-sight; iii) shadowing, when a person is obstructing the
line-of-sight component of the transmitter-receiver pair. Statistical and
spatial models for the three states are derived and the models are empirically
validated. Based on the models, a simplistic device-free localization
application is designed which aims to: first, estimate the temporal state of
the channel using a hidden Markov model; second, track a person using a
particle filter. The results suggest that the tracking accuracy is enhanced by
at least 65% while the link's sensitivity region is increased by 100% or more
with respect to empirical models presented in earlier works.
"
1050,"On the Behavioral Interpretation of System-Environment Fit and
  Auto-Resilience","  Already 71 years ago Rosenblueth, Wiener, and Bigelow introduced the concept
of the ""behavioristic study of natural events"" and proposed a classification of
systems according to the quality of the behaviors they are able to exercise. In
this paper we consider the problem of the resilience of a system when deployed
in a changing environment, which we tackle by considering the behaviors both
the system organs and the environment mutually exercise. We then introduce a
partial order and a metric space for those behaviors, and we use them to define
a behavioral interpretation of the concept of system-environment fit. Moreover
we suggest that behaviors based on the extrapolation of future environmental
requirements would allow systems to proactively improve their own
system-environment fit and optimally evolve their resilience. Finally we
describe how we plan to express a complex optimization strategy in terms of the
concepts introduced in this paper.
"
1051,"Frequency-Shift Filtering for OFDM Signal Recovery in Narrowband Power
  Line Communications","  Power line communications (PLC) has been drawing considerable interest in
recent years due to the growing interest in smart grid implementation.
Specifically, network control and grid applications are allocated the frequency
band of 0-500 kHz, commonly referred to as the narrowband PLC channel. This
frequency band is characterized by strong periodic noise which results in low
signal to noise ratio (SNR). In this work we propose a receiver which uses
frequency shift filtering to exploit the cyclostationary properties of both the
narrowband power line noise, as well as the information signal, digitally
modulated using orthogonal frequency division multiplexing. An adaptive
implementation for the proposed receiver is presented as well. The proposed
receiver is compared to existing receivers via analysis and simulation. The
results show that the receiver proposed in this work obtains a substantial
performance gain over previously proposed receivers, without requiring any
coordination with the transmitter.
"
1052,"The Unambiguous Distance in a Phase-based Ranging System with Hopping
  Frequencies","  It is a challenge to specify unambiguous distance (UD) in a phase-based
ranging system with hopping frequencies (PRSHF). In this letter, we propose to
characterize the UD in a PRSHF by the probability that it takes on its maximum
value. We obtain a very simple and elegant expression of the probability with
growth estimation techniques from analytic number theory. It is revealed that
the UD in a PRSHF usually takes on the maximum value with as few as 10
frequencies in measurement, almost independent of the specific distribution of
available bandwidth.
"
1053,"ARM 7 Based Controller Area Network for Accident Avoidance in
  Automobiles","  Based on requirements of modern vehicle, in- vehicle Controller Area Network
(CAN) architecture has been implemented. In order to reduce point to point
wiring harness in vehicle automation, CAN is suggested as a means for data
communication within the vehicle environment. The benefits of CAN bus based
network over traditional point to point schemes will offer increased
flexibility and expandability for future technology insertions.
  This paper describes system which uses sensors to measure various parameters
of the car like speed, distance from the other car, presence of alcohol in car
and accidental change of lane and sends a warning signal to the driver if any
of the parameter goes out of range to avoid accidents . In addition to this if
accident occurs in any remote area then using bump sensor accident is detected
and SMS is send immediately using GSM. A situation that provides a good example
of how the system works is when a driver is about to change lanes, and there is
a car in his blind spot. The sensors will detect that car and inform the driver
before he starts turning, preventing him from potentially getting into a
serious accident.
"
1054,Distributed Transformer Monitoring System Based On Zigbee Technology,"  A Distributed transformer networks remote monitoring system is developed and
constructed,for monitor and record the parameters like temperature, oil level
status, of a distribution transformer.The system consists of a micro controller
based circuit,with solid-state components for handling sensors,power
back-up,real time clock and data communication module which based on ZigBee
protocol.
"
1055,"Behavior, Organization, Substance: Three Gestalts of General Systems
  Theory","  The term gestalt, when used in the context of general systems theory, assumes
the value of ""systemic touchstone"", namely a figure of reference used to
categorize the properties or qualities of a set of systems. Typical gestalts
used in biology are those based on anatomical or physiological characteristics,
which correspond respectively to architectural and organizational design
choices in natural and artificial systems. In this paper we discuss three
gestalts of general systems theory: behavior, organization, and substance,
which refer respectively to the works of Wiener, Boulding, and Leibniz. Our
major focus here is the system introduced by the latter. Through a discussion
of some of the elements of the Leibnitian System, and by means of several novel
interpretations of those elements in terms of today's computer science, we
highlight the debt that contemporary research still has with this Giant among
the giant scholars of the past.
"
1056,A Quantitative Approach to Painting Styles,"  This research extends a method previously applied to music and
philosophy,representing the evolution of art as a time-series where relations
like dialectics are measured quantitatively. For that, a corpus of paintings of
12 well-known artists from baroque and modern art is analyzed. A set of 93
features is extracted and the features which most contributed to the
classification of painters are selected. The projection space obtained provides
the basis to the analysis of measurements. This quantitative measures underlie
revealing observations about the evolution of painting styles, specially when
compared with other humanity fields already analyzed: while music evolved along
a master-apprentice tradition (high dialectics) and philosophy by opposition,
painting presents another pattern: constant increasing skewness, low opposition
between members of the same movement and opposition peaks in the transition
between movements. Differences between baroque and modern movements are also
observed in the projected ""painting space"": while baroque paintings are
presented as an overlapped cluster, the modern paintings present minor
overlapping and are disposed more widely in the projection than the baroque
counterparts. This finding suggests that baroque painters shared aesthetics
while modern painters tend to ""break rules"" and develop their own style.
"
1057,"Load flow analysis of radial distribution network using linear data
  structure","  Distribution systems hold a very significant position in the power system
since it is the main point of link between bulk power and consumers. A planned
and effective distribution network is the key to cope up with the ever
increasing demand for domestic, industrial and commercial load. The load-flow
study of radial distribution network is of prime importance for effective
planning of load transfers. Power companies are interested in finding the most
efficient configuration for minimization of real power loses and load balancing
among distribution feeders to save energy and enhance the over all performance
of distribution system.
  This thesis presents a fast and efficient method for load-flow analysis of
radial distribution networks. The proposed method is based on linear data
structure. The order of time and space complexity is reported here. There is
significant saving in no. of steps execution. Using graph theory concept and
exploiting multi-cores architecture, the proposed method for load flow can be
further investigated for obtaining more optimized solutions.
"
1058,Mapping parcel-level urban areas for a large geographical area,"  As a vital indicator for measuring urban development, urban areas are
expected to be identified explicitly and conveniently with widely available
dataset thereby benefiting the planning decisions and relevant urban studies.
Existing approaches to identify urban areas normally based on mid-resolution
sensing dataset, socioeconomic information (e.g. population density) generally
associate with low-resolution in space, e.g. cells with several square
kilometers or even larger towns/wards. Yet, few of them pay attention to
defining urban areas with micro data in a fine-scaled manner with large extend
scale by incorporating the morphological and functional characteristics. This
paper investigates an automated framework to delineate urban areas in the
parcel level, using increasingly available ordnance surveys for generating all
parcels (or geo-units) and ubiquitous points of interest (POIs) for inferring
density of each parcel. A vector cellular automata model was adopted for
identifying urban parcels from all generated parcels, taking into account
density, neighborhood condition, and other spatial variables of each parcel. We
applied this approach for mapping urban areas of all 654 Chinese cities and
compared them with those interpreted from mid-resolution remote sensing images
and inferred by population density and road intersections. Our proposed
framework is proved to be more straight-forward, time-saving and fine-scaled,
compared with other existing ones, and reclaim the need for consistency,
efficiency and availability in defining urban areas with well-consideration of
omnipresent spatial and functional factors across cities.
"
1059,"Marine Buoy Location Finding and Tracking System for Linux Supporting
  Mobiles","  Marine buoy is an important part of underwater acoustic communication system.
It is of great significance to track and locate it. It is widely used in ocean
environment three - dimensional monitoring, underwater multimedia
communication, underwater mobile carrier navigation and positioning, marine
resources detection, remote control of submarine topography mapping and
offshore oil industry, data acquisition, etc. This paper describes the
application of the monitoring service of GPRS / GPS module at Marine buoy. It
can achieve real - time location of underwater acoustic communication devices
and route tracking to avoid the loss of the device, as well as assist to
retrieve the lost device.
"
1060,"Multiple-Population Moment Estimation: Exploiting Inter-Population
  Correlation for Efficient Moment Estimation in Analog/Mixed-Signal Validation","  Moment estimation is an important problem during circuit validation, in both
pre-Silicon and post-Silicon stages. From the estimated moments, the
probability of failure and parametric yield can be estimated at each circuit
configuration and corner, and these metrics are used for design optimization
and making product qualification decisions. The problem is especially difficult
if only a very small sample size is allowed for measurement or simulation, as
is the case for complex analog/mixed-signal circuits. In this paper, we propose
an efficient moment estimation method, called Multiple-Population Moment
Estimation (MPME), that significantly improves estimation accuracy under small
sample size. The key idea is to leverage the data collected under different
corners/configurations to improve the accuracy of moment estimation at each
individual corner/configuration. Mathematically, we employ the hierarchical
Bayesian framework to exploit the underlying correlation in the data. We apply
the proposed method to several datasets including post-silicon measurements of
a commercial high-speed I/O link, and demonstrate an average error reduction of
up to 2$\times$, which can be equivalently translated to significant reduction
of validation time and cost.
"
1061,"Development of Wearable Systems for Ubiquitous Healthcare Service
  Provisioning","  This paper reports on the development of a wearable system using wireless
biomedical sensors for ubiquitous healthcare service provisioning. The
prototype system is developed to address current healthcare challenges such as
increasing cost of services, inability to access diverse services, low quality
services and increasing population of elderly as experienced globally. The
biomedical sensors proactively collect physiological data of remote patients to
recommend diagnostic services. The prototype system is designed to monitor
oxygen saturation level (SpO2), Heart Rate (HR), activity and location of the
elderly. Physiological data collected are uploaded to a Health Server (HS) via
GPRS/Internet for analysis.
"
1062,"The UK-DALE dataset, domestic appliance-level electricity demand and
  whole-house demand from five UK homes","  Many countries are rolling out smart electricity meters. These measure a
home's total power demand. However, research into consumer behaviour suggests
that consumers are best able to improve their energy efficiency when provided
with itemised, appliance-by-appliance consumption information. Energy
disaggregation is a computational technique for estimating
appliance-by-appliance energy consumption from a whole-house meter signal.
  To conduct research on disaggregation algorithms, researchers require data
describing not just the aggregate demand per building but also the `ground
truth' demand of individual appliances. In this context, we present UK-DALE: an
open-access dataset from the UK recording Domestic Appliance-Level Electricity
at a sample rate of 16 kHz for the whole-house and at 1/6 Hz for individual
appliances. This is the first open access UK dataset at this temporal
resolution. We recorded from five houses, one of which was recorded for 655
days, the longest duration we are aware of for any energy dataset at this
sample rate. We also describe the low-cost, open-source, wireless system we
built for collecting our dataset.
"
1063,Design of Reversible Counter,"  This article presents a research work on the design and synthesis of
sequential circuits and flip-flops that are available in digital arena; and
describes a new synthesis design of reversible counter that is optimized in
terms of quantum cost, delay and garbage outputs compared to the existing
designs. We proposed a new model of reversible T flip-flop in designing
reversible counter
"
1064,Steerable Antennas for Automotive Communication Systems,"  This research project undertakes a comprehensive analysis of RF beamforming
techniques for design, simulation, fabrication, and measurement of Butler
Matrix and Rotman Lens beamforming networks. It is aimed to develop novel and
well-established designs for steerable antenna systems that can be used in
vehicular telematics and automotive communication systems based on microwave
and millimeter-wave techniques.
"
1065,"Visual and Predictive Analytics on Singapore News: Experiments on GDELT,
  Wikipedia, and ^STI","  The open-source Global Database of Events, Language, and Tone (GDELT) is the
most comprehensive and updated Big Data source of important terms extracted
from international news articles . We focus only on GDELT's Singapore events to
better understand the data quality of its news articles, accuracy of its term
extraction, and potential for prediction. To test news completeness and
validity, we visually compared GDELT (Singapore news articles' terms from 1979
to 2013) to Wikipedia's timeline of Singaporean history. To test term
extraction accuracy, we visually compared GDELT (CAMEO codes and TABARI system
of extraction from Singapore news articles' text from April to December 2013)
to SAS Text Miner's term and topic extraction. To perform predictive analytics,
we propose a novel feature engineering method to transform row-level GDELT from
articles to a user-specified temporal resolution. For example, we apply a
decision tree using daily counts of feature values from GDELT to predict
Singapore stock market's Straits Times Index (^STI). Of practical interest from
the above results is SAS Visual Analytics' ability to highlight the various
impacts of June 2013 Southeast Asian haze and December 2013 Little India riot
on Singapore. Although Singapore is unique as a sovereign city-state, a leading
financial centre, has strong international influence, and consists of a highly
multi-cultural population, the visual and predictive analytics reported here
are highly applicable to another country's GDELT data.
"
1066,Apple IOS Devices for Network Administrators,"  As tablet devices continue to gain market share at the expense of the
traditional PC, they become a more integral part of the corporate landscape.
Tablets are no longer being utilized only by sales executives for presentation
purposes, or as addition to the traditional laptop. Users are attempting to
perform significant amounts of their daily work on tablet devices, some even
abandoning the ubiquitous laptop or desktop entirely. Operating exclusively
from a tablet device, specifically Apple IOS tablet devices creates unique
challenges in a corporate environment traditionally dominated by Microsoft
Windows operating systems. Interactions with file shares, presentation media,
VPN, and remote access present barriers that users and helpdesk support are
unfamiliar with in a relation to an iPad or iPhone. Many solutions are being
offered to these challenges some of which are analyzed by this manuscript.
"
1067,Teaching Network Storage Technology Assessment Outcomes and Directions,"  The paper presents academic content, delivery and assessment mechanisms used,
available resources including initial lessons from teaching Networked Storage
Technology as a special topics course to students enrolled in two specific
programs - IT and CS. The course is based on the EMC s vendor-neutral Storage
Technology Fundamentals course. Furthermore, this manuscript provides a
detailed review of how the course fits into our curriculum, particularly, how
it helps achieving the 2008 ABET assessment requirements.
"
1068,Automation Security,"  Web-based Automated Process Control systems are a new type of applications
that use the Internet to control industrial processes with the access to the
real-time data. Supervisory control and data acquisition (SCADA) networks
contain computers and applications that perform key functions in providing
essential services and commodities (e.g., electricity, natural gas, gasoline,
water, waste treatment, transportation) to all Americans. As such, they are
part of the nation s critical infrastructure and require protection from a
variety of threats that exist in cyber space today.
"
1069,"Energy and Latency Aware Application Mapping Algorithm & Optimization
  for Homogeneous 3D Network on Chip","  Energy efficiency is one of the most critical issue in design of System on
Chip. In Network On Chip (NoC) based system, energy consumption is influenced
dramatically by mapping of Intellectual Property (IP) which affect the
performance of the system. In this paper we test the antecedently extant
proposed algorithms and introduced a new energy proficient algorithm stand for
3D NoC architecture. In addition a hybrid method has also been implemented
using bioinspired optimization (particle swarm optimization) technique. The
proposed algorithm has been implemented and evaluated on randomly generated
benchmark and real life application such as MMS, Telecom and VOPD. The
algorithm has also been tested with the E3S benchmark and has been compared
with the existing algorithm (spiral and crinkle) and has shown better reduction
in the communication energy consumption and shows improvement in the
performance of the system. Comparing our work with spiral and crinkle,
experimental result shows that the average reduction in communication energy
consumption is 19% with spiral and 17% with crinkle mapping algorithms, while
reduction in communication cost is 24% and 21% whereas reduction in latency is
of 24% and 22% with spiral and crinkle. Optimizing our work and the existing
methods using bio-inspired technique and having the comparison among them an
average energy reduction is found to be of 18% and 24%.
"
1070,"Substrate integrated waveguide power divider, circulator and coupler in
  [10-15] GHz band","  The Substrate Integrated Waveguide (SIW) technology is an attractive approach
for the design of high performance microwave and millimeter wave components, as
it combines the advantages of planar technology, such as low fabrication costs,
with the low loss inherent to the waveguide solution. In this study, a
substrate integrated waveguide power divider, circulator and coupler are
conceived and optimized in [10-15] GHz band by Ansoft HFSS code. Thus, results
of this modeling are presented, discussed and allow to integrate these devices
in planar circuits.
"
1071,"Elevation Contour Analysis and Water body Extraction for Finding Water
  Scarcity Locations using DEM","  The present study was aimed to create new methods for extraction and analysis
of land elevation contour lines, automatic extraction of water bodies (river
basins and lakes), from the digital elevation models (DEM) of a test area. And
extraction of villages which are fell under critical water scarcity regions for
agriculture and drinking water with respect to their elevation data and
available natural water resources.
"
1072,"Computer Simulation Codes for the Quine-McCluskey Method of Logic
  Minimization","  The Quine-McCluskey method is useful in minimizing logic expressions for
larger number of variables when compared with minimization by Karnaugh Map or
Boolean algebra. In this paper, we have tried to put together all of the
computer codes which are available on the internet, edited and modified them as
well as rewritten some parts of those collected codes our self, which are used
in the implementation of the Quine- McCluskey method. A brief introduction and
the logic of this method are discussed following which the codes have been
provided. The Quine-McCluskey Method has been implemented using computer
languages like C and C++ using some amount of variations. Our effort is to list
them all, so that the readers well versed in any of the particular computer
language will find it easy to follow the code written in that particular
language.
"
1073,"Invisibility System Using Image Processing and Optical Camouflage
  Technology","  Invisible persons are seen in fiction stories only, but in the real world it
is proved that invisibility is possible. This paper describes the creation of
invisibility with the help of technologies like Optical camouflage; Image based
rendering and Retro reflective projection. The object that needs to be made
transparent or invisible is painted or covered with retro reflective material.
Then a projector projects the background image on it making the masking object
virtually transparent. Capturing the background image requires a video camera,
which sits behind the person wearing the cloak. The video from the camera must
be in a digital format so it can be sent to a computer for image processing
using image based rendering technical. There are some useful applications for
this simple but astonishing technology.
"
1074,Big Data: Overview,"  Big data is data that exceeds the processing capacity of traditional
databases. The data is too big to be processed by a single machine. New and
innovative methods are required to process and store such large volumes of
data. This paper provides an overview on big data, its importance in our live
and some technologies to handle big data.
"
1075,Web Mining using Artificial Ant Colonies: A Survey,"  Web mining has been very crucial to any organization as it provides useful
insights to business patterns. It helps the company to understand its customers
better. As the web is growing in pace, so is its importance and hence it
becomes all the more necessary to find useful patterns. Here in this paper, web
mining using ant colony optimization has been reviewed with some of its
experimental results.
"
1076,Journey from Data Mining to Web Mining to Big Data,"  This paper describes the journey of big data starting from data mining to web
mining to big data. It discusses each of this method in brief and also provides
their applications. It states the importance of mining big data today using
fast and novel approaches.
"
1077,Trust Evaluation using an Improved Context Similarity Measurement,"  In context-aware trust evaluation, using ontology tree is a popular approach
to represent the relation between contexts. Usually, similarity between two
contexts is computed using these trees. Therefore, the performance of trust
evaluation highly depends on the quality of ontology trees. Fairness or
granularity consistency is one of the major limitations affecting the quality
of ontology tree. This limitation refers to inequality of semantic similarity
in the most ontology trees. In other words, semantic similarity of every two
adjacent nodes is unequal in these trees. It deteriorates the performance of
contexts similarity computation. We overcome this limitation by weighting tree
edges based on their semantic similarity. Weight of each edge is computed using
Normalized Similarity Score (NSS) method. This method is based on frequencies
of concepts (words) co-occurrences in the pages indexed by search engines. Our
experiments represent the better performance of the proposed approach in
comparison with established trust evaluation approaches. The suggested approach
can enhance efficiency of any solution which models semantic relations by
ontology tree.
"
1078,"Training-Free Non-Intrusive Load Monitoring of Electric Vehicle Charging
  with Low Sampling Rate","  Non-intrusive load monitoring (NILM) is an important topic in smart-grid and
smart-home. Many energy disaggregation algorithms have been proposed to detect
various individual appliances from one aggregated signal observation. However,
few works studied the energy disaggregation of plug-in electric vehicle (EV)
charging in the residential environment since EVs charging at home has emerged
only recently. Recent studies showed that EV charging has a large impact on
smart-grid especially in summer. Therefore, EV charging monitoring has become a
more important and urgent missing piece in energy disaggregation. In this
paper, we present a novel method to disaggregate EV charging signals from
aggregated real power signals. The proposed method can effectively mitigate
interference coming from air-conditioner (AC), enabling accurate EV charging
detection and energy estimation under the presence of AC power signals.
Besides, the proposed algorithm requires no training, demands a light
computational load, delivers high estimation accuracy, and works well for data
recorded at the low sampling rate 1/60 Hz. When the algorithm is tested on
real-world data recorded from 11 houses over about a whole year (total 125
months worth of data), the averaged error in estimating energy consumption of
EV charging is 15.7 kwh/month (while the true averaged energy consumption of EV
charging is 208.5 kwh/month), and the averaged normalized mean square error in
disaggregating EV charging load signals is 0.19.
"
1079,"Design of a Low Voltage Class-AB CMOS Super Buffer Amplifier with Sub
  Threshold and Leakage Control","  This paper describes a CMOS analogy voltage supper buffer designed to have
extremely low static current Consumption as well as high current drive
capability. A new technique is used to reduce the leakage power of class-AB
CMOS buffer circuits without affecting dynamic power dissipation. The name of
applied technique is TRANSISTOR GATING TECHNIQUE, which gives the high speed
buffer with the reduced low power dissipation (1.105%), low leakage and reduced
area (3.08%) also. The proposed buffer is simulated at 45nm CMOS technology and
the circuit is operated at 3.3V supply[11]. Consumption is comparable to the
switching component. Reports indicate that 40% or even higher percentage of the
total power consumption is due to the leakage of transistors. This percentage
will increase with technology scaling unless effective techniques are
introduced to bring leakage under control. This article focuses on circuit
optimization and Design automation techniques to accomplish this goal [9].
"
1080,Design and considerations of ADC0808 as interleaved ADCs,"  Here in this paper we are presenting a digital system background technique
for correcting the time offset error rate and gain mismatches in a time
interleaved analog to digital converter system for N channel communication
using 8 bit ADC0808 ICs. A time interleaved A to D converter system is an
effective way to implement a high sampling rate ADC with relatively slow
circuits. This paper analyses the benefits and derives an upper band on the
performance by considering kT/C noise and slewing requirement of the circuit
driving the system. In the system, several channel ADCs operate at interleaved
sampling times as if they were effectively a single ADC operating at a much
higher sampling rate. A timing mismatch calibration technique is proposed that
covers linear and non linear channel mismatches, unifies, and extends the
channel models. A novel foreground channel mismatch identification method has
been developed, which can be used to fully characterize dynamic linear
mismatches. A background identification method provides accurate timing
mismatch estimates.
"
1081,Big Spectrum Data: The New Resource for Cognitive Wireless Networking,"  The era of Big Data is here now, which has brought both unprecedented
opportunities and critical challenges. In this article, from a perspective of
cognitive wireless networking, we start with a definition of Big Spectrum Data
by analyzing its characteristics in terms of six Vs, i.e., volume, variety,
velocity, veracity, viability, and value. We then present a high-level tutorial
on research frontiers in Big Spectrum Data analytics to guide the development
of practical algorithms. We also highlight Big Spectrum Data as the new
resource for cognitive wireless networking by presenting the emerging use
cases.
"
1082,Implementation of Sensor Network using Efficient CAN Interface,"  Sensors monitored by centralized system, that may be used for controlling and
monitoring industrial parameters (Temp, Pressure, Speed, Torque) by using CAN
interface. In this paper we presents a comprehensive overview of controller
area networks, their architecture, protocol, and standards. Also, this paper
gives an overview of CAN applications, in both the industrial and nonindustrial
fields. Due to CAN reliability, efficiency and robustness, we also propose the
extension of CAN applications to sensor network. In this paper, a framework of
sensor network for monitoring industrial parameters is explained where sensors
are physically distributed and CAN is used to exchange system information. CAN
(Controller Area Network) is a high integrity serial bus protocol that is
designed to operate at high speeds ranging from 20kbit/s to 1Mbit/s which
provide an efficient, reliable and very economical link
"
1083,Data Driven Energy Efficiency in Buildings,"  Buildings across the world contribute significantly to the overall energy
consumption and are thus stakeholders in grid operations. Towards the
development of a smart grid, utilities and governments across the world are
encouraging smart meter deployments. High resolution (often at every 15
minutes) data from these smart meters can be used to understand and optimize
energy consumptions in buildings. In addition to smart meters, buildings are
also increasingly managed with Building Management Systems (BMS) which control
different sub-systems such as lighting and heating, ventilation, and air
conditioning (HVAC). With the advent of these smart meters, increased usage of
BMS and easy availability and widespread installation of ambient sensors, there
is a deluge of building energy data. This data has been leveraged for a variety
of applications such as demand response, appliance fault detection and
optimizing HVAC schedules. Beyond the traditional use of such data sets, they
can be put to effective use towards making buildings smarter and hence driving
every possible bit of energy efficiency. Effective use of this data entails
several critical areas from sensing to decision making and participatory
involvement of occupants. Picking from wide literature in building energy
efficiency, we identify five crust areas (also referred to as 5 Is) for
realizing data driven energy efficiency in buildings : i) instrument optimally;
ii) interconnect sub-systems; iii) inferred decision making; iv) involve
occupants and v) intelligent operations. We classify prior work as per these 5
Is and dis-cuss challenges, opportunities and applications across them.
Building upon these 5 Is we discuss a well studied problem in building energy
efficiency -non-intrusive load monitoring (NILM) and how research in this area
spans across the 5 Is.
"
1084,"Estimation of Optimized Energy and Latency Constraints for Task
  Allocation in 3d Network on Chip","  In Network on Chip (NoC) rooted system, energy consumption is affected by
task scheduling and allocation schemes which affect the performance of the
system. In this paper we test the pre-existing proposed algorithms and
introduced a new energy skilled algorithm for 3D NoC architecture. An efficient
dynamic and cluster approaches are proposed along with the optimization using
bio-inspired algorithm. The proposed algorithm has been implemented and
evaluated on randomly generated benchmark and real life application such as
MMS, Telecom and VOPD. The algorithm has also been tested with the E3S
benchmark and has been compared with the existing mapping algorithm spiral and
crinkle and has shown better reduction in the communication energy consumption
and shows improvement in the performance of the system. On performing
experimental analysis of proposed algorithm results shows that average
reduction in energy consumption is 49%, reduction in communication cost is 48%
and average latency is 34%. Cluster based approach is mapped onto NoC using
Dynamic Diagonal Mapping (DDMap), Crinkle and Spiral algorithms and found DDmap
provides improved result. On analysis and comparison of mapping of cluster
using DDmap approach the average energy reduction is 14% and 9% with crinkle
and spiral.
"
1085,"Coal Blending: Business Value, Analysis, and Optimization","  Coal blending is a critically important process in the coal mining industry
as it directly influences the number of product tonnes and the total revenue
generated by a mine site. Coal blending represents a challenging and complex
problem with numerous blending possibilities, multiple constraints and
competing objectives. At many mine sites, blending decisions are made using
heuristics that have been developed through experience or made by using
computer assisted control algorithms or linear programming. While current
blending procedures have achieved profitable outcomes in the past, they often
result in a sub-optimal utilization of high quality coal. This sub-optimality
has a considerable negative impact on mine site productivity as it can reduce
the amount of lower quality ROM that is blended and sold. This article reviews
the coal blending problem and discusses some of the difficult trade-offs and
challenges that arise in trying to address this problem. We highlight some of
the risks from making simplifying assumptions and the limitations of current
software optimization systems. We conclude by explaining how the mining
industry would significantly benefit from research and development into
optimization algorithms and technologies that are better able to combine
computer optimization algorithm capabilities with the important insights of
engineers and quality control specialists.
"
1086,"Rapture in the Cartesian Wall between Real World Entities and their
  Abstract Models","  This short paper envisages that the advancements made with respect to Big
Data (BD), High Performance Computing, etc. would give rise to a new paradigm
of concrete information models, which would closely replicate the real world
and the consequences such as self-verifying information models, BD warehouses
as intermediaries between data sources and information systems, etc.
"
1087,"Use of ARAS 360 to Facilitate Rapid Development of Articulated Total
  Body Biomechanical Physics Simulations","  The development of 3-dimensional environments to be used within a
biomechanical physics simulation framework, such as Articulated Total Body, can
be laborious and time intensive. This brief article demonstrates how the ARAS
360 software package can aid the user by speeding up development time.
"
1088,A New Multi-Tiered Solid State Disk Using Slc/Mlc Combined Flash Memory,"  Storing digital information, ensuring the accuracy, steady and uninterrupted
access to the data are considered as fundamental challenges in enterprise-class
organizations and companies. In recent years, new types of storage systems such
as solid state disks (SSD) have been introduced. Unlike hard disks that have
mechanical structure, SSDs are based on flash memory and thus have electronic
structure. Generally a SSD consists of a number of flash memory chips, some
buffers of the volatile memory type, and an embedded microprocessor, which have
been interconnected by a port. This microprocessor run a small file system
which called flash translation layer (FTL). This software controls and
schedules buffers, data transfers and all flash memory tasks. SSDs have some
advantages over hard disks such as high speed, low energy consumption, lower
heat and noise, resistance against damage, and smaller size. Besides, some
disadvantages such as limited endurance and high price are still challenging.
In this study, the effort is to combine two common technologies - SLC and MLC
chips - used in the manufacture of SSDs in a single SSD to decrease the side
effects of current SSDs. The idea of using multi-layer SSD is regarded as an
efficient solution in this field.
"
1089,"The Eve of 3D Printing in Telemedicine: State of the Art and Future
  Challenges","  3D printing has raised a lot of attention from fields outside the
manufacturing one in the last years. In this paper, we will illustrate some
recent advances of 3D printing technology, applied to the field of telemedicine
and remote patient care. The potentiality of this technology will be detailed
without lab examples. Some crucial aspect such as the regulation of these
devices and the need of some standards will also be discussed. The purpose of
this paper is to present some of the most promising applications of such
technology.
"
1090,Mending the Big-Data Missing Information,"  Consider a high-dimensional data set, in which for every data-point there is
incomplete information. Each object in the data set represents a real entity,
which is described by a point in high-dimensional space. We model the lack of
information for a given object as an affine subspace in $\mathbb{R}^d$ whose
dimension $k$ is the number of missing features.
  Our goal in this study is to find clusters of objects where the main problem
is to cope with partial information and high dimension. Assuming the data set
is separable, namely, its emergence from clusters that can be modeled as a set
of disjoint ball in $\mathbb{R}^d$, we suggest a simple data clustering
algorithm. Our suggested algorithm use the affine subspaces minimum distance
and calculates pair-wise projection of the data achieving poly-logarithmic time
complexity.
  We use probabilistic considerations to prove the algorithm's correctness.
These probabilistic results are of independent interest, and can serve to
better understand the geometry of high dimensional objects.
"
1091,"A Layered Modeling and Simulation Approach to investigate Resource-aware
  Computing in MPSoCs","  Increasing complexity of modern multi-processor system on chip (MPSoC) and
the decreasing feature size have introduced new challenges. System designers
have to consider now aspects which were not part of the design process in past
times. Resource-aware Computing is one of such emerging design concerns which
can help to improve performance, dependability and resource utilization of
overall system. Resource-aware execution takes into account the resource status
when executing tasks on MPSoCs. Exploration of resource-aware computing at
early design stages of complex systems is mandatory and appropriate
methodologies to do this in an efficient manner are thus required. In this
paper, we present a modular approach which provides modeling and simulation
support for investigation of resource-aware execution in MPSoCs. The proposed
methodology enables rapid exploration of the design space by modeling and
simulating the resource-awareness in a separate layer while widely reusing the
legacy system model in the other layer. Our experiments illustrate the benefits
of our approach for the exploration of resource-aware execution on MPSoCs.
"
1092,GREEND: An Energy Consumption Dataset of Households in Italy and Austria,"  Home energy management systems can be used to monitor and optimize
consumption and local production from renewable energy. To assess solutions
before their deployment, researchers and designers of those systems demand for
energy consumption datasets. In this paper, we present the GREEND dataset,
containing detailed power usage information obtained through a measurement
campaign in households in Austria and Italy. We provide a description of
consumption scenarios and discuss design choices for the sensing
infrastructure. Finally, we benchmark the dataset with state-of-the-art
techniques in load disaggregation, occupancy detection and appliance usage
mining.
"
1093,"Recognition and Ranking Critical Success Factors of Business
  Intelligence in Hospitals -- Case Study: Hasheminejad Hospital","  Business Intelligence, not as a tool of a product but as a new approach is
propounded in organizations to make tough decisions in business as shortly as
possible. Hospital managers often need business intelligence in their fiscal,
operational, and clinical reports and indices. The main goal of recognition and
ranking CSF is implementation of a business intelligent system in hospitals to
increase success factor of application of business intelligence in health and
treatment sector. This paper is an application and descriptive-analytical one,
in which we use questionnaires to gather data and we used SPSS and LISREL to
analyze them. Its statistical society is managers and personnel of Hasheminejad
hospital and case studies are selected by Cochran formula. The findings show
that all three organizational, process, and technological factors equally
affect implementation of business intelligence based on Yeoh & Koronis
approach, where the assumptions are based upon it. The proposed model for CSFs
of business intelligence in hospitals include: declaring perspective, goals and
strategies, development of human and financial resources, clarification of
organizational culture, documentation and process mature, management support,
etc. Business intelligence implementation is affected by different components.
Center of Hasheminejad hospital BI system as a leader in providing quality
health care, partially succeeded to take advantage of the benefits the
organization in passing the information revolution but the development of this
system to achieve intelligent hospital and its certainty is a high priority,
thus it can`t be said that the hospital-wide BI system is quite favorable. In
this regard, it can be concluded that Hasheminejad hospital requires practical
model for business intelligence systems development.
"
1094,"Using the Expectation Maximization Algorithm with Heterogeneous Mixture
  Components for the Analysis of Spectrometry Data","  Coupling a multi-capillary column (MCC) with an ion mobility (IM)
spectrometer (IMS) opened a multitude of new application areas for gas
analysis, especially in a medical context, as volatile organic compounds (VOCs)
in exhaled breath can hint at a person's state of health. To obtain a potential
diagnosis from a raw MCC/IMS measurement, several computational steps are
necessary, which so far have required manual interaction, e.g., human
evaluation of discovered peaks. We have recently proposed an automated pipeline
for this task that does not require human intervention during the analysis.
Nevertheless, there is a need for improved methods for each computational step.
In comparison to gas chromatography / mass spectrometry (GC/MS) data, MCC/IMS
data is easier and less expensive to obtain, but peaks are more diffuse and
there is a higher noise level. MCC/IMS measurements can be described as samples
of mixture models (i.e., of convex combinations) of two-dimensional probability
distributions. So we use the expectation-maximization (EM) algorithm to
deconvolute mixtures in order to develop methods that improve data processing
in three computational steps: denoising, baseline correction and peak
clustering. A common theme of these methods is that mixture components within
one model are not homogeneous (e.g., all Gaussian), but of different types.
Evaluation shows that the novel methods outperform the existing ones. We
provide Python software implementing all three methods and make our evaluation
data available at http://www.rahmannlab.de/research/ims.
"
1095,Extended AIGER Format for Synthesis,"  We extend the AIGER format, as used in HWMCC, to a format that is suitable to
define synthesis problems with safety specifications. We recap the original
format and define one format for posing synthesis problems and one for
solutions of synthesis problems in this setting.
"
1096,"Real-Time Estimation of the Distribution of Brake Response Times for an
  Individual Driver Using Vehicular Ad Hoc Network","  Adapting the functioning of the collision warning systems to the specific
drivers' characteristics is of great benefit to drivers. For example, by
customizing collision warning algorithms we can minimize false alarms, thereby
reducing injuries and deaths in highway traffic accidents. In order to take the
behaviors of individual drivers into account, the system needs to have a
Real-Time estimation of the distribution of brake response times for an
individual driver. In this paper, we propose a method for doing this estimation
which is not computationally intensive and can take advantage of the
information contained in all data points.
"
1097,"Revised Version of a JCIT Paper-Comparison of Feature Point Extraction
  Algorithms for Vision Based Autonomous Aerial Refueling","  This is a revised version of our paper published in Journal of Convergence
Information Technology(JCIT): ""Comparison of Feature Point Extraction
Algorithms for Vision Based Autonomous Aerial Refueling"". We corrected some
errors including measurement unit errors, spelling errors and so on. Since the
published papers in JCIT are not allowed to be modified, we submit the revised
version to arXiv.org to make the paper more rigorous and not to confuse other
researchers.
"
1098,Adaptive Minimum-Maximum Exclusive Mean Filter for Impulse Noise Removal,"  Many filters are proposed for impulse noise removal. However, they are hard
to keep excellent denoising performance with high computational efficiency. In
response to this difficulty, this paper presents a novel fast filter, adaptive
minimum-maximum exclusive mean (AMMEM) filter to remove impulse noise. Although
the AMMEM filter is a variety of the maximum-minimum exclusive mean (MMEM)
filter, however, the AMMEM filter inherits the advantages, and overcomes the
drawbacks, compared with the MMEM filter. To increase the various performances
of noise removal, the AMMEM filter uses an adaptive size window, introduces two
flexible factors, projection factor P and detection factor T, and limits the
calculation scope of the AVG. The experimental results show the AMMEM filter
makes a significant improvement in terms of noise detection, image restoration,
and computational efficiency. Even at noise level as high as 95%, the AMMEM
filter still can restore the images with good visual effect.
"
1099,Mobile Application for GBAS Air Traffic Status Unit,"  At present, the Air Traffic Status Unit is a windows PC based application,
which receives the status of ground based augmentation system station over
Ethernet and displays on the screen. The objective of this project is to
convert the PC based Application into Mobile application using Android OS.
"
1100,Odr\v{z}avanje ra\v{c}unarskih sistema,"  Computer hardware and software are resources without which the modern
business of any organization, from manufacturing to services, is impossible.
Not enough attention is being payed to maintenance of computer systems as an
aspect of business. This paper gives some recommendations for the selection of
the computer systems maintenance approach, based on many years of experience
maintaining these systems at the University of Zenica.
"
1101,Cognitive Coordination of Global Service Delivery,"  Formal coordination mechanisms are of growing importance as human-based
service delivery becomes more globalized and informal mechanisms are no longer
effective. Further it is becoming apparent that business environments,
communication among distributed teams, and work performance are all subject to
endogenous and exogenous uncertainty.
  This paper describes a stochastic model of service requests in global service
delivery and then puts forth a cognitive approach for coordination in the face
of uncertainty, based on a perception-action loop and receding horizon control.
Optimization algorithms used are a mix of myopic dynamic programming and
constraint-based programming. The coordination approach described has been
deployed by a globally integrated enterprise in a very large-scale global
delivery system and has been demonstrated to improve work efficiency by 10-15%
as compared to manual planning.
"
1102,VirtuMob : Remote Desktop Virtualization Solution for Smarphones,"  Mobility is an important attribute in todays computing world. Mobile
devices,smartphone and tablet PC are becoming an integral part of human life
because they are most effective and convenient communication tools. This paper
proposes a system to connect and access the desktops of remote computer systems
using an android based Smartphone. Virtual Network Computing based architecture
is used to develop the proposed system. Through a VirtuMob viewer provided on
the users Smartphone, the user will be able to access and manipulate the
desktops of remote computers. Several functionality such as viewing the
desktop, mouse operations, keyboard operations, manipulation of documents can
be performed from the Smartphone. VirtuMob server should be running on the
remote system and it must be attached to a network. VirtuMob Accelerator is
used to process the RFB frames of the desktop, perform Encoding of the frames
and then relay the frames to the viewer over the internet. Several Encoding
techniques are studied and analysed to determine which is best suited for the
proposed system.
"
1103,Optimizing the flash-RAM energy trade-off in deeply embedded systems,"  Deeply embedded systems often have the tightest constraints on energy
consumption, requiring that they consume tiny amounts of current and run on
batteries for years. However, they typically execute code directly from flash,
instead of the more energy efficient RAM. We implement a novel compiler
optimization that exploits the relative efficiency of RAM by statically moving
carefully selected basic blocks from flash to RAM. Our technique uses integer
linear programming, with an energy cost model to select a good set of basic
blocks to place into RAM, without impacting stack or data storage.
  We evaluate our optimization on a common ARM microcontroller and succeed in
reducing the average power consumption by up to 41% and reducing energy
consumption by up to 22%, while increasing execution time. A case study is
presented, where an application executes code then sleeps for a period of time.
For this example we show that our optimization could allow the application to
run on battery for up to 32% longer. We also show that for this scenario the
total application energy can be reduced, even if the optimization increases the
execution time of the code.
"
1104,"A.Q.M.E.I.S.: Air Quality Meteorological and Enviromental Information
  System in Western Macedonia, Hellas","  An operational monitoring, as well as high resolution local-scale
meteorological and air quality forecasting information system for Western
Macedonia, Hellas, has been developed and is operated by the Laboratory of
Atmospheric Pollution and Environmental Physics / TEI Western Macedonia since
2002, continuously improved. In this paper the novelty of information system is
presented, in a dynamic, easily accessible and user-friendly manner. It
consists of a structured system that users have access to and they can
manipulate thoroughly, as well as of a system for accessing and managing
results of measurements in a direct and dynamic way. It provides updates about
the weather and pollution forecast for the next few days (based on current day
information) in Western Macedonia. These forecasts are displayed through
dynamic-interactive web charts and the visual illustration of the atmospheric
pollution of the region in a map using images and animation images.
"
1105,Load Hiding of Household's Power Demand,"  With the development and introduction of smart metering, the energy
information for costumers will change from infrequent manual meter readings to
fine-grained energy consumption data. On the one hand these fine-grained
measurements will lead to an improvement in costumers' energy habits, but on
the other hand the fined-grained data produces information about a household
and also households' inhabitants, which are the basis for many future privacy
issues. To ensure household privacy and smart meter information owned by the
household inhabitants, load hiding techniques were introduced to obfuscate the
load demand visible at the household energy meter. In this work, a
state-of-the-art battery-based load hiding (BLH) technique, which uses a
controllable battery to disguise the power consumption and a novel load hiding
technique called load-based load hiding (LLH) are presented. An LLH system uses
an controllable household appliance to obfuscate the household's power demand.
We evaluate and compare both load hiding techniques on real household data and
show that both techniques can strengthen household privacy but only LLH can
increase appliance level privacy.
"
1106,Faithful Glitch Propagation in Binary Circuit Models,"  Modern digital circuit design relies on fast digital timing simulation tools
and, hence, on accurate binary-valued circuit models that faithfully model
signal propagation, even throughout a complex design. Unfortunately, it was
recently proved [F\""ugger et al., ASYNC'13] that no existing binary-valued
circuit model proposed so far, including the two most commonly used pure and
inertial delay channels, faithfully captures glitch propagation: For the simple
Short-Pulse Filtration (SPF) problem, which is related to a circuit's ability
to suppress a single glitch, we showed that the quite broad class of bounded
single-history channels either contradict the unsolvability of SPF in bounded
time or the solvability of SPF in unbounded time in physical circuits.
  In this paper, we propose a class of binary circuit models that do not suffer
from this deficiency: Like bounded single-history channels, our involution
channels involve delays that may depend on the time of the previous output
transition. Their characteristic property are delay functions which are based
on involutions, i.e., functions that form their own inverse. A concrete example
of such a delay function, which is derived from a generalized first-order
analog circuit model, reveals that this is not an unrealistic assumption. We
prove that, in sharp contrast to what is possible with bounded single-history
channels, SPF cannot be solved in bounded time due to the nonexistence of a
lower bound on the delay of involution channels, whereas it is easy to provide
an unbounded SPF implementation. It hence follows that binary-valued circuit
models based on involution channels allow to solve SPF precisely when this is
possible in physical circuits. To the best of our knowledge, our model is hence
the very first candidate for a model that indeed guarantees faithful glitch
propagation.
"
1107,Optimal Gaussian Filter for Effective Noise Filtering,"  In this paper we show that the knowledge of noise statistics contaminating a
signal can be effectively used to choose an optimal Gaussian filter to
eliminate noise. Very specifically, we show that the additive white Gaussian
noise (AWGN) contaminating a signal can be filtered best by using a Gaussian
filter of specific characteristics. The design of the Gaussian filter bears
relationship with the noise statistics and also some basic information about
the signal. We first derive a relationship between the properties of the
Gaussian filter, noise statistics and the signal and later show through
experiments that this relationship can be used effectively to identify the
optimal Gaussian filter that can effectively filter noise.
"
1108,Integration of Legacy Appliances into Home Energy Management Systems,"  The progressive installation of renewable energy sources requires the
coordination of energy consuming devices. At consumer level, this coordination
can be done by a home energy management system (HEMS). Interoperability issues
need to be solved among smart appliances as well as between smart and
non-smart, i.e., legacy devices. We expect current standardization efforts to
soon provide technologies to design smart appliances in order to cope with the
current interoperability issues. Nevertheless, common electrical devices affect
energy consumption significantly and therefore deserve consideration within
energy management applications. This paper discusses the integration of smart
and legacy devices into a generic system architecture and, subsequently,
elaborates the requirements and components which are necessary to realize such
an architecture including an application of load detection for the
identification of running loads and their integration into existing HEM
systems. We assess the feasibility of such an approach with a case study based
on a measurement campaign on real households. We show how the information of
detected appliances can be extracted in order to create device profiles
allowing for their integration and management within a HEMS.
"
1109,"Simulation of Pedestrian Movements Using Fine Grid Cellular Automata
  Model","  Crowd simulation is used for evacuation and crowd safety inspections, study
of performance in crowd systems and animations. Cellular automata has been
extensively used in modelling the crowd. In regular cellular automata models,
each pedestrian occupies a single cell with the size of a pedestrian body.
Since the space is divided into relatively large cells, the movements of
pedestrians look like the movements of pieces on a chess board. Furthermore,
all pedestrians have the same body size and speed. In this paper, a method
called fine grid cellular automata is proposed in which smaller cells are used
and pedestrian body may occupy several cells. The model allows the use of
different body sizes, shapes and speeds for pedestrian.
  The proposed model is used for simulating movements of pedestrians toward a
target. A typical walkway scenario is used to test and evaluate the model. The
movements of pedestrians are smoother because of the finer grain discretization
of movements and the simulation results match empirical speed-density graphs
with good accuracy.
"
1110,"High Performance Network-on-Chips (NoCs) Design: Performance Modeling,
  Routing Algorithm and Architecture Optimization","  With technology scaling down, hundreds and thousands processing elements
(PEs) can be integrated on a single chip. Network-on-chip (NoC) has been
proposed as an efficient solution to handle this distinctive challenge. In this
thesis, we have explored the high performance NoC design for MPSoC and CMP
structures from the performance modeling in the offline design phase to the
routing algorithm and NoC architecture optimization. More specifically, we
first deal with the issue of how to estimate an NoC design fast and accurately
in the synthesis inner loop. For this purpose, we propose a machine learning
based latency regression model to evaluate the NoC designs with respect to
different configurations. Then, for high performance NoC designs, we tackle one
of the most important problems, i.e., the routing algorithms design. For
avoiding temperature hotspots, a thermal-aware routing algorithm is proposed to
achieve an even temperature profile for application-specific Network-on-chips
(NoCs). For improving the reliability, a routing algorithm to achieve maximum
performance under fault is proposed. Finally, in the architecture level, we
propose two new NoC structures using bi-directional links for the performance
optimization. In particular, we propose a flit-level speedup scheme to enhance
the network-on-chip(NoC) performance utilizing bidirectional channels. We also
propose a flexible NoC architecture which takes advantage of a dynamic
distributed routing algorithm and improves the NoC communication performance
with moderate energy overhead. From the simulation results on both synthetic
traffic and real workload traces, significant performance improvement in terms
of latency and throughput can be achieved.
"
1111,Implementation of Tic-Tac-Toe Game in LabVIEW,"  Tic-Tac-Toe game can be played by two players where the square block (3 x 3)
can be filled with a cross (X) or a circle (O). The game will toggle between
the players by giving the chance for each player to mark their move. When one
of the players make a combination of 3 same markers in a horizontal, vertical
or diagonal line the program will display which player has won, whether X or O.
In this paper, we implement a 3x3 tic-tac-toe game in LabVIEW. The game is
designed so that two players can play tic-tac-toe using LabVIEW software. The
program will contain a display function and a select function to place the
symbol as well as toggle between the symbols allowing each player a turn to
play the game. The program will update after each player makes their move and
check for the conditions of game as it goes on. Overall program works without
any bugs and is able to use
"
1112,"Midiendo la calidad de la informacion gestionada: algunas reflexiones
  conceptuales-metodologicas","  The study, based on a documental classic analysis, presents conceptual and
methodological guidelines concerning the design of methodologies that help to
measure the quality of information that is managed in organizations. It is
described the process of information management and the importance of
implementing quality principles in it. There are exposed the four dimensions of
information quality as part of an indicators integration which characterize the
informational contents. There are defined each of the phases in the
methodological design to evaluate the information. There also are indicated the
implications of this activity for information professionals.
"
1113,Big Models: From Beijing to the whole China,"  This paper propose the concept of big model as a novel research paradigm for
regional and urban studies. Big models are fine-scale regional/urban simulation
models for a large geographical area, and they overcome the trade-off between
simulated scale and spatial unit by tackling both of them at the same time
enabled by emerging big/open data, increasing computation power and matured
regional/urban modeling methods. The concept, characteristics, and potential
applications of big models have been elaborated. We addresse several case
studies to illustrate the progress of research and utilization on big models,
including mapping urban areas for all Chinese cities, performing parcel-level
urban simulation, and several ongoing research projects. Most of these
applications can be adopted across the country, and all of them are focusing on
a fine-scale level, such as a parcel, a block, or a township (sub-district),
which is not the same with the existing studies using conventional models that
are only suitable for a certain single or two cities or regions, or for a
larger area but have to significantly sacrifice the data resolution. It is
expected that big models will mark a promising new era for the urban and
regional study in the age of big data.
"
1114,"Low-Complexity Variable Forgetting Factor Constrained Constant Modulus
  RLS Algorithm for Adaptive Beamforming","  In this paper, a recursive least squares (RLS) based blind adaptive
beamforming algorithm that features a new variable forgetting factor (VFF)
mechanism is presented. The beamformer is designed according to the constrained
constant modulus (CCM) criterion, and the proposed adaptive algorithm operates
in the generalized sidelobe canceler (GSC) structure. A detailed study of its
operating properties is carried out, including a convexity analysis and a mean
squared error (MSE) analysis of its steady-state behavior. The results of
numerical experiments demonstrate that the proposed VFF mechanism achieves a
superior learning and tracking performance compared to other VFF mechanisms.
"
1115,E-Learning Quality Criteria and Aspects,"  As IT grows the impact of new technology reflects in more or less every
field. Education also gets new dimensions with the advancement in IT sector.
Nowadays education is not limited to books and black boards only it gets a new
way i.e. electronic media. Although with e-learning, the education having
broader phenomena, yet it is in budding stage. Quality is a crucial issue for
education as well as e-learning. It is required to serve qualitative and
standardization education. Quality cannot be expressed and set by a simple
definition, since in itself quality is a very abstract notion. The specified
context and the perspectives of users need to be taken into account when
defining quality in e-learning. It is also essential to classify suitable
criteria to address quality.
"
1116,"A Dynamic Simulation-Optimization Model for Adaptive Management of Urban
  Water Distribution System Contamination Threats","  Urban water distribution systems hold a critical and strategic position in
preserving public health and industrial growth. Despite the ubiquity of these
urban systems, aging infrastructure, and increased risk of terrorism, decision
support models for a timely and adaptive contamination emergency response still
remain at an undeveloped stage. Emergency response is characterized as a
progressive, interactive, and adaptive process that involves parallel
activities of processing streaming information and executing response actions.
This study develops a dynamic decision support model that adaptively simulates
the time-varying emergency environment and tracks changing best health
protection response measures at every stage of an emergency in real-time.
Feedback mechanisms between the contaminated network, emergency managers, and
consumers are incorporated in a dynamic simulation model to capture
time-varying characteristics of an emergency environment. An
evolutionary-computation-based dynamic optimization model is developed to
adaptively identify time-dependant optimal health protection measures during an
emergency. This dynamic simulation-optimization model treats perceived
contaminant source attributes as time-varying parameters to account for
perceived contamination source updates as more data stream in over time.
Performance of the developed dynamic decision support model is analyzed and
demonstrated using a mid-size virtual city that resembles the dynamics and
complexity of real-world urban systems. This adaptive emergency response
optimization model is intended to be a major component of an all-inclusive
cyberinfrastructure for efficient contamination threat management, which is
currently under development.
"
1117,How to Track Online SLA,"  SLA (Service level agreement) is defined by an organization to fulfil its
client requirements, the time within which the deliverables should be turned
over to the clients. Tracking of SLA can be done manually by checking the
status, priority of any particular task. Manual SLA tracking takes time as one
has to go over each and every task that needs to be completed. For instance,
you ordered a product from a website and you are not happy with the quality of
the product and want to replace the same on urgent basis, You send mail to the
customer support department, the query/complaint will be submitted in a queue
and will be processed basis of its priority and urgency (The SLA for responding
back to customers concern are listed in the policy). This online SLA tracking
system will ensure that no queries/complaints are missed and are processed in
an organized manner as per their priority and the date by when it should be
handled. The portal will provide the status of the complaints for that
particular day and the ones which have been pending since last week. The
information can be refreshed as per the client need (within what time frame the
complaint should be addressed).
"
1118,"Business types classification via e-commerce stage model in oil industry
  in Iran","  Since the strategies and plans for e-commerce development are different for
different industries and since the oil industry is one of the most important
industries in Iran, the scope of this research is thus confined to that of the
oil industry in Iran. The main aim of this study is to identify and classify
the different features of e-commerce development stages and features based on
the different business types present in companies in the oil industry in Iran.
In order to achieve both of these objectives a questionnaire was developed and
administered online. The questionnaire was distributed to forty representatives
working in different companies. The collected data was classified and sorted
and the priority e-commerce features was classified and displayed as triangles
for each business type. Furthermore, the experts were asked to indicate the
features which they implemented in their companies in order to know the most
used features in each stage. The results of this study give an insight to the
practice of e-commerce for Iranian oil companies and can be used to strategize
future directions for the industry in terms of e- commerce.
"
1119,Sequential Data Mining using Correlation Matrix Memory,"  This paper proposes a method for sequential data mining using correlation
matrix memory. Here, we use the concept of the Logical Match to mine the
indices of the sequential pattern. We demonstrate the uniqueness of the method
with both the artificial and the real datum taken from NCBI databank.
"
1120,"A Wavelet Based Algorithm for the Identification of Oscillatory
  Event-Related Potential Components","  Event Related Potentials (ERPs) are very feeble alterations in the ongoing
Electroencephalogram (EEG) and their detection is a challenging problem. Based
on the unique time-based parameters derived from wavelet coefficients and the
asymmetry property of wavelets a novel algorithm to separate ERP components in
single-trial EEG data is described. Though illustrated as a specific
application to N170 ERP detection, the algorithm is a generalized approach that
can be easily adapted to isolate different kinds of ERP components. The
algorithm detected the N170 ERP component with a high level of accuracy. We
demonstrate that the asymmetry method is more accurate than the matching
wavelet algorithm and t-CWT method by 48.67 and 8.03 percent respectively. This
paper provides an off-line demonstration of the algorithm and considers issues
related to the extension of the algorithm to real-time applications.
"
1121,Intelligent Fatigue Detection and Automatic Vehicle Control System,"  This paper describes method for detecting the early signs of fatigue in train
drivers. As soon as the train driver is falling in symptoms of fatigue
immediate message will be transfer to the control room indicating the status of
the drivers. In addition of the advance technology of heart rate sensors is
also added in the system for correct detection of status of driver if in either
case driver is falling to fatigue due to any sever medical problems .The
fatigue is detected in the system by the image processing method of comparing
the image(frames) in the video and by using the human features we are able to
estimate the indirect way of detecting fatigue. The technique also focuses on
modes of person when driving the train i.e. awake, drowsy state or sleepy and
sleep state. The system is very efficient to detect the fatigue and control the
train also train can be controlled if it cross any such signal by which the
train may collide on another train.
"
1122,"A New Fuzzy DEMATEL-TODIM Hybrid Method for evaluation criteria of
  Knowledge management in supply chain","  Knowledge management (KM) adoption in the supply chain network needs a good
investment as well as few changes in the culture of the entire SC. Knowledge
management is the process of creating, distributing and transferring
information. The goal of this study is to Rank KM criteria in supply chain
network in Iran which is important for firms these days. Criterion used in this
paper were extracted from the literature review and were confirmed by supply
chain experts. The proposed approach for ranking and finding out about these
criterion is hybrid fuzzy DEMATEL-TODIM, with using fuzzy number as data for
our studies we could avoid uncertainty. The data was gathered from PhD. And Ms.
Students in industrial engineering of Kharrazmi university of Tehran and PhD.
And Ms. Students of the management department of Semnan university. A new
hybrid approach was used for achieving the results of this study. This new
hybrid approach ranks data criteria respect to each other, then by using TODIM
for ranking respect to the best situation (gains), the rates of criterion were
determined which is a very important advantage
"
1123,Compressive Periodogram Reconstruction Using Uniform Binning,"  In this paper, two problems that show great similarities are examined. The
first problem is the reconstruction of the angular-domain periodogram from
spatial-domain signals received at different time indices. The second one is
the reconstruction of the frequency-domain periodogram from time-domain signals
received at different wireless sensors. We split the entire angular or
frequency band into uniform bins. The bin size is set such that the received
spectra at two frequencies or angles, whose distance is equal to or larger than
the size of a bin, are uncorrelated. These problems in the two different
domains lead to a similar circulant structure in the so-called coset
correlation matrix. This circulant structure allows for a strong compression
and a simple least-squares reconstruction method. The latter is possible under
the full column rank condition of the system matrix, which can be achieved by
designing the spatial or temporal sampling patterns based on a circular sparse
ruler. We analyze the statistical performance of the compressively
reconstructed periodogram including bias and variance. We further consider the
case when the bins are so small that the received spectra at two frequencies or
angles, with a spacing between them larger than the size of the bin, can still
be correlated. In this case, the resulting coset correlation matrix is
generally not circulant and thus a special approach is required.
"
1124,Data Transfer between Two USB Flash SCSI Disks using a Touch Screen,"  Under normal circumstances, as an intermediate device, if we want to move or
copy data from one mass storage device to another, we use a computer in the
form of desktops, laptops, etc. We need a device which can be used as an
intermediate device, also which is a complete blend of hardware & software.
This device is a gadget that can be used to transfer data between two flash
SCSI devices via a touch screen. This is a user friendly device which uses the
most popular bus USB (Universal Serial Bus) with Type-A connector. It is
governed by the USB 2.0 Protocol. One of the major advantage of this device is
its portability.
"
1125,"M$^2$I: Channel Modeling for Metamaterial-Enhanced Magnetic Induction
  Communications","  Magnetic Induction (MI) communication technique has shown great potentials in
complex and RF-challenging environments, such as underground and underwater,
due to its advantage over EM wave-based techniques in penetrating lossy medium.
However, the transmission distance of MI techniques is limited since magnetic
field attenuates very fast in the near field. To this end, this paper proposes
Metamaterial-enhanced Magnetic Induction (M$^2$I) communication mechanism,
where a MI coil antenna is enclosed by a metamaterial shell that can enhance
the magnetic fields around the MI transceivers. As a result, the M$^2$I
communication system can achieve tens of meters communication range by using
pocket-sized antennas. In this paper, an analytical channel model is developed
to explore the fundamentals of the M$^2$I mechanism, in the aspects of
communication range and channel capacity, and the susceptibility to various
hostile and complex environments. The theoretical model is validated through
the finite element simulation software, Comsol Multiphysics. Proof-of-concept
experiments are also conducted to validate the feasibility of M$^2$I.
"
1126,"Supporting Read/Write Applications in Embedded Real-time Systems via
  Suspension-aware Analysis","  In many embedded real-time systems, applications often interact with I/O
devices via read/write operations, which may incur considerable suspension
delays. Unfortunately, prior analysis methods for validating timing correctness
in embedded systems become quite pessimistic when suspension delays are
present. In this paper, we consider the problem of supporting two common types
of I/O applications in a multiprocessor system, that is, write-only
applications and read-write applications. For the write-only application model,
we present a much improved analysis technique that results in only O(m)
suspension-related utilization loss, where m is the number of processors. For
the second application model, we present a flexible I/O placement strategy and
a corresponding new scheduling algorithm, which can completely circumvent the
negative impact due to read- and write-induced suspension delays. We illustrate
the feasibility of the proposed I/O-placement-based schedule via a case study
implementation. Furthermore, experiments presented herein show that the
improvement with respect to system utilization over prior methods is often
significant.
"
1127,A Novel Hybrid Algorithm for Permutation Flow Shop Scheduling,"  In the present scenario the recent engineering and industrial built-up units
are facing hodgepodge of problems in a lot of aspects such as machining time,
electricity, man power, raw material and customers constraints. The job-shop
scheduling is one of the most significant industrial behaviours, particularly
in manufacturing planning. This paper proposes the permutation flow shop
sequencing problem with the objective of makespan minimization using the new
modified proposed method of johnsons algorithm as well as the guptas heuristic
algorithm. This paper involves the determination of the order of processing of
n jobs in m machines. Although since the problem is known to be np-hard for
three or more machines, that produces near optimal solution of the given
problem. The proposed method is very simple and easy to understand followed by
a numerical illustration is given.
"
1128,Adaptive Fault Diagnosis using Self-Referential Reasoning,"  The problem is to determine which processors are reliable in a remote
location by asking ""Yes or No"" questions. The processors are of three types:
those that always tell the truth, those that always lie, and those the
sometimes tell the truth and sometimes lie. Using self-referential reasoning,
along with earlier techniques, we can regard both the truth-tellers and liars
as reliable and thus the tackle situations when fewer than half the processors
are truth-tellers.
"
1129,"Simulation and optimization of container terminal operations: a case
  study","  Container terminals are facing a set of interrelated problems. Container
handling problems at container terminals are NP-hard problems. The docking time
of container ships at the port must be optimized. In this paper we have built a
simulation model that integrates all the activities of a container terminal.
The proposed approach is applied on a real case study data of container
terminal at El-Dekheilla port. The results show that the proposed approach
reduced the ship turnaround time in port where 51% reduction in ship service
time (loading/unloading) in port is achieved.
"
1130,Solving container terminals problems using computer-based modeling,"  This paper addresses the applications of different techniques for solving
container terminals problems. We have built a simulation model that can be used
to analyze the performance of container terminal operations. The proposed
approach is intended to be applied for a real case study in Alexandria
Container Terminal (ACT) at Alexandria port. The implementation of our approach
shows that a proposed model increases the efficiency of Alexandria container
terminal at Alexandria port.
"
1131,"Study on FLOWSIM and its Application for Isolated Signal-ized
  Intersection Assessment","  Recently the traffic related problems have become strategically important,
due to the continuously increasing vehicle number. As a result, microscopic
simulation software has become an efficient method in traffic engineering for
its cost-effectiveness and safety characteristics. In this paper, a new fuzzy
logic based simulation software (FLOWSIM) is introduced, which can reflect the
mixed traffic flow phenomenon in China better. The fuzzy logic based
car-following model and lane-changing model are explained in detail.
Furthermore, its applications for mixed traffic flow management in mid-size
cities and for signalized intersection management assessment in large cities
are illustrated by examples in China. Finally, further study objectives are
discussed.
"
1132,"Triple Patterning Lithography (TPL) Layout Decomposition using
  End-Cutting (JM3 Special Session)","  Triple patterning lithography (TPL) is one of the most promising techniques
in the 14nm logic node and beyond. Conventional LELELE type TPL technology
suffers from native conflict and overlapping problems. Recently, as an
alternative process, triple patterning lithography with end cutting (LELE-EC)
was proposed to overcome the limitations of LELELE manufacturing. In LELE-EC
process the first two masks are LELE type double patterning, while the third
mask is used to generate the end-cuts. Although the layout decomposition
problem for LELELE has been well-studied in the literature, only few attempts
have been made to address the LELE-EC layout decomposition problem. In this
paper we propose the comprehensive study for LELE-EC layout decomposition.
Conflict graph and end-cut graph are constructed to extract all the geometrical
relationships of both input layout and end-cut candidates. Based on these
graphs, integer linear programming (ILP) is formulated to minimize the conflict
number and the stitch number. The experimental results demonstrate the
effectiveness of the proposed algorithms.
"
1133,"Precision of Pulse-Coupled Oscillator Synchronization on FPGA-Based
  Radios","  The precision of synchronization algorithms based on the theory of
pulse-coupled oscillators is evaluated on FPGA-based radios for the first time.
Measurements show that such algorithms can reach precision in the low
microsecond range when being implemented in the physical layer. Furthermore, we
propose an algorithm extension accounting for phase rate deviations of the
hardware and show that an improved precision below one microsecond is possible
with this extension in the given setup. The resulting algorithm can thus be
applied in ad hoc wireless systems for fully distributed synchronization of
transmission slots or sleep cycles, in particular, if centralized
synchronization is impossible.
"
1134,"e-Installation: Synesthetic Documentation of Media Art via Telepresence
  Technologies","  In this paper, a new synesthetic documentation method that contributes to
media art conservation is presented. This new method is called e-Installation
in analogy to the idea of the e-Book as the electronic version of a real book.
An e-Installation is a virtualized media artwork that reproduces all
synesthesia, interaction, and meaning levels of the artwork. Advanced 3D
modeling and telepresence technologies with a very high level of immersion
allow the virtual re-enactment of works of media art that are no longer
performable or rarely exhibited. The virtual re-enactment of a media artwork
can be designed with a scalable level of complexity depending on whether it
addresses professionals such as curators, art restorers, and art theorists or
the general public. An e-Installation is independent from the artwork's
physical location and can be accessed via head-mounted display or similar data
goggles, computer browser, or even mobile devices. In combination with
informational and preventive conservation measures, the e-Installation offers
an intermediate and long-term solution to archive, disseminate, and pass down
the milestones of media art history as a synesthetic documentation when the
original work can no longer be repaired or exhibited in its full function.
"
1135,"A New Approach to Customization of Collision Warning Systems to
  Individual Drivers","  This paper discusses the need for individualizing safety systems and proposes
an approach including the Real-Time estimation of the distribution of brake
response times for an individual driver. While maintaining high level of
safety, the collision warning system should send ""tailored"" responses to the
driver. This method could be the first step to show that safety applications
would potentially benefit from customizing to individual drivers'
characteristics using VANET. Our simulation results show that, as one of the
imminent and preliminary outcomes of the new improved system, the number of
false alarms will be reduced by more than 40%. We think this tactic can reach
to even beyond the safety applications for designing the future innovative
systems.
"
1136,Stress-Minimizing Orthogonal Layout of Data Flow Diagrams with Ports,"  We present a fundamentally different approach to orthogonal layout of data
flow diagrams with ports. This is based on extending constrained stress
majorization to cater for ports and flow layout. Because we are minimizing
stress we are able to better display global structure, as measured by several
criteria such as stress, edge-length variance, and aspect ratio. Compared to
the layered approach, our layouts tend to exhibit symmetries, and eliminate
inter-layer whitespace, making the diagrams more compact.
"
1137,Tools and Techniques for Efficient High-Level System Design on FPGAs,"  In order for FPGAs to be successful outside traditional markets, tools which
enable software programmers to achieve high levels of system performance while
abstracting away the FPGA-specific details are needed. DSPB Builder Advanced
(DSPBA) is one such tool. DSPBA provides model-based design environment using
Matlab's Simulink frontend that decouples the fully-algorithmic design
description from the details of FPGA system generation. DSPBA offers several
levels of debugging: from Simulink scopes to bit-accurate-simulation and silver
reference models. It also offers the most comprehensive set of fixed-point,
floating-point and signal-processing IPs available today. The combination of 7
floating-point precisions, fused-datapath support, custom operator support and
automated folding allows exploring the best tradeoffs between accuracy, size
and throughput. The DSPBA backend protects users from the details of
device-dependent operator mapping offering both efficiency and prompt support
for new devices and features such as the Arria10 floating-point cores. The
collection of features available in DSPBA allows both unexperienced and expert
users to efficiently migrate performance-crucial systems to the FPGA
architecture.
"
1138,"OpenHEC: A Framework for Application Programmers to Design FPGA-based
  Systems","  Today, there is a trend to incorporate more intelligence (e.g., vision
capabilities) into a wide range of devices, which makes high performance a
necessity for computing systems. Furthermore, for embedded systems, low power
consumption should be generally considered together with high computing
performance. FPGAs, as programmable logic devices able to support different
types of fine-grained parallelisms, their power and performance advantages were
recognized widely. However, designing applications on FPGA-based systems is
traditionally far from a task can be carried out by software programmers.
Generally, hardware engineers and even system-level software engineers have
more hardware/architectural knowledge but fewer algorithm and application
knowledge. Thus, it is critical for computing systems to allow
application-level programmers to realize their idea conveniently, which is
popular in computing systems based on the general processor. In this paper, the
OpenHEC (Open Framework for High-Efficiency Computing) framework is proposed to
provide a design framework for application-level software programmers to use
FPGA-based platforms. It frees users from hardware and architectural details to
let them focus more on algorithms/applications. This framework was integrated
with the commercial Xilinx ISE/Vivado to make it to be used immediately. After
implementing a widely-used feature detection algorithm on OpenHEC from the
perspective of software programmers, it shows this framework is applicable for
application programmers with little hardware knowledge.
"
1139,High-Level Design of Portable and Scalable FPGA Accelerators,"  This paper presents our approach for making FPGA accelerators accessible to
software (SW) programmers. It is intended as a starting point for
collaborations with other groups pursuing similar objectives. We report on our
current SAccO platform (Scalable Accelerator platform Osnabr\""uck) and the
planned project extending this platform.
"
1140,"Stream Processor Generator for HPC to Embedded Applications on
  FPGA-based System Platform","  This paper presents a stream processor generator, called SPGen, for
FPGA-based system-on-chip platforms. In our research project, we use an FPGA as
a common platform for applications ranging from HPC to embedded/robotics
computing. Pipelining in application-specific stream processors brings FPGAs
power-efficient and high-performance computing. However, poor productivity in
developing custom pipelines prevents the reconfigurable platform from being
widely and easily used. SPGen aims at assisting developers to design and
implement high-throughput stream processors by generating their HDL codes with
our domain-specific high-level stream processing description, called SPD.With
an example of fluid dynamics computation, we validate SPD for describing a real
application and verify SPGen for synthesis with a pipelined data-flow graph. We
also demonstrate that SPGen allows us to easily explore a design space for
finding better implementation than a hand-designed one.
"
1141,High-Level Synthesis Case Study: Implementation of a Memcached Server,"  High-Level Synthesis (HLS) aspires to raise the level of abstraction in
hardware design without sacrificing hardware efficiency. It has so far been
successfully employed in signal and video processing but has found only limited
use in other areas. This paper utilizes a commercial HLS tool, namely Vivado(R)
HLS, to implement the processing of a common data center application, the
Key-Value Store (KVS) application memcached, as a deeply pipelined dataflow
architecture. We compared our results to a fully equivalent RTL implementation
done previously in our group and found that it matches its performance, yields
tangible improvements in latency (between 7-30%) and resource consumption (22%
in LUTs and 35% in registers), all while requiring 3x less lines of code and 2x
less development time. The implementation was validated in hardware on a
Xilinx(R) VC709 development board, meeting timing requirements for 10Gbps line
rate processing.
"
1142,SFA Referee Allocation Scheme,"  For many sports, the allocation of officials to matches is performed manually
and is a very time consuming procedure. For the Scottish Football Association
(SFA), the allocation of referees and other officials to matches is governed by
a number of rules specifying the expertise required from the different types of
official at each level, e.g. Scottish Premiership League referee must be a
grade 1 with high experience. The allocation requires an SFA secretary to
expend several hours to find suitable officials, contact them and assign them.
Most of the time, the secretary is a volunteer who performs the allocation as a
hobby and it would be useful to reduce his costs and time.
  The project aims to reduce the burden on SFA, and potentially other
secretaries, by developing a program to assign SFA officials. A suitable
algorithm must be devised to search through the set of data about matches and
officials and find a potential allocation. The program then updates the
database with the new data, and provides a web interface for both secretaries
and officials.
  A prototype system using the new greedy algorithm has been implemented and
evaluated with SFA secretaries. A final usable referee allocation system has
been designed that uses the greedy algorithm, and is extended after evaluation
of the prototype. The final allocation system based provides both a command
line and a web interface and has also been evaluated by SFA secretaries. In
their letters of recommendation in Appendix F the SFA secretaries indicate that
the final allocation system it will be used again in the future.
"
1143,"Modeling, Stability Analysis, and Testing of a Hybrid Docking Simulator","  A hybrid docking simulator is a hardware-in-the-loop (HIL) simulator that
includes a hardware element within a numerical simulation loop. One of the
goals of performing a HIL simulation at the European Proximity Operation
Simulator (EPOS) is the verification and validation of the docking phase in an
on-orbit servicing mission.....
"
1144,Population spatialization and synthesis with open data,"  Individuals together with their locations & attributes are essential to feed
micro-level applied urban models (for example, spatial micro-simulation and
agent-based modeling) for policy evaluation. Existed studies on population
spatialization and population synthesis are generally separated. In developing
countries like China, population distribution in a fine scale, as the input for
population synthesis, is not universally available. With the open-government
initiatives in China and the emerging Web 2.0 techniques, more and more open
data are becoming achievable. In this paper, we propose an automatic process
using open data for population spatialization and synthesis. Specifically, the
road network in OpenStreetMap is used to identify and delineate parcel
geometries, while crowd-sourced POIs are gathered to infer urban parcels with a
vector cellular automata model. Housing-related online Check-in records are
then applied to distinguish residential parcels from all of the identified
urban parcels. Finally the published census data, in which the sub-district
level of attributes distribution and relationships are available, is used for
synthesizing population attributes with a previously developed tool Agenter
(Long and Shen, 2013). The results are validated with ground truth
manually-prepared dataset by planners from Beijing Institute of City Planning.
"
1145,"Evaluating the Electrification of Vehicle Fleets Using the Veins
  Framework","  The case study discussed in this paper involves a company maintaining a
vehicle fleet of one hundred vehicles. In this article we will discuss how we
extend and deploy the Veins framework, which couples OMNeT++ and SUMO, to help
in the process of electrifying this vehicle fleet, i.e., replacing combustion
engine cars with electric vehicles to save money and lower CO$_2$ emissions.
"
1146,Exercises for Children with Dyslalia-Software Infrastructure,"  In order to help children with dyslalia we created a set of software
exercises. This set has a unitary software block (data base, programming
language, programming philosophy). In this paper we present this software
infrastructure with its advantage and disadvantage. The exercises are part of a
software system named LOGOMON. Therefore, besides horizontal compatibilities
(between exercises) vertical compatibilities are also presented (with LOGOMON
system). Concerning database tables used for modulus of exercises, a part of
them is ""inherited"" from LOGOMON application and another are specific for
exercises application. We also need to specify that there were necessary minor
changes of database tables used by LOGOMON. As programming language we used C#,
implemented in Visual Studio 2005. We developed specific interfaces elements
and classes. We also used multimedia resources that were necessary for
exercises (images, correct pronouncing obtained from speech therapist
recording, video clips). Another section of this application is related to
loading of exercises on mobile devices (Pocket PC). A part of code has been
imported directly, but there were a lot of files that need to be rewritten.
Anyway, the multimedia resources were used without any processing.
"
1147,V2V Propagation Modeling with Imperfect RSSI Samples,"  We describe three in-field data collection efforts yielding a large database
of RSSI values vs. time or distance from vehicles communicating with each other
via DSRC. We show several data processing schemes we have devised to develop
Vehicle-to-Vehicle (V2V) propagation models from such data. The database is
limited in several important ways, not least, the presence of a high noise
floor that limits the distance over which good modeling is feasible. Another is
the presence of interference from multiple active transmitters. Our methodology
makes it possible to obtain, despite these limitations, accurate models of
median path loss vs. distance, shadow fading, and fast fading caused by
multipath. We aim not to develop a new V2V model, but to show the methods
enabling such a model to be obtained from in-field RSSI data.
"
1148,"Design and Realization of an S-Band Microwave Low-Noise Amplifier for
  Wireless RF Subsystems","  This study undertakes the theoretical design, CAD modeling, realization, and
performance analysis of a microwave low-noise amplifier (LNA) which has been
accurately developed for operation at 3.0 GHz (S-band). The objective of this
research is to thoroughly analyze and develop a reliable microstrip LNA
intended for a potential employment in wireless communication systems, and
satellite applications. The S-band microwave LNA demonstrates the
appropriateness to develop a high-performance and well-established device
realization for wireless RF systems. The microwave amplifier simulations have
been conducted using the latest version of the AWR Design Environment software.
"
1149,Co-Emulation of Scan-Chain Based Designs Utilizing SCE-MI Infrastructure,"  As the complexity of the scan algorithm is dependent on the number of design
registers, large SoC scan designs can no longer be verified in RTL simulation
unless partitioned into smaller sub-blocks. This paper proposes a methodology
to decrease scan-chain verification time utilizing SCE-MI, a widely used
communication protocol for emulation, and an FPGA-based emulation platform. A
high-level (SystemC) testbench and FPGA synthesizable hardware transactor
models are developed for the scan-chain ISCAS89 S400 benchmark circuit for
high-speed communication between the host CPU workstation and the FPGA
emulator. The emulation results are compared to other verification
methodologies (RTL Simulation, Simulation Acceleration, and Transaction-based
emulation), and found to be 82% faster than regular RTL simulation. In
addition, the emulation runs in the MHz speed range, allowing the incorporation
of software applications, drivers, and operating systems, as opposed to the Hz
range in RTL simulation or sub-megahertz range as accomplished in
transaction-based emulation. In addition, the integration of scan testing and
acceleration/emulation platforms allows more complex DFT methods to be
developed and tested on a large scale system, decreasing the time to market for
products.
"
1150,YoMo - The Arduino based Smart Metering Board,"  Smart meters are an enabling technology for many smart grid applications.
This paper introduces a design for a low-cost smart meter system as well as the
fundamentals of smart metering. The smart meter platform, provided as open
hardware, is designed with a connector interface compatible to the Arduino
platform, thus opening the possibilities for smart meters with flexible
hardware and computation features, starting from low-cost 8 bit micro
controllers up to powerful single board computers that can run Linux. The
metering platform features a current transformer which allows a non-intrusive
installation of the current measurement unit. The suggested design can switch
loads, offers a variable sampling frequency, and provides measurement data such
as active power, reactive and apparent power. Results indicate that measurement
accuracy and resolution of the proposed metering platform are sufficient for a
range of different applications and loads from a few watts up to five
kilowatts.
"
1151,Computational Gravitational Dynamics with Modern Numerical Accelerators,"  We review the recent optimizations of gravitational $N$-body kernels for
running them on graphics processing units (GPUs), on single hosts and massive
parallel platforms. For each of the two main $N$-body techniques, direct
summation and tree-codes, we discuss the optimization strategy, which is
different for each algorithm. Because both the accuracy as well as the
performance characteristics differ, hybridizing the two algorithms is essential
when simulating a large $N$-body system with high-density structures containing
few particles, and with low-density structures containing many particles. We
demonstrate how this can be realized by splitting the underlying Hamiltonian,
and we subsequently demonstrate the efficiency and accuracy of the hybrid code
by simulating a group of 11 merging galaxies with massive black holes in the
nuclei.
"
1152,Explicit Integration with GPU Acceleration for Large Kinetic Networks,"  We demonstrate the first implementation of recently-developed fast explicit
kinetic integration algorithms on modern graphics processing unit (GPU)
accelerators. Taking as a generic test case a Type Ia supernova explosion with
an extremely stiff thermonuclear network having 150 isotopic species and 1604
reactions coupled to hydrodynamics using operator splitting, we demonstrate the
capability to solve of order 100 realistic kinetic networks in parallel in the
same time that standard implicit methods can solve a single such network on a
CPU. This orders-of-magnitude decrease in compute time for solving systems of
realistic kinetic networks implies that important coupled, multiphysics
problems in various scientific and technical fields that were intractible, or
could be simulated only with highly schematic kinetic networks, are now
computationally feasible.
"
1153,"Profiling underprivileged residents with mid-term public transit
  smartcard data of Beijing","  Mobility of economically underprivileged residents in China has seldom been
well profiled due to privacy issue and the characteristics of Chinese over
poverty. In this paper, we identify and characterize underprivileged residents
in Beijing using ubiquitous public transport smartcard transactions in 2008 and
2010, respectively. We regard these frequent bus/metro riders (FRs) in China,
especially in Beijing, as economically underprivileged residents. Our argument
is tested against (1) the household travel survey in 2010, (2) a small-scale
survey in 2012, as well as (3) our interviews with local residents in Beijing.
Cardholders' job and residence locations are identified using Smart Card Data
(SCD) in 2008 and 2010. Our analysis is restricted to cardholders that use the
same cards in both years. We then classify all identified FRs into 20 groups by
residence changes (change, no change), workplace changes (change, no change,
finding a job, losing a job, and all-time employed) during 2008-2010 and
housing place in 2010 (within the fourth ring road or not). The underprivileged
degree of each FR is then evaluated using the 2014 SCD. To the best of our
knowledge, this is one of the first studies for understanding long- or mid-term
urban dynamics using immediate ""big data"", and also for profiling
underprivileged residents in Beijing in a fine-scale.
"
1154,"Poster Abstract: Bits and Watts: Improving energy disaggregation
  performance using power line communication modems","  Non-intrusive load monitoring (NILM) or energy disaggregation, aims to
disaggregate a household's electricity consumption into constituent appliances.
More than three decades of work in NILM has resulted in the development of
several novel algorithmic approaches. However, despite these advancements, two
core challenges still exist: i) disaggregating low power consumption appliances
and ii) distinguishing between multiple instances of similar appliances. These
challenges are becoming increasingly important due to an increasing number of
appliances and increased usage of electronics in homes. Previous approaches
have attempted to solve these problems using expensive hardware involving high
sampling rates better suited to laboratory settings, or using additional number
of sensors, limiting the ease of deployment. In this work, we explore using
commercial-off-the-shelf (COTS) power line communication (PLC) modems as an
inexpensive and easy to deploy alternative solution to these problems. We use
the reduction in bandwidth between two PLC modems, caused due to the change in
PLC modulation scheme when different appliances are operated as a signature for
an appliance. Since the noise generated in the powerline is dependent both on
type and location of an appliance, we believe that our technique based on PLC
modems can be a promising addition for solving NILM.
"
1155,"Demo Abstract: NILMTK v0.2: A Non-intrusive Load Monitoring Toolkit for
  Large Scale Data Sets","  In this demonstration, we present an open source toolkit for evaluating
non-intrusive load monitoring research; a field which aims to disaggregate a
household's total electricity consumption into individual appliances. The
toolkit contains: a number of importers for existing public data sets, a set of
preprocessing and statistics functions, a benchmark disaggregation algorithm
and a set of metrics to evaluate the performance of such algorithms.
Specifically, this release of the toolkit has been designed to enable the use
of large data sets by only loading individual chunks of the whole data set into
memory at once for processing, before combining the results of each chunk.
"
1156,"An Efficient Topology-Based Algorithm for Transient Analysis of Power
  Grid","  In the design flow of integrated circuits, chip-level verification is an
important step that sanity checks the performance is as expected. Power grid
verification is one of the most expensive and time-consuming steps of
chip-level verification, due to its extremely large size. Efficient power grid
analysis technology is highly demanded as it saves computing resources and
enables faster iteration. In this paper, a topology-base power grid transient
analysis algorithm is proposed. Nodal analysis is adopted to analyze the
topology which is mathematically equivalent to iteratively solving a positive
semi-definite linear equation. The convergence of the method is proved.
"
1157,Filtering from Observations on Stiefel Manifolds,"  This paper considers the problem of optimal filtering for partially observed
signals taking values on the rotation group. More precisely, one or more
components are considered not to be available in the measurement of the
attitude of a 3D rigid body. In such cases, the observed signal takes its
values on a Stiefel manifold. It is demonstrated how to filter the observed
signal through the anti-development built from observations. A particle filter
implementation is proposed to perform the estimation of the signal partially
observed and corrupted by noise. The sampling issue is also addressed and
interpolation methods are introduced. Illustration of the proposed technique on
synthetic data demonstrates the ability of the approach to estimate the angular
velocity of a partially observed 3D system partially observed.
"
1158,"Requisite Variety, Autopoiesis, and Self-organization","  Ashby's law of requisite variety states that a controller must have at least
as much variety (complexity) as the controlled. Maturana and Varela proposed
autopoiesis (self-production) to define living systems. Living systems also
require to fulfill the law of requisite variety. A measure of autopoiesis has
been proposed as the ratio between the complexity of a system and the
complexity of its environment. Self-organization can be used as a concept to
guide the design of systems towards higher values of autopoiesis, with the
potential of making technology more ""living"", i.e. adaptive and robust.
"
1159,An Application of Topological Data Analysis to Hockey Analytics,"  This paper applies the major computational tool from Topological Data
Analysis (TDA), persistent homology, to discover patterns in the data related
to professional sports teams. I will use official game data from the
North-American National Hockey League (NHL) 2013-2014 season to discover the
correlation between the composition of NHL teams with the currently preferred
offensive performance markers. Specifically, I develop and use the program
TeamPlex (based on the JavaPlex software library) to generate the persistence
bar-codes. TeamPlex is applied to players as data points in a multidimensional
(up to 12-D) data space where each coordinate corresponds to a selected
performance marker.
  The conclusion is that team's offensive performance (measured by the popular
characteristic used in NHL called the Corsi number) correlates with two
bar-code characteristics: greater \textit{sparsity} reflected in the longer
bars in dimension 0 and lower \textit{tunneling} reflected in the low
number/length of the 1-dimensional classes. The methodology can be used by team
managers in identifying deficiencies in the present composition of the team and
analyzing player trades and acquisitions. We give an example of a proposed
trade which should improve the Corsi number of the team.
"
1160,Modeling In vivo Wireless Path Loss,"  Our long-term research goal is to model the in vivo wireless channel. As a
first step towards this goal, in this paper we performed in vivo path loss
measurements at 2.4GHz and make a comparison with free space path loss. We
calculate the path loss by using the electric field radiated by a
Hertzian-Dipole located inside the abdominal cavity. The simulations quantify
and confirm that the path loss falls more rapidly inside the body than outside
the body. We also observe fluctuations of the path loss caused by the
inhomogeneity of the human body. In comparison with the path loss measured with
monopole antennas, we conclude that the significant variations in Received
Signal Strength is caused by both the angular dependent path loss and the
significantly modified in vivo antenna effects.
"
1161,A Smart Cushion for Real-Time Heart Rate Monitoring,"  This paper presents a smart cushion for real time heart rate monitoring. The
cushion comprises of an integrated micro-bending fiber sensor, which records
the BCG (Ballistocardiogram) signal without direct skin-electrode contact, and
an optical transceiver that does signal amplification, digitization, and
pre-filtering. To remove the artifacts and extract heart rate from BCG signal,
a computationally efficient heart rate detection algorithm is developed. The
system doesn't require any pre-training and is highly responsive with the
outputs updated every 3 sec and initial response within first 10 sec. Tests
conducted on human subjects show the detected heart rate closely matches the
one from a commercial SpO2 device.
"
1162,GraphState - a tool for graph identification and labelling,"  We present python libraries for Feynman graphs manipulation. The key feature
of these libraries is usage of generalization of graph representation offered
by B. G. Nickel et al. In this approach graph is represented in some unique
'canonical' form that depends only on its combinatorial type. The uniqueness of
graph representation gives an efficient way for isomorphism finding, searching
for subgraphs and other graph manipulation tasks. Though offered libraries were
originally designed for Feynman graphs, they might be useful for more general
graph problems.
"
1163,"Asynchronous Linear Modulation Classification with Multiple Sensors via
  Generalized EM Algorithm","  In this paper, we consider the problem of automatic modulation classification
with multiple sensors in the presence of unknown time offset, phase offset and
received signal amplitude. We develop a novel hybrid maximum likelihood (HML)
classification scheme based on a generalized expectation maximization (GEM)
algorithm. GEM is capable of finding ML estimates numerically that are
extremely hard to obtain otherwise. Assuming a good initialization technique is
available for GEM, we show that the classification performance can be greatly
improved with multiple sensors compared to that with a single sensor,
especially when the signal-to-noise ratio (SNR) is low. We further demonstrate
the superior performance of our approach when simulated annealing (SA) with
uniform as well as nonuniform grids is employed for initialization of GEM in
low SNR regions. The proposed GEM based approach employs only a small number of
samples (in the order of hundreds) at a given sensor node to perform both time
and phase synchronization, signal power estimation, followed by modulation
classification. We provide simulation results to show the computational
efficiency and effectiveness of the proposed algorithm.
"
1164,"Theoretical Analysis of Radiative Cooling for Mobile and Embedded
  Systems","  A new global analytical model of the heat dissipation process that occurs in
passively-cooled embedded systems is introduced, and we explicit under what
circumstances the traditional assumption that exponential cooling laws apply in
such context is valid. Since the power consumption and reliability of
microprocessors are highly dependent on temperature, management units need
accurate thermal models. Exponential cooling models are justified for
actively-cooled systems. Here, we analyze the tractability of the cooling law
for a passively cooled body, subject to radiative and convective cooling,
including internal heat generation. Focusing then on embedded system-like
objects, we compare the performance difference between our new passive cooling
law and the conventionally-used exponential one. We show that, for quasi
isothermal cooling surfaces of the order of 1\,dm$^2$ or greater, the radiative
cooling effect may become comparable to the convective cooling one. In other
words, radiation becomes non-negligible for systems with a cooling surface
larger than about 1\,dm$^2$. Otherwise for surfaces below 1\,dm$^2$, we show
that the differences between the exact solution and the exponential cooling law
becomes negligible. In the absence of accurate temperature measurements, an
exponential cooling model is shown to be accurate enough for systems, such as
small-sized SoCs, that require low processing overhead.
"
1165,Adaptive two-dimensional wavelet transformation based on the Haar system,"  The purpose is to study qualitative and quantitative rates of image
compression by using different Haar wavelet banks. The experimental results of
adaptive compression are provided. The paper deals with specific examples of
orthogonal Haar bases generated by multiresolution analysis. Bases consist of
three piecewise constant wavelet functions with a support $[0,1] \times [0,1]
$.
"
1166,"Programing implementation of the Quine-McCluskey method for minimization
  of Boolean expression","  A Boolean function is a function that produces a Boolean value output by
logical calculation of Boolean inputs. It plays key roles in programing
algorithms and design of circuits. Minimization of Boolean function is able to
optimize the algorithms and circuits. Quine-McCluskey (QM) method is one of the
most powerful techniques to simplify Boolean expressions. Compared to other
techniques, QM method is more executable and can handle more variables. In
addition, QM method is easier to be implemented in computer programs, which
makes it an efficient technique. There are several versions of QM simulation
codes online, whereas some of them appear to have limitations of variables
numbers or lack the consideration of Dont-Care conditions. Here a QM simulation
code based on C programing is introduced. Theoretically it is able to handle
any number of variables and has taken the Dont-Care conditions into account.
"
1167,"A Novel Design of IEEE 802.15.4 and Solar Based Autonomous Water Quality
  Monitoring Prototype using ECHERP","  The recently advancement in Wireless Sensor Network (WSN) technology has
brought new distributed sensing applications such as water quality monitoring.
With sensing capabilities and using parameters like pH, conductivity and
temperature, the quality of water can be known. This paper proposes a novel
design based on IEEE 802.15.4 (Zig-Bee protocol) and solar energy called
Autonomous Water Quality Monitoring Prototype (AWQMP). The prototype is
designed to use ECHERP routing protocol and Adruino Mega 2560, an open-source
electronic prototyping platform for data acquisition. AWQMP is expected to give
real time data acquirement and to reduce the cost of manual water quality
monitoring due to its autonomous characteristic. Moreover, the proposed
prototype will help to study the behavior of aquatic animals in deployed water
bodies.
"
1168,"Area Versus Speed Trade-off Analysis of a WiMAX Deinterleaver Circuit
  Design","  Trade-off is one of the main design parameters in the field of electronic
circuit design. Whereas smaller electronics devices which use less hardware due
to techniques like hardware multiplexing or due to smaller devices created due
to techniques developed by nanotechnology and MEMS, are more appealing, a
trade-off between area, power and speed is inevitable. This paper analyses the
trade-off in the design of WiMAX deinterleaver. The main aim is to reduce the
hardware utilization in a deinterleaver but speed and power consumption are
important parameters which cannot be overlooked.
"
1169,"3D simulation of complex shading affecting PV systems taking benefit
  from the power of graphics cards developed for the video game industry","  Shading reduces the power output of a photovoltaic (PV) system. The design
engineering of PV systems requires modeling and evaluating shading losses. Some
PV systems are affected by complex shading scenes whose resulting PV energy
losses are very difficult to evaluate with current modeling tools. Several
specialized PV design and simulation software include the possibility to
evaluate shading losses. They generally possess a Graphical User Interface
(GUI) through which the user can draw a 3D shading scene, and then evaluate its
corresponding PV energy losses. The complexity of the objects that these tools
can handle is relatively limited. We have created a software solution, 3DPV,
which allows evaluating the energy losses induced by complex 3D scenes on PV
generators. The 3D objects can be imported from specialized 3D modeling
software or from a 3D object library. The shadows cast by this 3D scene on the
PV generator are then directly evaluated from the Graphics Processing Unit
(GPU). Thanks to the recent development of GPUs for the video game industry,
the shadows can be evaluated with a very high spatial resolution that reaches
well beyond the PV cell level, in very short calculation times. A PV simulation
model then translates the geometrical shading into PV energy output losses.
3DPV has been implemented using WebGL, which allows it to run directly from a
Web browser, without requiring any local installation from the user. This also
allows taken full benefits from the information already available from
Internet, such as the 3D object libraries. This contribution describes, step by
step, the method that allows 3DPV to evaluate the PV energy losses caused by
complex shading. We then illustrate the results of this methodology to several
application cases that are encountered in the world of PV systems design.
"
1170,Crawford-Sobel meet Lloyd-Max on the grid,"  The main contribution of this work is twofold. First, we apply, for the first
time, a framework borrowed from economics to a problem in the smart grid
namely, the design of signaling schemes between a consumer and an electricity
aggregator when these have non-aligned objectives. The consumer's objective is
to meet its need in terms of power and send a request (a message) to the
aggregator which does not correspond, in general, to its actual need. The
aggregator, which receives this request, not only wants to satisfy it but also
wants to manage the cost induced by the residential electricity distribution
network. Second, we establish connections between the exploited framework and
the quantization problem. Although the model assumed for the payoff functions
for the consumer and aggregator is quite simple, it allows one to extract
insights of practical interest from the analysis conducted. This allows us to
establish a direct connection with quantization, and more importantly, to open
a much more general challenge for source and channel coding.
"
1171,On The Dynamical Nature Of Computation,"  Dynamical Systems theory generally deals with fixed point iterations of
continuous functions. Computation by Turing machine although is a fixed point
iteration but is not continuous. This specific category of fixed point
iterations can only be studied using their orbits. Therefore the standard
notion of chaos is not immediately applicable. However, when a suitable
definition is used, it is found that the notion of chaos and fractal sets
exists even in computation. It is found that a non terminating Computation will
be almost surely chaotic, and autonomous learning will almost surely identify
fractal only sets.
"
1172,Photomapping Using Aerial Vehicle,"  Creating a photomap plays a critical role in navigation. Therefore, flying
vehicles are usually used to create topdown maps of the environment. In this
report we used two different aerial vehicles to create a map in a simulated
environment
"
1173,"Systems, Resilience, and Organization: Analogies and Points of Contact
  with Hierarchy Theory","  Aim of this paper is to provide preliminary elements for discussion about the
implications of the Hierarchy Theory of Evolution on the design and evolution
of artificial systems and socio-technical organizations. In order to achieve
this goal, a number of analogies are drawn between the System of Leibniz; the
socio-technical architecture known as Fractal Social Organization; resilience
and related disciplines; and Hierarchy Theory. In so doing we hope to provide
elements for reflection and, hopefully, enrich the discussion on the above
topics with considerations pertaining to related fields and disciplines,
including computer science, management science, cybernetics, social systems,
and general systems theory.
"
1174,"Semantic-based Detection of Segment Outliers and Unusual Events for
  Wireless Sensor Networks","  Environmental scientists have increasingly been deploying wireless sensor
networks to capture valuable data that measures and records precise information
about our environment. One of the major challenges associated with wireless
sensor networks is the quality of the data and more specifically the detection
of segment outliers and unusual events. Most previous research has focused on
detecting outliers that are errors that are caused by unreliable sensors and
sensor nodes. However, there is an urgent need for the development of new tools
capable of identifying, tagging and visualizing erroneous segment outliers and
unusual events from sensor data streams. In this paper, we present a SOUE
Detector (Segment Outlier and Unusual Event-Detector) system for wireless
sensor networks that combines statistical analyses using Dynamic Time Warping
(DTW) with domain expert knowledge (captured via an ontology and semantic
inferencing rules). The resulting Web portal enables scientist to efficiently
search across a collection of wireless sensor data streams and identify,
retrieve and display segment outliers (both erroneous and genuine) within the
data streams. In this paper, we firstly describe the detection algorithms, the
implementation details and the functionality of the SOUE Detector system.
Secondly we evaluate our approach using data that comprises sensor observations
collected from a sensor network deployed in the Springbrook National Park in
Queensland, Australia. The experimental results show that the SOUE-Detector can
efficiently detect segment outliers and unusual events with high levels of
precision and recall.
"
1175,"High gain two-stage amplifier with positive capacitive feedback
  compensation","  A novel topology for a high gain two-stage amplifier is proposed. The
proposed circuit is designed in a way that the non-dominant pole is at output
of the first stage. A positive capacitive feedback (PCF) around the second
stage introduces a left half plane (LHP) zero which cancels the phase shift
introduced by the non-dominant pole, considerably. The dominant pole is at the
output node which means that increasing the load capacitance has minimal effect
on stability. Moreover, a simple and effective method is proposed to enhance
slew rate. Simulation shows that slew rate is improved by a factor of 2.44
using the proposed method. The proposed amplifier is designed in a 0.18um CMOS
process. It consumes 0.86mW power from a 1.8V power supply and occupies
3038.5um2 of chip area. The DC gain is 82.7dB and gain bandwidth (GBW) is 88.9
MHz when driving a 5pF capacitive load. Also low frequency CMRR and PSRR+ are
127dB and 83.2dB, respectively. They are 24.8dB and 24.2dB at GBW frequency,
which are relatively high and are other important properties of the proposed
amplifier. Moreover, Simulations show convenient performance of the circuit in
process corners and also presence of mismatch.
"
1176,Routing Diverse Crowds in Emergency with Dynamic Grouping,"  Evacuee routing algorithms in emergency typically adopt one single criterion
to compute desired paths and ignore the specific requirements of users caused
by different physical strength, mobility and level of resistance to hazard. In
this paper, we present a quality of service (QoS) driven multi-path routing
algorithm to provide diverse paths for different categories of evacuees. This
algorithm borrows the concept of Cognitive Packet Network (CPN), which is a
flexible protocol that can rapidly solve optimal solution for any user-defined
goal function. Spatial information regarding the location and spread of hazards
is taken into consideration to avoid that evacuees be directed towards
hazardous zones. Furthermore, since previous emergency navigation algorithms
are normally insensitive to sudden changes in the hazard environment such as
abrupt congestion or injury of civilians, evacuees are dynamically assigned to
several groups to adapt their course of action with regard to their on-going
physical condition and environments. Simulation results indicate that the
proposed algorithm which is sensitive to the needs of evacuees produces better
results than the use of a single metric. Simulations also show that the use of
dynamic grouping to adjust the evacuees' category and routing algorithms with
regard for their on-going health conditions and mobility, can achieve higher
survival rates.
"
1177,A dynamic mechanism of Alzheimer based on artificial neural network,"  In this paper, we provide another angle to analyze the reasons why Alzheimer
Disease exists. We analyze the dynamic mechanism of Alzheimer Disease based on
the cognitive model that established from artificial neural network. We can
provide some theoretic explanations to Alzheimer Disease through the analyzing
of this model.
"
1178,Hostile Intent Enumeration using Soft Computing Techniques,"  In any tactical scenario, the successful quantification and triangulation of
potential hostile elements is instrumental to minimize any casualties which
might be incurred. The most commonly deployed infrastructures to cater to this
have mostly been surveillance systems which only extract some data pertaining
to the targets of interest in the area of observation and convey the
information to the human operators. Accordingly, with the ever increasing rate
at which warfare tactics are evolving, there has been a growing need for
smarter solutions to this problem of hostile intent enumeration. Recently, a
number of developments have been made to ameliorate the efficacy and the
certitude with which this task is performed. This paper discusses two of the
most prominent approaches which address this problem and posits the outline of
a novel solution which seeks to address the shortcomings faced by the existing
approaches.
"
1179,"The Solving of the Problems with Random Division of an Interval with Use
  of Computer Analytic Programs","  An original approach to solving rather difficult probabilistic problems
arising in studying the readout of random discrete fields and having no exact
analytical solutions at the moment is proposed. Several algorithms for direct,
iterative, and combinatorial-recursive calculations of multidimensional
integral expressions, which can describe partial solutions of these problems,
are presented (these solutions are further used to search for the common closed
analytical regularities). The huge volume of necessary calculations forced us
to formalize completely the algorithms and to transfer all the burden of
routine analytical transforms to a computer. The calculations performed helped
us to establish (and to prove later) a number of new earlier unknown
probabilistic formulas responsible for random division of an interval. One more
important feature of this study is the fact that we introduced a new concept of
'three-dimensional generalized Catalan numbers' and found their explicit form
in solving problems associated with random division of an interval.
"
1180,Spatiotemporal Modeling of a Pervasive Game,"  Given pervasive games that maintain a virtual spatiotemporal model of the
physical world, game designers must contend with space and time in the virtual
and physical, but an integrated conceptual model is lacking. Because the
problem domains of GIS and Pervasive Games overlap, Peuquet's Triad
Representational Framework is exapted, from the domain of GIS, and applied to
Pervasive Games. Using Dix et al.'s three types of space and Langran's notion
of time, virtual time and space are then be mapped to the physical world and
vice versa. The approach is evaluated using the pervasive game called Codename:
Heroes, as case study.
"
1181,Qubit Data Structures for Analyzing Computing Systems,"  Qubit models and methods for improving the performance of software and
hardware for analyzing digital devices through increasing the dimension of the
data structures and memory are proposed. The basic concepts, terminology and
definitions necessary for the implementation of quantum computing when
analyzing virtual computers are introduced. The investigation results
concerning design and modeling computer systems in a cyberspace based on the
use of two-component structure <memory - transactions> are presented.
"
1182,"Autonomous Load Disaggregation Approach based on Active Power
  Measurements","  With the help of smart metering valuable information of the appliance usage
can be retrieved. In detail, non-intrusive load monitoring (NILM), also called
load disaggregation, tries to identify appliances in the power draw of an
household. In this paper an unsupervised load disaggregation approach is
proposed that works without a priori knowledge about appliances. The proposed
algorithm works autonomously in real time. The number of used appliances and
the corresponding appliance models are learned in operation and are
progressively updated. The proposed algorithm is considering each useful and
suitable detected power state. The algorithm tries to detect power states
corresponding to on/off appliances as well as to multi-state appliances based
on active power measurements in 1s resolution. We evaluated the novel
introduced load disaggregation approach on real world data by testing the
possibility to disaggregate energy demand on appliance level.
"
1183,Personal Multi-threading,"  Multi-threading allows agents to pursue a heterogeneous collection of tasks
in an orderly manner. The view of multi-threading that emerges from thread
algebra is applied to the case where a single agent, who may be human,
maintains a hierarchical multithread as an architecture of its own activities.
"
1184,Domotic Embedded System,"  This paper presents an original domotic embedded system for room temperature
monitoring. The OpenRemote is the main software interface between the user and
the system, but other software components and communication protocols are used,
such as 1-Wire protocol for temperature monitoring devices, RS-232 for the
central PC unit and OWFS software for remote control using Android mobile
devices. The system architecture consists in hardware and software components
to remote control a room temperature parameter for energy efficiency
increasing.
"
1185,Neuronal noise as a physical resource for human cognition,"  A new class of energy-efficient digital microprocessor is being developed
which is susceptible to thermal noise and consequently operates in
probabilistic rather than conventional deterministic mode. Hybrid computing
systems which combine probabilistic and deterministic processors can provide
robust and efficient tools for computational problems that hitherto would be
intractable by conventional deterministic algorithm. These developments suggest
a revised perspective on the consequences of ion-channel noise in slender
axons, often regarded as a hindrance to neuronal computations. It is proposed
that the human brain is such an energy-efficient hybrid computational system
whose remarkable characteristics emerge from constructive synergies between
probabilistic and deterministic modes of operation. In particular, the capacity
for intuition and creative problem solving appears to arise naturally from such
a hybrid system. Bearing in mind that physical thermal noise is both pure and
available at no cost, our proposal has implications for attempts to emulate the
energy-efficient human brain on conventional energy-intensive deterministic
supercomputers.
"
1186,"Cloud Enabled Emergency Navigation Using Faster-than-real-time
  Simulation","  State-of-the-art emergency navigation approaches are designed to evacuate
civilians during a disaster based on real-time decisions using a pre-defined
algorithm and live sensory data. Hence, casualties caused by the poor decisions
and guidance are only apparent at the end of the evacuation process and cannot
then be remedied. Previous research shows that the performance of routing
algorithms for evacuation purposes are sensitive to the initial distribution of
evacuees, the occupancy levels, the type of disaster and its as well its
locations. Thus an algorithm that performs well in one scenario may achieve bad
results in another scenario. This problem is especially serious in
heuristic-based routing algorithms for evacuees where results are affected by
the choice of certain parameters. Therefore, this paper proposes a
simulation-based evacuee routing algorithm that optimises evacuation by making
use of the high computational power of cloud servers. Rather than guiding
evacuees with a predetermined routing algorithm, a robust Cognitive Packet
Network based algorithm is first evaluated via a cloud-based simulator in a
faster-than-real-time manner, and any ""simulated casualties"" are then re-routed
using a variant of Dijkstra's algorithm to obtain new safe paths for them to
exits. This approach can be iterated as long as corrective action is still
possible.
"
1187,"Modified Design of Microstrip Patch Antenna for WiMAX Communication
  System","  In this paper, a new design for U-shaped microstrip patch antenna is
proposed, which can be used in WiMAX communication systems. The aim of this
paper is to optimize the performance of microstrip patch antenna. Nowadays,
WiMAX communication applications are widely using U-shaped microstrip patch
antenna and it has become very popular. Our proposed antenna design uses 4-4.5
GHZ frequency band and it is working at narrowband within this band. RT/DUROID
5880 material is used for creating the substrate of the microstrip antenna.
This modified design of the microstrip patch antenna gives high performance in
terms of gain and return loss.
"
1188,Time-Symmetric Physics: A Radical Approach to the Decoherence Problem,"  The most powerful form of quantum learning system possible would somehow
learn the parameters W of a quantum system f(X, W), for f representing the
largest, most powerful set of possible input-output relations. This paper
addresses the issue of how to enlarge the set represented by f, by using a new
formulation of time-symmetric physics to model analog quantum computers based
on spin and by exploring possible sources of backwards-time free energy so as
to address problems of decoherence and dissipation.
"
1189,Mapping and Matching Algorithms: Data Mining by Adaptive Graphs,"  Assume we have two bijective functions $U(x)$ and $M(x)$ with $M(x)\neq U(x)$
for all $x$ and $M,N: \N \rightarrow \N$ . Every day and in different
locations, we see the different results of $U$ and $M$ without seeing $x$. We
are not assured about the time stamp nor the order within the day but at least
the location is fully defined. We want to find the matching between $U(x)$ and
$M(x)$ (i.e., we will not know $x$). We formulate this problem as an adaptive
graph mining: we develop the theory, the solution, and the implementation. This
work stems from a practical problem thus our definitions. The solution is
simple, clear, and the implementation parallel and efficient. In our
experience, the problem and the solution are novel and we want to share our
finding.
"
1190,PC Guided Automatic Vehicle System,"  The main objective of this paper is to design and develop an automatic
vehicle, fully controlled by a computer system. The vehicle designed in the
present work can move in a pre-determined path and work automatically without
the need of any human operator and it also controlled by human operator. Such a
vehicle is capable of performing wide variety of difficult tasks in space
research, domestic, scientific and industrial fields. For this purpose, an IBM
compatible PC with Pentium microprocessor has been used which performed the
function of the system controller. Its parallel printer port has been used as
data communication port to interface the vehicle. A suitable software program
has been developed for the system controller to send commands to the vehicle.
"
1191,Micro-location for Internet of Things equipped Smart Buildings,"  Micro-location is the process of locating any entity with high accuracy
(possibly in centimeters), while geofencing is the process of creating a
virtual fence around a so-called Point of Interest (PoI). In this paper, we
present an insight into various micro-location enabling technologies and
services. We also discuss how these can accelerate the incorporation of
Internet of Things (IoT) in smart buildings. We argue that micro-location based
location-aware solutions can play a significant role in facilitating the
tenants of an IoT equipped smart building. Also, such advanced technologies
will enable the smart building control system through minimal actions performed
by the tenants. We also highlight the existing and envisioned services to be
provided by using micro-location enabling technologies. We describe the
challenges and propose some potential solutions such that micro-location
enabling technologies and services are thoroughly integrated with IoT equipped
smart building.
"
1192,"Design, Analysis, and Simulation of a Pipe-Welding Robot with Fixed
  Plinth","  Industrial requirements concerning the increased efficiency and high rate of
manufacturing result in the development of manufacturer robots, and a vast
group of these types of robots is used for welding. This study presented the
design, analysis, and simulation of a pipe-welding robot with fixed plinth for
a constant circular welding around the pipes. Design of a welding robot capable
of keeping the electrode orientation, welding speed, and distance between
electrode and pipe surface constant can improve the quality of welding; thus, a
five-linked articulated robot was designed for this purpose. Solving of direct
and diverse kinematics and dynamics equations of the robot was done by means of
Matlab software. The robot was also simulated using a program written in Matlab
and the diagrams of angles, velocities, and accelerations of all the arms, and
the applied force and torque of each arm required for drive the mechanism were
obtained.
"
1193,"Multiple-Campaign Ad-Targeting Deployment: Parallel Response Modeling,
  Calibration and Scoring Without Personal User Information","  We present a vertical introduction to campaign optimization; that is, the
ability to predict the user response to an ad campaign without any users'
profiles on average and for each exposed ad. In practice, we present an
approach to build a polytomous model, multi response, composed by several
hundred binary models using generalized linear models. The theory has been
introduced twenty years ago and it has been applied in different fields since
then. Here, we show how we optimize hundreds campaigns and how this large
number of campaigns may overcome a few characteristic caveats of single
campaign optimization. We discuss the problem and solution of training and
calibration at scale. We present statistical performance as {\em coverage},
{\em precision} and {\em recall} used in classification. We present also a
discussion about the potential performance as throughput: how many decisions
can be done per second streaming the bid auctions also by using dedicated
hardware.
"
1194,"Image Processing Code for Sharpening Photoelastic Fringe Patterns and
  Its Usage in Determination of Stress Intensity Factors in a Sample Contact
  Problem","  This study presented a type of image processing code which is used for
sharpening photoelastic fringe patterns of transparent materials in
photoelastic experiences to determine the stress distribution. C-Sharp software
was utilized for coding the algorithm of this image processing method. For
evaluation of this code, the results of a photoelastic experience of a sample
contact problem between a half-plane with an oblique edge crack and a tilted
wedge using this image processing method was compared with the FEM results of
the same problem in order to obtain the stress intensity factors (SIF) of the
specimen. A good agreement between experimental results extracted from this
method of image processing and computational results was observed.
"
1195,Complexity of Power Draws for Load Disaggregation,"  Non-Intrusive Load Monitoring (NILM) is a technology offering methods to
identify appliances in homes based on their consumption characteristics and the
total household demand. Recently, many different novel NILM approaches were
introduced, tested on real-world data and evaluated with a common evaluation
metric. However, the fair comparison between different NILM approaches even
with the usage of the same evaluation metric is nearly impossible due to
incomplete or missing problem definitions. Each NILM approach typically is
evaluated under different test scenarios. Test results are thus influenced by
the considered appliances, the number of used appliances, the device type
representing the appliance and the pre-processing stages denoising the
consumption data. This paper introduces a novel complexity measure of
aggregated consumption data providing an assessment of the problem complexity
affected by the used appliances, the appliance characteristics and the
appliance usage over time. We test our load disaggregation complexity on
different real-world datasets and with a state-of-the-art NILM approach. The
introduced disaggregation complexity measure is able to classify the
disaggregation problem based on the used appliance set and the considered
measurement noise.
"
1196,"Extended Report: Fine-grained Recognition of Abnormal Behaviors for
  Early Detection of Mild Cognitive Impairment","  According to the World Health Organization, the rate of people aged 60 or
more is growing faster than any other age group in almost every country, and
this trend is not going to change in a near future. Since senior citizens are
at high risk of non communicable diseases requiring long-term care, this trend
will challenge the sustainability of the entire health system. Pervasive
computing can provide innovative methods and tools for early detecting the
onset of health issues. In this paper we propose a novel method to detect
abnormal behaviors of elderly people living at home. The method relies on
medical models, provided by cognitive neuroscience researchers, describing
abnormal activity routines that may indicate the onset of early symptoms of
mild cognitive impairment. A non-intrusive sensor-based infrastructure acquires
low-level data about the interaction of the individual with home appliances and
furniture, as well as data from environmental sensors. Based on those data, a
novel hybrid statistical-symbolical technique is used to detect the abnormal
behaviors of the patient, which are communicated to the medical center.
Differently from related works, our method can detect abnormal behaviors at a
fine-grained level, thus providing an important tool to support the medical
diagnosis. In order to evaluate our method we have developed a prototype of the
system and acquired a large dataset of abnormal behaviors carried out in an
instrumented smart home. Experimental results show that our technique is able
to detect most anomalies while generating a small number of false positives.
"
1197,Sparsity based Efficient Cross-Correlation Techniques in Sensor Networks,"  Cross-correlation is a popular signal processing technique used in numerous
location tracking systems for obtaining reliable range information. However,
its efficient design and practical implementation has not yet been achieved on
mote platforms that are typical in wireless sensor network due to resource
constrains. In this paper, we propose SparseS-XCorr: cross-correlation via
structured sparse representation, a new computing framework for ranging based
on L1-minimization and structured sparsity. The key idea is to compress the
ranging signal samples on the mote by efficient random projections and transfer
them to a central device; where a convex optimization process estimates the
range by exploiting the sparse signal structure in the proposed correlation
dictionary. Through theoretical validation, extensive empirical studies and
experiments on an end-to-end acoustic ranging system implemented on resource
limited off-the-shelf sensor nodes, we show that the proposed framework can
achieve up to two orders of magnitude better performance compared to other
approaches such as working on DCT domain and downsampling. Compared to the
standard cross-correlation, it is able to obtain range estimates with a bias of
2-6cm with 30% and approximately 100cm with 5% compressed measurements. Its
structured sparsity model is able to improve the ranging accuracy by 40% under
challenging recovery conditions (such as high compression factor and low
signal-to-noise ratio) by overcoming limitations due to dictionary coherence.
"
1198,"A Simulation Modeling Approach for Optimization of Storage Space
  Allocation in Container Terminal","  Container handling problems at container terminals are NP-hard problems. This
paper presents an approach using discrete-event simulation modeling to optimize
solution for storage space allocation problem, taking into account all various
interrelated container terminal handling activities. The proposed approach is
applied on a real case study data of container terminal at Alexandria port. The
computational results show the effectiveness of the proposed model for
optimization of storage space allocation in container terminal where 54%
reduction in containers handling time in port is achieved.
"
1199,Contemporary Internet of Things platforms,"  This document regroups a representative, but non-exhaustive, list of
contemporary IoT platforms. The platforms are ordered alphabetically. The aim
of this document is to provide the a quick review of current IoT platforms, as
well as relevant information.
"
1200,"ECPR: Environment- and Context-aware Combined Power and Rate Distributed
  Congestion Control for Vehicular Communications","  Safety and efficiency applications in vehicular networks rely on the exchange
of periodic messages between vehicles. These messages contain position, speed,
heading, and other vital information that makes the vehicles aware of their
surroundings. The drawback of exchanging periodic cooperative messages is that
they generate significant channel load. Decentralized Congestion Control (DCC)
algorithms have been proposed to minimize the channel load. However, while the
rationale for periodic message exchange is to improve awareness, existing DCC
algorithms do not use awareness as a metric for deciding when, at what power,
and at what rate the periodic messages need to be sent in order to make sure
all vehicles are informed. We propose an environment- and context-aware DCC
algorithm combines power and rate control in order to improve cooperative
awareness by adapting to both specific propagation environments (e.g., urban
intersections, open highways, suburban roads) as well as application
requirements (e.g., different target cooperative awareness range). Studying
various operational conditions (e.g., speed, direction, and application
requirement), ECPR adjusts the transmit power of the messages in order to reach
the desired awareness ratio at the target distance while at the same time
controlling the channel load using an adaptive rate control algorithm. By
performing extensive simulations, including realistic propagation as well as
environment modeling and realistic vehicle operational environments (varying
demand on both awareness range and rate), we show that ECPR can increase
awareness by 20% while keeping the channel load and interference at almost the
same level. When permitted by the awareness requirements, ECPR can improve the
average message rate by 18% compared to algorithms that perform rate adaptation
only.
"
1201,"E-BLOW: E-Beam Lithography Overlapping aware Stencil Planning for MCC
  System","  Electron beam lithography (EBL) is a promising maskless solution for the
technology beyond 14nm logic node. To overcome its throughput limitation,
industry has proposed character projection (CP) technique, where some complex
shapes (characters) can be printed in one shot. Recently the traditional EBL
system is extended into multi-column cell (MCC) system to further improve the
throughput. In MCC system, several independent CPs are used to further speed-up
the writing process. Because of the area constraint of stencil, MCC system
needs to be packed/planned carefully to take advantage of the characters. In
this paper, we prove that the overlapping aware stencil planning (OSP) problem
is NP-hard. To solve OSP problem in MCC system, we present a tool, E-BLOW, with
several novel speedup techniques, such as successive relaxation, dynamic
programming, and KD-Tree based clustering. Experimental results show that,
compared with previous works, E-BLOW demonstrates better performance for both
conventional EBL system and MCC system.
"
1202,"CHAOS: Accurate and Realtime Detection of Aging-Oriented Failure Using
  Entropy","  Even well-designed software systems suffer from chronic performance
degradation, also named ""software aging"", due to internal (e.g. software bugs)
and external (e.g. resource exhaustion) impairments. These chronic problems
often fly under the radar of software monitoring systems before causing severe
impacts (e.g. system failure). Therefore it's a challenging issue how to timely
detect these problems to prevent system crash. Although a large quantity of
approaches have been proposed to solve this issue, the accuracy and
effectiveness of these approaches are still far from satisfactory due to the
insufficiency of aging indicators adopted by them. In this paper, we present a
novel entropy-based aging indicator, Multidimensional Multi-scale Entropy
(MMSE). MMSE employs the complexity embedded in runtime performance metrics to
indicate software aging and leverages multi-scale and multi-dimension
integration to tolerate system fluctuations. Via theoretical proof and
experimental evaluation, we demonstrate that MMSE satisfies Stability,
Monotonicity and Integration which we conjecture that an ideal aging indicator
should have. Based upon MMSE, we develop three failure detection approaches
encapsulated in a proof-of-concept named CHAOS. The experimental evaluations in
a Video on Demand (VoD) system and in a real-world production system,
AntVision, show that CHAOS can detect the failure-prone state in an
extraordinarily high accuracy and a near 0 Ahead-Time-To-Failure (ATTF).
Compared to previous approaches, CHAOS improves the detection accuracy by about
5 times and reduces the ATTF even by 3 orders of magnitude. In addition, CHAOS
is light-weight enough to satisfy the realtime requirement.
"
1203,"Etude des d\'eterminants psychologiques de la persistance dans l'usage
  d'un jeu s\'erieux : \'evaluation de l'environnement optimal d'apprentissage
  avec Mecagenius?","  The aim of this paper is to show the relevance of motivational key concepts
in evaluating the use of serious game. This research involves 115 students
training with Mecagenius (serious game in mechanical engineering). The results
of the exploratory study also confirm the relevance of the use of flow in
Education scale (EduFlow) to evaluate the optimal learning experienc ewith a
serious game. It also appears that EduFlow is related to specific actions
within the school context such as self-efficacy, motivational climate and
interest.
"
1204,"FRAME: Fast and Realistic Attacker Modeling and Evaluation for Temporal
  Logical Correlation in Static Noise","  We propose a method called Fast and Realistic Attacker Modeling and
Evaluation (FRAME) that can reduce pessimism in static noise analysis by
exploiting temporal logical correlation of attackers and using novel techniques
termed envelopes and $\sigma$ functions. Unlike conventional pruning-based
approaches, FRAME efficiently considers all relevant attackers, thereby
producing more realistic results. FRAME was tested with complex industrial
design and successfully reduced the pessimism of conventional techniques by
30.4% on average, with little computational overhead.
"
1205,Physical Biomodeling: a new field enabled by 3-D printing in biomodeling,"  Accurate physical modeling with 3D-printing techniques could lead to new
approaches to study structure and dynamics of biological systems complementing
computational methods. Computational biology has become an important part of
research over the last couple of decades. Now 3D printing technology opens the
door for a new field, Physical Biomodeling, at the intersection of experimental
data, computational biology and physical modeling for study of biological
systems, such as protein folding at nano-scale. Here I explore this new domain
of precision physical modeling and correlate it with existing visualization and
computational systems and future possibilities. Dynamic physical models can be
designed to-scale that can serve as research tools in future along with
existing biocomputational tools and databases, adding a third angle to tackle
unsolved scientific problems.
"
1206,"Privately Information Sharing with Delusive Paths for Data Forwarding in
  Vehicular Networks","  We discuss how to efficiently forward data in vehicular networks. Existing
solutions do not make full use of trajectory planning of nearby vehicles, or
social attributes. The development of onboard navigation system provides
drivers some traveling route information. The main novelty of our approach is
to envision sharing partial traveling information to the encountered vehicles
for better service. Our data forwarding algorithm utilizes this lightweight
information under the delusive paths privacy preservation together with the
social community structure in vehicular networks. We assume that data
transmission is carried by vehicles and road side units (RSUs), while cellular
network manages and coordinates relevant global information. The approximate
destination set is the set of RSUs that are often passed by the destination
vehicle. RSU importance is raised by summing encounter ratios of RSUs in the
same connected component. We first define a concept of space-time
approachability which is derived from shared partial traveling route and
encounter information. It describes the capability of a vehicle to advance
messages toward destination. Then, we design a novel data forwarding algorithm,
called approachability based algorithm, which combines the space-time
approachability with the social community attribute in vehicular networks. We
evaluate our approachability based algorithm on data sets from San Francisco
Cabspotting and Shanghai Taxi Movement. Results show that the partially shared
traveling information plays a positive role in data forwarding in vehicular
networks. Approachability based data forwarding algorithm achieves a better
performance than existing social based algorithms in vehicular networks.
"
1207,"Analysis of Lithography Based Approaches In development of Semi
  Conductors","  The end of the 19th century brought about a change in the dynamics of
computing by the development of the microprocessor. Huge bedroom size computers
began being replaced by portable, smaller sized desktops. Today the world is
dominated by silicon, which has circumscribed chip development for computers
through microprocessors. Majority of the integrated circuits that are
manufactured at present are developed using the concept of Lithography. This
paper presents a detailed analysis of multiple Lithography methodologies as a
means for advanced integrated circuit development. The study paper primarily
restricts to examples in the context of Lithography, surveying the various
existing techniques of Lithography in literature, examining feasible and
efficient methods, highlighting the various pros and cons of each of them.
"
1208,"Converting ECG and other paper legated biomedical maps into digital
  signals","  This paper presents a digital signal processing tool developed using
MatlabTM, which provides a very low-cost and effective strategy for
analog-to-digital conversion of legated paper biomedical maps without requiring
dedicated hardware. This software-based approach is particularly helpful for
digitalizing biomedical signals acquired from analogical devices equipped with
a plottingter. Albeit signals used in biomedical diagnosis are the primary
concern, this imaging processing tool is suitable to modernize facilities in a
non-expensive way. Legated paper ECG and EEG charts can be fast and efficiently
digitalized in order to be added in existing up-to-date medical data banks,
improving the follow-up of patients.
"
1209,"Improved Model for Wire-Length Estimation in Stochastic Wiring
  Distribution","  This paper presents a pair of improved stochastic wiring distribution model
for better estimation of on-chip wire lengths. The proposed models provide 28 -
50% reduction in error when estimating the average on-chip wire length compared
to the estimation using the existing models. The impact of Rent's exponent on
the average wire length estimation is also investigated to demonstrate
limitations of the approximations used in some of the current models. To
improve the approximations of the model a new threshold for Rent's constant is
recommended. Simulation results demonstrate that proposed models with the new
threshold reduce the error of estimation by 38 to 75 percent compared to the
previous works.
"
1210,"Euclidean Distance Matrices: Essential Theory, Algorithms and
  Applications","  Euclidean distance matrices (EDM) are matrices of squared distances between
points. The definition is deceivingly simple: thanks to their many useful
properties they have found applications in psychometrics, crystallography,
machine learning, wireless sensor networks, acoustics, and more. Despite the
usefulness of EDMs, they seem to be insufficiently known in the signal
processing community. Our goal is to rectify this mishap in a concise tutorial.
We review the fundamental properties of EDMs, such as rank or
(non)definiteness. We show how various EDM properties can be used to design
algorithms for completing and denoising distance data. Along the way, we
demonstrate applications to microphone position calibration, ultrasound
tomography, room reconstruction from echoes and phase retrieval. By spelling
out the essential algorithms, we hope to fast-track the readers in applying
EDMs to their own problems. Matlab code for all the described algorithms, and
to generate the figures in the paper, is available online. Finally, we suggest
directions for further research.
"
1211,GREAT Process Modeller user manual,"  This report contains instructions to install, uninstall and use GREAT Process
Modeller, a tool that supports Communication Analysis, a communication-oriented
business process modelling method. GREAT allows creating communicative event
diagrams (i.e. business process models), specifying message structures (which
describe the messages associated to each communicative event), and
automatically generating a class diagram (representing the data model of an
information system that would support such organisational communication). This
report briefly describes the methodological background of the tool. This
handbook explains the modelling techniques in detail: Espa\~na, S., A.
Gonz\'alez, \'O. Pastor and M. Ruiz (2012). Communication Analysis modelling
techniques. Technical report ProS-TR-2012-02, PROS Research Centre, Universitat
Polit\`ecnica de Val\`encia, Spain, arXiv:1205.0987.
"
1212,Globally Optimal Crowdsourcing Quality Management,"  We study crowdsourcing quality management, that is, given worker responses to
a set of tasks, our goal is to jointly estimate the true answers for the tasks,
as well as the quality of the workers. Prior work on this problem relies
primarily on applying Expectation-Maximization (EM) on the underlying maximum
likelihood problem to estimate true answers as well as worker quality.
Unfortunately, EM only provides a locally optimal solution rather than a
globally optimal one. Other solutions to the problem (that do not leverage EM)
fail to provide global optimality guarantees as well. In this paper, we focus
on filtering, where tasks require the evaluation of a yes/no predicate, and
rating, where tasks elicit integer scores from a finite domain. We design
algorithms for finding the global optimal estimates of correct task answers and
worker quality for the underlying maximum likelihood problem, and characterize
the complexity of these algorithms. Our algorithms conceptually consider all
mappings from tasks to true answers (typically a very large number), leveraging
two key ideas to reduce, by several orders of magnitude, the number of mappings
under consideration, while preserving optimality. We also demonstrate that
these algorithms often find more accurate estimates than EM-based algorithms.
This paper makes an important contribution towards understanding the inherent
complexity of globally optimal crowdsourcing quality management.
"
1213,"Parameter Estimation of Jelinski-Moranda Model Based on Weighted
  Nonlinear Least Squares and Heteroscedasticity","  Parameter estimation method of Jelinski-Moranda (JM) model based on weighted
nonlinear least squares (WNLS) is proposed. The formulae of resolving the
parameter WNLS estimation (WNLSE) are derived, and the empirical weight
function and heteroscedasticity problem are discussed. The effects of
optimization parameter estimation selection based on maximum likelihood
estimation (MLE) method, least squares estimation (LSE) method and weighted
nonlinear least squares estimation (WNLSE) method are also investigated. Two
strategies of heteroscedasticity decision and weighting methods embedded in JM
model prediction process are also investigated. The experimental results on
standard software reliability analysis database-Naval Tactical Data System
(NTDS) and three datasets used by J.D. Musa demonstrate that WNLSE method can
be superior to LSE and MLE under the relative error (RE) criterion.
"
1214,Google-based Mode Choice Modeling Approach,"  Microsimulation based frameworks have become very popular in many research
areas including travel demand modeling where activity-based models have been in
the center of attention for the past decade. Advanced activity-based models
synthesize the entire population of the study region and simulate their
activities in a way that they can keep track of agents resources as well as
their spatial location. However, the models that are built for these frameworks
do not take into account this information mainly because they do not have them
at the modeling stage. This paper tries to describe the importance of this
information by analyzing a travel survey and generate the actual alternatives
that individuals had when making their trips. With a focus on transit, the
study reveals how transit alternatives are limited\unavailable in certain areas
which must be taken in to account in our mode choice models. Some statistics
regarding available alternatives and the constraints people encounter when
making a choice are presented with a comprehensive choice set formation. A mode
choice model is then developed based on this approach to represent the
importance of such information.
"
1215,What Is an Emerging Technology?,"  There is considerable and growing interest in the emergence of novel
technologies, especially from the policy-making perspective. Yet as an area of
study, emerging technologies lacks key foundational elements, namely a
consensus on what classifies a technology as 'emergent' and strong research
designs that operationalize central theoretical concepts. The present paper
aims to fill this gap by developing a definition of 'emerging technologies' and
linking this conceptual effort with the development of a framework for the
operationalisation of technological emergence. The definition is developed by
combining a basic understanding of the term and in particular the concept of
'emergence' with a review of key innovation studies dealing with definitional
issues of technological emergence. The resulting definition identifies five
attributes that feature in the emergence of novel technologies. These are: (i)
radical novelty, (ii) relatively fast growth, (iii) coherence, (iv) prominent
impact, and (v) uncertainty and ambiguity. The framework for operationalising
emerging technologies is then elaborated on the basis of the proposed
attributes. To do so, we identify and review major empirical approaches (mainly
in, although not limited to, the scientometric domain) for the detection and
study of emerging technologies (these include indicators and trend analysis,
citation analysis, co-word analysis, overlay mapping, and combinations thereof)
and elaborate on how these can be used to operationalise the different
attributes of emergence.
"
1216,"Photoplethysmography-Based Heart Rate Monitoring in Physical Activities
  via Joint Sparse Spectrum Reconstruction","  Goal: A new method for heart rate monitoring using photoplethysmography (PPG)
during physical activities is proposed. Methods: It jointly estimates spectra
of PPG signals and simultaneous acceleration signals, utilizing the multiple
measurement vector model in sparse signal recovery. Due to a common sparsity
constraint on spectral coefficients, the method can easily identify and remove
spectral peaks of motion artifact (MA) in PPG spectra. Thus, it does not need
any extra signal processing modular to remove MA as in some other algorithms.
Furthermore, seeking spectral peaks associated with heart rate is simplified.
Results: Experimental results on 12 PPG datasets sampled at 25 Hz and recorded
during subjects' fast running showed that it had high performance. The average
absolute estimation error was 1.28 beat per minute and the standard deviation
was 2.61 beat per minute. Conclusion and Significance: These results show that
the method has great potential to be used for PPG-based heart rate monitoring
in wearable devices for fitness tracking and health monitoring.
"
1217,Fault Analysis Using Gegenbauer Multiresolution Analysis,"  This paper exploits the multiresolution analysis in the fault analysis on
transmission lines. Faults were simulated using the ATP (Alternative Transient
Program), considering signals at 128/cycle. A nonorthogonal multiresolution
analysis was provided by Gegenbauer scaling and wavelet filters. In the cases
where the signal reconstruction is not required, orthogonality may be
immaterial. Gegenbauer filter banks are thereby offered in this paper as a tool
for analyzing fault signals on transmission lines. Results are compared to
those ones derived from a 4-coefficient Daubechies filter. The main advantages
in favor of Gegenbauer filters are their smaller computational effort and their
constant group delay, as they are symmetric filters.
"
1218,"Towards an intelligent VNS heuristic for the k-labelled spanning forest
  problem","  In a currently ongoing project, we investigate a new possibility for solving
the k-labelled spanning forest (kLSF) problem by an intelligent Variable
Neighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given
an undirected input graph G and an integer positive value k, and the aim is to
find a spanning forest of G having the minimum number of connected components
and the upper bound k on the number of labels to use. The problem is related to
the minimum labelling spanning tree (MLST) problem, whose goal is to get the
spanning tree of the input graph with the minimum number of labels, and has
several applications in the real world, where one aims to ensure connectivity
by means of homogeneous connections. The Int-VNS metaheuristic that we propose
for the kLSF problem is derived from the promising intelligent VNS strategy
recently proposed for the MLST problem, and integrates the basic VNS for the
kLSF problem with other complementary approaches from machine learning,
statistics and experimental algorithmics, in order to produce high-quality
performance and to completely automate the resulting strategy.
"
1219,"Modeling and Improving the Energy Performance of GPS Receivers for
  Mobile Applications","  Integrated GPS receivers have become a basic module in today's mobile
devices. While serving as the cornerstone for location based services, GPS
modules have a serious battery drain problem due to high computation load. This
paper aims to reveal the impact of key software parameters on hardware energy
consumption, by establishing an energy model for a standard GPS receiver
architecture as found in both academic and industrial designs. In particular,
our measurements show that the receiver's energy consumption is in large part
linear with the number of tracked satellites. This leads to a design of
selective tracking algorithm that provides similar positioning accuracy (around
12m) with a subset of selected satellites, which translates to an energy saving
of 20.9-23.1\% on the Namuru board.
"
1220,When In-Memory Computing is Slower than Heavy Disk Usage,"  Disk access latency and transfer times are often considered to have a major
and detrimental impact on the running time of software. Developers are often
advised to favour in-memory operations and minimise disk access. Furthermore,
diskless computer architectures are being studied and designed to remove this
bottleneck all together, to improve application performance in areas such as
High Performance Computing, Big Data, and Business Intelligence. In this paper
we use code inspired by real, production software, to show that in-memory
operations are not always a guarantee for high performance, and may actually
cause a considerable slow-down. We also show how small code changes can have
dramatic effects on running times. We argue that a combination of system-level
improvements and better developer awareness and coding practices are necessary
to ensure in-memory computing can achieve its full potential.
"
1221,Hardware Probing Interface and Test Robustness,"  Computerized integrity test of an electronic product hardware interface and
product probing validation are considered. Integrity testing is based on a
current voltage characteristic measurement, when a small voltage and/or current
stimuli are applied to the product pads including power supply circuitry pads,
so that the product is not normally powered on. Test fixture needles validation
is a part of a self test maintenance scenario designed to predict deterioration
of product probing.
"
1222,Distributed Computation Particle PHD filter,"  Particle probability hypothesis density filtering has become a promising
means for multi-target tracking due to its capability of handling an unknown
and time-varying number of targets in non-linear non-Gaussian system. However,
its computational complexity grows linearly with the number of measurements and
particles assigned to each target, and this can be very time consuming
especially when numerous targets and clutter exist in the surveillance region.
Addressing this issue, we present a distributed computation particle PHD filter
for target tracking. Its framework consists of several local particle PHD
filters at each processing element and a central unit. Each processing element
takes responsibility for part particles but full measurements and provides
local estimates; central unit controls particle exchange between processing
elements and specifies a fusion rule to match and fuse the estimates from
different local filters. The proposed framework is suitable for parallel
implementation and maintains the tracking accuracy. Simulations verify the
proposed method can provide comparative accuracy as well as a significant
speedup with the standard particle PHD filter.
"
1223,Synthesis of all Maximum Length Cellular Automata of Cell Size up to 12,"  Maximum length CA has wide range of applications in design of linear block
code, cryptographic primitives and VLSI testing particularly in
Built-In-Self-Test. In this paper, an algorithm to compute all $n$-cell maximum
length CA-rule vectors is proposed. Also rule vectors for each primitive
polynomial in GF(2^2) to GF(2^{12} have been computed by simulation and they
have been listed.Programmable rule vectors based maximum length CA can be used
to design cryptographic primitives.
"
1224,"Designing and Building a Three-dimensional Projective Scanner for
  Smartphones","  One of the frustrating things in the digital fabrication era is that its
media are neither affordable nor easily accessible and usable.
Three-dimensional (3D) fabrication media (DFM) such as 3D Printers and 3D
Scanners have experienced an upsurge in popularity, while the latter remain
expensive and hard to function. With this paper, we aim to present you the
RhoScanner Project - a an affordable and efficient Three-dimensional Projective
Scanner for Smart-phones, hence shedding light on the extended capabilities of
digital fabrication media on popular use.
"
1225,"Feeder Load Balancing using Fuzzy Logic and Combinatorial
  Optimization-based Implementation","  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes a novel
reconfiguration of the phase balancing using the fuzzy logic and the
combinatorial optimization-based implementation step back to back. Input to the
fuzzy step is the total load per phase of the feeders. Output of the fuzzy step
is the load change values, negative value for load releasing and positive value
for load receiving. The output of the fuzzy step is the input to the load
changing system. The load changing system uses combinatorial optimization
techniques to translate the change values (kW) into number of load points and
then selects the specific load points. It also performs the inter-changing of
the load points between the releasing and the receiving phases in an optimal
fashion. Application results using the distribution feeder network of South
Africa are presented in this paper.
"
1226,"Adjusted Haar Wavelet for Application in the Power Systems Disturbance
  Analysis","  Abrupt change detection based on the wavelet transform and threshold method
is very effective in detecting the abrupt changes and hence segmenting the
signals recorded during disturbances in the electrical power network. The
wavelet method estimates the time-instants of the changes in the signal model
parameters during the pre-fault condition, after initiation of fault, after
circuit-breaker opening and auto-reclosure. Certain kinds of disturbance
signals do not show distinct abrupt changes in the signal parameters. In those
cases, the standard mother wavelets fail to achieve correct event-specific
segmentations. A new adjustment technique to the standard Haar wavelet is
proposed in this paper, by introducing 2n adjusting zeros in the Haar wavelet
scaling filter, n being a positive integer. This technique is quite effective
in segmenting those fault signals into pre- and post-fault segments, and it is
an improvement over the standard mother wavelets for this application. This
paper presents many practical examples where recorded signals from the power
network in South Africa have been used.
"
1227,"A software controlled voltage tuning system using multi-purpose ring
  oscillators","  This paper presents a novel software driven voltage tuning method that
utilises multi-purpose Ring Oscillators (ROs) to provide process variation and
environment sensitive energy reductions. The proposed technique enables voltage
tuning based on the observed frequency of the ROs, taken as a representation of
the device speed and used to estimate a safe minimum operating voltage at a
given core frequency. A conservative linear relationship between RO frequency
and silicon speed is used to approximate the critical path of the processor.
  Using a multi-purpose RO not specifically implemented for critical path
characterisation is a unique approach to voltage tuning. The parameters
governing the relationship between RO and silicon speed are obtained through
the testing of a sample of processors from different wafer regions. These
parameters can then be used on all devices of that model. The tuning method and
software control framework is demonstrated on a sample of XMOS XS1-U8A-64
embedded microprocessors, yielding a dynamic power saving of up to 25% with no
performance reduction and no negative impact on the real-time constraints of
the embedded software running on the processor.
"
1228,Normalization: A Preprocessing Stage,"  As we know that the normalization is a pre-processing stage of any type
problem statement. Especially normalization takes important role in the field
of soft computing, cloud computing etc. for manipulation of data like scale
down or scale up the range of data before it becomes used for further stage.
There are so many normalization techniques are there namely Min-Max
normalization, Z-score normalization and Decimal scaling normalization. So by
referring these normalization techniques we are going to propose one new
normalization technique namely, Integer Scaling Normalization. And we are going
to show our proposed normalization technique using various data sets.
"
1229,Practical Denoising of MEG Data using Wavelet Transform,"  Magnetoencephalography (MEG) is an important noninvasive, nonhazardous
technology for functional brain mapping, measuring the magnetic fields due to
the intracellular neuronal current flow in the brain. However, the inherent
level of noise in the data collection process is large enough to obscure the
signal(s) of interest most often. In this paper, a practical denoising
technique based on the wavelet transform and the multiresolution signal
decomposition technique is presented. The proposed technique is substantiated
by the application results using three different mother wavelets on the
recorded MEG signal.
"
1230,"Optimization of gridshell bar orientation using a simplified genetic
  approach","  Gridshells are defined as structures that have the shape and rigidity of a
double curvature shell but consist of a grid instead of a continuous surface.
This study concerns those obtained by elastic deformation of an initially flat
two-way grid. This paper presents a novel approach to generate gridshells on an
imposed shape under imposed boundary conditions. A numerical tool based on a
geometrical method, the compass method, is developed. It is coupled with
genetic algorithms to optimize the orientation of gridshell bars in order to
minimize the stresses and therefore to avoid bar breakage during the
construction phase. Examples of application are shown.
"
1231,"A parametric study of window-to-floor ratio of three window types using
  dynamic simulation","  The windows can be responsible for unnecessary energy consumption in a
building, if incorrectly designed, shadowed or oriented. Considering an annual
thermal comfort assessment of a space, if windows are over-dimensioned, they
can contribute to the increase of the heating needs due to heat losses, and
also to the increase of cooling needs due to over-exposure to solar radiation.
When under-dimensioned, the same space may benefit from reduced heat losses
through the glazing surface but does not benefit from solar radiation gains.
Therefore, it is important to find the optimum design that minimizes both the
heating and cooling needs. This paper presents a parametric study of window
type (single, double and triple glazing), orientation and opening size, located
in the city of Coimbra, Portugal. An annual and a seasonal assessment were
done, in order to obtain the set of optimum values around 360 degree
orientation.
"
1232,An embedded system for real-time feedback neuroscience experiments,"  A complete data acquisition and signal output control system for synchronous
stimuli generation, geared towards in vivo neuroscience experiments, was
developed using the Terasic DE2i-150 board. All emotions and thoughts are an
emergent property of the chemical and electrical activity of neurons. Most of
these cells are regarded as excitable cells (spiking neurons), which produce
temporally localized electric patterns (spikes). Researchers usually consider
that only the instant of occurrence (timestamp) of these spikes encodes
information. Registering neural activity evoked by stimuli demands timing
determinism and data storage capabilities that cannot be met without dedicated
hardware and a hard real-time operational system (RTOS). Indeed, research in
neuroscience usually requires dedicated electronic instrumentation for studies
in neural coding, brain machine interfaces and closed loop in vivo or in vitro
experiments. We developed a complete embedded system solution consisting of a
hardware/software co-design with the Intel Atom processor running a free RTOS
and a FPGA communicating via a PCIe-to-Avalon bridge. Our system is capable of
registering input event timestamps with 1{\mu}s precision and digitally
generating stimuli output in hard real-time. The whole system is controlled by
a Linux-based Graphical User Interface (GUI). Collected results are
simultaneously saved in a local file and broadcasted wirelessly to mobile
device web-browsers in an user-friendly graphic format, enhanced by HTML5
technology. The developed system is low-cost and highly configurable, enabling
various neuroscience experimental setups, while the commercial off-the-shelf
systems have low availability and are less flexible to adapt to specific
experimental configurations.
"
1233,Preprint Traffic Management and Forecasting System Based on 3D GIS,"  This is the preprint version of our paper on 2015 15th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing (CCGrid). This paper takes
Shenzhen Futian comprehensive transportation junction as the case, and makes
use of continuous multiple real-time dynamic traffic information to carry out
monitoring and analysis on spatial and temporal distribution of passenger flow
under different means of transportation and service capacity of junction from
multi-dimensional space-time perspectives such as different period and special
period. Virtual reality geographic information system is employed to present
the forecasting result.
"
1234,Understanding Soft Errors in Uncore Components,"  The effects of soft errors in processor cores have been widely studied.
However, little has been published about soft errors in uncore components, such
as memory subsystem and I/O controllers, of a System-on-a-Chip (SoC). In this
work, we study how soft errors in uncore components affect system-level
behaviors. We have created a new mixed-mode simulation platform that combines
simulators at two different levels of abstraction, and achieves 20,000x speedup
over RTL-only simulation. Using this platform, we present the first study of
the system-level impact of soft errors inside various uncore components of a
large-scale, multi-core SoC using the industrial-grade, open-source OpenSPARC
T2 SoC design. Our results show that soft errors in uncore components can
significantly impact system-level reliability. We also demonstrate that uncore
soft errors can create major challenges for traditional system-level checkpoint
recovery techniques. To overcome such recovery challenges, we present a new
replay recovery technique for uncore components belonging to the memory
subsystem. For the L2 cache controller and the DRAM controller components of
OpenSPARC T2, our new technique reduces the probability that an application run
fails to produce correct results due to soft errors by more than 100x with
3.32% and 6.09% chip-level area and power impact, respectively.
"
1235,Definition and Research of Internet Neurology,"  More and more scientific research shows that there is a close correlation
between the Internet and brain science. This paper presents the idea of
establishing the Internet neurology, which means to make a cross-contrast
between the two in terms of physiology and psychology, so that a complete
infrastructure system of the Internet is established, predicting the
development trend of the Internet in the future as well as the brain structure
and operation mechanism, and providing theoretical support for the generation
principle of intelligence, cognition and emotion. It also proposes the
viewpoint that the Internet can be divided into Internet neurophysiology,
Internet neuropsychology, Brain Internet physiology, Brain Internet psychology
and the Internet in cognitive science.
"
1236,Any non-affine one-to-one binary gate suffices for computation,"  Any non-affine one-to-one binary gate can be wired together with suitable
inputs to give AND, OR, NOT and fan-out gates, and so suffices to construct a
general-purpose computer.
"
1237,Android based security and home automation system,"  The smart mobile terminal operator platform Android is getting popular all
over the world with its wide variety of applications and enormous use in
numerous spheres of our daily life. Considering the fact of increasing demand
of home security and automation, an Android based control system is presented
in this paper where the proposed system can maintain the security of home main
entrance and also the car door lock. Another important feature of the designed
system is that it can control the overall appliances in a room. The mobile to
security system or home automation system interface is established through
Bluetooth. The hardware part is designed with the PIC microcontroller.
"
1238,"An Improved Variable Step-size Zero-point Attracting Projection
  Algorithm","  This paper proposes an improved variable step-size (VSS) scheme for
zero-point attracting projection (ZAP) algorithm. The proposed VSS is
proportional to the sparseness difference between filter coefficients and the
true impulse response. Meanwhile, it works for both sparse and non-sparse
system identification, and simulation results demonstrate that the proposed
algorithm could provide both faster convergence rate and better tracking
ability than previous ones.
"
1239,"A parametric study on window-to-floor ratio of double window glazing and
  its shadowing using dynamic simulation","  When incorrectly designed, windows can be responsible for unnecessary energy
consumption in a building. This may result from its dimensions, orientation and
shadowing. In a moderate climate like the Portuguese, and considering an annual
thermal comfort assessment of a space, if windows are under-dimensioned or
over-shadowed, they can contribute to the increase of heating needs. However,
when over-dimensioned or under-shadowed, they contribute to the increase of
cooling requirements. Therefore, it is important to find the optimum design
that balances orientation, dimension and shadowing, contributing to minimize
both the heating and cooling needs. This study presents a parametric analysis
of a double glazing window in its orientation and dimension, located in the
Portuguese city of Coimbra. For each window orientation and dimension, the
optimum overhang depth is determined. The objective is to minimize degree-hours
of thermal discomfort. Results show that overhangs are mainly a corrective
mechanism to over-dimensioned openings, thus allowing that building
practitioners may choose a wider range of windows dimensions.
"
1240,"STC: Coarse-Grained Vehicular Data Based Travel Speed Sensing by
  Leveraging Spatial-Temporal Correlation","  As an important information for traffic condition evaluation, trip planning,
transportation management, etc., average travel speed for a road means the
average speed of vehicles travelling through this road in a given time
duration. Traditional ways for collecting travel-speed oriented traffic data
always depend on dedicated sensors and supporting infrastructures, and are
therefore financial costly. Differently, vehicular crowdsensing as an
infrastructure-free way, can be used to collect data including real-time
locations and velocities of vehicles for road travel speed estimation, which is
a quite low-cost way. However, vehicular crowdsensing data is always
coarse-grained. This coarseness can lead to the incompleteness of travel
speeds. Aiming to handle this problem as well as estimate travel speed
accurately, in this paper, we propose an approach named STC that exploits the
spatial-temporal correlation among travel speeds for roads by introducing the
time-lagged cross correlation function. The time lagging factor describes the
time consumption of traffic feature diffusion along roads. To properly
calculate cross correlation, we novelly make the determination of the time
lagging factor self-adaptive by recording the locations of vehicles at
different roads. Then, utilizing the local stationarity of cross correlation,
we further reduce the problem of single-road travel speed vacancy completion to
a minimization problem. Finally, we fill all the vacancies of travel speed for
roads in a recursive way using the geometric structure of road net. Elaborate
experiments based on real taxi trace data show that STC can settle the
incompleteness problem of vehicle crowdsensing data based travel speed
estimation and ensure the accuracy of estimated travel speed better, in
comparison with representative existing methods such as KNN, Kriging and ARIMA.
"
1241,Highest Trees of Random Mappings,"  We prove the exact asymptotic
$1-\left({\frac{2\pi}{3}-\frac{827}{288\pi}}+o(1)\right)/{\sqrt{n}}$ for the
probability that the underlying graph of a random mapping of $n$ elements
possesses a unique highest tree. The property of having a unique highest tree
turned out to be crucial in the solution of the famous Road Coloring Problem as
well as the generalization of this property in the proof of the author's result
about the probability of being synchronizable for a random automaton.
"
1242,"A Simple and General Problem and its Optimal Randomized Online Algorithm
  Design with Competitive Analysis","  The online algorithm design was proposed to handle the caching problem when
the future information is unknown. And currently, it draws more and more
attentions from the researchers from the areas of microgrid, where the
production of renewables are unpredictable.
  In this note, we present a framework of randomized online algorithm design
for the \textit{simple and tractable} problem. This framework hopes to provide
a tractable design to design a randomized online algorithm, which can be proved
to achieve the best competitive ratio by \textit{Yao's Principle}.
"
1243,"A Digital/Analog Signal Processing Method to Compute the Number of
  Hamiltonian Paths","  This paper explores finding the number $n_h$ of undirected hamiltonian paths
in an undirected graph $G=(V,E)$ using lumped/ideal circuits, specifically
low-pass filters. Ideal analog computation allows one to computer $n_h$ in a
short period of time, but in practice, precision problems disturb this ideal
nature. A digital/algorithmic approach is proposed, and then it is shown that
the approach/method operates under theoretically feasible (polynomial) time:
$O(n^{230})+O(n_d PC(p_2,n_d))$, where $n=|V|$ and $PC(p_2,n_d)$ refers to the
time complexity of finding a root of a polynomial with polynomial coefficients
being $p_2$ digits (with coefficients limited within $2^{p_2/2}$ and
$1/2^{p_2/2}$ in magnitude) and $n_d$ referring to number of degree, with $p_2
=n^{50}$ and $n_d = n^{10}$.
"
1244,"State of the Art of the Intra-Task Dynamic Voltage and Frequency Scaling
  Technique","  In recent years there has been an increasing use of embedded systems because
of advances in technology, the reduction of the costs of electronic equipment
and mainly the popularity of mobile devices. Many of these systems implement
low power consumption policies to extend their autonomy, usually because they
have a reduced amount of resources and the great majority of them use electric
power from batteries. One way to minimize the power consumption of these
devices is through of the application of low power consumption techniques. From
the various techniques presented in the literature - the intra-task Dynamic
Voltage and Frequency Scaling (DVFS) has played an important role. The main aim
of DVFS is to allow each task to manage the minimum resources necessary for
tasks execution, this way reducing the processor power consumption and, at the
same time, respecting the task deadlines when considered a real-time system
context. Therefore, this paper aims to apply a systematic literature review
with the goal of identifying and presenting the main methods using the
intra-task DVFS technique, applied in the context of real-time systems to
reduce energy consumption on the processor. Finally, this work will show the
advantages and disadvantages of each cataloged methodology.
"
1245,"The Fallacy of Favoring Gradual Replacement Mind Uploading Over
  Scan-and-Copy","  Mind uploading speculation and debate often concludes that a procedure
described as gradual in-place replacement preserves personal identity while a
procedure described as destructive scan-and-copy produces some other identity
in the target substrate such that personal identity is lost along with the
biological brain. This paper demonstrates a chain of reasoning that establishes
metaphysical equivalence between these two methods in terms of preserving
personal identity.
"
1246,"Aashiyana: Design and Evaluation of a Smart Demand-Response System for
  Highly-stressed Grids","  This paper targets the unexplored problem of demand response within the
context of power-grids that are allowed to regularly enforce blackouts as a
mean to balance supply with demand:highly-stressed grids. Currently these
utilities use as a cyclic and binary (power/no-power) schedule over consumer
groups leading to significant wastage of capacity and long hours of no-power.
We present here a novel building DLC system, Aashiyana, that can enforce
several user-defined low-power states. We evaluate distributed and centralized
load-shedding schemes using Aashiyana that can, compared to current
load-shedding strategy, reduce the number of homes with no power by 80% for
minor change in the fraction of homes with full-power.
"
1247,"Server component installation and testing of the university information
  and educational environment on the Moodle LMS platform","  The informational educational environment (IEE) of an institution is a
complex multilevel system which, along with methodical, organizational and
cultural resources, accumulates the intellectual and technical potential of a
university, as well as the informative and activity components of the learners
and teachers. In practice, the formation of IEE is actually based on the
creation of information technologies and their integration into the existing
educational environment of the institution. The management of this system is
carried out using specialized equipment and software. For the successful
formation and operation of IEE, in the present work we review software products
that form the basis of the organization of interactive and web interactions
between students, teachers and all participants of the educational process. We
analyse the technical capabilities that have provided users with IEE services
such as the Apache web server with connected modules PHP, MySQL, the Java
virtual machine and the Red5 server. We demonstrate the possibility of
obtaining results from the interaction of these products, and reports on users'
work in webinars, video conferences and web conferences.
"
1248,"Preprint Virtual Reality GIS and Cloud Service Based Traffic Analysis
  Platform","  This is the preprint version of our paper on The 23rd International
Conference on Geoinformatics (Geoinformatics2015). City traffic data has
several characteristics, such as large scale, diverse predictable and
real-time, which falls in the range of definition of Big Data. This paper
proposed a cloud service platform which targets for wise transportation is to
carry out unified management and mining analysis of the huge number of the
multivariate and heterogeneous dynamic transportation information, provides
real-time transportation information, increase the utilization efficiency of
transportation, promote transportation management and service level of travel
information and provide decision support of transportation management by
virtual reality as visual.
"
1249,A Monte Carlo Study of Pairwise Comparisons,"  Consistent approximations obtained by geometric means ($GM$) and the
principal eigenvector ($EV$), turned out to be close enough for 1,000,000
not-so-inconsistent pairwise comparisons matrices. In this respect both methods
are accurate enough for most practical applications. As the enclosed Table 1
demonstrates, the biggest difference between average deviations of $GM$ and
$EV$ solutions is 0.00019 for the Euclidean metric and 0.00355 for the
Tchebychev metric.
  For practical applications, this precision is far better than expected. After
all we are talking, in most cases, about relative subjective comparisons and
one tenth of a percent is usually below our threshold of perception.
"
1250,An LP-based inconsistency monitoring of pairwise comparison matrices,"  A distance-based inconsistency indicator, defined by the third author for the
consistency-driven pairwise comparisons method, is extended to the incomplete
case. The corresponding optimization problem is transformed into an equivalent
linear programming problem. The results can be applied in the process of
filling in the matrix as the decision maker gets automatic feedback. As soon as
a serious error occurs among the matrix elements, even due to a misprint, a
significant increase in the inconsistency index is reported. The high
inconsistency may be alarmed not only at the end of the process of filling in
the matrix but also during the completion process. Numerical examples are also
provided.
"
1251,"An Algorithm for the Optimal Consistent Approximation to a Pairwise
  Comparisons Matrix by Orthogonal Projections","  The algorithm for finding the optimal consistent approximation of an
inconsistent pairwise comparisons matrix is based on a logarithmic
transformation of a pairwise comparisons matrix into a vector space with the
Euclidean metric. Orthogonal basis is introduced in the vector space. The
orthogonal projection of the transformed matrix onto the space formed by the
images of consistent matrices is the required consistent approximation.
"
1252,"FPGA-Based Bandwidth Selection for Kernel Density Estimation Using High
  Level Synthesis Approach","  FPGA technology can offer significantly hi\-gher performance at much lower
power consumption than is available from CPUs and GPUs in many computational
problems. Unfortunately, programming for FPGA (using ha\-rdware description
languages, HDL) is a difficult and not-trivial task and is not intuitive for
C/C++/Java programmers. To bring the gap between programming effectiveness and
difficulty the High Level Synthesis (HLS) approach is promoting by main FPGA
vendors. Nowadays, time-intensive calculations are mainly performed on GPU/CPU
architectures, but can also be successfully performed using HLS approach. In
the paper we implement a bandwidth selection algorithm for kernel density
estimation (KDE) using HLS and show techniques which were used to optimize the
final FPGA implementation. We are also going to show that FPGA speedups,
comparing to highly optimized CPU and GPU implementations, are quite
substantial. Moreover, power consumption for FPGA devices is usually much less
than typical power consumption of the present CPUs and GPUs.
"
1253,UbiBreathe: A Ubiquitous non-Invasive WiFi-based Breathing Estimator,"  Monitoring breathing rates and patterns helps in the diagnosis and potential
avoidance of various health problems. Current solutions for respiratory
monitoring, however, are usually invasive and/or limited to medical facilities.
In this paper, we propose a novel respiratory monitoring system, UbiBreathe,
based on ubiquitous off-the-shelf WiFi-enabled devices. Our experiments show
that the received signal strength (RSS) at a WiFi-enabled device held on a
person's chest is affected by the breathing process. This effect extends to
scenarios when the person is situated on the line-of-sight (LOS) between the
access point and the device, even without holding it. UbiBreathe leverages
these changes in the WiFi RSS patterns to enable ubiquitous non-invasive
respiratory rate estimation, as well as apnea detection.
  We propose the full architecture and design for UbiBreathe, incorporating
various modules that help reliably extract the hidden breathing signal from a
noisy WiFi RSS. The system handles various challenges such as noise
elimination, interfering humans, sudden user movements, as well as detecting
abnormal breathing situations. Our implementation of UbiBreathe using
off-the-shelf devices in a wide range of environmental conditions shows that it
can estimate different breathing rates with less than 1 breaths per minute
(bpm) error. In addition, UbiBreathe can detect apnea with more than 96%
accuracy in both the device-on-chest and hands-free scenarios. This highlights
its suitability for a new class of anywhere respiratory monitoring.
"
1254,"How Resilient Are Our Societies? Analyses, Models, and Preliminary
  Results","  Traditional social organizations such as those for the management of
healthcare and civil defence are the result of designs and realizations that
matched well with an operational context considerably different from the one we
are experiencing today: A simpler world, characterized by a greater amount of
resources to match less users producing lower peaks of requests. The new
context reveals all the fragility of our societies: unmanageability is just
around the corner unless we do not complement the ""old recipes"" with smarter
forms of social organization. Here we analyze this problem and propose a
refinement to our fractal social organizations as a model for resilient
cyber-physical societies. Evidence to our claims is provided by simulating our
model in terms of multi-agent systems.
"
1255,Unique Sense: Smart Computing Prototype,"  Unique sense: Smart computing prototype is a part of unique sense computing
architecture, which delivers alternate solution for todays computing
architecture. This computing is one step towards future generation needs, which
brings extended support to the ubiquitous environment. This smart computing
prototype is the light weight compact architecture which is designed to satisfy
all the needs of this society. The proposed solution is based on the hybrid
combination of cutting edge technologies and techniques from the various
layers. In addition it achieves low cost architecture and eco-friendly to meet
all the levels of peoples needs.
"
1256,Advances in Computational Biology: A Real Boost or a Wishful Thinking,"  Computational biology is on the verge of a paradigm shift in its research
practice - from a data-based (computational) paradigm to an information-based
(cognitive) paradigm. As in the other research fields, this transition is
impeded by lack of a right understanding about what is actually hidden behind
the term ""information"". The paper is intended to clarify this issue and
introduces two new notions of ""physical information"" and ""semantic
information"", which together constitute the term ""information"". Some
implications of this introduction are considered.
"
1257,Managing Null Entries in Pairwise Comparisons,"  This paper shows how to manage null entries in pairwise comparisons matrices.
Although assessments can be imprecise, since subjective criteria are involved,
the classical pairwise comparisons theory expects all of them to be available.
In practice, some experts may not be able (or available) to provide all
assessments. Therefore managing null entries is a necessary extension of the
pairwise comparisons method. It is shown that certain null entries can be
recovered on the basis of the transitivity property which each pairwise
comparisons matrix is expected to satisfy.
"
1258,"Lossless Layout Image Compression Algorithms for Electron-Beam
  Direct-Write Lithography","  Electron-beam direct-write (EBDW) lithography systems must in the future
transmit terabits of information per second to be viable for commercial
semiconductor manufacturing. Lossless layout image compression algorithms with
high decoding throughputs and modest decoding resources are tools to address
the data transfer portion of the throughput problem. The earlier lossless
layout image compression algorithm Corner2 is designed for binary layout images
on raster-scanning systems. We propose variations of Corner2 collectively
called Corner2-EPC and Paeth-EPC which apply to electron-beam proximity
corrected layout images and offer interesting trade-offs between compression
ratios and decoding speeds. Most of our algorithms achieve better overall
compression performance than PNG, Block C4 and LineDiffEntropy while having low
decoding times and resources.
"
1259,A Novel Geographic Partitioning System for Anonymizing Health Care Data,"  With large volumes of detailed health care data being collected, there is a
high demand for the release of this data for research purposes. Hospitals and
organizations are faced with conflicting interests of releasing this data and
protecting the confidentiality of the individuals to whom the data pertains.
Similarly, there is a conflict in the need to release precise geographic
information for certain research applications and the requirement to censor or
generalize the same information for the sake of confidentiality. Ultimately the
challenge is to anonymize data in order to comply with government privacy
policies while reducing the loss in geographic information as much as possible.
In this paper, we present a novel geographic-based system for the anonymization
of health care data. This system is broken up into major components for which
different approaches may be supplied. We compare such approaches in order to
make recommendations on which of them to select to best match user
requirements.
"
1260,Fixed Rank Kriging for Cellular Coverage Analysis,"  Coverage planning and optimization is one of the most crucial tasks for a
radio network operator. Efficient coverage optimization requires accurate
coverage estimation. This estimation relies on geo-located field measurements
which are gathered today during highly expensive drive tests (DT); and will be
reported in the near future by users' mobile devices thanks to the 3GPP
Minimizing Drive Tests (MDT) feature~\cite{3GPPproposal}. This feature consists
in an automatic reporting of the radio measurements associated with the
geographic location of the user's mobile device. Such a solution is still
costly in terms of battery consumption and signaling overhead. Therefore,
predicting the coverage on a location where no measurements are available
remains a key and challenging task. This paper describes a powerful tool that
gives an accurate coverage prediction on the whole area of interest: it builds
a coverage map by spatially interpolating geo-located measurements using the
Kriging technique. The paper focuses on the reduction of the computational
complexity of the Kriging algorithm by applying Fixed Rank Kriging (FRK). The
performance evaluation of the FRK algorithm both on simulated measurements and
real field measurements shows a good trade-off between prediction efficiency
and computational complexity. In order to go a step further towards the
operational application of the proposed algorithm, a multicellular use-case is
studied. Simulation results show a good performance in terms of coverage
prediction and detection of the best serving cell.
"
1261,An Analogy Based Method for Freight Forwarding Cost Estimation,"  The author explored estimation by analogy (EBA) as a means of estimating the
cost of international freight consignment. A version of the k-Nearest Neighbors
algorithm (k-NN) was tested by predicting job costs from a database of over
5000 actual jobs booked by an Irish freight forwarding firm over a seven year
period. The effect of a computer intensive training process on overall accuracy
of the method was found to be insignificant when the method was implemented
with four or fewer neighbors. Overall, the accuracy of the analogy based
method, while still significantly less accurate than manually working up
estimates, might be worthwhile to implement in practice, depending labor costs
in an adopting firm. A simulation model was used to compare manual versus
analytical estimation methods. The point of indifference occurs when it takes a
firm more than 1.5 worker hours to prepare a manual estimate (at current Irish
labor costs). Suggestions are given for future experiments to improve the
sampling policy of the method to improve accuracy and to improve overall
scalability.
"
1262,"Improving Time Estimation by Blind Deconvolution: with Applications to
  TOFD and Backscatter Sizing","  In this paper we present a blind deconvolution scheme based on statistical
wavelet estimation. We assume no prior knowledge of the wavelet, and do not
select a reflector from the signal. Instead, the wavelet (ultrasound pulse) is
statistically estimated from the signal itself by a kurtosis-based metric. This
wavelet is then used to deconvolve the RF (radiofrequency) signal through
Wiener filtering, and the resultant zero phase trace is subjected to spectral
broadening by Autoregressive Spectral Extrapolation (ASE). These steps increase
the time resolution of diffraction techniques. Results on synthetic and real
cases show the robustness of the proposed method.
"
1263,"Manufacturing Pathway and Experimental Demonstration for Nanoscale
  Fine-Grained 3-D Integrated Circuit Fabric","  At sub-20nm technologies CMOS scaling faces severe challenges primarily due
to fundamental device scaling limitations, interconnection overhead and complex
manufacturing. Migration to 3D has been long sought as a possible pathway to
continue scaling, however, intrinsic requirements of CMOS are not compatible
for fine-grained 3D integration. We proposed a truly fine-grained 3D integrated
circuit fabric called Skybridge that solves nanoscale challenges and achieves
orders of magnitude benefits over CMOS. In Skybridge, device, circuit,
connectivity, thermal management and manufacturing issues are addressed in an
integrated 3D compatible manner. At the core of Skybridge assembly are uniform
vertical nanowires, which are functionalized with architected features for
fabric integration. All active components are created primarily using
sequential material deposition steps on these nanowires. Lithography and doping
are performed prior to any functionalization and their precision requirements
are significantly reduced. This paper introduces Skybridge manufacturing
pathway that is developed based on extensive process, device simulations and
experimental metrology, and uses established processes. Experimental
demonstrations of key process steps are also shown.
"
1264,Sensitivity Analysis of Resonant Circuits,"  We use first-order perturbation theory to provide a local linear relation
between the circuit parameters and the poles of an RLC network. The sensitivity
matrix, which defines this relationship, is obtained from the systems
eigenvectors and the derivative of its eigenvalues. In general, the sensitivity
matrix is related to the equilibrium fluctuations of the system. In particular,
it may be used as the basis for a statistical model to efficiently predict the
sensitivity of the circuit response to small component variations. The method
is illustrated with a calculation of conditional probabilities by Monte Carlo
Simulation.
"
1265,Low Power Wideband Sensing for One-Bit Quantized Cognitive Radio Systems,"  We proposes an ultra low power wideband spectrum sensing architecture by
utilizing a one-bit quantization at the cognitive radio (CR) receiver. The
impact of this aggressive quantization is quantified and it is shown that the
proposed method is robust to low signal-to-noise ratios (SNR). We derive
closed-form expressions for both false alarm and detection probabilities. The
sensing performance and the analytical results are assessed through comparisons
with respective results from computer simulations. The results indicate that
the proposed method provides significant saving in power, complexity, and
sensing period on the account of an acceptable range of performance
degradation.
"
1266,"A few reflections on the quality of emergence in complex collective
  systems","  A number of elements towards a classification of the quality of emergence in
emergent collective systems are provided. By using those elements, several
classes of emergent systems are exemplified, ranging from simple aggregations
of simple parts up to complex organizations of complex collective systems. In
so doing, the factors likely to play a a significant role in the persistence of
emergence and its opposite are highlighted. From this, new elements for
discussion are identified also considering elements from the System of Leibniz.
"
1267,Limitations of PLL simulation: hidden oscillations in MatLab and SPICE,"  Nonlinear analysis of the phase-locked loop (PLL) based circuits is a
challenging task, thus in modern engineering literature simplified mathematical
models and simulation are widely used for their study. In this work the
limitations of numerical approach is discussed and it is shown that, e.g.
hidden oscillations may not be found by simulation. Corresponding examples in
SPICE and MatLab, which may lead to wrong conclusions concerning the
operability of PLL-based circuits, are presented.
"
1268,"Increasing loyalty using predictive modeling in Business-to-Business
  Telecommunication","  Customer Relationship Management (CRM) is a key element of modern marketing
strategies. One of the most practical way to build useful knowledge on
customers in a CRM system to produce scores to forecast churn behavior,
propensity to subscribe to a new service... In AMEA zone (Asia, Middle East and
Africa zone), the context of fierce competition may represent a higher
percentage, and particularly in B2B market. But by contrast, to our knowledge,
no scientific papers were dedicated and published to detail the way to improve
loyalty in B2B Telco market. If we can assume at low segments similarities
between B2B and B2C, some research is required in order to model B2B user
behavior versus B2C behavior. This problematic stands actual as ""Bring Your own
Device"" (BYOD) becomes more and more trendy. Answering business requirements,
our team applied some B2C predictive tools with adapting them to B2B in an AMEA
country Orange affiliate.
"
1269,Review on the Design of Web Based SCADA Systems Based on OPC DA Protocol,"  One of the most familiar SCADA (supervisory control and data acquisition)
application protocols now is OPC protocol. This interface is supported by
almost all SCADA, visualization, and process control systems. There are many
research efforts tried to design and implement an approach to access an OPC DA
server through the Internet. To achieve this goal they used diverse of modern
IT technologies like XML, Web services, Java and AJAX. In this paper, we
present a complete classification of the different approaches introduced in the
literature. A comparative study is also introduced. Finally we study the
feasibility of the realization of these approaches based on the real time
constraints imposed by the nature of the problem.
"
1270,"Determining rural areas vulnerable to illegal dumping using GIS
  techniques. Case study: Neamt county, Romania","  The paper aims to mapping the potential vulnerable areas to illegal dumping
of household waste from rural areas in the extra- Carpathian region of Neamt
County. These areas are ordinary in the proximity of built-up areas and buffers
areas of 1 km were delimited for every locality. Based on various map layers in
vector formats (land use, rivers, built-up areas, roads etc) an assessment
method is performed to highlight the potential areas vulnerable to illegal
dumping inside these buffer areas at local scale. The results are correlated to
field observations and current situation of waste management systems. The maps
outline local disparities due to various geographical conditions of county.
This approach is a necessary tool in EIA studies particularly for rural waste
management systems at local and regional scale which are less studied in
current literature than urban areas.
"
1271,"Coherent 100G Nonlinear Compensation with Single-Step Digital
  Backpropagation","  Enhanced-SSFM digital backpropagation (DBP) is experimentally demonstrated
and compared to conventional DBP. A 112 Gb/s PM-QPSK signal is transmitted over
a 3200 km dispersion-unmanaged link. The intradyne coherent receiver includes
single-step digital backpropagation based on the enhanced-SSFM algorithm. In
comparison, conventional DBP requires twenty steps to achieve the same
performance. An analysis of the computational complexity and structure of the
two algorithms reveals that the overall complexity and power consumption of DBP
are reduced by a factor of 16 with respect to a conventional implementation,
while the computation time is reduced by a factor of 20. As a result, the
proposed algorithm enables a practical and effective implementation of DBP in
real-time optical receivers, with only a moderate increase of the computational
complexity, power consumption, and latency with respect to a simple
feed-forward equalizer for dispersion compensation.
"
1272,Fractal surfaces from simple arithmetic operations,"  Fractal surfaces ('patchwork quilts') are shown to arise under most general
circumstances involving simple bitwise operations between real numbers. A
theory is presented for all deterministic bitwise operations on a finite
alphabet. It is shown that these models give rise to a roughness exponent $H$
that shapes the resulting spatial patterns, larger values of the exponent
leading to coarser surfaces.
"
1273,"On environments as systemic exoskeletons: Crosscutting optimizers and
  antifragility enablers","  Classic approaches to General Systems Theory often adopt an individual
perspective and a limited number of systemic classes. As a result, those
classes include a wide number and variety of systems that result equivalent to
each other. This paper introduces a different approach: First, systems
belonging to a same class are further differentiated according to five major
general characteristics. This introduces a ""horizontal dimension"" to system
classification. A second component of our approach considers systems as nested
compositional hierarchies of other sub-systems. The resulting ""vertical
dimension"" further specializes the systemic classes and makes it easier to
assess similarities and differences regarding properties such as resilience,
performance, and quality-of-experience. Our approach is exemplified by
considering a telemonitoring system designed in the framework of Flemish
project ""Little Sister"". We show how our approach makes it possible to design
intelligent environments able to closely follow a system's horizontal and
vertical organization and to artificially augment its features by serving as
crosscutting optimizers and as enablers of antifragile behaviors.
"
1274,Can JSP Code be Generated Using XML Tags?,"  Over the years, a variety of web services have started using server-side
scripting to deliver results back to a client as a paid or free service; one
such server-side scripting language is Java Server Pages (JSP). Also Extensible
markup language (XML), is being adopted by most web developers as a tool to
describe data.Therefore, we present a conversion method which uses predefined
XML tags as input and generates the corresponding JSP code. However, the end
users are required to have a basic experience with web pages. This conversion
method aims to reduce the time and effort spent by the user (web developer) to
get acquainted with JSP. The conversion process abstracts the user from the
intricacies of JSP and enables him to focus on the business logic.
"
1275,Cracking Intel Sandy Bridge's Cache Hash Function,"  On Intel Sandy Bridge processor, last level cache (LLC) is divided into cache
slices and all physical addresses are distributed across the cache slices using
an hash function. With this undocumented hash function existing, it is
impossible to implement cache partition based on page coloring. This article
cracks the hash functions on two types of Intel Sandy processors by converting
the problem of cracking the hash function to the problem of classifying data
blocks into different groups based on eviction relationship existing between
data blocks that are mapped to the same cache set. Based on the cracking
result, this article proves that it's possible to implement cache partition
based on page coloring on cache indexed by hashing.
"
1276,"Passenger Flow Predictions at Sydney International Airport: A
  Data-Driven Queuing Approach","  Time spent in processing zones at an airport are an important part of the
passenger's airport experience. It undercuts the time spent in the rest of the
airport, and therefore the revenue that could be generated from shopping and
dining. It can also result in passengers missing flights and connections, which
has significant operational repercussions. Inadequate staffing levels are often
to blame for large congestion at an airport. In this paper, we present a
stochastic simulation that estimates the operational uncertainty in passenger
processing at immigration. Congestion and delays are estimated on arrivals and
departures based on scheduled flight departures and arrivals. We demonstrate
the use of cellular tracking data in refining the model, and an approach to
controlling congestion by adjusting staffing levels.
"
1277,"Missing Spectrum-Data Recovery in Cognitive Radio Networks Using
  Piecewise Constant Nonnegative Matrix Factorization","  In this paper, we propose a missing spectrum data recovery technique for
cognitive radio (CR) networks using Nonnegative Matrix Factorization (NMF). It
is shown that the spectrum measurements collected from secondary users (SUs)
can be factorized as product of a channel gain matrix times an activation
matrix. Then, an NMF method with piecewise constant activation coefficients is
introduced to analyze the measurements and estimate the missing spectrum data.
The proposed optimization problem is solved by a Majorization-Minimization
technique. The numerical simulation verifies that the proposed technique is
able to accurately estimate the missing spectrum data in the presence of noise
and fading.
"
1278,"GCC-Plugin for Automated Accelerator Generation and Integration on
  Hybrid FPGA-SoCs","  In recent years, architectures combining a reconfigurable fabric and a
general purpose processor on a single chip became increasingly popular. Such
hybrid architectures allow extending embedded software with application
specific hardware accelerators to improve performance and/or energy efficiency.
Aiding system designers and programmers at handling the complexity of the
required process of hardware/software (HW/SW) partitioning is an important
issue. Current methods are often restricted, either to bare-metal systems, to
subsets of mainstream programming languages, or require special coding
guidelines, e.g., via annotations. These restrictions still represent a high
entry barrier for the wider community of programmers that new hybrid
architectures are intended for. In this paper we revisit HW/SW partitioning and
present a seamless programming flow for unrestricted, legacy C code. It
consists of a retargetable GCC plugin that automatically identifies code
sections for hardware acceleration and generates code accordingly. The proposed
workflow was evaluated on the Xilinx Zynq platform using unmodified code from
an embedded benchmark suite.
"
1279,"A Comparison of High-Level Design Tools for SoC-FPGA on Disparity Map
  Calculation Example","  Modern SoC-FPGA that consists of FPGA with embedded ARM cores is being
popularized as an embedded vision system platform. However, the design approach
of SoC-FPGA applications still follows traditional hardware-software separate
workflow, which becomes the barrier of rapid product design and iteration on
SoC-FPGA. High-Level Synthesis (HLS) and OpenCL-based system-level design
approaches provide programmers the possibility to design SoC-FGPA at
system-level with an unified development environment for both hardware and
software. To evaluate the feasibility of high-level design approach especially
for embedded vision applications, Vivado HLS and Altera SDK for OpenCL,
representative and most popular commercial tools in market, are selected as
evaluation design tools, disparity map calculation as targeting application. In
this paper, hardware accelerators of disparity map calculation are designed
with both tools and implemented on Zedboard and SoCKit development board,
respectively. Comparisons between design tools are made in aspects of
supporting directives, accelerator design process, and generated hardware
performance. The results show that both tools can generate efficient hardware
for disparity map calculation application with much less developing time.
Moreover, we can also state that, more directives (e.g., interface type, array
reshape, resource type specification) are supported, but more hardware
knowledge is required, in Vivado HLS. In contrast, Altera SDK for OpenCL is
relatively easier for software programmers who is new to hardware, but with the
price of more resources usage on FPGA for similar hardware accelerator
generation.
"
1280,Target Tracking in Confined Environments with Uncertain Sensor Positions,"  To ensure safety in confined environments such as mines or subway tunnels, a
(wireless) sensor network can be deployed to monitor various environmental
conditions. One of its most important applications is to track personnel,
mobile equipment and vehicles. However, the state-of-the-art algorithms assume
that the positions of the sensors are perfectly known, which is not necessarily
true due to imprecise placement and/or dropping of sensors. Therefore, we
propose an automatic approach for simultaneous refinement of sensors' positions
and target tracking. We divide the considered area in a finite number of cells,
define dynamic and measurement models, and apply a discrete variant of belief
propagation which can efficiently solve this high-dimensional problem, and
handle all non-Gaussian uncertainties expected in this kind of environments.
Finally, we use ray-tracing simulation to generate an artificial mine-like
environment and generate synthetic measurement data. According to our extensive
simulation study, the proposed approach performs significantly better than
standard Bayesian target tracking and localization algorithms, and provides
robustness against outliers.
"
1281,Confluent Orthogonal Drawings of Syntax Diagrams,"  We provide a pipeline for generating syntax diagrams (also called railroad
diagrams) from context free grammars. Syntax diagrams are a graphical
representation of a context free language, which we formalize abstractly as a
set of mutually recursive nondeterministic finite automata and draw by
combining elements from the confluent drawing, layered drawing, and smooth
orthogonal drawing styles. Within our pipeline we introduce several heuristics
that modify the grammar but preserve the language, improving the aesthetics of
the final drawing.
"
1282,Axiomatization of Inconsistency Indicators for Pairwise Comparisons,"  This study proposes revised axioms for defining inconsistency indicators in
pairwise comparisons. It is based on the new findings that ""PC submatrix cannot
have a worse inconsistency indicator than the PC matrix containing it"" and that
there must be a PC submatrix with the same inconsistency as the given PC
matrix.
  This study also provides better reasoning for the need of normalization. It
is a revision of axiomatization by Koczkodaj and Szwarc, 2014 which proposed
axioms expressed informally with some deficiencies addressed in this study.
"
1283,Validation of daylighting model in CODYRUN building simulation code,"  CODYRUN is a multi-zone software integrating thermal building simulation,
airflow, and pollutant transfer. A first question thus arose as to the
integration of indoor lighting conditions into the simulation, leading to a new
model calculating natural and artificial lighting. The results of this new
daylighting module were then compared with results of other simulation codes
and experimental cases both in artificial and natural environments. Excellent
agreements were obtained, such as the values for luminous efficiencies in a
tropical and humid climate. In this paper, a comparison of the model output
with detailed measures is presented using a dedicated test cell in Reunion
Island (French overseas territory in the Indian Ocean), thus confirming the
interest for thermal and daylighting designs in low-energy buildings.
Introduction Several software packages are available for thermal and airflow
simulation in buildings. The most frequently used are ENERGY+ [1], ESP-r [2],
and TRNSYS [3]. These applications allow an increasing number of models to be
integrated, such as airflow, pollutant transport, and daylighting. In the
latter category, we may note ENERGY+, ESP-r and ECOTECT [4] software. After
more than 20 years of developing a specific code named CODYRUN, we decided to
add a lighting module to our software. This paper therefore provides some
details on this evolution and elements of validation. The CODYRUN initial
software and its validation Developed by the Physics and Mathematical
Engineering Laboratory for Energy and Environment at the University of Reunion
Island, CODYRUN [5-14] is a multi-zone software program integrating ventilation
and moisture transport transfer in buildings. The software employs a zone
approach based on nodal analysis and resolves a coupled system describing
thermal and airflow phenomena. Numerous validation tests of the CODYRUN code
were successfully applied to the software. Apart from the daylighting model,
the majority applied the BESTEST procedure [15]. The International Energy
Agency (IEA) sponsors a number of programs to improve the use and associated
technologies of energy. The National Renewable Energy Laboratory (NREL)
developed BESTEST, which is a method based on comparative testing of building
simulation programs, on the IEA's behalf. The procedure consists of a series of
test cases buildings that are designed to isolate individual aspects of
building energy and test the extremes of a program. As the modelling approach
is very different between codes, the test cases are specified so that input
equivalency can be defined thus allowing the different cases to be modelled by
most of codes. The basis for comparison is a range of results from a number of
programs considered to be a state-of-art in United States and Europe.
Associated with other specific comparisons, a very confident level of
validation was obtained for the CODYRUN initial software [8].
"
1284,"Problem of optimization of a transport traffic at preliminary
  registration of queires with use of CBSMAP-model","  The problem of optimization of a transport traffic at preliminary
registration of demands with use of the CBSMAP model is investigated. For the
solution of an objective application of the queueing theory and the theory of
controlled processes is supposed.
"
1285,A balanced rail-to-rail all digital comparator using only standard cells,"  An all-digital comparator with full input range is presented. It outperforms
the nowaday all-digital comparators with its large rail-to-rail input range.
This is achieved by the proposed Yin-yang balance mechanism between the two
logic gates: NAND3 and OAI (Or-And-Invert). The important design considerations
to achieve this balance are presented, such as the driving strength
manipulation and the use of pre-distortion technique. Constructed only by
commercially available digital standard cells, the layout of the proposed
comparator is generated automatically by standard digital Place & Route routine
within several minutes. The Verilog code for the proposed circuit is given, and
the circuit is successfully implemented in 130nm CMOS technology with the power
consumption of 0.176mW at the clock of 330MHz.
"
1286,"Properties of Farey Sequence and their Applications to Digital Image
  Processing","  Farey sequence has been a topic of interest to the mathematicians since the
very beginning of last century. With the emergence of various algorithms
involving the digital plane in recent times, several interesting works related
with the Farey sequence have come up. Our work is related with the problem of
searching an arbitrary fraction in a Farey sequence and its relevance to image
processing. Given an arbitrary fraction p/q (0 < p < q) and a Farey sequence Fn
of order n, we propose a novel algorithm using the Regula Falsi method and the
concept of Farey table to efficiently find the fraction of Fn closest to p/q.
All computations are in the integer domain only, which is its added benefit.
Some contemporary applications of image processing have also been shown where
such concepts can be incorporated. Experimental results have been furnished to
demonstrate its efficiency and elegance.
"
1287,"Yield, Area and Energy Optimization in Stt-MRAMs using failure aware ECC","  Spin Transfer Torque MRAMs are attractive due to their non-volatility, high
density and zero leakage. However, STT-MRAMs suffer from poor reliability due
to shared read and write paths. Additionally, conflicting requirements for data
retention and write-ability (both related to the energy barrier height of the
magnet) makes design more challenging. Furthermore, the energy barrier height
depends on the physical dimensions of the free layer. Any variations in the
dimensions of the free layer lead to variations in the energy barrier height.
In order to address poor reliability of STT-MRAMs, usage of Error Correcting
Codes (ECC) have been proposed. Unlike traditional CMOS memory technologies,
ECC is expected to correct both soft and hard errors in STT_MRAMs. To achieve
acceptable yield with low write power, stronger ECC is required, resulting in
increased number of encoded bits and degraded memory efficiency. In this paper,
we propose Failure aware ECC (FaECC), which masks permanent faults while
maintaining the same correction capability for soft errors without increased
encoded bits. Furthermore, we investigate the impact of process variations on
run-time reliability of STT-MRAMs. We provide an analysis on the impact of
process variations on the life-time of the free layer and retention failures.
In order to analyze the effectiveness of our methodology, we developed a
cross-layer simulation framework that consists of device, circuit and array
level analysis of STT-MRAM memory arrays. Our results show that using FaECC
relaxes the requirements on the energy barrier height, which reduces the write
energy and results in smaller access transistor size and memory array area.
Keywords: STT-MRAM, reliability, Error Correcting Codes, ECC, magnetic memory
"
1288,Model-independent comparison of simulation output,"  Computational models of complex systems are usually elaborate and sensitive
to implementation details, characteristics which often affect their
verification and validation. Model replication is a possible solution to this
issue. It avoids biases associated with the language or toolkit used to develop
the original model, not only promoting its verification and validation, but
also fostering the credibility of the underlying conceptual model. However,
different model implementations must be compared to assess their equivalence.
The problem is, given two or more implementations of a stochastic model, how to
prove that they display similar behavior? In this paper, we present a model
comparison technique, which uses principal component analysis to convert
simulation output into a set of linearly uncorrelated statistical measures,
analyzable in a consistent, model-independent fashion. It is appropriate for
ascertaining distributional equivalence of a model replication with its
original implementation. Besides model-independence, this technique has three
other desirable properties: a) it automatically selects output features that
best explain implementation differences; b) it does not depend on the
distributional properties of simulation output; and, c) it simplifies the
modelers' work, as it can be used directly on simulation outputs. The proposed
technique is shown to produce similar results to the manual or empirical
selection of output features when applied to a well-studied reference model.
"
1289,"On The Evolution Of User Support Topics in Computational Science and
  Engineering Software","  We investigate ten years of user support emails in the large-scale solver
library PETSc in order to identify changes in user requests. For this purpose
we assign each email thread to one or several categories describing the type of
support request. We find that despite several changes in hardware architecture
as well programming models, the relative share of emails for the individual
categories does not show a notable change over time. This is particularly
remarkable as the total communication volume has increased four-fold in the
considered time frame, indicating a considerable growth of the user base. Our
data also demonstrates that user support cannot be substituted with what is
often referred to as 'better documentation' and that the involvement of core
developers in user support is essential.
"
1290,Evolvable Autonomic Management,"  Autonomic management is aimed at adapting to uncertainty. Hence, it is
devised as m-connected k-dominating set problem, resembled by dominator and
dominate, such that dominators are resilient up to m-1 uncertainty among them
and dominate are resilient up to k-1 uncertainty on their way to dominators.
Therefore, an evolutionary algorithm GENESIS is proposed, which resolves
uncertainty by evolving population of solutions, while considering uncertain
constraints as sub-problems, started by initial populations by a greedy
algorithm AVIDO. Theoretical analysis first justifies original problem as
NP-hard problem. Eventually, the absence of polynomial time approximation
scheme necessitates justification of original problem as multiobjective
optimization problem. Furthermore, approximation to Pareto front is verified to
be decomposed into scalar optimization sub-problems, which lays out the
theoretical foundation for decomposition based evolutionary solution. Finally,
case-study, feasibility analysis and exemplary implication are presented for
evolvable autonomic management in combined cancer treatment with in-vivo sensor
networks.
"
1291,Implications of Burn-In Stress on NBTI Degradation,"  Burn-in is accepted as a way to evaluate ageing effects in an accelerated
manner. It has been suggested that burn-in stress may have a significant effect
on the Negative Bias Temperature Instability (NBTI) of subthreshold CMOS
circuits. This paper analyses the effect of burn-in on NBTI in the context of a
Digital to Analogue Converter (DAC) circuit. Analogue circuits require matched
device pairs; NBTI may cause mismatches and hence circuit failure. The NBTI
degradation observed in the simulation analysis indicates that under severe
stress conditions, a significant voltage threshold mismatch in the DAC beyond
the design specification of 2 mV limit can result. Experimental results confirm
the sensitivity of the DAC circuit design to NBTI resulting from burn-in.
"
1292,Spatial Prediction Under Location Uncertainty In Cellular Networks,"  Coverage optimization is an important process for the operator as it is a
crucial prerequisite towards offering a satisfactory quality of service to the
end-users. The first step of this process is coverage prediction, which can be
performed by interpolating geo-located measurements reported to the network by
mobile users' equipments. In previous works, we proposed a low complexity
coverage prediction algorithm based on the adaptation of the Geo-statistics
Fixed Rank Kriging (FRK) algorithm. We supposed that the geo-location
information reported with the radio measurements was perfect, which is not the
case in reality. In this paper, we study the impact of location uncertainty on
the coverage prediction accuracy and we extend the previously proposed
algorithm to include geo-location error in the prediction model. We validate
the proposed algorithm using both simulated and real field measurements. The
FRK extended to take into account the location uncertainty proves to enhance
the prediction accuracy while keeping a reasonable computational complexity.
"
1293,"Comparison of different Methods for Univariate Time Series Imputation in
  R","  Missing values in datasets are a well-known problem and there are quite a lot
of R packages offering imputation functions. But while imputation in general is
well covered within R, it is hard to find functions for imputation of
univariate time series. The problem is, most standard imputation techniques can
not be applied directly. Most algorithms rely on inter-attribute correlations,
while univariate time series imputation needs to employ time dependencies. This
paper provides an overview of univariate time series imputation in general and
an in-detail insight into the respective implementations within R packages.
Furthermore, we experimentally compare the R functions on different time series
using four different ratios of missing data. Our results show that either an
interpolation with seasonal kalman filter from the zoo package or a linear
interpolation on seasonal loess decomposed data from the forecast package were
the most effective methods for dealing with missing data in most of the
scenarios assessed in this paper.
"
1294,Fine-Grained Energy Modeling for the Source Code of a Mobile Application,"  Energy efficiency has a significant influence on user experience of
battery-driven devices such as smartphones and tablets. The goal of an energy
model for source code is to lay a foundation for the application of
energy-saving techniques during software development. The challenge is to
relate hardware energy consumption to high-level application code, considering
the complex run-time context and software stack. Traditional techniques build
the energy model by mapping a hardware energy model onto software constructs;
this approach faces obstacles when the software stack consists of a number of
abstract layers. Another approach that has been followed is to utilize hardware
or operating system features to estimate software energy information at a
coarse level of granularity such as blocks, methods or even applications. In
this paper, we explain how to construct a fine-grained energy model for the
source code, which is based on ""energy operations"" identified directly from the
source code and able to provide more valuable information for code
optimization. We apply the approach to a class of applications based on a
game-engine, and explain the wider applicability of the method.
"
1295,"Wireless communication, identification and sensing technologies enabling
  integrated logistics: a study in the harbor environment","  In the last decade, integrated logistics has become an important challenge in
the development of wireless communication, identification and sensing
technology, due to the growing complexity of logistics processes and the
increasing demand for adapting systems to new requirements. The advancement of
wireless technology provides a wide range of options for the maritime container
terminals. Electronic devices employed in container terminals reduce the manual
effort, facilitating timely information flow and enhancing control and quality
of service and decision made. In this paper, we examine the technology that can
be used to support integration in harbor's logistics. In the literature, most
systems have been developed to address specific needs of particular harbors,
but a systematic study is missing. The purpose is to provide an overview to the
reader about which technology of integrated logistics can be implemented and
what remains to be addressed in the future.
"
1296,"The Virtual Experiences Lab - a platform for global collaborative
  engineering and beyond","  We are developing the Virtual Experiences (Vx)Lab, a research and research
training infrastructure and capability platform for global collaboration. VxLab
comprises labs with visualisation capabilities, including underpinning
networking to global points of presence, videoconferencing and high-performance
computation, simulation and rendering, and sensors and actuators such as
robotic instruments locally and in connected remote labs. VxLab has been used
for industry projects in industrial automation, experimental research in cloud
deployment, workshops and remote capability demonstrations, teaching
advanced-level courses in games development, and student software engineering
projects. Our goal is for resources to become a ""catalyst"" for IT-driven
research results both within the university and with external industry
partners. Use cases include: multi-disciplinary collaboration, prototyping and
troubleshooting requiring multiple viewpoints and architectures, dashboards and
decision support for global remote planning and operations.
"
1297,"Using Raspberry Pi for scientific video observation of pedestrians
  during a music festival","  The document serves as a reference for researchers trying to capture a large
portion of a mass event on video for several hours, while using a very limited
budget.
"
1298,"Building a Decision Tree Model for Academic Advising Affairs Based on
  the Algorithm C 4-5","  The ability to recognize students weakness and solve any problem that may
confront them in timely fashion is always a target for all educational
institutions. Thus, colleges and universities implement the so-called academic
advising affairs. On the academic advisor relies the responsibility of solving
any problem that may confront students learning progress. This paper shows how
the adviser can benefit from data mining techniques, namely decision trees
techniques. The C 4.5 algorithm is used as a method for building such trees.
The output is evaluated based on the accuracy measure, Kappa measure, and ROC
area. The difference between the registered and gained credit hours is
considered as the main attribute on which advisor can rely
"
1299,Preprint WebVRGIS Based Traffic Analysis and Visualization System,"  This is the preprint version of our paper on Advances in Engineering
Software. With several characteristics, such as large scale, diverse
predictability and timeliness, the city traffic data falls in the range of
definition of Big Data. A Virtual Reality GIS based traffic analysis and
visualization system is proposed as a promising and inspiring approach to
manage and develop traffic big data. In addition to the basic GIS interaction
functions, the proposed system also includes some intelligent visual analysis
and forecasting functions. The passenger flow forecasting algorithm is
introduced in detail.
"
1300,Analysis of SVN Repositories for Remote Access,"  Software Evolution is considered to be essential and challenging
characteristic in the field of software engineering. Version control system is
an incremental versions tracking system, introduced to avoid unnecessary
overwriting of files such as programming code, web pages and records. It also
helps to decrease the confusion affected by duplicate or outdated data. In this
proposed research SVN repository is maintained and analyzed for
msitone.wikispaces.com to minimize the efforts as well as resources for the
future users. We have used two semester data for the analysis purpose that is
observed SVN repository. The result shows that, implementing the SVN
repositories are helpful for maintenance of the Wikispaces as it also reduce
the cost, time and efforts for their evolution. Whereas without implementing
the SVN repositories Wikispaces were just supposed to be building the house by
putting each brick from start.
"
1301,WLAN Specific IoT Enable Power Efficient RAM Design on 40nm FPGA,"  Increasing the speed of computer is one of the important aspects of the
Random Access Memory (RAM) and for better and fast processing it should be
efficient. In this work, the main focus is to design energy efficient RAM and
it also can be accessed through internet. A 128-bit IPv6 address is added to
the RAM in order to control it via internet. Four different types of Low
Voltage CMOS (LCVMOS) IO standards are used to make it low power under five
different WLAN frequencies is taken. At WLAN frequency 2.4GHz, there is maximum
power reduction of 85% is achieved when LVCMOS12 is taken in place of LVCMOS25.
This design is implemented using Virtex-6 FPGA, Device xc6vlx75t and Package
FF484
"
1302,Pairwise Comparisons Rating Scale Paradox,"  This study demonstrates that incorrect data are entered into a pairwise
comparisons matrix for processing into weights for the data collected by a
rating scale. Unprocessed rating scale data lead to a paradox. A solution to
it, based on normalization, is proposed. This is an essential correction for
virtually all pairwise comparisons methods using rating scales. The
illustration of the relative error currently, taking place, is discussed.
"
1303,"moGrams: a network-based methodology for visualizing the set of
  non-dominated solutions in multiobjective optimization","  An appropriate visualization of multiobjective non-dominated solutions is a
valuable asset for decision making. Although there are methods for visualizing
the solutions in the design space, they do not provide any information about
their relationship. In this work, we propose a novel methodology that allows
the visualization of the non-dominated solutions in the design space and their
relationships by means of a network. The nodes represent the solutions in the
objective space, while the edges show the relationships between the solutions
in the design space. Our proposal (called moGrams) thus provides a joint
visualization of both objective and design spaces. It aims at helping the
decision maker to get more understanding of the problem so that (s)he can
choose the more appropriate final solution. moGrams can be applied to any
multicriteria problem in which the solutions are related by a similarity
metric. Besides, the decision maker interaction is facilitated by modifying the
network based on the current preferences to obtain a clearer view. An
exhaustive experimental study is performed using three multiobjective problems
in order to show both the usefulness and versatility of moGrams. The results
exhibit interesting characteristics of our methodology for visualizing and
analyzing solutions of multiobjective problems.
"
1304,The Quasi cellular nets-based models of transport and logistic systems,"  There are many systems in different subjects such as industry, medicine,
transport, social and others, can be discribed on their dynamic of flows.
Nowadays models of flows consist of micro- and macro-models. In practice there
is a problem of convertation from different levels of simulation. In the
different articles author descriptes quasi cellular nets. Quasi cellular nets
are new type of discrete structures without signature. It may be used for
simulation instruments. This structures can simulate flows on micro- and macro
levels on the single model structure. In this article described using quasi
cellular nets in transport and logistics of open-cast mining.
"
1305,"Computation of Transition Adjacency Relations Based on Complete Prefix
  Unfolding (Technical Report)","  An increasing number of works have devoted to the application of Transition
Adjacency Relation (TAR) as a means to capture behavioral features of business
process models. In this paper, we systematically study the efficient TAR
derivation from process models using unfolding technique which previously has
been used to address the state space explosion when dealing with concurrent
behaviors of a Petri net. We reveal and formally describe the equivalence
between TAR and Event Adjacency Relation (EAR), the manifestation of TAR in the
Complete Prefix Unfolding (CPU) of a Petri net. By computing TARs from CPU
using this equivalence, we can alleviate the concurrency caused state-explosion
issues. Furthermore, structural boosting rules are categorized, proved and
added to the TAR computing algorithm. Formal proofs of correctness and
generality of CPU-based TAR computation are provided for the first time by this
work, and they significantly expand the range of Petri nets from which TARs can
be efficiently derived. Experiments on both industrial and synthesized process
models show the effectiveness of proposed CPU-based algorithms as well as the
observation that they scale well with the increase in size and concurrency of
business process models.
"
1306,"Introduction of the Residue Number Arithmetic Logic Unit With Brief
  Computational Complexity Analysis","  Digital System Research has pioneered the mathematics and design for a new
class of computing machine using residue numbers. Unlike prior art, the new
breakthrough provides methods and apparatus for general purpose computation
using several new residue based fractional representations. The result is that
fractional arithmetic may be performed without carry. Additionally, fractional
operations such as addition, subtraction and multiplication of a fraction by an
integer occur in a single clock period, regardless of word size. Fractional
multiplication is of the order O(p), where p equals the number of residues.
More significantly, complex operations, such as sum of products, may be
performed in an extended format, where fractional products are performed and
summed using single clock instructions, regardless of word width, and where a
normalization operation with an execution time of the order O(p) is performed
as a final step.
"
1307,Emerging Cloud Computing Security Threats,"  Cloud computing is one of the latest emerging innovations of the modern
internet and technological landscape. With everyone from the White house to
major online technological leaders like Amazon and Google using or offering
cloud computing services it is truly presents itself as an exciting and
innovative method to store and use data on the internet.
"
1308,"Motion model transitions in GPS-IMU sensor fusion for user tracking in
  augmented reality","  Finding the position of the user is an important processing step for
augmented reality (AR) applications. This paper investigates the use of
different motion models in order to choose the most suitable one, and
eventually reduce the Kalman filter errors in sensor fusion for such
applications where the accuracy of user tracking is crucial. A Deterministic
Finite Automaton (DFA) was employed using the innovation parameters of the
filter. Results show that the approach presented here reduces the filter error
compared to a static model and prevents filter divergence. The approach was
tested on a simple AR game in order to justify the accuracy and performance of
the algorithm.
"
1309,"Web application for size and topology optimization of trusses and gusset
  plates","  With its ever growing popularity, providing Internet based applications tuned
towards practical applications is on the rise. Advantages such as no external
plugins and additional software, ease of use, updating and maintenance have
increased the popularity of web applications. In this work, a web-based
application has been developed which can perform size optimization of truss
structure as a whole as well as topology optimization of individual gusset
plate of each joint based on specified joint displacements and load conditions.
This application is developed using cutting-edge web technologies such as
Three.js and HTML5. The client side boasts of an intuitive interface which in
addition to its modeling capabilities also recommends configurations based on
user input, provides analysis options and finally displays the results. The
server side, using a combination of Scilab and DAKOTA, computes solution and
also provides the user with comparisons of the optimal design with that
conforming to Indian Standard (IS 800-2007). It is a freely available one-stop
web-based application to perform optimal and/or code based design of trusses.
"
1310,"Simulation and Analysis of Container Freight Train Operations at Port
  Botany","  Over two million containers crossed the docks at Sydney's Port Botany in
2011/12; a figure that is forecast increase more than threefold by the end of
the next decade. To cope with such large growth in volumes the NSW Government
plans to double rail mode share at the port by the year 2020. Conventional
wisdom from industry and the media says that existing infrastructure cannot
handle such volumes. In this paper we use a combination of data analytics and
simulation to examine operations at the port and evaluate the efficacy of
current infrastructure to handle projected growth in volumes. Contrary to
conventional wisdom we find that current rail resources appear distinctly
under-utilised. Moreover: (i) the peak rail capacity of Port Botany is 1.78
million TEU per annum; over six times higher than 2011/12 rail volumes; (ii)
there are no infrastructural impediments to the achievement of peak rail
capacity; (iii) operational changes, not infrastructural investment, are the
key to unlocking the potential of the port; (iv) Port Botany is well positioned
to handle projected increases in container volumes over the next decade and
beyond, including the 28% rail mode share target established by the New South
Wales State Government.
"
1311,"DGD Gallery: Storage, sharing, and publication of digital research data","  We describe a project, called the ""Discretization in Geometry and Dynamics
Gallery"", or DGD Gallery for short, whose goal is to store geometric data and
to make it publicly available. The DGD Gallery offers an online web service for
the storage, sharing, and publication of digital research data.
"
1312,"Finding Needles in a Haystack: Missing Tag Detection in Large RFID
  Systems","  Radio frequency identification (RFID) technology has been widely used in
missing tag detection to reduce and avoid inventory shrinkage. In this
application, promptly finding out the missing event is of paramount importance.
However, existing missing tag detection protocols cannot efficiently handle the
presence of a large number of unexpected tags whose IDs are not known to the
reader, which shackles the time efficiency. To deal with the problem of
detecting missing tags in the presence of unexpected tags, this paper
introduces a two-phase Bloom filter-based missing tag detection protocol
(BMTD). The proposed BMTD exploits Bloom filter in sequence to first deactivate
the unexpected tags and then test the membership of the expected tags, thus
dampening the interference from the unexpected tags and considerably reducing
the detection time. Moreover, the theoretical analysis of the protocol
parameters is performed to minimize the detection time of the proposed BMTD and
achieve the required reliability simultaneously. Extensive experiments are then
conducted to evaluate the performance of the proposed BMTD. The results
demonstrate that the proposed BMTD significantly outperforms the
state-of-the-art solutions.
"
1313,"Two improved normalized subband adaptive filter algorithms with good
  robustness against impulsive interferences","  To improve the robustness of subband adaptive filter (SAF) against impulsive
interferences, we propose two modified SAF algorithms with an individual scale
function for each subband, which are derived by maximizing correntropy-based
cost function and minimizing logarithm-based cost function, respectively,
called MCC-SAF and LC-SAF. Whenever the impulsive interference happens, the
subband scale functions can sharply drop the step size, which eliminate the
influence of outliers on the tap-weight vector update. Therefore, the proposed
algorithms are robust against impulsive interferences, and exhibit the faster
convergence rate and better tracking capability than the sign SAF (SSAF)
algorithm. Besides, in impulse-free interference environments, the proposed
algorithms achieve similar convergence performance as the normalized SAF (NSAF)
algorithm. Simulation results have demonstrated the performance of our proposed
algorithms.
"
1314,"On A Testing and Implementation of Quantum Gate and Measurement Emulator
  (QGAME)","  Today, people are looking forward to get an awesome computational power. This
kind of desire can be answered by quantum computing. By adopting quantum
mechanics theory, it can generate a very fast computation result. As known,
quantum mechanics can establish that particle can also become wave; it shows
that electron can be in duality. Through this theory, even a human
teleportation is issued can be really happened in the future. However, it needs
a high requirement of hardware support to implement the real quantum computing.
That is why it is difficult to bring quantum computing into reality. This
research presents a study about quantum computing. Here it is studied, a
specialty of quantum computing, like superposition, as if the classical
computer can do it. Since there was a marvellous research about quantum
computer simulation that runs on classical computer, this research provides an
analysis about our testing and implementation of Quantum Gate and Measurement
Emulator (QGAME). Our analysis, testing and implementation are based on a
method that always use in the software engineering field.
"
1315,Energy Consumption Forecasting for Smart Meters,"  Earth, water, air, food, shelter and energy are essential factors required
for human being to survive on the planet. Among this energy plays a key role in
our day to day living including giving lighting, cooling and heating of
shelter, preparation of food. Due to this interdependency, energy, specifically
electricity, production and distribution became a high tech industry. Unlike
other industries, the key differentiator of electricity industry is the product
itself. It can be produced but cannot be stored for future; production and
consumption happen almost in near real-time. This particular peculiarity of the
industry is the key driver for Machine Learning and Data Science based
innovations in this industry. There is always a gap between the demand and
supply in the electricity market across the globe. To fill the gap and improve
the service efficiency through providing necessary supply to the market,
commercial as well as federal electricity companies employ forecasting
techniques to predict the future demand and try to meet the demand and provide
curtailment guidelines to optimise the electricity consumption/demand. In this
paper the authors examine the application of Machine Learning algorithms,
specifically Boosted Decision Tree Regression, to the modelling and forecasting
of energy consumption for smart meters. The data used for this exercise is
obtained from DECC data website. Along with this data, the methodology has been
tested in Smart Meter data obtained from EMA Singapore. This paper focuses on
feature engineering for time series forecasting using regression algorithms and
deriving a methodology to create personalised electricity plans offers for
household users based on usage history.
"
1316,"Cloud Computation and Google Earth Visualization of Heat/Cold Waves: A
  Nonanticipative Long-Range Forecasting Case Study","  Long-range forecasting of heat/cold waves is a topical issue nowadays. High
computational complexity of the design of numerical and statistical models is a
bottleneck for the forecast process. In this work, Windows Server 2012 R2
virtual machines are used as a high-performance tool for the speed-up of the
computational process. Six D-series and one standard tier A-series virtual
machines were hosted in Microsoft Azure public cloud for this purpose.
Visualization of the forecasted data is based on the Google Earth Pro virtual
globe in ASP.NET web-site against http://gearth.azurewebsites.net (prototype),
where KMZ file represents geographic placemarks. The long-range predictions of
the heat/cold waves are computed for several specifically located places based
on nonanticipative analog algorithm. The arguments of forecast models are
datasets from around the world, which reflects the concept of teleconnections.
This methodology does not require the probability distribution to design the
forecast models and/or calculate the predictions. Heat weaves at Annaba
(Algeria) are discussed in detail. Up to 36.4% of heat waves are specifically
predicted. Up to 33.3% of cold waves are specifically predicted for other four
locations around the world. The proposed approach is 100% accurate if the signs
of predicted and actual values are compared according to climatological
baseline. These high-accuracy predictions were achieved due to the
interdisciplinary approach, but advanced computer science techniques, public
cloud computing and Google Earth Pro virtual globe mainly, form the major part
of the work.
"
1317,"A Novel Approach to Compress Centralized Text Data using Indexed
  Dictionary","  Data compression is very important feature in terms of saving the memory
space. In this proposal, an indexed dictionary based compression is used for
text data, where the word's reference in dictionary is used for compression.
This approach is not file based, a common dictionary is used for compression.
Which contains the words, the position of the word in dictionary is one of the
key parts of encoded frame which is compressed form of the text word. This is
loss-less compression. This compression approach is also take cares of small
words like one or two characters words which usually decrease the efficiency of
compression algorithms. This approach is also deals with file having special
characters as a word. Special character words, alpha numeric words, normal
texted words and small words all deals differently which makes this approach
more efficient. Since a centralized dictionary is used for data compression,
therefore, this approach is not preferred for transfer compressed file, while
it is suitable to store text data in compressed form in hard disk drive and
centralized storage or cloud drive for memory utilization.
"
1318,"Design of portable power consumption measuring system for green
  computing needs","  The article presents the design of a digital power measurement device
intended for the green IT. Article comprises: use case analysis, accuracy and
precision measurements and real life test of apache web server as exemplary
application.
"
1319,ePlace-3D: Electrostatics based Placement for 3D-ICs,"  We propose a flat, analytic, mixed-size placement algorithm ePlace-3D for
three-dimension integrated circuits (3D-ICs) using nonlinear optimization. Our
contributions are (1) electrostatics based 3D density function with globally
uniform smoothness (2) 3D numerical solution with improved spectral formulation
(3) 3D nonlinear pre-conditioner for convergence acceleration (4) interleaved
2D-3D placement for efficiency enhancement. Our placer outperforms the leading
work mPL6-3D and NTUplace3-3D with 6.44% and 37.15% shorter wirelength, 9.11%
and 10.27% fewer 3D vertical interconnects (VI) on average of IBM-PLACE
circuits. Validation on the large-scale modern mixed-size (MMS) 3D circuits
shows high performance and scalability.
"
1320,Crowds for Clouds: Recent Trends in Humanities Research Infrastructures,"  Humanities have convincingly argued that they need transnational research
opportunities and through the digital transformation of their disciplines also
have the means to proceed with it on an up to now unknown scale. The digital
transformation of research and its resources means that many of the artifacts,
documents, materials, etc. that interest humanities research can now be
combined in new and innovative ways. Due to the digital transformations, (big)
data and information have become central to the study of culture and society.
Humanities research infrastructures manage, organise and distribute this kind
of information and many more data objects as they becomes relevant for social
and cultural research.
"
1321,"Novel velocity model to improve indoor localization using inertial
  navigation with sensors on a smartphone","  We present a generalized velocity model to improve localization when using an
Inertial Navigation System (INS). This algorithm was applied to correct the
velocity of a smart phone based indoor INS system to increase the accuracy by
counteracting the accumulation of large drift caused by sensor reading errors.
We investigated the accuracy of the algorithm with three different velocity
models which were derived from the actual velocity measured at the hip of
walking person. Our results show that the proposed method with Gaussian
velocity model achieves competitive accuracy with a 50\% less variance over
Step and Heading approach proving the accuracy and robustness of proposed
method. We also investigated the frequency of applying corrections and found
that a minimum of 5\% corrections per step is sufficient for improved accuracy.
The proposed method is applicable in indoor localization and tracking
applications based on smart phone where traditional approaches such as GNSS
suffers from many issues.
"
1322,Transit directions at global scale,"  A novel approach to integrated ground and air public transport journey
planning, operating at continent scale. Flexible date search, prerequisite for
long distance trips given their typical low and irregular service frequencies,
is core functionality. The algorithm is especially suited for irregular and
poorly structured networks. Almost all of the described functionality is
implemented in a working prototype. Using ground transport only, the system is
on par with Google Transit on random country-wide trips in the US.
"
1323,Predictive and statistical analyses for academic advisory support,"  The ability to recognize weakness of students and solving any problem may
confront them in timely fashion is always a target of all educational
institutions. This study was designed to explore how can predictive and
statistical analysis support the academic work of adviser mainly in analysis
progress of students . The sample consisted of a total of 249 undergraduate
students: 46 % of them were Female and 54% Male. A one-way analysis of variance
and t-test were conducted to analysis if there was different behavior in
registering courses. Predictive data mining is used for support adviser in
decision making. Several classification techniques with 10-fold Cross
validation were applied. Among of them, C 4.5 constitutes the best agreement
among the finding results.
"
1324,"Multi-Number CVT-XOR Arithmetic Operations in any Base System and its
  Significant Properties","  Carry Value Transformation (CVT) is a model of discrete dynamical system
which is one special case of Integral Value Transformations (IVTs). Earlier in
[5] it has been proved that sum of two non-negative integers is equal to the
sum of their CVT and XOR values in any base system. In the present study, this
phenomenon is extended to perform CVT and XOR operations for many non-negative
integers in any base system. To achieve that both the definition of CVT and XOR
are modified over the set of multiple integers instead of two. Also some
important properties of these operations have been studied. With the help of
cellular automata the adder circuit designed in [14] on using CVT-XOR
recurrence formula is used to design a parallel adder circuit for multiple
numbers in binary number system.
"
1325,Pulse processing routines for neutron time-of-flight data,"  A pulse shape analysis framework is described, which was developed for
n_TOF-Phase3, the third phase in the operation of the n_TOF facility at CERN.
The most notable feature of this new framework is the adoption of generic pulse
shape analysis routines, characterized by a minimal number of explicit
assumptions about the nature of pulses. The aim of these routines is to be
applicable to a wide variety of detectors, thus facilitating the introduction
of the new detectors or types of detectors into the analysis framework. The
operational details of the routines are suited to the specific requirements of
particular detectors by adjusting the set of external input parameters. Pulse
recognition, baseline calculation and the pulse shape fitting procedure are
described. Special emphasis is put on their computational efficiency, since the
most basic implementations of these conceptually simple methods are often
computationally inefficient.
"
1326,Superposition principle in linear networks with controlled sources,"  The manuscript discusses a well-known issue that, despite its fundamental
role in basic electric circuit theory, seems to be tackled without the needful
attention. The question if the Principle of Superposition (POS) can be applied
to linear networks containing linear dependent sources still appears as an
addressed point unworthy to be further discussed. Conversely, the analysis of
this point has been recently re-proposed [5,6] and an alternative conclusion
has been drawn. From this result, the manuscript provides an alternative
approach to such issue from a more general point of view. It is oriented to
clarify the issue from the didactic viewpoint, rather than provide a more
efficient general technique for circuit analysis. By starting from a linear
system of equations, representing a general linear circuit containing
controlled sources, the correct interpretation of turning off the controlled
elements in terms of circuit equivalent is provided, so allowing a statement of
the POS for linear circuits in a wider context. Further, this approach is
sufficiently intuitive and straightforward to fit the needs of a Basic Electric
Circuit Theory class.
"
1327,50+ Metrics for Calendar Mining,"  In this report we propose 50+ metrics which can be measured by organizations
in order to identify improvements in various areas such as meeting efficiency,
capacity planning or leadership skills, just to new a few. The notion of
calendar mining is introduced and support is provided for performing the
measurement by a reference data model and queries for all metrics defined.
"
1328,"NexMon: A Cookbook for Firmware Modifications on Smartphones to Enable
  Monitor Mode","  Full control over a Wi-Fi chip for research purposes is often limited by its
firmware, which makes it hard to evolve communication protocols and test
schemes in practical environments. Monitor mode, which allows eavesdropping on
all frames on a wireless communication channel, is a first step to lower this
barrier. Use cases include, but are not limited to, network packet analyses,
security research and testing of new medium access control layer protocols.
Monitor mode is generally offered by SoftMAC drivers that implement the media
access control sublayer management entity (MLME) in the driver rather than in
the Wi-Fi chip. On smartphones, however, mostly FullMAC chips are used to
reduce power consumption, as MLME tasks do not need to wake up the main
processor. Even though, monitor mode is also possible in FullMAC scenarios, it
is generally not implemented in today's Wi-Fi firmwares used in smartphones.
This work focuses on bringing monitor mode to Nexus 5 smartphones to enhance
the interoperability between applications that require monitor mode and BCM4339
Wi-Fi chips. The implementation is based on our new C-based programming
framework to extend existing Wi-Fi firmwares.
"
1329,"A Method for RFO Estimation Using Phase Analysis of Pilot Symbols in
  OFDM Systems","  In this paper, a method for CFO/RFO estimation based on proportional
coefficients extraction in OFDM system is proposed, which may be applied to any
pilot symbol pattern.
"
1330,The Pilot Alignment Pattern Design in OFDM Systems,"  In this paper, we propose optimal pilot pattern of downlink OFDM (Orthogonal
Frequency Division Multiplexing) communication system.
"
1331,"A Formal Approach to Power Optimization in CPSs with Delay-Workload
  Dependence Awareness","  The design of cyber-physical systems (CPSs) faces various new challenges that
are unheard of in the design of classical real-time systems. Power optimization
is one of the major design goals that is witnessing such new challenges. The
presence of interaction between the cyber and physical components of a CPS
leads to dependence between the time delay of a computational task and the
amount of workload in the next iteration. We demonstrate that it is essential
to take this delay-workload dependence into consideration in order to achieve
low power consumption.
  In this paper, we identify this new challenge, and present the first formal
and comprehensive model to enable rigorous investigations on this topic. We
propose a simple power management policy, and show that this policy achieves a
best possible notion of optimality. In fact, we show that the optimal power
consumption is attained in a ""steady-state"" operation and a simple policy of
finding and entering this steady state suffices, which can be quite surprising
considering the added complexity of this problem. Finally, we validated the
efficiency of our policy with experiments.
"
1332,"Reliability of Checking an Answer Given by a Mathematical Expression in
  Interactive Learning Systems","  In this article we address the problem of automatic answer checking in
interactive learning systems that support mathematical notation. This problem
consists of the problem of establishing identities in formal mathematical
systems and hence is formally unsolvable. However, there is a way to cope with
the issue. We suggest to reinforce the standard algorithm for function
comparison with an additional pointwise checking procedure. An error might
appear in this case. The article provides a detailed analysis of the
probability of this error. It appears that the error probability is extremely
low in most common cases. Generally speaking, this means that such an
additional checking procedure can be quite successfully used in order to
support standard algorithms for functions comparison. The results, obtained in
this article, help avoiding some sudden effects of the identity problem, and
provide a way to estimate the reliability of answer checking procedure in
interactive learning systems.
"
1333,Improving Data Quality in Intelligent Transportation Systems,"  Intelligent Transportation Systems (ITS) use data and information technology
to improve the operation of our transportation network. ITS contributes to
sustainable development by using technology to make the transportation system
more efficient; improving our environment by reducing emissions, reducing the
need for new construction and improving our daily lives through reduced
congestion. A key component of ITS is traveler information. The Oregon
Department of Transportation (ODOT) recently implemented a new traveler
information system on selected freeways to provide drivers with travel time
estimates that allow them to make more informed decisions about routing to
their destinations. The ODOT project aims to improve traffic flow and promote
efficient traffic movement, which can reduce emissions rates and improve air
quality. The new ODOT system is based on travel data collected from a
recently-increased set of sensors installed on its freeways. Our current
project investigates novel data cleaning methodologies and the integration of
those methodologies into the prediction of travel times. We use machine
learning techniques on our archive to identify suspect data, and calculate
revised travel times excluding this suspect data. We compare the resulting
travel time predictions to ground-truth data, and to predictions based on
simple, rule-based data cleaning. We report on the results of our study using
qualitative and quantitative methods.
"
1334,"Customizable Precision of Floating-Point Arithmetic with Bitslice Vector
  Types","  Customizing the precision of data can provide attractive trade-offs between
accuracy and hardware resources. We propose a novel form of vector computing
aimed at arrays of custom-precision floating point data. We represent these
vectors in bitslice format. Bitwise instructions are used to implement
arithmetic circuits in software that operate on customized bit-precision.
Experiments show that this approach can be efficient for vectors of
low-precision custom floating point types, while providing arbitrary bit
precision.
"
1335,"Encoding Distortion Modeling For DWT-Based Wireless EEG Monitoring
  System","  Recent advances in wireless body area sensor net- works leverage wireless and
mobile communication technologies to facilitate development of innovative
medical applications that can significantly enhance healthcare services and
improve quality of life. Specifically, Electroencephalography (EEG)-based
applications lie at the heart of these promising technologies. However, the
design and operation of such applications is challenging. Power consumption
requirements of the sensor nodes may turn some of these applications
impractical. Hence, implementing efficient encoding schemes are essential to
reduce power consumption in such applications. In this paper, we propose an
analytical distortion model for the EEG-based encoding systems. Using this
model, the encoder can effectively reconfigure its complexity by adjusting its
control parameters to satisfy application constraints while maintaining
reconstruction accuracy at the receiver side. The simulation results illustrate
that the main parameters that affect the distortion are compression ratio and
filter length of the considered DWT-based encoder. Furthermore, it is found
that the wireless channel variations have a significant influence on the
estimated distortion at the receiver side.
"
1336,An Estimation Method Using Periodic Inspection of Indicators,"  This paper proposes a new approach for estimating the failure time
distribution using the indicator data. The indicators, which are checked by
periodic inspection of a standby redundant system, only convey whether at least
one failure occurs per interval. The estimation procedure first obtains the
estimation of the forward recurrence time using the indicator data. Then the
mean is estimated based on its relationship with the forward recurrence time.
And the estimation of the sampled Cdf is thus derived based on its relationship
with the forward recurrence time and the mean. Finally, the Cdf function is
estimated using interpolation method. The simulation results showed that the
estimation method performed well for the four Weibull distributions.
"
1337,Spatio-Temporal Analysis of Team Sports -- A Survey,"  Team-based invasion sports such as football, basketball and hockey are
similar in the sense that the players are able to move freely around the
playing area; and that player and team performance cannot be fully analysed
without considering the movements and interactions of all players as a group.
State of the art object tracking systems now produce spatio-temporal traces of
player trajectories with high definition and high frequency, and this, in turn,
has facilitated a variety of research efforts, across many disciplines, to
extract insight from the trajectories. We survey recent research efforts that
use spatio-temporal data from team sports as input, and involve non-trivial
computation. This article categorises the research efforts in a coherent
framework and identifies a number of open research questions.
"
1338,"{\delta}-MAPS: From spatio-temporal data to a weighted and lagged
  network between functional domains","  We propose {\delta}-MAPS, a method that analyzes spatio-temporal data to
first identify the distinct spatial components of the underlying system,
referred to as ""domains"", and second to infer the connections between them. A
domain is a spatially contiguous region of highly correlated temporal activity.
The core of a domain is a point or subregion at which a metric of local
homogeneity is maximum across the entire domain. We compute a domain as the
maximum-sized set of spatially contiguous cells that include the detected core
and satisfy a homogeneity constraint, expressed in terms of the average
pairwise cross-correlation across all cells in the domain. Domains may be
spatially overlapping. Different domains may have correlated activity,
potentially at a lag, because of direct or indirect interactions. The proposed
edge inference method examines the statistical significance of each lagged
cross-correlation between two domains, infers a range of lag values for each
edge, and assigns a weight to each edge based on the covariance of the two
domains. We illustrate the application of {\delta}-MAPS on data from two
domains: climate science and neuroscience.
"
1339,Philosophical Fictionalism and Problem of Artificial Intelligence,"  The artificial intelligence received broad interpretation as a literary
image. This approach did not have unambiguous refering to the scopes of logical
studies and mathematical investigations. An author applied methods peculiar to
the semiotic approach, offered by Boris Uspensky and Yury Lotman. In addition,
the article presented the criticism of modern versions of educational
technologies, which led to the unconditional expectations for possibilities of
information and telecommunication technologies. Methodological culture's
growth, which was described on the base of semiotics and functional approach to
word formation of new meanings for the description of the studied subjects,
provided the development of pupils' thought. As a result, the research opened
new prospects on understanding of artificial intelligence within educational
practice.
"
1340,Investigating Drivers' Head and Glance Correspondence,"  The relationship between a driver's glance pattern and corresponding head
rotation is highly complex due to its nonlinear dependence on the individual,
task, and driving context. This study explores the ability of head pose to
serve as an estimator for driver gaze by connecting head rotation data with
manually coded gaze region data using both a statistical analysis approach and
a predictive (i.e., machine learning) approach. For the latter, classification
accuracy increased as visual angles between two glance locations increased. In
other words, the greater the shift in gaze, the higher the accuracy of
classification. This is an intuitive but important concept that we make
explicit through our analysis. The highest accuracy achieved was 83% using the
method of Hidden Markov Models (HMM) for the binary gaze classification problem
of (1) the forward roadway versus (2) the center stack. Results suggest that
although there are individual differences in head-glance correspondence while
driving, classifier models based on head-rotation data may be robust to these
differences and therefore can serve as reasonable estimators for glance
location. The results suggest that driver head pose can be used as a surrogate
for eye gaze in several key conditions including the identification of
high-eccentricity glances. Inexpensive driver head pose tracking may be a key
element in detection systems developed to mitigate driver distraction and
inattention.
"
1341,Loongson IoT Gateway: A Technical Review,"  A prototype of Loongson IoT (Internet of Things) ZigBee gateway is already
designed and implemented. However, this prototype is not perfect enough because
of the lack of a number of functions. And a lot of things should be done to
improve this prototype, such as adding widely used IEEE 802.11 function, using
a fully open source ZigBee protocol stack to get rid of proprietary implement
or using a fully open source embedded operating system to support 6LoWPAN, and
implementing multiple interfaces.
"
1342,"Exponential capacity of associative memories under quantum annealing
  recall","  Associative memory models, in theoretical neuro- and computer sciences, can
generally store a sublinear number of memories. We show that using quantum
annealing for recall tasks endows associative memory models with exponential
storage capacities. Theoretically, we obtain the radius of attractor basins,
$R(N)$, and the capacity, $C(N)$, of such a scheme and their tradeoffs. Our
calculations establish that for randomly chosen memories the capacity of a
model using the Hebbian learning rule with recall via quantum annealing is
exponential in the size of the problem, $C(N)=\mathcal{O}(e^{C_1N}),~C_1\geq0$,
and succeeds on randomly chosen memory sets with a probability of
$(1-e^{-C_2N}),~C_2\geq0$ with $C_1+C_2=(.5-f)^2/(1-f)$, where,
$f=R(N)/N,~0\leq f\leq .5$ is the radius of attraction in terms of Hamming
distance of an input probe from a stored memory as a fraction of the problem
size. We demonstrate the application of this scheme on a programmable quantum
annealing device - the Dwave processor.
"
1343,On short-term traffic flow forecasting and its reliability,"  Recent advances in time series, where deterministic and stochastic modelings
as well as the storage and analysis of big data are useless, permit a new
approach to short-term traffic flow forecasting and to its reliability, i.e.,
to the traffic volatility. Several convincing computer simulations, which
utilize concrete data, are presented and discussed.
"
1344,Fast inference of ill-posed problems within a convex space,"  In multiple scientific and technological applications we face the problem of
having low dimensional data to be justified by a linear model defined in a high
dimensional parameter space. The difference in dimensionality makes the problem
ill-defined: the model is consistent with the data for many values of its
parameters. The objective is to find the probability distribution of parameter
values consistent with the data, a problem that can be cast as the exploration
of a high dimensional convex polytope. In this work we introduce a novel
algorithm to solve this problem efficiently. It provides results that are
statistically indistinguishable from currently used numerical techniques while
its running time scales linearly with the system size. We show that the
algorithm performs robustly in many abstract and practical applications. As
working examples we simulate the effects of restricting reaction fluxes on the
space of feasible phenotypes of a {\em genome} scale E. Coli metabolic network
and infer the traffic flow between origin and destination nodes in a real
communication network.
"
1345,"Using Newton's method to model a spatial light distribution of a LED
  with attached secondary optics","  In design of optical systems based on LED (Light emitting diode) technology,
a crucial task is to handle the unstructured data describing properties of
optical elements in standard formats. This leads to the problem of data fitting
within an appropriate model. Newton's method is used as an upgrade of
previously developed most promising discrete optimization heuristics showing
improvement of both performance and quality of solutions. Experiment also
indicates that a combination of an algorithm that finds promising initial
solutions as a preprocessor to Newton's method may be a winning idea, at least
on some datasets of instances.
"
1346,In-Vehicle PLC: In-Car and In-Ship Channel Characterization,"  This paper deals with power line communication (PLC) in the context of
in-vehicle data networks. This technology can provide high-speed data
connectivity via the exploitation of the existing power network, with clear
potential benefits in terms of cost and weight reduction. The focus is on two
scenarios: an electric car and a cruise ship. An overview of the wiring
infrastructure and the network topology in these two scenarios is provided. The
main findings reported in the literature related to the channel characteristics
are reported. Noise is also assessed with emphasis to the electric car context.
Then, new results from the statistical analysis of measurements made in a
compact electric car and in a large cruise ship are shown. The channel
characteristics are analysed in terms of average channel gain, delay spread,
coherence bandwidth and achievable transmission rate. Finally, an overall
comparison is made, highlighting similarities and differences taking into
account also the conventional (combustion engine) car and the largely
investigated in-home scenario.
"
1347,Microprocessor Optimizations for the Internet of Things: A Survey,"  The Internet of Things (IoT) refers to a pervasive presence of interconnected
and uniquely identifiable physical devices. These devices' goal is to gather
data and drive actions in order to improve productivity, and ultimately reduce
or eliminate reliance on human intervention for data acquisition,
interpretation, and use. The proliferation of these connected low-power devices
will result in a data explosion that will significantly increase data
transmission costs with respect to energy consumption and latency. Edge
computing reduces these costs by performing computations at the edge nodes,
prior to data transmission, to interpret and/or utilize the data. While much
research has focused on the IoT's connected nature and communication
challenges, the challenges of IoT embedded computing with respect to device
microprocessors has received much less attention. This paper explores IoT
applications' execution characteristics from a microarchitectural perspective
and the microarchitectural characteristics that will enable efficient and
effective edge computing. To tractably represent a wide variety of
next-generation IoT applications, we present a broad IoT application
classification methodology based on application functions, to enable quicker
workload characterizations for IoT microprocessors. We then survey and discuss
potential microarchitectural optimizations and computing paradigms that will
enable the design of right-provisioned microprocessors that are efficient,
configurable, extensible, and scalable. This paper provides a foundation for
the analysis and design of a diverse set of microprocessor architectures for
next-generation IoT devices.
"
1348,The Design Principles of Konrad Zuse's Mechanical Computers,"  Konrad Zuse built the Z1, a mechanical programmable computing machine,
between 1935/36 and 1937/38. The Z1 was a binary floating-point computing
device. The individual logical gates were constructed using metallic plates and
interconnection rods. This paper describes the design principles Zuse followed
in order to complete a complex calculating machine, as the Z1 was. Zuse called
his basic switching elements ""mechanical relays"" in analogy to the electrical
relays used in telephony.
"
1349,Mapping EU fishing activities using ship tracking data,"  Information and understanding of fishing activities at sea are fundamental
components of marine knowledge and maritime situational awareness. Such
information is important to fisheries science, public authorities and policy
makers. In this paper we introduce a first map at European scale of EU fishing
activities extracted using Automatic Identification System (AIS) ship tracking
data. The resulting map is a density of points that identify fishing
activities. A measure of the reliability of such information is also presented
as a map of coverage reception capabilities.
"
1350,A (Basis for a) Philosophy of a Theory of Fuzzy Computation,"  Vagueness is a linguistic phenomenon as well as a property of physical
objects. Fuzzy set theory is a mathematical model of vagueness that has been
used to define vague models of computation. The prominent model of vague
computation is the fuzzy Turing machine. This conceptual computing device gives
an idea of what computing under vagueness means, nevertheless, it is not the
most natural model. Based on the properties of this and other models of vague
computing, it is aimed to formulate a basis for a philosophy of a theory of
fuzzy computation.
"
1351,"Norm-1 Regularized Consensus-based ADMM for Imaging with a Compressive
  Antenna","  This paper presents a novel norm-one-regularized, consensus-based imaging
algorithm, based on the Alternating Direction Method of Multipliers (ADMM).
This algorithm is capable of imaging composite dielectric and metallic targets
by using limited amount of data. The distributed capabilities of the ADMM
accelerates the convergence of the imaging. Recently, a Compressive Reflector
Antenna (CRA) has been proposed as a way to provide high-sensing-capacity with
a minimum cost and complexity in the hardware architecture. The ADMM algorithm
applied to the imaging capabilities of the Compressive Antenna (CA) outperforms
current state of the art iterative reconstruction algorithms, such as
Nesterov-based methods, in terms of computational cost; and it ultimately
enables the use of a CA in quasi-real-time, compressive sensing imaging
applications.
"
1352,Analyzing In-Game Movements of Soccer Players at Scale,"  It is challenging to get access to datasets related to the physical
performance of soccer players. The teams consider such information highly
confidential, especially if it covers in-game performance.Hence, most of the
analysis and evaluation of the players' performance do not contain much
information on the physical aspect of the game, creating a blindspot in
performance analysis. We propose a novel method to solve this issue by deriving
movement characteristics of soccer players. We use event-based datasets from
data provider companies covering 50+ soccer leagues allowing us to analyze the
movement profiles of potentially tens of thousands of players without any major
investment. Our methodology does not require expensive, dedicated player
tracking system deployed in the stadium. We also compute the similarity of the
players based on their movement characteristics and as such identify potential
candidates who may be able to replace a given player. Finally, we quantify the
uniqueness and consistency of players in terms of their in-game movements. Our
study is the first of its kind that focuses on the movements of soccer players
at scale, while it derives novel, actionable insights for the soccer industry
from event-based datasets.
"
1353,"Multiprocessor Scheduling of a Multi-mode Dataflow Graph Considering
  Mode Transition Delay","  Synchronous Data Flow (SDF) model is widely used for specifying signal
processing or streaming applications. Since modern embedded applications become
more complex with dynamic behavior changes at run-time, several extensions of
the SDF model have been proposed to specify the dynamic behavior changes while
preserving static analyzability of the SDF model. They assume that an
application has a finite number of behaviors (or modes) and each behavior
(mode) is represented by an SDF graph. They are classified as multi-mode
dataflow models in this paper. While there exist several scheduling techniques
for multi-mode dataflow models, no one allows task migration between modes. By
observing that the resource requirement can be additionally reduced if task
migration is allowed, we propose a multiprocessor scheduling technique of a
multi-mode dataflow graph considering task migration between modes. Based on a
genetic algorithm, the proposed technique schedules all SDF graphs in all modes
simultaneously to minimize the resource requirement. To satisfy the throughput
constraint, the proposed technique calculates the actual throughput requirement
of each mode and the output buffer size for tolerating throughput jitter. We
compare the proposed technique with a method which analyzes SDF graphs in each
execution mode separately and a method that does not allow task migration for
synthetic examples and three real applications: H.264 decoder, vocoder, and LTE
receiver algorithms.
"
1354,"Beyond Binary Computers: How To Implement Multi-Switch Computer Hardware
  and Software and; The Advantage of a Multi-Switched Computer","  This paper explores the possibilities of using a computing methodology
--hardware and software-- that employs technology other than binary. I refer to
this as ""supra - binary"" computing. Software constructs that use more than
binary techniques are discussed. The gains in supra - binary software are
demonstrated, which includes supra - binary code being RISC. Possible hardware
implementations of a computer with other than binary based architecture are
demonstrated and considered. The advantages and possible disadvantages of these
hardware implementations are discussed. The gain in computing speed is
evaluated and demonstrated. Supra - binary processing would streamline parallel
processing and make its implementation a built - in feature of the software and
hardware. Also, supra - binary would bring an advancement to neural networking.
This is discussed and demonstrated. In addition, possible applications of supra
- binary computing to database and neural networking are discussed. Also, the
possible implementations could be applied to telecommunications with dramatic
results.
"
1355,Continuous-Flow Graph Transportation Distances,"  Optimal transportation distances are valuable for comparing and analyzing
probability distributions, but larger-scale computational techniques for the
theoretically favorable quadratic case are limited to smooth domains or
regularized approximations. Motivated by fluid flow-based transportation on
$\mathbb{R}^n$, however, this paper introduces an alternative definition of
optimal transportation between distributions over graph vertices. This new
distance still satisfies the triangle inequality but has better scaling and a
connection to continuous theories of transportation. It is constructed by
adapting a Riemannian structure over probability distributions to the graph
case, providing transportation distances as shortest-paths in probability
space. After defining and analyzing theoretical properties of our new distance,
we provide a time discretization as well as experiments verifying its
effectiveness.
"
1356,Ni\'epce-Bell or Turing: How to Test Odor Reproduction?,"  In a 1950 article in Mind, decades before the existence of anything
resembling an artificial intelligence system, Alan Turing addressed the
question of how to test whether machines can think, or in modern terminology,
whether a computer claimed to exhibit intelligence indeed does so. The current
paper raises the analogous issue for olfaction: how to test the validity of a
system claimed to reproduce arbitrary odors artificially, in a way recognizable
to humans, in face of the unavailability of a general naming method for odors.
Although odor reproduction systems are still far from being viable, the
question of how to test candidates thereof is claimed to be interesting and
nontrivial, and a novel method is proposed. To some extent, the method is
inspired by Turing`s test for AI, in that it involves a human challenger and
the real and artificial entities, yet it is very different: our test is
conditional, requiring from the artificial no more than is required from the
original, and it employs a novel method of immersion that takes advantage of
the availability of near-perfect reproduction methods for sight and sound.
"
1357,PGR: A Graph Repository of Protein 3D-Structures,"  Graph theory and graph mining constitute rich fields of computational
techniques to study the structures, topologies and properties of graphs. These
techniques constitute a good asset in bioinformatics if there exist efficient
methods for transforming biological data into graphs. In this paper, we present
Protein Graph Repository (PGR), a novel database of protein 3D-structures
transformed into graphs allowing the use of the large repertoire of graph
theory techniques in protein mining. This repository contains graph
representations of all currently known protein 3D-structures described in the
Protein Data Bank (PDB). PGR also provides an efficient online converter of
protein 3D-structures into graphs, biological and graph-based description,
pre-computed protein graph attributes and statistics, visualization of each
protein graph, as well as graph-based protein similarity search tool. Such
repository presents an enrichment of existing online databases that will help
bridging the gap between graph mining and protein structure analysis. PGR data
and features are unique and not included in any other protein database. The
repository is available at http://wjdi.bioinfo.uqam.ca/.
"
1358,"A General World Model with Poiesis: Poppers Three Worlds updated with
  Software","  With the famous Three Worlds of Karl Popper as template, the paper rigorously
introduces the concept of software to define the counterpart of the physical
subworld. Digesting the scientific-technical view of biology and neurology on a
high level, results in an updated Three Worlds scheme consistent with an
information technical view. Chance and mathematics complete the world model.
Some simple examples illustrate the move from Poppers view of the world with
physics, psyche and World 3, to a new extended model with physics, extended
software (which we call Poiesis), and Geist (the notion which embodies spirit,
mind and soul).
"
1359,"Cooperative communications for sleep monitoring in wireless body area
  networks","  This paper investigates the performance of cooperative receive diversity, for
the wireless body area network (WBAN) radio channel, compliant with the IEEE
802.15.6 Standard, in the case of monitoring a sleeping person. Extensive WBAN
measurements near the 2.4 GHz ISM band were used. Up to 7 dB and 20%
improvement for two-hop communications with the use of relays are empirically
demonstrated with respect to outage probability and outage duration, with
3-branch cooperative selection combining and 3-branch cooperative
switch-and-examine combining.
"
1360,Designing robust watermark barcodes for multiplex long-read sequencing,"  A method for designing sequencing barcodes that can withstand a large number
of insertion, deletion and substitution errors and are suitable for use in
multiplex single-molecule real-time sequencing is presented. The manuscript
focuses on the design of barcodes for full-length single-pass reads, impaired
by challenging error rates in the order of 11%. To the authors' knowledge, this
is the first method to specifically address this problem without requiring
upstream quality improvement. The proposed barcodes can multiplex hundreds or
thousands of samples while achieving sample misassignment probabilities as low
as $10^{-7}$, and are designed to be compatible with chemical constraints
imposed by the sequencing process. Software for constructing watermark barcode
sets and demultiplexing barcoded reads, together with example sets of barcodes
and synthetic barcoded reads, are freely available at
www.cifasis-conicet.gov.ar/ezpeleta/NS-watermark.
"
1361,"A Framework for Predictive Analysis of Stock Market Indices : A Study of
  the Indian Auto Sector","  Analysis and prediction of stock market time series data has attracted
considerable interest from the research community over the last decade. Rapid
development and evolution of sophisticated algorithms for statistical analysis
of time series data, and availability of high-performance hardware has made it
possible to process and analyze high volume stock market time series data
effectively, in real-time. Among many other important characteristics and
behavior of such data, forecasting is an area which has witnessed considerable
focus. In this work, we have used time series of the index values of the Auto
sector in India during January 2010 to December 2015 for a deeper understanding
of the behavior of its three constituent components, e.g., the trend, the
seasonal component, and the random component. Based on this structural
analysis, we have also designed five approaches for forecasting and also
computed their accuracy in prediction using suitably chosen training and test
data sets. Extensive results are presented to demonstrate the effectiveness of
our proposed decomposition approaches of time series and the efficiency of our
forecasting techniques, even in presence of a random component and a sharply
changing trend component in the time-series.
"
1362,"Real-Time Contingency Analysis with Corrective Transmission Switching -
  Part I: Methodology","  Transmission switching (TS) has gained significant attention recently.
However, barriers still remain and must be overcome before the technology can
be adopted by the industry. The state of the art challenges include AC
feasibility and performance, computational complexity, the ability to handle
large-scale real power systems, and dynamic stability. This two-part paper
investigates these challenges by developing an AC TS-based real-time
contingency analysis (RTCA) tool that can handle large-scale systems within a
reasonable time. The tool proposes multiple corrective switching actions, after
detection of a contingency with potential violations. To reduce the
computational complexity, three heuristic algorithms are proposed to generate a
small set of candidates for switching. Parallel computing is implemented to
further speed up the solution time. Furthermore, stability analysis is
performed to check for dynamic stability of proposed TS solutions. Part I of
the paper presents a comprehensive literature review and the methodology. The
promising results, tested on the Tennessee Valley Authority (TVA) system and
actual energy management system (EMS) snapshots from Pennsylvania New Jersey
Maryland (PJM) and the Electric Reliability Council of Texas (ERCOT), are
presented in Part II. It is concluded that RTCA with corrective TS
significantly reduces potential post-contingency violations and is ripe for
industry adoption.
"
1363,"Real-Time Contingency Analysis with Corrective Transmission Switching -
  Part II: Results and Discussion","  This paper presents the performance of an AC transmission switching (TS)
based real-time contingency analysis (RTCA) tool that is introduced in Part I
of this paper. The approach quickly proposes high quality corrective switching
actions for relief of potential post-contingency network violations. The
approach is confirmed by testing it on actual EMS snapshots of two large-scale
systems, the Electric Reliability Council of Texas (ERCOT) and the Pennsylvania
New Jersey Maryland (PJM) Interconnection; the approach is also tested on data
provided by the Tennessee Valley Authority (TVA). The results show that the
tool effectively reduces post-contingency violations. Fast heuristics are used
along with parallel computing to reduce the computational difficulty of the
problem. The tool is able to handle the PJM system in about five minutes with a
standard desktop computer. Time-domain simulations are performed to check
system stability with corrective transmission switching (CTS). In conclusion,
the paper shows that corrective switching is ripe for industry adoption. CTS
can provide significant reliability benefits that can be translated into
significant cost savings.
"
1364,"Non-convex Global Minimization and False Discovery Rate Control for the
  TREX","  The TREX is a recently introduced method for performing sparse
high-dimensional regression. Despite its statistical promise as an alternative
to the lasso, square-root lasso, and scaled lasso, the TREX is computationally
challenging in that it requires solving a non-convex optimization problem. This
paper shows a remarkable result: despite the non-convexity of the TREX problem,
there exists a polynomial-time algorithm that is guaranteed to find the global
minimum. This result adds the TREX to a very short list of non-convex
optimization problems that can be globally optimized (principal components
analysis being a famous example). After deriving and developing this new
approach, we demonstrate that (i) the ability of the preexisting TREX heuristic
to reach the global minimum is strongly dependent on the difficulty of the
underlying statistical problem, (ii) the new polynomial-time algorithm for TREX
permits a novel variable ranking and selection scheme, (iii) this scheme can be
incorporated into a rule that controls the false discovery rate (FDR) of
included features in the model. To achieve this last aim, we provide an
extension of the results of Barber & Candes (2015) to establish that the
knockoff filter framework can be applied to the TREX. This investigation thus
provides both a rare case study of a heuristic for non-convex optimization and
a novel way of exploiting non-convexity for statistical inference.
"
1365,Towards a characterization of the uncertainty curve for graphs,"  Signal processing on graphs is a recent research domain that aims at
generalizing classical tools in signal processing, in order to analyze signals
evolving on complex domains. Such domains are represented by graphs, for which
one can compute a particular matrix, called the normalized Laplacian. It was
shown that the eigenvalues of this Laplacian correspond to the frequencies of
the Fourier domain in classical signal processing. Therefore, the frequency
domain is not the same for every support graph. A consequence of this is that
there is no non-trivial generalization of Heisenberg's uncertainty principle,
that states that a signal cannot be fully localized both in the time domain and
in the frequency domain. A way to generalize this principle, introduced by
Agaskar and Lu, consists in determining a curve that represents a lower bound
on the compromise between precision in the graph domain and precision in the
spectral domain. The aim of this paper is to propose a characterization of the
signals achieving this curve, for a larger class of graphs than the one studied
by Agaskar and Lu.
"
1366,"EDF-VD Scheduling of Mixed-Criticality Systems with Degraded Quality
  Guarantees","  This paper studies real-time scheduling of mixed-criticality systems where
low-criticality tasks are still guaranteed some service in the high-criticality
mode, with reduced execution budgets. First, we present a utilization-based
schedulability test for such systems under EDF-VD scheduling. Second, we
quantify the suboptimality of EDF-VD (with our test condition) in terms of
speedup factors. In general, the speedup factor is a function with respect to
the ratio between the amount of resource required by different types of tasks
in different criticality modes, and reaches 4/3 in the worst case. Furthermore,
we show that the proposed utilization-based schedulability test and speedup
factor results apply to the elastic mixed-criticality model as well.
Experiments show effectiveness of our proposed method and confirm the
theoretical suboptimality results.
"
1367,2.4GHZ Class AB power Amplifier For Healthcare Application,"  The objective of this research was to design a 2.4 GHz class AB Power
Amplifier, with 0.18 um SMIC CMOS technology by using Cadence software, for
health care applications. The ultimate goal for such application is to minimize
the trade-offs between performance and cost, and between performance and low
power consumption design. The performance of the power amplifier meets the
specification requirements of the desired.
"
1368,Process Information Model for Sheet Metal Operations,"  The paper extracts the process parameters from a sheet metal part model
(B-Rep). These process parameters can be used in sheet metal manufacturing to
control the manufacturing operations. By extracting these process parameters
required for manufacturing, CAM program can be generated automatically using
the part model and resource information. A Product model is generated in
modeling software and converted into STEP file which is used for extracting
B-Rep which interned is used to classify and extract feature by using sheet
metal feature recognition module. The feature edges are classified as CEEs,
IEEs, CIEs and IIEs based on topological properties. Database is created for
material properties of the sheet metal and machine tools required to
manufacture features in a part model. The extracted feature, feature's edge
information and resource information are then used to compute process
parameters and values required to control manufacturing operations. The
extracted feature, feature's edge information, resource information and process
parameters are the integral components of the proposed process information
model for sheet metal operations.
"
1369,Scientific notations for the digital era,"  Computers have profoundly changed the way scientific research is done.
Whereas the importance of computers as research tools is evident to everyone,
the impact of the digital revolution on the representation of scientific
knowledge is not yet widely recognized. An ever increasing part of today's
scientific knowledge is expressed, published, and archived exclusively in the
form of software and electronic datasets. In this essay, I compare these
digital scientific notations to the the traditional scientific notations that
have been used for centuries, showing how the digital notations optimized for
computerized processing are often an obstacle to scientific communication and
to creative work by human scientists. I analyze the causes and propose
guidelines for the design of more human-friendly digital scientific notations.
"
1370,"An Alternative Framework for Time Series Decomposition and Forecasting
  and its Relevance for Portfolio Choice: A Comparative Study of the Indian
  Consumer Durable and Small Cap Sectors","  One of the challenging research problems in the domain of time series
analysis and forecasting is making efficient and robust prediction of stock
market prices. With rapid development and evolution of sophisticated algorithms
and with the availability of extremely fast computing platforms, it has now
become possible to effectively extract, store, process and analyze high volume
stock market time series data. Complex algorithms for forecasting are now
available for speedy execution over parallel architecture leading to fairly
accurate results. In this paper, we have used time series data of the two
sectors of the Indian economy: Consumer Durables sector and the Small Cap
sector for the period January 2010 to December 2015 and proposed a
decomposition approach for better understanding of the behavior of each of the
time series. Our contention is that various sectors reveal different time
series patterns and understanding them is essential for portfolio formation.
Further, based on this structural analysis, we have also proposed several
robust forecasting techniques and analyzed their accuracy in prediction using
suitably chosen training and test data sets. Extensive results are presented to
demonstrate the effectiveness of our propositions.
"
1371,"Improvement of algorithms to identify transportation modes for
  MobilitApp, an Android Application to anonymously track citizens in Barcelona","  MobilitApp is an Android application whose objective is to obtain mobility
data from the citizens of the metropolitan area of Barcelona. The current
project is based on the research of more trustful and stronger transport
decision algorithms using advantages of accelerometry techniques. The developed
algorithm reads data from the mobile's accelerometer and gyroscope and writes
it in a file that is afterwards sent to the server. This process is executed in
background without interfering in the main application activity. Collected data
has been processed and used to analyze the behaviour of the mobility pattern of
the distinct transport modalities. The obtained result has been parameters
which allow us to configure a model for each mean activity and designing a
transport mode detection algorithm which would use information obtained from
the mobile's own sensors. MobilitApp is still executing its main functionality,
monitoring mobility data from the citizens. In future versions the solution
proposed to detect the transport systems will be integrated into the
application, in this way the app will work with information obtained from the
device and will not depend on any other services.
"
1372,The bitwise operations in relation to obtaining Latin squares,"  The main thrust of the article is to provide interesting example, useful for
students of using bitwise operations in the programming languages C ++ and
Java. As an example, we describe an algorithm for obtaining a Latin square of
arbitrary order. We will outline some techniques for the use of bitwise
operations.
"
1373,Requirements for storing electrophysiology data,"  The purpose of this document is to specify the basic data types required for
storing electrophysiology and optical imaging data to facilitate computer-based
neuroscience studies and data sharing. These requirements are being developed
within a working group of the Electrophysiology Task Force in the International
Neuroinformatics Coordinating Facility (INCF) Program on Standards for Data
Sharing. While this document describes the requirements of the standard
independent of the actual storage technology, the Task Force has recommended
basing a standard on HDF5. This is in line with a number of groups who are
already using HDF5 to store electrophysiology data, although currently without
being based on a standard.
"
1374,"Mobile phone data for public health: towards data-sharing solutions that
  protect individual privacy and national security","  We outline the constraints faced by operators when deciding to share
de-identified data with researchers or policy makers. We describe a
conservative approach that we have taken to harness the value of CDRs for
infectious disease epidemiology while ensuring that identification of
individuals is impossible. We believe this approach serves as a useful and
highly conservative model for productive partnerships between mobile operators,
researchers, and public health practitioners.
"
1375,"Exploiting AIS Data for Intelligent Maritime Navigation: A Comprehensive
  Survey","  The Automatic Identification System (AIS) tracks vessel movement by means of
electronic exchange of navigation data between vessels, with onboard
transceiver, terrestrial and/or satellite base stations. The gathered data
contains a wealth of information useful for maritime safety, security and
efficiency. This paper surveys AIS data sources and relevant aspects of
navigation in which such data is or could be exploited for safety of seafaring,
namely traffic anomaly detection, route estimation, collision prediction and
path planning.
"
1376,Axodraw Version 2,"  We present version two of the Latex graphical style file Axodraw. It has a
number of new drawing primitives and many extra options, and it can now work
with \program{pdflatex} to directly produce output in PDF file format (but with
the aid of an auxiliary program).
"
1377,One-dimensional Cutting Stock Problem with Divisible Items,"  This paper considers the one-dimensional cutting stock problem with divisible
items, which is a new problem in the cutting stock literature. The problem
exists in steel industries. In the new problem, each item can be divided into
smaller pieces, then they can be recombined again by welding. The objective is
to minimize both the trim loss and the number of the welds. We present a
mathematical model and a dynamic programming based heuristic for the problem.
Furthermore, a software, which is based on the proposed heuristic algorithm, is
developed to use in MKA company, and its performance is analyzed by solving
real-life problems in the steel industry. The computational experiments show
the efficiency of the proposed algorithm.
"
1378,"Leveraging ERP Implementation to Create Intellectual Capital: the Role
  of Organizational Learning Capability","  The extent to which enterprise resource planning (ERP) systems deliver value
for organizations has been debated. In this study, we argue that the presence
of appropriate organizational resources is essential for capturing the
potential of ERP implementation. We investigate the relationship between ERP
implementation and two organizational resources, specifically, Intellectual
Capital (IC) and Organizational Learning Capability (OLC) to enrich the
understanding of the way the value of ERP implementations can be realized. A
sample of 226 manufacturing firms in Vietnam was surveyed to test the
theoretical model. Structural equation modelling with partial least square
method and two approaches for moderation analysis were used to analyze the
data. The results indicate that ERP implementation scope has a positive impact
on intellectual capital (IC). However, firms need to build a certain level of
OLC to utilize ERP implementation for the enhancement of IC.
"
1379,"Undecidability and Irreducibility Conditions for Open-Ended Evolution
  and Emergence","  Is undecidability a requirement for open-ended evolution (OEE)? Using methods
derived from algorithmic complexity theory, we propose robust computational
definitions of open-ended evolution and the adaptability of computable
dynamical systems. Within this framework, we show that decidability imposes
absolute limits to the stable growth of complexity in computable dynamical
systems. Conversely, systems that exhibit (strong) open-ended evolution must be
undecidable, establishing undecidability as a requirement for such systems.
Complexity is assessed in terms of three measures: sophistication, coarse
sophistication and busy beaver logical depth. These three complexity measures
assign low complexity values to random (incompressible) objects. As time grows,
the stated complexity measures allow for the existence of complex states during
the evolution of a computable dynamical system. We show, however, that finding
these states involves undecidable computations. We conjecture that for similar
complexity measures that assign low complexity values, decidability imposes
comparable limits to the stable growth of complexity, and that such behaviour
is necessary for non-trivial evolutionary systems. We show that the
undecidability of adapted states imposes novel and unpredictable behaviour on
the individuals or populations being modelled. Such behaviour is irreducible.
Finally, we offer an example of a system, first proposed by Chaitin, that
exhibits strong OEE.
"
1380,Big Data Refinement,"  ""Big data"" has become a major area of research and associated funding, as
well as a focus of utopian thinking. In the still growing research community,
one of the favourite optimistic analogies for data processing is that of the
oil refinery, extracting the essence out of the raw data. Pessimists look for
their imagery to the other end of the petrol cycle, and talk about the ""data
exhausts"" of our society.
  Obviously, the refinement community knows how to do ""refining"". This paper
explores the extent to which notions of refinement and data in the formal
methods community relate to the core concepts in ""big data"". In particular, can
the data refinement paradigm can be used to explain aspects of big data
processing?
"
1381,"Adaptive Quantization Matrices for HD and UHD Display Resolutions in
  Scalable HEVC","  HEVC contains an option to enable custom quantization matrices, which are
designed based on the Human Visual System and a 2D Contrast Sensitivity
Function. Visual Display Units, capable of displaying video data at High
Definition and Ultra HD display resolutions, are frequently utilized on a
global scale. Video compression artifacts that are present due to high levels
of quantization, which are typically inconspicuous in low display resolution
environments, are clearly visible on HD and UHD video data and VDUs. The
default QM technique in HEVC does not take into account the video data
resolution, nor does it take into consideration the associated display
resolution of a VDU to determine the appropriate levels of quantization
required to reduce unwanted video compression artifacts. Based on this fact, we
propose a novel, adaptive quantization matrix technique for the HEVC standard,
including Scalable HEVC. Our technique, which is based on a refinement of the
current HVS-CSF QM approach in HEVC, takes into consideration the display
resolution of the target VDU for the purpose of minimizing video compression
artifacts. In SHVC SHM 9.0, and compared with anchors, the proposed technique
yields important quality and coding improvements for the Random Access
configuration, with a maximum of 56.5% luma BD-Rate reductions in the
enhancement layer. Furthermore, compared with the default QMs and the Sony QMs,
our method yields encoding time reductions of 0.75% and 1.19%, respectively.
"
1382,On the optimality of grid cells,"  Grid cells, discovered more than a decade ago [5], are neurons in the brain
of mammals that fire when the animal is located near certain specific points in
its familiar terrain. Intriguingly, these points form, for a single cell, a
two-dimensional triangular grid, not unlike our Figure 3. Grid cells are widely
believed to be involved in path integration, that is, the maintenance of a
location state through the summation of small displacements. We provide
theoretical evidence for this assertion by showing that cells with grid-like
tuning curves are indeed well adapted for the path integration task. In
particular we prove that, in one dimension under Gaussian noise, the
sensitivity of measuring small displacements is maximized by a population of
neurons whose tuning curves are near-sinusoids -- that is to say, with peaks
forming a one-dimensional grid. We also show that effective computation of the
displacement is possible through a second population of cells whose sinusoid
tuning curves are in phase difference from the first. In two dimensions, under
additional assumptions it can be shown that measurement sensitivity is
optimized by the product of two sinusoids, again yielding a grid-like pattern.
We discuss the connection of our results to the triangular grid pattern
observed in animals.
"
1383,"Design and Implementation of a Novel Compatible Encoding Scheme in the
  Time Domain for Image Sensor Communication","  This paper presents a modulation scheme in the time domain based on
On-Off-Keying and proposes various compatible supports for different types of
image sensors. The content of this article is a sub-proposal to the IEEE
802.15.7r1 Task Group (TG7r1) aimed at Optical Wireless Communication (OWC)
using an image sensor as the receiver. The compatibility support is
indispensable for Image Sensor Communications (ISC) because the rolling shutter
image sensors currently available have different frame rates, shutter speeds,
sampling rates, and resolutions. However, focusing on unidirectional
communications (i.e., data broadcasting, beacons), an asynchronous
communication prototype is also discussed in the paper. Due to the physical
limitations associated with typical image sensors (including low and varying
frame rates, long exposures, and low shutter speeds), the link speed
performance is critically considered. Based on the practical measurement of
camera response to modulated light, an operating frequency range is suggested
along with the similar system architecture, decoding procedure, and algorithms.
A significant feature of our novel data frame structure is that it can support
both typical frame rate cameras (in the oversampling mode) as well as very low
frame rate cameras (in the error detection mode for a camera whose frame rate
is lower than the transmission packet rate). A high frame rate camera, i.e., no
less than 20 fps, is supported in an oversampling mode in which a majority
voting scheme for decoding data is applied. A low frame rate camera, i.e., when
the frame rate drops to less than 20 fps at some certain time, is supported by
an error detection mode in which any missing data sub-packet is detected in
decoding and later corrected by external code. Numerical results and valuable
analysis are also included to indicate the capability of the proposed schemes.
"
1384,"Evaluating the predicted reliability of mechatronic systems: state of
  the art","  Reliability analysis of mechatronic systems is a recent field and a dynamic
branch of research. It is addressed whenever there is a need for reliable,
available, and safe systems. The studies of reliability must be conducted
earlier during the design phase, in order to reduce costs and the number of
prototypes required in the validation of the system. The process of reliability
is then deployed throughout the full cycle of development. This process is
broken down into three major phases: the predictive reliability, the
experimental reliability and operational reliability. The main objective of
this article is a kind of portrayal of the various studies enabling a
noteworthy mastery of the predictive reliability. The weak points are
highlighted. Presenting an overview of all the quantitative and qualitative
approaches concerned with modelling and evaluating the prediction of
reliability is so important for future reliability studies and for academic
research to come up with new methods and tools. The mechatronic system is a
hybrid system, it is dynamic, reconfigurable, and interactive. The modeling
carried out of reliability prediction must take into account these criteria.
Several methodologies have been developed in this track of research. In this
regard, the aforementioned methodologies will be analytically sketched in this
paper.
"
1385,Some comments on the reliability of NOAA's Storm Events Database,"  Storms and other severe weather events can result in fatalities, injuries,
and property damage. Therefore, preventing such outcomes to the extent possible
is a key concern, and the scientific community faces an increasing demand for
regularly updated appraisals of evolving climate conditions and extreme
weather. NOAA's Storm Events Database is undoubtedly an invaluable resource to
the general public, to the professional, and to the researcher. Due to such
importance, the primary objective of this study was to explore this database
and get clues about its reliability. A complete investigation of the damage
estimates, injuries or fatalities figures is unfeasible due to the extension of
the database. However, an exploratory data analysis with the resources of the R
statistical data analysis language found that damage reports are missing in
more than half of the records, that part of the damage values are incorrect,
and that, despite all efforts of standardizations, non-standard event type
names are still finding their way into the database. These few results are
enough to demonstrate that the database suffers from incompleteness and
inconsistencies and should not be used without taking reservations and
appropriate precautions before advancing any inferences from the data.
"
1386,Artificial Fun: Mapping Minds to the Space of Fun,"  Yampolskiy and others have shown that the space of possible minds is vast,
actually infinite (Yampolskiy, 2015). A question of interest is 'Which
activities can minds perform during their lifetime?' This question is very
broad, thus in this article restricted to 'Which non-boring activities can
minds perform?' The space of potential non-boring activities has been called by
Yudkowsky 'fun space' (Yudkowsky, 2009). This paper aims to discuss the
relation between various types of minds and the part of the fun space, which is
accessible for them.
"
1387,Project Based Learning of Embedded Systems,"  Traditional teaching, usually based on lectures and tutorials fosters the
idea of instruction-driven learning model where students are passive listeners.
Besides this approach, Project Based Learning (PBL) as a different learning
paradigm is standing behind constructivism learning theory, where learning from
real-world situations is put on the first place. The purpose of this paper is
to present our approach in learning embedded systems at our University. It is
based on combination of traditional (face-to-face) learning and PBL. Our PBL
represents an interdisciplinary project based on wireless sensor monitoring of
real-world environment (greenhouse). The students use UML that was shown as an
excellent tool for developing such a projects. From the student perspective, we
found that this high level of interdisciplinary is very valuable from the point
of view of facing the students with real-life problems.
"
1388,Want Drugs? Use Python,"  We describe how Python can be leveraged to streamline the curation, modelling
and dissemination of drug discovery data as well as the development of
innovative, freely available tools for the related scientific community. We
look at various examples, such as chemistry toolkits, machine-learning
applications and web frameworks and show how Python can glue it all together to
create efficient data science pipelines.
"
1389,Probabilistic Programming and PyMC3,"  In recent years sports analytics has gotten more and more popular. We propose
a model for Rugby data - in particular to model the 2014 Six Nations
tournament. We propose a Bayesian hierarchical model to estimate the
characteristics that bring a team to lose or win a game, and predict the score
of particular matches. This is intended to be a brief introduction to
Probabilistic Programming in Python and in particular the powerful library
called PyMC3.
"
1390,PyCells for an Open Semiconductor Industry,"  In the modern semiconductor industry, automatic generation of parameterized
and recurring layout structures plays an important role and should be present
as a feature in Electronic Design Automation (EDA)-tools. Currently these
layout generators are developed with a proprietary programming language and can
be used with a specific EDA-tool. Therefore, the semiconductor companies find
the development of the layout generators that can be used in all state of the
art EDA-tools which support OpenAccess database appealing. The goal of this
project is to develop computationally efficient layout generators with Python
(PyCells), for ams AG technologies, that possess all the features of
comprehensive layout generators.
"
1391,Accelerated Evaluation of Automated Vehicles in Car-Following Maneuvers,"  The safety of Automated Vehicles (AVs) must be assured before their release
and deployment. The current approach to evaluation relies primarily on (i)
testing AVs on public roads or (ii) track testing with scenarios defined in a
test matrix. These two methods have completely opposing drawbacks: the former,
while offering realistic scenarios, takes too much time to execute; the latter,
though it can be completed in a short amount of time, has no clear correlation
to safety benefits in the real world. To avoid the aforementioned problems, we
propose Accelerated Evaluation, focusing on the car-following scenario. The
stochastic human-controlled vehicle (HV) motions are modeled based on 1.3
million miles of naturalistic driving data collected by the University of
Michigan Safety Pilot Model Deployment Program. The statistics of the HV
behaviors are then modified to generate more intense interactions between HVs
and AVs to accelerate the evaluation procedure. The Importance Sampling theory
was used to ensure that the safety benefits of AVs are accurately assessed
under accelerated tests. Crash, injury and conflict rates for a simulated AV
are simulated to demonstrate the proposed approach. Results show that test
duration is reduced by a factor of 300 to 100,000 compared with the
non-accelerated (naturalistic) evaluation. In other words, the proposed
techniques have great potential for accelerating the AV evaluation process.
"
1392,Generating Cycloidal Gears for 3D Printing,"  (Shortened version of abstract in article itself)
  This article describes an algorithm for producing, for any desired resolution
and any desired numbers of wheel and pinion teeth, polygonal approximations to
the shapes of a pair of cycloidal gears that mesh correctly. An Octave
implementation of the algorithm, mostly written in 2014, is included. The
Octave implementation contains a (crude, but evidently adequate, at least for
reasonable numbers of wheel and pinion teeth) solution of the problem of
iteratively finding the generating wheel angle corresponding to the tips of the
tooth addenda.
  However, this Octave implementation does not contain a good solution to the
problem of automatically determining the generating wheel angles required to
produce a polygon which approximates the curved addenda to a resolution
specified by the user. A proposed better solution to this problem, involving a
priority queue, is discussed.
"
1393,8th European Conference on Python in Science (EuroSciPy 2015),"  The 8th edition of the European Conference on Python in Science, EuroSciPy
was held for the second time in the beautiful city of Cambridge, UK from
August, 26th to 29th, 2014. More than 200 participants, both from academia and
industry, attended the conference.
  As usual, the conference kicked off with two days of tutorials, divided into
an introductory and an advanced track. The introductory track, presented by
Joris Vankerschaver, Valerio Maggio Joris Van den Bossche, Stijn Van Hoey and
Nicolas Rougier, gave a quick but thorough overview of the SciPy stack, while
the experience track focused on different advanced topics. This second track
began with an introduction to Bokeh, by Bryan Van den Ven, followed by an image
processing tutorial with scikit-image by Emmanuelle Gouillart and Juan
Nunez-Iglesias. The afternoon continued with two tutorials on data analysis:
the first, intitulated ""How 'good' is your model, and how can you make it
better?"" (by Chih-Chun Chen, Dimitry Foures, Elena Chatzimichali, Giuseppe
Vettigli) focused on the challenges face while attempting model selections, and
the first day concluded with a statistics in python tutorial by Gael Varoquaux.
During the second day, the attendees tackled an in depth 4 hour tutorial on
Cython, presented by Stefan Behnel, and a crash course on ""Evidence-Based
Teaching: What We Know and How to Use It"", by Greg Wilson.
"
1394,"Spectral Clustering for Optical Confirmation and Redshift Estimation of
  X-ray Selected Galaxy Cluster Candidates in the SDSS Stripe 82","  We develop a galaxy cluster finding algorithm based on spectral clustering
technique to identify optical counterparts and estimate optical redshifts for
X-ray selected cluster candidates. As an application, we run our algorithm on a
sample of X-ray cluster candidates selected from the third XMM-Newton
serendipitous source catalog (3XMM-DR5) that are located in the Stripe 82 of
the Sloan Digital Sky Survey (SDSS). Our method works on galaxies described in
the color-magnitude feature space. We begin by examining 45 galaxy clusters
with published spectroscopic redshifts in the range of 0.1 to 0.8 with a median
of 0.36. As a result, we are able to identify their optical counterparts and
estimate their photometric redshifts, which have a typical accuracy of 0.025
and agree with the published ones. Then, we investigate another 40 X-ray
cluster candidates (from the same cluster survey) with no redshift information
in the literature and found that 12 candidates are considered as galaxy
clusters in the redshift range from 0.29 to 0.76 with a median of 0.57. These
systems are newly discovered clusters in X-rays and optical data. Among them 7
clusters have spectroscopic redshifts for at least one member galaxy.
"
1395,Characterizing Smartphone Power Management in the Wild,"  For better reliability and prolonged battery life, it is important that users
and vendors understand the quality of charging and the performance of
smartphone batteries. Considering the diverse set of devices and user behavior
it is a challenge. In this work, we analyze a large collection of battery
analytics dataset collected from 30K devices of 1.5K unique smartphone models.
We analyze their battery properties and state of charge while charging, and
reveal the characteristics of different components of their power management
systems: charging mechanisms, state of charge estimation techniques, and their
battery properties. We explore diverse charging behavior of devices and their
users.
"
1396,"A Step towards Advanced Metering for the Smart Grid: A Survey of Energy
  Monitors","  The smart grid initiative has encouraged utility companies worldwide to
rollout new and smarter versions of energy meters. Before an extensive rollout,
which is both labor-intensive and incurs high capital costs, consumers need to
be incentivized to reap the long-term benefits of smart grid. Off-the-shelf
energy monitors can provide consumers with an insight of such potential
benefits. Since energy monitors are owned by the consumer, the consumer has
greater control over data which significantly reduces privacy and data
confidentiality concerns. We evaluate several existing energy monitors using an
online technical survey and online product literature. For consumers, the use
of different off-the-shelf energy monitors can help demonstrate the potential
gains of smart grid. Our survey indicates a trend towards incorporation of
state-of-the-art capabilities, like appliance level monitoring through load
disaggregation in energy monitors, which can encourage effective consumer
participation. Multiple sensor types and ratings allow some monitors to operate
in various configurations and environments.
"
1397,"Phase Noise Influence in Optical OFDM Systems employing RF Pilot Tone
  for Phase Noise Cancellation","  For coherent and direct-detection Orthogonal Frequency Division Multiplexed
(OFDM) systems employing radio frequency (RF) pilot tone phase noise
cancellation the influence of laser phase noise is evaluated. Novel analytical
results for the common phase error and for the (modulation dependent) inter
carrier interference are evaluated based upon Gaussian statistics for the laser
phase noise. In the evaluation it is accounted for that the laser phase noise
is filtered in the correlation signal detection. Numerical results are
presented for OFDM systems with 4 and 16 PSK modulation, 200 OFDM bins and baud
rate of 1 GS/s. It is found that about 225 km transmission is feasible for the
coherent 4PSK-OFDM system over normal (G.652) fiber.
"
1398,Example Data Sets and Collections for BeSpaceD Explained,"  In this report, we present example data sets and collections for the BeSpaceD
platform. BeSpaceD is a spatio-temporal modelling and reasoning software
framework. We describe the content of a number of the data sets and how the
data was obtained. We also present the programming API in BeSpaceD used to
store and access these data sets so that future BeSpaceD users can utilise the
data collections in their own experiments with minimal effort and expand the
library of data collections for BeSpaceD.
"
1399,RFID-Cloud Smart Cart System,"  The main purpose of this work is in reducing the queuing delays in major
supermarkets or other shopping centers by means of an Electronic Smart Cart
System which will introduce an intellectual approach to billing process through
RFID technology. Smart Cart System is a cooperative performance of three
separate systems: a website developed for the shopping market, electronic smart
cart device and anti-theft RFID gates. This project focuses on developing the
electronic smart cart device itself. It involves an embedded electronic
hardware that consists of an OLED display, Arduino Mega 2560 board, a
specifically designed PCB, a Wi-Fi module, 13.56 MHz HF RFID reader, a power
supply and a shopping cart.
"
1400,"On the Origin of Samples: Attribution of Output to a Particular
  Algorithm","  With unprecedented advances in genetic engineering we are starting to see
progressively more original examples of synthetic life. As such organisms
become more common it is desirable to be able to distinguish between natural
and artificial life forms. In this paper, we present this challenge as a
generalized version of Darwin's original problem, which he so brilliantly
addressed in On the Origin of Species. After formalizing the problem of
determining origin of samples we demonstrate that the problem is in fact
unsolvable, in the general case, if computational resources of considered
originator algorithms have not been limited and priors for such algorithms are
known to be equal. Our results should be of interest to astrobiologists and
scientists interested in producing a more complete theory of life, as well as
to AI-Safety researchers.
"
1401,A Generalization of the Directed Graph Layering Problem,"  The Directed Layering Problem (DLP) solves a step of the widely used
layer-based approach to automatically draw directed acyclic graphs. To cater
for cyclic graphs, usually a preprocessing step is used that solves the
Feedback Arc Set Problem (FASP) to make the graph acyclic before a layering is
determined. Here we present the Generalized Layering Problem (GLP), which
solves the combination of DLP and FASP simultaneously, allowing general graphs
as input. We present an integer programming model and a heuristic to solve the
NP-complete GLP and perform thorough evaluations on different sets of graphs
and with different implementations for the steps of the layer-based approach.
We observe that GLP reduces the number of dummy nodes significantly, can
produce more compact drawings, and improves on graphs where DLP yields poor
aspect ratios.
"
1402,Energy Transparency for Deeply Embedded Programs,"  Energy transparency is a concept that makes a program's energy consumption
visible, from hardware up to software, through the different system layers.
Such transparency can enable energy optimizations at each layer and between
layers, and help both programmers and operating systems make energy-aware
decisions. In this paper, we focus on deeply embedded devices, typically used
for Internet of Things (IoT) applications, and demonstrate how to enable energy
transparency through existing Static Resource Analysis (SRA) techniques and a
new target-agnostic profiling technique, without hardware energy measurements.
Our novel mapping technique enables software energy consumption estimations at
a higher level than the Instruction Set Architecture (ISA), namely the LLVM
Intermediate Representation (IR) level, and therefore introduces energy
transparency directly to the LLVM optimizer. We apply our energy estimation
techniques to a comprehensive set of benchmarks, including single- and also
multi-threaded embedded programs from two commonly used concurrency patterns,
task farms and pipelines. Using SRA, our LLVM IR results demonstrate a high
accuracy with a deviation in the range of 1% from the ISA SRA. Our profiling
technique captures the actual energy consumption at the LLVM IR level with an
average error of 3%.
"
1403,DELTA: Data Extraction and Logging Tool for Android,"  In the past few years, the use of smartphones has increased exponentially,
and so have the capabilities of such devices. Together with an increase in raw
processing power, modern smartphones are equipped with a wide variety of
sensors and expose an extensive set of API (Accessible Programming Interface).
These capabilities allow us to extract a wide spectrum of data that ranges from
information about the environment (e.g., position, orientation) to user habits
(e.g., which apps she uses and when), as well as about the status of the
operating system itself (e.g., memory, network adapters). This data can be
extremely valuable in many research fields such as user authentication,
intrusion detection and detection of information leaks. For these reasons,
researchers need to use a solid and reliable logging tool to collect data from
mobile devices.
  In this paper, we first survey the existing logging tools available on the
Android platform, comparing the features offered by different tools and their
impact on the system, and highlighting some of their shortcomings. Then, we
present DELTA - Data Extraction and Logging Tool for Android, which improves
the existing Android logging solutions in terms of flexibility, fine-grained
tuning capabilities, extensibility, and available set of logging features. We
performed a full implementation of DELTA and we run a thorough evaluation on
its performance. The results show that our tool has low impact on the
performance of the system, on battery consumption, and on user experience.
Finally, we make the DELTA source code and toolset available to the research
community.
"
1404,"Online Charging Scheduling Algorithms of Electric Vehicles in Smart
  Grid: An Overview","  As an environment-friendly substitute for conventional fuel-powered vehicles,
electric vehicles (EVs) and their components have been widely developed and
deployed worldwide. The large-scale integration of EVs into power grid brings
both challenges and opportunities to the system performance. On one hand, the
load demand from EV charging imposes large impact on the stability and
efficiency of power grid. On the other hand, EVs could potentially act as
mobile energy storage systems to improve the power network performance, such as
load flattening, fast frequency control, and facilitating renewable energy
integration. Evidently, uncontrolled EV charging could lead to inefficient
power network operation or even security issues. This spurs enormous research
interests in designing charging coordination mechanisms. A key design challenge
here lies in the lack of complete knowledge of events that occur in the future.
Indeed, the amount of knowledge of future events significantly impacts the
design of efficient charging control algorithms. This article focuses on
introducing online EV charging scheduling techniques that deal with different
degrees of uncertainty and randomness of future knowledge. Besides, we
highlight the promising future research directions for EV charging control.
"
1405,Knowledge management and measurement in Public Sector Organizations,"  Knowledge Management (KM) is a strategic component that enables development,
growth and continuous improvement of Public Sector Organizations (PSO). This
thesis is bounded to this specific context. Indeed, we critically and
comprehensively study the factors that characterize KM strategies and those
that foster its development and success in PSO, then finally we propose metrics
to measure and evaluate, in order to continuous and systematically improve KM
practices in PSO. Main problems are related to the lack of academic literature
that explains the elements that KM address in the given context. In addition,
it is identified as a problem the lack of criteria for measuring and evaluating
KM in PSO, from a different viewpoint than business perspective. The
contribution of this thesis is that it provides valuable elements for an
academic debate on the previous factors, strategies and metrics to promote KM
initiatives in PSO. To achieve the research objective of this thesis we
performed a comprehensive systematic literature review in order to discover
what the KM critical success factors are, and also we integrated and proposed
some metrics to evaluate and assess the performance of KM in PSO. We conducted
an in-depth study among different Public Sector Organizations in order to learn
what are the critical success factors that must be first considered before
implement KM initiatives. based on this. Finally, based on this methodological
proposal and as a result of this research, we were able to analyze and explain
the critical success factors, we identified some strategies to encourage the
success of KM and integrated a proposal of metrics, from different approaches,
for KM in PSO.
"
1406,Toward an Algebraic Theory of Systems,"  We propose the concept of a system algebra with a parallel composition
operation and an interface connection operation, and formalize
composition-order invariance, which postulates that the order of composing and
connecting systems is irrelevant, a generalized form of associativity.
Composition-order invariance explicitly captures a common property that is
implicit in any context where one can draw a figure (hiding the drawing order)
of several connected systems, which appears in many scientific contexts. This
abstract algebra captures settings where one is interested in the behavior of a
composed system in an environment and wants to abstract away anything internal
not relevant for the behavior. This may include physical systems, electronic
circuits, or interacting distributed systems.
  One specific such setting, of special interest in computer science, are
functional system algebras, which capture, in the most general sense, any type
of system that takes inputs and produces outputs depending on the inputs, and
where the output of a system can be the input to another system. The behavior
of such a system is uniquely determined by the function mapping inputs to
outputs. We consider several instantiations of this very general concept. In
particular, we show that Kahn networks form a functional system algebra and
prove their composition-order invariance.
  Moreover, we define a functional system algebra of causal systems,
characterized by the property that inputs can only influence future outputs,
where an abstract partial order relation captures the notion of ""later"". This
system algebra is also shown to be composition-order invariant and appropriate
instantiations thereof allow to model and analyze systems that depend on time.
"
1407,"Virtual Micromagnetics: A Framework for Accessible and Reproducible
  Micromagnetic Simulation","  Computational micromagnetics requires numerical solution of partial
differential equations to resolve complex interactions in magnetic
nanomaterials. The Virtual Micromagnetics project described here provides
virtual machine simulation environments to run open-source micromagnetic
simulation packages. These environments allow easy access to simulation
packages that are often difficult to compile and install, and enable
simulations and their data to be shared and stored in a single virtual hard
disk file, which encourages reproducible research. Virtual Micromagnetics can
be extended to automate the installation of micromagnetic simulation packages
on non-virtual machines, and to support closed-source and new open-source
simulation packages, including packages from disciplines other than
micromagnetics, encouraging reuse. Virtual Micromagnetics is stored in a public
GitHub repository under a three-clause Berkeley Software Distribution (BSD)
license.
"
1408,Organized Complexity: is Big History a Big Computation?,"  The concept of ""logical depth"" introduced by Charles H. Bennett (1988) seems
to capture, at least partially, the notion of organized complexity, so central
in big history. More precisely, the increase in organized complexity refers
here to the wealth, variety and intricacy of structures, and should not be
confused with the increase of random complexity, formalized by Kolmogorov
(1965). If Bennett is right in proposing to assimilate organized complexity
with ""computational content"", then the fundamental cause of the increase of
complexity in the universe is the existence of computing mechanisms with
memory, and able to cumulatively create and preserve computational contents. In
this view, the universe computes, remembers its calculations, and reuses them
to conduct further computations. Evolutionary mechanisms are such forms of
cumulative computation with memory and we owe them the organized complexity of
life. Language, writing, culture, science and technology can also be analyzed
as computation mechanisms generating, preserving and accelerating the increase
in organized complexity. The main unifying theme for big history is the energy
rate density, a metric based on thermodynamics. However useful, this metric
does not provide much insight into the role that information and computation
play in our universe. The concept of ""logical depth"" provides a new lens to
examine the increase of organized complexity. We argue in this paper that
organized complexity is a valid and useful way to make sense of big history.
Additionally, logical depth has a rigorous formal definition in theoretical
computer science that hints at a broader research program to quantify
complexity in the universe.
  Keywords: organized complexity, Kolmogorov complexity, logical depth, big
history, cosmic evolution, evolution, complexity, complexification,
computation, artificial life, philosophy of information
"
1409,"The curse of variety in computing, and what can be done about it","  Excess freedom in how computers are used creates problems that include: bit
rot, problems with big data, problems in the creation and debugging of
software, and problems with cyber security. To tame excess freedom, ""tough
love"" is needed in the form of a {\em universal framework for the
representation and processing of diverse kinds of knowledge} (UFK). The ""SP
machine"", based on the ""SP theory of intelligence"", has the potential to
provide that framework and to help solve the problems above. There is potential
to reduce the near-4000 different kinds of computer file to one, and to reduce
the hundreds of different computer languages to one.
"
1410,"SWoTSuite: A Development Framework for Prototyping Cross-domain Semantic
  Web of Things Applications","  Semantic Web of Things (SWoT) applications focus on providing a wide-scale
interoperability that allows the sharing of IoT devices across domains and the
reusing of available knowledge on the web. However, the application development
is difficult because developers have to do various tasks such as designing an
application, annotating IoT data, interpreting data, and combining application
domains.
  To address the above challenges, this paper demonstrates SWoTSuite, a toolkit
for prototyping SWoT applications. It hides the use of semantic web
technologies as much as possible to avoid the burden of designing SWoT
applications that involves designing ontologies, annotating sensor data, and
using reasoning mechanisms to enrich data. Taking inspiration from sharing and
reuse approaches, SWoTSuite reuses data and vocabularies. It leverages existing
technologies to build applications. We take a hello world naturopathy
application as an example and demonstrate an application development process
using SWoTSuite. The demo video is available at URL:
http://tinyurl.com/zs9flrt.
"
1411,Non-Intrusive Load Monitoring: A Review and Outlook,"  With the roll-out of smart meters the importance of effective non-intrusive
load monitoring (NILM) techniques has risen rapidly. NILM estimates the power
consumption of individual devices given their aggregate consumption. In this
way, the combined consumption must only be monitored at a single, central point
in the household, providing various advantages such as reduced cost for
metering equipment. In this paper we discuss the fundamental building-blocks of
NILM, first giving a taxonomy of appliance models and device signatures and
then explaining common supervised and unsupervised learning methods.
Furthermore, we outline a fundamental algorithm that tackles the task of NILM.
Subsequently, this paper reviews recent research that has brought novel insight
to the field and more effective techniques. Finally, we formulate future
challenges in the domain of NILM and smart meters.
"
1412,Experimental Characterization of In Vivo Wireless Communication Channels,"  In vivo wireless medical devices have a critical role in healthcare
technologies due to their continuous health monitoring and noninvasive surgery
capabilities. In order to fully exploit the potential of such devices, it is
necessary to characterize the in vivo wireless communication channel which will
help to build reliable and high-performance communication systems. This paper
presents preliminary results of experimental characterization for this
fascinating communications medium on a human cadaver and compares the results
with numerical studies.
"
1413,"Cerebral Signal Instantaneous Parameters Estimation MATLAB Toolbox -
  User Guide Version 2.3","  This document is meant to help individuals use the Cerebral Signal Phase
Analysis toolbox which implements different methods for estimating the
instantaneous phase and frequency of a signal and calculating some related
popular quantities.The toolbox -- which is distributed under the terms of the
GNU GENERAL PUBLIC LICENSE as a set of MATLAB routines -- can be downloaded at
the address http://oset.ir/category.php?dir=Tools.The purpose of this toolbox
is to calculate the instantaneous phase and frequency sequences of cerebral
signals (EEG, MEG, etc.) and some related popular features and quantities in
brain studies and Neuroscience such as Phase Shift, Phase Resetting, Phase
Locking Value (PLV), Phase Difference and more, to help researchers in these
fields.
"
1414,"CONE: Zero-Calibration Accurate Confidence Estimation for Indoor
  Localization Systems","  Accurate estimation of the confidence of an indoor localization system is
crucial for a number of applications including crowd-sensing applications,
map-matching services, and probabilistic location fusion techniques; all of
which lead to an enhanced user experience. Current approaches for quantifying
the output accuracy of a localization system in real-time either do not provide
a distance metric, require an extensive training process, and/or are tailored
to a specific localization system. In this paper, we present the design,
implementation, and evaluation of CONE: a novel calibration-free accurate
confidence estimation system that can work in real-time with any location
determination system. CONE builds on a sound theoretical model that allows it
to trade the required user confidence with tight bound on the estimated
confidence radius. We also introduce a new metric for evaluating confidence
estimation systems that can capture new aspects of their performance.
Evaluation of CONE on Android phones in a typical testbed using the iBeacons
BLE technology with a side-by-side comparison with traditional confidence
estimation techniques shows that CONE can achieve a consistent median absolute
error difference accuracy of less than 2.7m while estimating the user position
more than 80% of the time within the confidence circle. This is significantly
better than the state-of-the-art confidence estimation systems that are
tailored to the specific localization system in use. Moreover, CONE does not
require any calibration and therefore provides a scalable and ubiquitous
confidence estimation system for pervasive applications.
"
1415,"Doing Moore with Less -- Leapfrogging Moore's Law with Inexactness for
  Supercomputing","  Energy and power consumption are major limitations to continued scaling of
computing systems. Inexactness, where the quality of the solution can be traded
for energy savings, has been proposed as an approach to overcoming those
limitations. In the past, however, inexactness necessitated the need for highly
customized or specialized hardware. The current evolution of commercial
off-the-shelf(COTS) processors facilitates the use of lower-precision
arithmetic in ways that reduce energy consumption. We study these new
opportunities in this paper, using the example of an inexact Newton algorithm
for solving nonlinear equations. Moreover, we have begun developing a set of
techniques we call reinvestment that, paradoxically, use reduced precision to
improve the quality of the computed result: They do so by reinvesting the
energy saved by reduced precision.
"
1416,"Highly Robust Clustering of GPS Driver Data for Energy Efficient Driving
  Style Modelling","  This paper presents a novel approach to distinguish driving styles with
respect to their energy efficiency. A distinct property of our method is that
it relies exclusively on Global Positioning System (GPS) logs of drivers. This
setting is highly relevant in practice as these data can easily be acquired.
  Relying on positional data alone means that all derived features will be
correlated, so we strive to find a single quantity that allows us to perform
the driving style analysis. To this end we consider a robust variation of the
so called jerk of a movement. We show that our feature choice outperforms other
more commonly used jerk-based formulations and we discuss the handling of
noisy, inconsistent, and incomplete data as this is a notorious problem when
dealing with real-world GPS logs.
  Our solving strategy relies on an agglomerative hierarchical clustering
combined with an L-term heuristic to determine the relevant number of clusters.
It can easily be implemented and performs fast, even on very large, real-world
data sets. Experiments show that our approach is robust against noise and able
to discern different driving styles.
"
1417,Finite Computational Structures and Implementations,"  What is computable with limited resources? How can we verify the correctness
of computations? How to measure computational power with precision? Despite the
immense scientific and engineering progress in computing, we still have only
partial answers to these questions. In order to make these problems more
precise, we describe an abstract algebraic definition of classical computation,
generalizing traditional models to semigroups. The mathematical abstraction
also allows the investigation of different computing paradigms (e.g. cellular
automata, reversible computing) in the same framework. Here we summarize the
main questions and recent results of the research of finite computation.
"
1418,Design and Implementation of an Antenna Model for the Cooja simulator,"  COOJA is a network simulator developed for wireless sensor networks. It can
be used for high-level algorithm development as well as low-level device driver
implementations for accurate simulation of wireless sensor networks before
deployment. However, in a simulation Cooja assumes that the nodes are only
equipped with omnidirectional antennas. There is currently no support for
directional antennas. Due to the growing interest in the use of directional or
smart antennas in wireless sensor networks, a model that can support
directional antennas is essential for the realistic simulations of protocols
relying on directional communication. This paper presents work on extending
COOJA with a directional antenna model.
"
1419,A Survey: Embedded Systems Supporting By Different Operating Systems,"  In these days embedded system have an important role in different Fields and
applications like Network embedded system , Real-time embedded systems which
supports the mission-critical domains, mostly having the time constraints,
Stand-alone systems which includes the network router etc. A great deployment
in the processors made for completing the demanding needs of the users. There
is also a large-scale deployment occurs in sensor networks for providing the
advance facilities, for handled such type of embedded systems a specific
operating system must provide. This paper presents some software
infrastructures that have the ability of supporting such types of embedded
systems.
"
1420,Perspectives and Networks,"  The perspective we take on a system determines the features and properties of
this system that we are focusing on. It determines where we search for causes
to explain the effects on the system that we observe. It determines the terms
in which we expect the information about the system to be expressed. And it can
also influence the choice of formalism that will be used to convey the
information. using Boolean Automata Networks as prototypes of interaction
systems, this paper means to start making these considerations concrete in
order to draw a practical benefit out of them.
"
1421,Causality and Networks,"  Causality is omnipresent in scientists' verbalisations of their
understanding, even though we have no formal consensual scientific definition
for it. In Automata Networks, it suffices to say that automata ""influence"" one
another to introduce a notion of causality. One might argue that this merely is
an incidental side effect of preferring statements expressed in natural
languages to mathematical formulae. The discussion of this paper shows that if
this is the case, then it is worth considering the effects of those preferences
on the contents of the statements we make and the formulae we derive. And if it
is not the case, that causality is a mere incidental side effect of our
preferences of formulation, then causality must be worth some scientific
attention per se. In any case, the paper illustrates how the innate sense of
causality we have may be made deliberate and formal use of without having to
pin down the elusive notion of causality to anything fixed and formal that
wouldn't do justice to the wide range of ways it is involved in science-making.
"
1422,Novel Grid Topology Estimation Technique Exploiting PLC Modems,"  A fundamental requirement to develop routing strategies in power line
networks is the knowledge of the network topology, which might not be complete.
In this work, we present a novel method to derive the topology of a
distribution network that exploits the capability of Power Line Communication
modems to measure the network admittance, and we report the most significant
results.
"
1423,"Application Specific Instrumentation (ASIN): A Bio-inspired Paradigm to
  Instrumentation using recognition before detection","  In this paper we present a new scheme for instrumentation, which has been
inspired by the way small mammals sense their environment. We call this scheme
Application Specific Instrumentation (ASIN). A conventional instrumentation
system focuses on gathering as much information about the scene as possible.
This, usually, is a generic system whose data can be used by another system to
take a specific action. ASIN fuses these two steps into one. The major merit of
the proposed scheme is that it uses low resolution sensors and much less
computational overhead to give good performance for a highly specialised
application
"
1424,"Overview of Spintronic Sensors, Internet of Things, and Smart Living","  Smart living is a trending lifestyle that envisions lower energy consumption,
sound public services, and better quality of life for human being. The Internet
of Things (IoT) is a compelling platform connecting various sensors around us
to the Internet, providing great opportunities for the realization of smart
living. Spintronic sensors with superb measuring ability and multiple unique
advantages can be an important piece of cornerstone for IoT. In this review, we
discuss successful applications of spintronic sensors in electrical current
sensing, transmission and distribution lines monitoring, vehicle detection, and
biodetection. Traditional monitoring systems with limited sensors and wired
communication can merely collect fragmented data in the application domains. In
this paper, the wireless spintronic sensor networks (WSSNs) will be proposed
and illustrated to provide pervasive monitoring systems, which facilitate the
intelligent surveillance and management over building, power grid, transport,
and healthcare. The database of collected information will be of great use to
the policy making in public services and city planning. This work provides
insights for realizing smart living through the integration of IoT with
spintronic sensor technology.
"
1425,"Enerji \.Izleme Yaz{\i}l{\i}mlar{\i} i\c{c}in Merkezi ve Genel bir
  Mimari (A Centralized and Generic Architecture for Energy Monitoring
  Software)","  There is need for several software systems within the energy domain and
corresponding systems are being developed to satisfy these needs. These systems
include energy monitoring, information, wide area monitoring and control
systems, and SCADA systems. Energy monitoring systems are one of the most
important and common systems among them. In this study, after briefly reviewing
several of the software systems within the energy domain, a centralized and
generic software architecture for energy monitoring systems is presented. Next,
sample projects are described in which energy monitoring systems based on this
architecture have been implemented. We envisage that this study will be an
important resource for software projects in the energy domain.
"
1426,Activity Recognition Based on Micro-Doppler Signature with In-Home Wi-Fi,"  Device free activity recognition and monitoring has become a promising
research area with increasing public interest in pattern of life monitoring and
chronic health conditions. This paper proposes a novel framework for in-home
Wi-Fi signal-based activity recognition in e-healthcare applications using
passive micro-Doppler (m-D) signature classification. The framework includes
signal modeling, Doppler extraction and m-D classification. A data collection
campaign was designed to verify the framework where six m-D signatures
corresponding to typical daily activities are sucessfully detected and
classified using our software defined radio (SDR) demo system. Analysis of the
data focussed on potential discriminative characteristics, such as maximum
Doppler frequency and time duration of activity. Finally, a sparsity induced
classifier is applied for adaptting the method in healthcare application
scenarios and the results are compared with those from the well-known Support
Vector Machine (SVM) method.
"
1427,GSM based CommSense system to measure and estimate environmental changes,"  Facilitating the coexistence of radar systems with communication systems has
been a major area of research in radar engineering. The current work presents a
new way to sense the environment using the channel equalization block of
existing communication systems. We have named this system CommSense. In the
current paper we demonstrate the feasibility of the system using Global System
for Mobile Communications (GSM) signals. The implementation has been done using
open-source Software Defined Radio (SDR) environment. In the preliminary
results obtained in our work we show that it is possible to distinguish
environmental changes using the proposed system. The major advantage of the
system is that it is inexpensive as channel estimation is an inherent block in
any communication system and hence the added cost to make it work as an
environment sensor is minimal. The major challenge, on which we are continuing
our work, is how to characterize the features in the environmental changes.
This is an acute challenge given the fact that the bandwidth available is
narrow and the system is inherently a forward looking radar. However the
initial results, as shown in this paper, are encouraging and we intend to use
an application specific instrumentation (ASIN) scheme to distinguish the
environmental changes.
"
1428,A Brief Survey of Non-Residue Based Computational Error Correction,"  The idea of computational error correction has been around for over half a
century. The motivation has largely been to mitigate unreliable devices,
manufacturing defects or harsh environments, primarily as a mandatory measure
to preserve reliability, or more recently, as a means to lower energy by
allowing soft errors to occasionally creep. While residue codes have shown
great promise for this purpose, there have been several orthogonal non-residue
based techniques. In this article, we provide a high level outline of some of
these non-residual approaches.
"
1429,Length Matters: Clustering System Log Messages using Length of Words,"  The analysis techniques of system log messages (syslog messages) have a long
history from when the syslog mechanism was invented. Typically, the analysis
consists of two parts, one is a message template generation, and the other is
finding something interesting using the messages classified by the inferred
templates. It is important to generate better templates to achieve better,
precise, or convincible analysis results. In this paper, we propose a
classification methodology using the length of words of each message. Our
method is suitable for online template generation because it does not require
two-pass analysis to generate template messages, that is an important factor
considering increasing amount of log messages produced by a large number of
system components such as cloud infrastructure.
"
1430,"On the optimality of ternary arithmetic for compactness and hardware
  design","  In this paper, the optimality of ternary arithmetic is investigated under
strict mathematical formulation. The arithmetic systems are presented in
generic form, as the means to encode numeric values, and the choice of radix is
asserted as the main parameter to assess the efficiency of the representation,
in terms of information compactness and estimated implementation cost in
hardware. Using proper formulations for the optimization task, the universal
constant 'e' (base of natural logarithms) is proven as the most efficient radix
and ternary is asserted as the closest integer choice.
"
1431,"A Novel Approach for Learning How to Automatically Match Job Offers and
  Candidate Profiles","  Automatic matching of job offers and job candidates is a major problem for a
number of organizations and job applicants that if it were successfully
addressed could have a positive impact in many countries around the world. In
this context, it is widely accepted that semi-automatic matching algorithms
between job and candidate profiles would provide a vital technology for making
the recruitment processes faster, more accurate and transparent. In this work,
we present our research towards achieving a realistic matching approach for
satisfactorily addressing this challenge. This novel approach relies on a
matching learning solution aiming to learn from past solved cases in order to
accurately predict the results in new situations. An empirical study shows us
that our approach is able to beat solutions with no learning capabilities by a
wide margin.
"
1432,"An Efficient Framework for Floor-plan Prediction of Dynamic Runtime
  Reconfigurable Systems","  Several embedded application domains for reconfigurable systems tend to
combine frequent changes with high performance demands of their workloads such
as image processing, wearable computing and network processors. Time
multiplexing of reconfigurable hardware resources raises a number of new
issues, ranging from run-time systems to complex programming models that
usually form a Reconfigurable hardware Operating System (ROS). The Operating
System performs online task scheduling and handles resource management. There
are many challenges in adaptive computing and dynamic reconfigurable systems.
One of the major understudied challenges is estimating the required resources
in terms of soft cores, Programmable Reconfigurable Regions (PRRs), the
appropriate communication infrastructure, and to predict a near optimal layout
and floorplan of the reconfigurable logic fabric. Some of these issues are
specific to the application being designed, while others are more general and
relate to the underlying run-time environment. Static resource allocation for
Run- Time Reconfiguration (RTR) often leads to inferior and unacceptable
results. In this paper, we present a novel adaptive and dynamic methodology,
based on a Machine Learning approach, for predicting and estimating the
necessary resources for an application based on past historical information. An
important feature of the proposed methodology is that the system is able to
learn and generalize and, therefore, is expected to improve its accuracy over
time. The goal of the entire process is to extract useful hidden knowledge from
the data. This knowledge is the prediction and estimation of the necessary
resources for an unknown or not previously seen application.
"
1433,"COOLL: Controlled On/Off Loads Library, a Public Dataset of High-Sampled
  Electrical Signals for Appliance Identification","  This paper gives a brief description of the Controlled On/Off Loads Library
(COOLL) dataset. This latter is a dataset of high-sampled electrical current
and voltage measurements representing individual appliances consumption. The
measurements were taken in June 2016 in the PRISME laboratory of the University
of Orl\'eans, France. The appliances are mainly controllable appliances (i.e.
we can precisely control their turn-on/off time instants). 42 appliances of 12
types were measured at a 100 kHz sampling frequency.
"
1434,"Detection of Dangerous Magnetic Field Ranges from Tablets by Clustering
  Analysis","  The paper considers the problem of the extremely low frequency magnetic field
radiation generated by the tablet computers. Accordingly, the measurement of
the magnetic field radiation from a set of tablets is carried out. Furthermore,
the measurement results are analyzed and clustered according to the K-Medians
algorithm to obtain different magnetic field ranges. The obtained cluster
ranges are evaluated according to the reference level proposed by the TCO
standard in order to define dangerous areas in the neighborhood of tablet,
which are established during the typical work with tablet computers. Analysis
shows that dangerous areas correspond to specific inner components of tablet,
and gives suggestions to users for a safe usage of tablet and to companies
producing tablet components for limiting the risk of magnetic field exposure.
"
1435,"City traffic forecasting using taxi GPS data: A coarse-grained cellular
  automata model","  City traffic is a dynamic system of enormous complexity. Modeling and
predicting city traffic flow remains to be a challenge task and the main
difficulties are how to specify the supply and demands and how to parameterize
the model. In this paper we attempt to solve these problems with the help of
large amount of floating car data. We propose a coarse-grained cellular
automata model that simulates vehicles moving on uniform grids whose size are
much larger compared with the microscopic cellular automata model. The car-car
interaction in the microscopic model is replaced by the coupling between
vehicles and coarse-grained state variables in our model. To parameterize the
model, flux-occupancy relations are fitted from the historical data at every
grids, which serve as the coarse-grained fundamental diagrams coupling the
occupancy and speed. To evaluate the model, we feed it with the historical
travel demands and trajectories obtained from the floating car data and use the
model to predict road speed one hour into the future. Numerical results show
that our model can capture the traffic flow pattern of the entire city and make
reasonable predictions. The current work can be considered a prototype for a
model-based forecasting system for city traffic.
"
1436,Operations in the era of large distributed telescopes,"  The previous generation of astronomical instruments tended to consist of
single receivers in the focal point of one or more physical reflectors. Because
of this, most astronomical data sets were small enough that the raw data could
easily be downloaded and processed on a single machine.
  In the last decade, several large, complex Radio Astronomy instruments have
been built and the SKA is currently being designed. Many of these instruments
have been designed by international teams, and, in the case of LOFAR span an
area larger than a single country. Such systems are ICT telescopes and consist
mainly of complex software. This causes the main operational issues to be
related to the ICT systems and not the telescope hardware. However, it is
important that the operations of the ICT systems are coordinated with the
traditional operational work. Managing the operations of such telescopes
therefore requires an approach that significantly differs from classical
telescope operations.
  The goal of this session is to bring together members of operational teams
responsible for such large-scale ICT telescopes. This gathering will be used to
exchange experiences and knowledge between those teams. Also, we consider such
a meeting as very valuable input for future instrumentation, especially the SKA
and its regional centres.
"
1437,DEMoS Manifesto,"  This is a manifesto for DEMoS, which is a Distributed Embedded Modular
System, but also a manifesto addressing the need for more
inter-/cross-disciplinary mastery of working knowledge related to installing
this class of systems in the real world. There is somehow room for yet another
class of systems - complementary to existing embedded systems - complementing
distributed operating systems - which takes on an interdisciplinary
cyber-physical-materiality approach, a dedicated holistic perspective that
recognizes the true value of interdisciplinary mastery vs. the implicit and
overlooked expense of narrow intra-disciplinary focus dominating much of
systems development (e.g. EE, CE, CS, SE, and IS). Interdisciplinary mastery
yields its accumulated value across the development, deployment, use, re-use,
and decommission phases for this class of systems: DEMoS is a system
architected to be locally distributed, embedded, and modular as outlined herein
and with the additional goals of human interdisciplinary mastery in this
context: A potential set of goals for developing and applying DEMoS can be
found in UN Resolution 70/1.
"
1438,"Computational Intelligence: are you crazy? Since when has intelligence
  become computational?","  Computational Intelligence is a dead-end attempt to recreate human-like
intelligence in a computing machine. The goal is unattainable because the means
chosen for its accomplishment are mutually inconsistent and contradictory:
""Computational"" implies data processing ability while ""Intelligence"" implies
the ability to process information. In the research community, there is a lack
of interest in data versus information divergence. The cause of this
indifference is the Shannon's Information theory, which has dominated the
scientific community since the early 1950s. However, today it is clear that
Shannon's theory is applicable only to a specific case of data communication
and is inapplicable to the majority of other occasions, where information about
semantic properties of a message must be taken into account. The paper will try
to explain the devastating results of overlooking some of these very important
issues - what is intelligence, what is semantic information, how they are
interrelated and what happens when the relationship is disregarded.
"
1439,"Improving the Quality of Random Number Generators by Applying a Simple
  Ratio Transformation","  It is well-known that the quality of random number generators can often be
improved by combining several generators, e.g. by summing or subtracting their
results. In this paper we investigate the ratio of two random number generators
as an alternative approach: the smaller of two input random numbers is divided
by the larger, resulting in a rational number from $[0,1]$.
  We investigate theoretical properties of this approach and show that it
yields a good approximation to the ideal uniform distribution. To evaluate the
empirical properties we use the well-known test suite \textsc{TestU01}. We
apply the ratio transformation to moderately bad generators, i.e. those that
failed up to 40\% of the tests from the test battery \textsc{Crush} of
\textsc{TestU01}. We show that more than half of them turn into very good
generators that pass all tests of \textsc{Crush} and \textsc{BigCrush} from
\textsc{TestU01} when the ratio transformation is applied. In particular,
generators based on linear operations seem to benefit from the ratio, as this
breaks up some of the unwanted regularities in the input sequences. Thus the
additional effort to produce a second random number and to calculate the ratio
allows to increase the quality of available random number generators.
"
1440,Detecting Plagiarism based on the Creation Process,"  All methodologies for detecting plagiarism to date have focused on the final
digital ""outcome"", such as a document or source code. Our novel approach takes
the creation process into account using logged events collected by special
software or by the macro recorders found in most office applications. We look
at an author's interaction logs with the software used to create the work.
Detection relies on comparing the histograms of multiple logs' command use. A
work is classified as plagiarism if its log deviates too much from logs of
""honestly created"" works or if its log is too similar to another log. The
technique supports the detection of plagiarism for digital outcomes that stem
from \emph{unique} tasks, such as theses and \emph{equal} tasks such as
assignments for which the same problem sets are solved by multiple students.
Focusing on the latter case, we evaluate this approach using logs collected by
an interactive development environment (IDE) from more than sixty students who
completed three programming assignments.
"
1441,Picturing Indefinite Causal Structure,"  Following on from the notion of (first-order) causality, which generalises
the notion of being tracepreserving from CP-maps to abstract processes, we give
a characterization for the most general kind of map which sends causal
processes to causal processes. These new, second-order causal processes enable
us to treat the input processes as 'local laboratories' whose causal ordering
needs not be fixed in advance. Using this characterization, we give a
fully-diagrammatic proof of a non-trivial theorem: namely that being
causality-preserving on separable processes implies being 'completely'
causality preserving. That is, causality is preserved even when the 'local
laboratories' are allowed to have ancilla systems. An immediate consequence is
that preserving causality is separable processes is equivalence to preserving
causality for strongly non-signalling (a.k.a. localizable) processes.
"
1442,"Online characterization of planetary surfaces: PlanetServer, an
  open-source analysis and visualization tool","  The lack of open-source tools for hyperspectral data visualization and
analysiscreates a demand for new tools. In this paper we present the new
PlanetServer,a set of tools comprising a web Geographic Information System
(GIS) and arecently developed Python Application Programming Interface (API)
capableof visualizing and analyzing a wide variety of hyperspectral data from
differentplanetary bodies. Current WebGIS open-source tools are evaluated in
orderto give an overview and contextualize how PlanetServer can help in this
mat-ters. The web client is thoroughly described as well as the datasets
availablein PlanetServer. Also, the Python API is described and exposed the
reason ofits development. Two different examples of mineral characterization of
differenthydrosilicates such as chlorites, prehnites and kaolinites in the Nili
Fossae areaon Mars are presented. As the obtained results show positive outcome
in hyper-spectral analysis and visualization compared to previous literature,
we suggestusing the PlanetServer approach for such investigations.
"
1443,SOI RF Switch for Wireless Sensor Network,"  The objective of this research was to design a 0-5 GHz RF SOI switch, with
0.18um power Jazz SOI technology by using Cadence software, for health care
applications. This paper introduces the design of a RF switch implemented in
shunt-series topology. An insertion loss of 0.906 dB and an isolation of 30.95
dB were obtained at 5 GHz. The switch also achieved a third order distortion of
53.05 dBm and 1 dB compression point reached 50.06dBm. The RF switch
performance meets the desired specification requirements.
"
1444,Low Power SI Class E Power Amplifier and RF Switch For Health Care,"  This research was to design a 2.4 GHz class E Power Amplifier (PA) for health
care, with 0.18um Semiconductor Manufacturing International Corporation CMOS
technology by using Cadence software. And also RF switch was designed at
cadence software with power Jazz 180nm SOI process. The ultimate goal for such
application is to reach high performance and low cost, and between high
performance and low power consumption design. This paper introduces the design
of a 2.4GHz class E power amplifier and RF switch design. PA consists of
cascade stage with negative capacitance. This power amplifier can transmit
16dBm output power to a 50{\Omega} load. The performance of the power amplifier
and switch meet the specification requirements of the desired.
"
1445,Taxi-based Emergency Medical System,"  In case of a severe accident, the key to saving lives is the time between the
incident and when the victim receives treatment from the first-responders. In
areas with well designed emergency medical systems, the time for an ambulance
to arrive at the accident location is often not too long. However, in many low
and middle income countries, it usually takes much longer for an ambulance to
arrive at the accident location due to lack of proper services. On the other
hand, with ubiquitous wireless connectivity, and emergence of radio based
taxis, it seems feasible to build a low-cost emergency response system based on
taxi service. In this report, we explore one such solution for deployment of a
taxi-based emergency response systems using reinforcement learning.
"
1446,"MorphoNoC: Exploring the Design Space of a Configurable Hybrid NoC using
  Nanophotonics","  As diminishing feature sizes drive down the energy for computations, the
power budget for on-chip communication is steadily rising. Furthermore, the
increasing number of cores is placing a huge performance burden on the
network-on-chip (NoC) infrastructure. While NoCs are designed as regular
architectures that allow scaling to hundreds of cores, the lack of a flexible
topology gives rise to higher latencies, lower throughput, and increased energy
costs. In this paper, we explore MorphoNoCs - scalable, configurable, hybrid
NoCs obtained by extending regular electrical networks with configurable
nanophotonic links. In order to design MorphoNoCs, we first carry out a
detailed study of the design space for Multi-Write Multi-Read (MWMR)
nanophotonics links. After identifying optimum design points, we then discuss
the router architecture for deploying them in hybrid electronic-photonic NoCs.
We then study explore the design space at the network level, by varying the
waveguide lengths and the number of hybrid routers. This affords us to carry
out energy-latency trade-offs. For our evaluations, we adopt traces from
synthetic benchmarks as well as the NAS Parallel Benchmark suite. Our results
indicate that MorphoNoCs can achieve latency improvements of up to 3.0x or
energy improvements of up to 1.37x over the base electronic network.
"
1447,Stay-point Identification as Curve Extrema,"  In a nutshell, stay-points are locations that a person has stopped for some
amount of time. Previous work depends mainly on stay-point identification
methods using experimentally fine tuned threshold values. These behave well on
their experimental datasets but may exhibit reduced performance on other
datasets.
  In this work, we demonstrate the potential of a geometry-based method for
stay-point extraction. This is accomplished by transforming the user's
trajectory path to a two-dimensional discrete time series curve that in turn
transforms the stay-points to the local minima of the first derivative of this
curve.
  To demonstrate the soundness of the proposed method, we evaluated it on raw,
noisy trajectory data acquired over the period of 28 different days using four
different techniques. The results demonstrate, among others, that given a good
trajectory tracking technique, we can identify correctly 86% to 98% of the
stay-points.
"
1448,Validation of Internal Meters of Mobile Android Devices,"  In this paper we outline our results for validating the precision of the
internal power meters of smart-phones under different workloads. We compare its
results with an external power meter. This is the first step towards creating
customized energy models on the fly and towards optimizing battery efficiency
using genetic program improvements. Our experimental results indicate that the
internal meters are sufficiently precise when large enough time windows are
considered.
  This is part of our work on the ""dreaming smart-phone"". For a technical
demonstration please watch our videos
https://www.youtube.com/watch?v=xeeFz2GLFdU and
https://www.youtube.com/watch?v=C7WHoLW1KYw.
"
1449,An Introduction to Classic DEVS,"  DEVS is a popular formalism for modelling complex dynamic systems using a
discrete-event abstraction. At this abstraction level, a timed sequence
ofpertinent ""events"" input to a system (or internal, in the case of timeouts)
cause instantaneous changes to the state of the system. Between events, the
state does not change, resulting in a a piecewise constant state trajectory.
Main advantages of DEVS are its rigorous formal definition, and its support for
modular composition.
  This chapter introduces the Classic DEVS formalism in a bottom-up fashion,
using a simple traffic light example. The syntax and operational semantics of
Atomic (i.e., non-hierarchical) models are intruced first. The semantics of
Coupled (hierarchical) models is then given by translation into Atomic DEVS
models. As this formal ""flattening"" is not efficient, a modular abstract
simulator which operates directly on the coupled model is also presented. This
is the common basis for subsequent efficient implementations. We continue to
actual applications of DEVS modelling and simulation, as seen in performance
analysis for queueing systems. Finally, we present some of the shortcomings in
the Classic DEVS formalism, and show solutions to them in the form of variants
of the original formalism.
"
1450,"Persistent Entropy for Separating Topological Features from Noise in
  Vietoris-Rips Complexes","  Persistent homology studies the evolution of k-dimensional holes along a
nested sequence of simplicial complexes (called a filtration). The set of bars
(i.e. intervals) representing birth and death times of k-dimensional holes
along such sequence is called the persistence barcode. k-Dimensional holes with
short lifetimes are informally considered to be ""topological noise"", and those
with long lifetimes are considered to be ""topological features"" associated to
the filtration. Persistent entropy is defined as the Shannon entropy of the
persistence barcode of a given filtration. In this paper we present new
important properties of persistent entropy of Cech and Vietoris-Rips
filtrations. Among the properties, we put a focus on the stability theorem that
allows to use persistent entropy for comparing persistence barcodes. Later, we
derive a simple method for separating topological noise from features in
Vietoris-Rips filtrations.
"
1451,"Quantitative Characterization of Components of Computer Assisted
  Interventions","  Purpose: We propose a mathematical framework for quantitative analysis
weighting the impact of heterogeneous components of a surgery. While
multi-level approaches, surgical process modeling and other workflow analysis
methods exist, this is to our knowledge the first quantitative approach.
Methods: Inspired by the group decision making problem from the field of
operational research, we define event impact factors, which combine independent
and very diverse low-level functions. This allows us to rate surgical events by
their importance. Results: We conducted surveys with 4 surgeons to determine
the importance of roles, phases and their combinations within a laparoscopic
cholecystectomy. Applying this data on a recorded surgery, we showed that it is
possible to define a quantitative measure for deciding on acception or
rejection of calls to different roles and at different phases of surgery.
Conclusions: This methodology allows us to use components such as expertise and
role of the surgical staff and other aspects of a given surgery in order to
quantitatively analyze and evaluate events, actions, user interfaces or
procedures.
"
1452,Sensitivity Analysis of Expensive Black-Box Systems Using Metamodeling,"  Simulations are becoming ever more common as a tool for designing complex
products. Sensitivity analysis techniques can be applied to these simulations
to gain insight, or to reduce the complexity of the problem at hand. However,
these simulators are often expensive to evaluate and sensitivity analysis
typically requires a large amount of evaluations. Metamodeling has been
successfully applied in the past to reduce the amount of required evaluations
for design tasks such as optimization and design space exploration. In this
paper, we propose a novel sensitivity analysis algorithm for variance and
derivative based indices using sequential sampling and metamodeling. Several
stopping criteria are proposed and investigated to keep the total number of
evaluations minimal. The results show that both variance and derivative based
techniques can be accurately computed with a minimal amount of evaluations
using fast metamodels and FLOLA-Voronoi or density sequential sampling
algorithms.
"
1453,On absolutely normal numbers and their discrepancy estimate,"  We construct the base $2$ expansion of an absolutely normal real number $x$
so that, for every integer $b$ greater than or equal to $2$, the discrepancy
modulo $1$ of the sequence $(b^0 x, b^1 x, b^2 x , \ldots)$ is essentially the
same as that realized by almost all real numbers.
"
1454,"Evaluation of Trace Alignment Quality and its Application in Medical
  Process Mining","  Trace alignment algorithms have been used in process mining for discovering
the consensus treatment procedures and process deviations. Different alignment
algorithms, however, may produce very different results. No widely-adopted
method exists for evaluating the results of trace alignment. Existing
reference-free evaluation methods cannot adequately and comprehensively assess
the alignment quality. We analyzed and compared the existing evaluation
methods, identifying their limitations, and introduced improvements in two
reference-free evaluation methods. Our approach assesses the alignment result
globally instead of locally, and therefore helps the algorithm to optimize
overall alignment quality. We also introduced a novel metric to measure the
alignment complexity, which can be used as a constraint on alignment algorithm
optimization. We tested our evaluation methods on a trauma resuscitation
dataset and provided the medical explanation of the activities and patterns
identified as deviations using our proposed evaluation methods.
"
1455,Kharita: Robust Map Inference using Graph Spanners,"  The widespread availability of GPS information in everyday devices such as
cars, smartphones and smart watches make it possible to collect large amount of
geospatial trajectory information. A particularly important, yet technically
challenging, application of this data is to identify the underlying road
network and keep it updated under various changes. In this paper, we propose
efficient algorithms that can generate accurate maps in both batch and online
settings. Our algorithms utilize techniques from graph spanners so that they
produce maps can effectively handle a wide variety of road and intersection
shapes. We conduct a rigorous evaluation of our algorithms over two real-world
datasets and under a wide variety of performance metrics. Our experiments show
a significant improvement over prior work. In particular, we observe an
increase in Biagioni f-score of up to 20% when compared to the state of the art
while reducing the execution time by an order of magnitude. We also make our
source code open source for reproducibility and enable other researchers to
build on our work.
"
1456,"Correct Convergence of Min-Sum Loopy Belief Propagation in a Block
  Interpolation Problem","  This work proves a new result on the correct convergence of Min-Sum Loopy
Belief Propagation (LBP) in an interpolation problem on a square grid graph.
The focus is on the notion of local solutions, a numerical quantity attached to
each site of the graph that can be used for obtaining MAP estimates. The main
result is that over an $N\times N$ grid graph with a one-run boundary
configuration, the local solutions at each $i \in B$ can be calculated using
Min-Sum LBP by passing difference messages in $2N$ iterations, which parallels
the well-known convergence time in trees.
"
1457,"Characterizing Classes of Potential Outliers through Traffic Data Set
  Data Signature 2D nMDS Projection","  This paper presents a formal method for characterizing the potential outliers
from the data signature projection of traffic data set using Non-Metric
Multidimensional Scaling (nMDS) visualization. Previous work had only relied on
visual inspection and the subjective nature of this technique may derive false
and invalid potential outliers. The identification of correct potential
outliers had already been an open problem proposed in literature. This is due
to the fact that they pinpoint areas and time frames where traffic
incidents/accidents occur along the North Luzon Expressway (NLEX) in Luzon. In
this paper, potential outliers are classified into (1) absolute potential
outliers; (2) valid potential outliers; and (3) ambiguous potential outliers
through the use of confidence bands and confidence ellipse. A method is also
described to validate cluster membership of identified ambiguous potential
outliers. Using the 2006 NLEX Balintawak Northbound (BLK-NB) data set, we were
able to identify two absolute potential outliers, nine valid potential
outliers, and five ambiguous potential outliers. In a literature where Vector
Fusion was used, 10 potential outliers were identified. Given the results for
the nMDS visualization using the confidence bands and confidence ellipses, all
of these 10 potential outliers were also found and 8 new potential outliers
were also found.
"
1458,Insense: Incoherent Sensor Selection for Sparse Signals,"  Sensor selection refers to the problem of intelligently selecting a small
subset of a collection of available sensors to reduce the sensing cost while
preserving signal acquisition performance. The majority of sensor selection
algorithms find the subset of sensors that best recovers an arbitrary signal
from a number of linear measurements that is larger than the dimension of the
signal. In this paper, we develop a new sensor selection algorithm for sparse
(or near sparse) signals that finds a subset of sensors that best recovers such
signals from a number of measurements that is much smaller than the dimension
of the signal. Existing sensor selection algorithms cannot be applied in such
situations. Our proposed Incoherent Sensor Selection (Insense) algorithm
minimizes a coherence-based cost function that is adapted from recent results
in sparse recovery theory. Using six datasets, including two real-world
datasets on microbial diagnostics and structural health monitoring, we
demonstrate the superior performance of Insense for sparse-signal sensor
selection.
"
1459,Modulation and Multiple Access for 5G Networks,"  Fifth generation (5G) wireless networks face various challenges in order to
support large-scale heterogeneous traffic and users, therefore new modulation
and multiple access (MA) schemes are being developed to meet the changing
demands. As this research space is ever increasing, it becomes more important
to analyze the various approaches, therefore in this article we present a
comprehensive overview of the most promising modulation and MA schemes for 5G
networks. We first introduce the different types of modulation that indicate
their potential for orthogonal multiple access (OMA) schemes and compare their
performance in terms of spectral efficiency, out-of-band leakage, and bit-error
rate. We then pay close attention to various types of non-orthogonal multiple
access (NOMA) candidates, including power-domain NOMA, code-domain NOMA, and
NOMA multiplexing in multiple domains. From this exploration we can identify
the opportunities and challenges that will have significant impact on the
design of modulation and MA for 5G networks.
"
1460,"Building up user confidence for the spaceborne derived global and
  continental land cover products for the Mediterranean region: the case of
  Thessaly","  Across globe and space agencies nations recognize the importance of
homogenized land cover information, prone to regular updates, both in the
context of thematic and spatial resolutions. Recent sensor advances and the
free distribution policy promote the utilization of spaceborne products in an
unprecedented pace into an increasingly wider range of applications. Ensuring
credibility to the users is a major enabler in this process. To this end this
study contributes with a systematic accuracy performance measurement and
continental/global land cover layers' inter-comparison moving towards
confidence built up. Confidence levels during validation and a weighted overall
accuracy assessment were applied. Google Earth imagery was employed to assess
the accuracy of three land cover products, i.e., Globeland30, HRLs and CLC
2012, for the years 2010 and 2012. Reported rates indicate a minimum weighted
overall accuracy of 84%. Specific classes' performance deviations from the
general trend were noted and discussed on the basis of an unbiased sampling
approach. By integrating confidence levels during the ground truth annotation,
stratified sampling on the several Corine Level 3 subclasses and the weighted
overall accuracy assessment, the different aspects of the considered land cover
products can be highlighted more objectively.
"
1461,"Preventing Hospital Acquired Infections Through a Workflow-Based
  Cyber-Physical System","  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
"
1462,Stochastic Development Regression on Non-Linear Manifolds,"  We introduce a regression model for data on non-linear manifolds. The model
describes the relation between a set of manifold valued observations, such as
shapes of anatomical objects, and Euclidean explanatory variables. The approach
is based on stochastic development of Euclidean diffusion processes to the
manifold. Defining the data distribution as the transition distribution of the
mapped stochastic process, parameters of the model, the non-linear analogue of
design matrix and intercept, are found via maximum likelihood. The model is
intrinsically related to the geometry encoded in the connection of the
manifold. We propose an estimation procedure which applies the Laplace
approximation of the likelihood function. A simulation study of the performance
of the model is performed and the model is applied to a real dataset of Corpus
Callosum shapes.
"
1463,Bayesian Gates for Reliable Logical Operations under Noisy Condition,"  The reliability of logical operations is indispensable for the reliable
operation of computational systems. Since the down-sizing of micro-fabrication
generates non-negligible noise in these systems, a new approach for designing
noise-immune gates is required. In this paper, we demonstrate that noise-immune
gates can be designed by combining Bayesian inference theory with the idea of
computation over a noisy signal. To reveal their practical advantages, the
performance of these gates is evaluated in comparison with a stochastic
resonance-based gate proposed previously. This approach for computation is also
demonstrated to be better than a conventional one that conducts information
transmission and computation separately.
"
1464,"A Survey on Non-Intrusive Load Monitoring Methodies and Techniques for
  Energy Disaggregation Problem","  The rapid urbanization of developing countries coupled with explosion in
construction of high rising buildings and the high power usage in them calls
for conservation and efficient energy program. Such a program require
monitoring of end-use appliances energy consumption in real-time. The worldwide
recent adoption of smart-meter in smart-grid, has led to the rise of
Non-Intrusive Load Monitoring (NILM); which enables estimation of
appliance-specific power consumption from building's aggregate power
consumption reading. NILM provides households with cost-effective real-time
monitoring of end-use appliances to help them understand their consumption
pattern and become part and parcel of energy conservation strategy. This paper
presents an up to date overview of NILM system and its associated methods and
techniques for energy disaggregation problem. This is followed by the review of
the state-of-the art NILM algorithms. Furthermore, we review several
performance metrics used by NILM researcher to evaluate NILM algorithms and
discuss existing benchmarking framework for direct comparison of the state of
the art NILM algorithms. Finally, the paper discuss potential NILM use-cases,
presents an overview of the public available dataset and highlight challenges
and future research directions.
"
1465,"PACO: A System-Level Abstraction for On-Loading Contextual Data to
  Mobile Devices","  Spatiotemporal context is crucial in modern mobile applications that utilize
increasing amounts of context to better predict events and user behaviors,
requiring rich records of users' or devices' spatiotemporal histories.
Maintaining these rich histories requires frequent sampling and indexed storage
of spatiotemporal data that pushes the limits of resource-constrained mobile
devices. Today's apps offload processing and storing contextual information,
but this increases response time, often relies on the user's data connection,
and runs the very real risk of revealing sensitive information. In this paper
we motivate the feasibility of on-loading large amounts of context and
introduce PACO (Programming Abstraction for Contextual On-loading), an
architecture for on-loading data that optimizes for location and time while
allowing flexibility in storing additional context. The PACO API's innovations
enable on-loading very dense traces of information, even given devices'
resource constraints. Using real-world traces and our implementation for
Android, we demonstrate that PACO can support expressive application queries
entirely on-device. Our quantitative evaluation assesses PACO's energy
consumption, execution time, and spatiotemporal query accuracy. Further, PACO
facilitates unified contextual reasoning across multiple applications and also
supports user-controlled release of contextual data to other devices or the
cloud; we demonstrate these assets through a proof-of-concept case study.
"
1466,RapidProM: Mine Your Processes and Not Just Your Data,"  The number of events recorded for operational processes is growing every
year. This applies to all domains: from health care and e-government to
production and maintenance. Event data are a valuable source of information for
organizations that need to meet requirements related to compliance, efficiency,
and customer service. Process mining helps to turn these data into real value:
by discovering the real processes, by automatically identifying bottlenecks, by
analyzing deviations and sources of non-compliance, by revealing the actual
behavior of people, etc. Process mining is very different from conventional
data mining and machine learning techniques. ProM is a powerful open-source
process mining tool supporting hundreds of analysis techniques. However, ProM
does not support analysis based on scientific workflows. RapidProM, an
extension of RapidMiner based on ProM, combines the best of both worlds.
Complex process mining workflows can be modeled and executed easily and
subsequently reused for other data sets. Moreover, using RapidProM, one can
benefit from combinations of process mining with other types of analysis
available through the RapidMiner marketplace.
"
1467,"Algorithm/Architecture Co-design of Proportionate-type LMS Adaptive
  Filters for Sparse System Identification","  This paper investigates the problem of implementing proportionate-type LMS
family of algorithms in hardware for sparse adaptive filtering applications
especially the network echo cancelation. We derive a re-formulated
proportionate type algorithm through algorithm-architecture co-design
methodology that can be pipelined and has an efficient architecture for
hardware implementation. We study the convergence, steady-state and tracking
performances of these re-formulated algorithms for white, color and speech
inputs before implementing them in hardware. To the best of our knowledge this
is the first attempt to implement proportionate-type algorithms in hardware. We
show that Delayed $\mu$-law Proportionate LMS (DMPLMS) algorithm for white
input and Delayed Wavelet MPLMS (DWMPLMS) for colored input are the robust VLSI
solutions for network echo cancellation where the sparsity of the echo paths
can vary with time. We implemented all the designs considering $16$-bit fixed
point representation in hardware, synthesized the designs and synthesis results
show that DMPLMS algorithm with $\approx25\%$ increase in hardware over
conventional DLMS architecture, achieves $3X$ improvement in convergence rate
for white input and DWMPLMS algorithm with $\approx58\%$ increase in hardware
achieves $15X$ improvement in convergence rate for correlated input conditions.
"
1468,Wireless Health Monitoring using Passive WiFi Sensing,"  This paper presents a two-dimensional phase extraction system using passive
WiFi sensing to monitor three basic elderly care activities including breathing
rate, essential tremor and falls. Specifically, a WiFi signal is acquired
through two channels where the first channel is the reference one, whereas the
other signal is acquired by a passive receiver after reflection from the human
target. Using signal processing of cross-ambiguity function, various features
in the signal are extracted. The entire implementations are performed using
software defined radios having directional antennas. We report the accuracy of
our system in different conditions and environments and show that breathing
rate can be measured with an accuracy of 87% when there are no obstacles. We
also show a 98% accuracy in detecting falls and 93% accuracy in classifying
tremor. The results indicate that passive WiFi systems show great promise in
replacing typical invasive health devices as standard tools for health care.
"
1469,"Visual-Based Analysis of Classification Measures with Applications to
  Imbalanced Data","  With a plethora of available classification performance measures, choosing
the right metric for the right task requires careful thought. To make this
decision in an informed manner, one should study and compare general properties
of candidate measures. However, analysing measures with respect to complete
ranges of their domain values is a difficult and challenging task. In this
study, we attempt to support such analyses with a specialized visualization
technique, which operates in a barycentric coordinate system using a 3D
tetrahedron. Additionally, we adapt this technique to the context of imbalanced
data and put forward a set of properties which should be taken into account
when selecting a classification performance measure. As a result, we compare 22
popular measures and show important differences in their behaviour. Moreover,
for parametric measures such as the F$_{\beta}$ and IBA$_\alpha$(G-mean), we
analytically derive parameter thresholds that change measure properties.
Finally, we provide an online visualization tool that can aid the analysis of
complete domain ranges of performance measures.
"
1470,RootJS: Node.js Bindings for ROOT 6,"  We present rootJS, an interface making it possible to seamlessly integrate
ROOT 6 into applications written for Node.js, the JavaScript runtime platform
increasingly commonly used to create high-performance Web applications. ROOT
features can be called both directly from Node.js code and by JIT-compiling C++
macros. All rootJS methods are invoked asynchronously and support callback
functions, allowing non-blocking operation of Node.js applications using them.
Last but not least, our bindings have been designed to platform-independent and
should therefore work on all systems supporting both ROOT 6 and Node.js.
  Thanks to rootJS it is now possible to create ROOT-aware Web applications
taking full advantage of the high performance and extensive capabilities of
Node.js. Examples include platforms for the quality assurance of acquired,
reconstructed or simulated data, book-keeping and e-log systems, and even Web
browser-based data visualisation and analysis.
"
1471,"Participating in a Computer Science Linked-courses Learning Community
  Reduces Isolation","  In our previous work we reported on a linked-courses learning community for
underrepresented groups in computer science, finding differences in attitudes
and resource utilization between students in the community and other
programming students. Here we present the first statistically significant
differences in pre- to post-quarter student attitudes between those in the
learning community and others taking equivalent programming classes. We find
that students in the learning community are less likely to feel isolated
post-quarter than other programming students. We also present results showing
differences in resource utilization by learning-community participants.
"
1472,"A sub-mW IoT-endnode for always-on visual monitoring and smart
  triggering","  This work presents a fully-programmable Internet of Things (IoT) visual
sensing node that targets sub-mW power consumption in always-on monitoring
scenarios. The system features a spatial-contrast $128\mathrm{x}64$ binary
pixel imager with focal-plane processing. The sensor, when working at its
lowest power mode ($10\mu W$ at 10 fps), provides as output the number of
changed pixels. Based on this information, a dedicated camera interface,
implemented on a low-power FPGA, wakes up an ultra-low-power parallel
processing unit to extract context-aware visual information. We evaluate the
smart sensor on three always-on visual triggering application scenarios.
Triggering accuracy comparable to RGB image sensors is achieved at nominal
lighting conditions, while consuming an average power between $193\mu W$ and
$277\mu W$, depending on context activity. The digital sub-system is extremely
flexible, thanks to a fully-programmable digital signal processing engine, but
still achieves 19x lower power consumption compared to MCU-based cameras with
significantly lower on-board computing capabilities.
"
1473,Quantum Mechanical Approach to Modelling Reliability of Sensor Reports,"  Dempster-Shafer evidence theory is wildly applied in multi-sensor data
fusion. However, lots of uncertainty and interference exist in practical
situation, especially in the battle field. It is still an open issue to model
the reliability of sensor reports. Many methods are proposed based on the
relationship among collected data. In this letter, we proposed a quantum
mechanical approach to evaluate the reliability of sensor reports, which is
based on the properties of a sensor itself. The proposed method is used to
modify the combining of evidences.
"
1474,Reconstruction of Missing Big Sensor Data,"  With ubiquitous sensors continuously monitoring and collecting large amounts
of information, there is no doubt that this is an era of big data. One of the
important sources for scientific big data is the datasets collected by Internet
of things (IoT). It's considered that these datesets contain highly useful and
valuable information. For an IoT application to analyze big sensor data, it is
necessary that the data are clean and lossless. However, due to unreliable
wireless link or hardware failure in the nodes, data loss in IoT is very
common. To reconstruct the missing big sensor data, firstly, we propose an
algorithm based on matrix rank-minimization method. Then, we consider IoT with
multiple types of sensor in each node. Accounting for possible correlations
among multiple-attribute sensor data, we propose tensor-based methods to
estimate missing values. Moreover, effective solutions are proposed using the
alternating direction method of multipliers. Finally, we evaluate the
approaches using two real sensor datasets with two missing data-patterns, i.e.,
random missing pattern and consecutive missing pattern. The experiments with
real-world sensor data show the effectiveness of the proposed methods.
"
1475,Graph Partitioning using Quantum Annealing on the D-Wave System,"  In this work, we explore graph partitioning (GP) using quantum annealing on
the D-Wave 2X machine. Motivated by a recently proposed graph-based electronic
structure theory applied to quantum molecular dynamics (QMD) simulations, graph
partitioning is used for reducing the calculation of the density matrix into
smaller subsystems rendering the calculation more computationally efficient.
Unconstrained graph partitioning as community clustering based on the
modularity metric can be naturally mapped into the Hamiltonian of the quantum
annealer. On the other hand, when constraints are imposed for partitioning into
equal parts and minimizing the number of cut edges between parts, a quadratic
unconstrained binary optimization (QUBO) reformulation is required. This
reformulation may employ the graph complement to fit the problem in the Chimera
graph of the quantum annealer. Partitioning into 2 parts, 2^N parts
recursively, and k parts concurrently are demonstrated with benchmark graphs,
random graphs, and small material system density matrix based graphs. Results
for graph partitioning using quantum and hybrid classical-quantum approaches
are shown to equal or out-perform current ""state of the art"" methods.
"
1476,"Increasing the Discovery Power and Confidence Levels of Disease
  Association Studies: A Survey","  The majority of common diseases are influenced by multiple genetic and
environmental factors such as Cancer. Even though uncovering the main causes of
disease is deemed difficult due to the complexity of gene-gene and
gene-environment interactions, major research efforts aim at identifying
disease risk factors, especially genetic ones. Over the past decade, disease
association studies have been used to uncover the susceptibility, aetiology and
mechanisms of action pertaining to common diseases. In disease association
studies, genetic data is analyzed in order to reveal the relationship between
different types of variants, and a disease of interest. The ultimate goal of
association studies is to facilitate susceptibility testing for disease
prediction, early diagnosis and enhanced prognosis . Susceptibility testing and
disease prediction are particularly important for diseases that can be
prevented by diet, drugs or change in lifestyle. The discovered associations
assist in understanding the molecular mechanisms influenced by the reported
variants, and in identifying important risk factors. Current association
studies suffer from several shortcomings. This report surveys the literature
that addresses the shortcomings of current methods the identify genetic disease
associations. In addition, it reviews the suggested solutions that either
enhance some aspect of the methodologies, or complement them.
"
1477,Hardware Automated Dataflow Deployment of CNNs,"  Deep Convolutional Neural Networks (CNNs) are the state of the art systems
for image classification and scene understating. However, such techniques are
computationally intensive and involve highly regular parallel computation. CNNs
can thus benefit from a significant acceleration in execution time when running
on fine grain programmable logic devices. As a consequence, several studies
have proposed FPGA-based accelerators for CNNs. However, because of the huge
amount of the required hardware resources, none of these studies directly was
based on a direct mapping of the CNN computing elements onto the FPGA physical
resources. In this work, we demonstrate the feasibility of this so-called
direct hardware mapping approach and discuss several associated implementation
issues. As a proof of concept, we introduce the haddoc2 open source tool, that
is able to automatically transform a CNN description into a platform
independent hardware description for FPGA implementation.
"
1478,Cloud-based Fault Detection and Classification for Oil & Gas Industry,"  Oil & Gas industry relies on automated, mission-critical equipment and
complex systems built upon their interaction and cooperation. To assure
continuous operation and avoid any supervision, architects embed Distributed
Control Systems (DCS), a.k.a. Supervisory Control and Data Acquisition (SCADA)
systems, on top of their equipment to generate data, monitor state and make
critical online & offline decisions.
  In this paper, we propose a new Lambda architecture for oil & gas industry
for unified data and analytical processing on data received from DCS, discuss
cloud integration issues and share our experiences with the implementation of
sensor fault-detection and classification modules inside the proposed
architecture.
"
1479,"Zampa's systems theory: a comprehensive theory of measurement in dynamic
  systems","  The article outlines in memoriam Prof. Pavel Zampa's concepts of system
theory which enable to devise a measurement in dynamic systems independently of
the particular system behaviour. From the point of view of Zampa's theory,
terms like system time, system attributes, system link, system element, input,
output, subsystems, and state variables are defined. In Conclusions, Zampa's
theory is discussed together with another mathematical approaches of
qualitative dynamics known since the 19th century. In Appendices, we present
applications of Zampa's technical approach to measurement of complex dynamical
(chemical and biological) systems at the Institute of Complex Systems,
University of South Bohemia in Ceske Budejovice.
"
1480,A Proposed Architecture for Big Data Driven Supply Chain Analytics,"  Advancement in information and communication technology (ICT) has given rise
to explosion of data in every field of operations. Working with the enormous
volume of data (or Big Data, as it is popularly known as) for extraction of
useful information to support decision making is one of the sources of
competitive advantage for organizations today. Enterprises are leveraging the
power of analytics in formulating business strategy in every facet of their
operations to mitigate business risk. Volatile global market scenario has
compelled the organizations to redefine their supply chain management (SCM). In
this paper, we have delineated the relevance of Big Data and its importance in
managing end to end supply chains for achieving business excellence. A Big
Data-centric architecture for SCM has been proposed that exploits the current
state of the art technology of data management, analytics and visualization.
The security and privacy requirements of a Big Data system have also been
highlighted and several mechanisms have been discussed to implement these
features in a real world Big Data system deployment in the context of SCM. Some
future scope of work has also been pointed out. Keyword: Big Data, Analytics,
Cloud, Architecture, Protocols, Supply Chain Management, Security, Privacy.
"
1481,"Statistical Timing Analysis for Latch-Controlled Circuits with Reduced
  Iterations and Graph Transformations","  Level-sensitive latches are widely used in high- performance designs. For
such circuits efficient statistical timing analysis algorithms are needed to
take increasing process vari- ations into account. But existing methods solving
this problem are still computationally expensive and can only provide the yield
at a given clock period. In this paper we propose a method combining reduced
iterations and graph transformations. The reduced iterations extract setup time
constraints and identify a subgraph for the following graph transformations
handling the constraints from nonpositive loops. The combined algorithms are
very efficient, more than 10 times faster than other existing methods, and
result in a parametric minimum clock period, which together with the hold time
constraints can be used to compute the yield at any given clock period very
easily.
"
1482,Annotation of Car Trajectories based on Driving Patterns,"  Nowadays, the ubiquity of various sensors enables the collection of
voluminous datasets of car trajectories. Such datasets enable analysts to make
sense of driving patterns and behaviors: in order to understand the behavior of
drivers, one approach is to break a trajectory into its underlying patterns and
then analyze that trajectory in terms of derived patterns. The process of
trajectory segmentation is a function of various resources including a set of
ground truth trajectories with their driving patterns. To the best of our
knowledge, no such ground-truth dataset exists in the literature. In this
paper, we describe a trajectory annotation framework and report our results to
annotate a dataset of personal car trajectories. Our annotation methodology
consists of a crowd-sourcing task followed by a precise process of aggregation.
Our annotation process consists of two granularity levels, one to specify the
annotation (segment border) and the other one to describe the type of the
segment (e.g. speed-up, turn, merge, etc.). The output of our project, Dataset
of Annotated Car Trajectories (DACT), is available online at
https://figshare.com/articles/dact_dataset_of_annotated_car_trajectories/5005289 .
"
1483,Discrete Event Simulation of Personal Rapid Transit (PRT) Systems,"  The article discusses issues related to the construction of the PRT network
simulator and the simulation process: the elements of PRT network structure,
their representation in the simulator, the simulation process itself,
animation, and automation of the experiments. An example of a simulation
environment Feniks is described, elaborated within the framework of the
Eco-Mobility project.
"
1484,"RAE: The Rainforest Automation Energy Dataset for Smart Grid Meter Data
  Analysis","  Datasets are important for researchers to build models and test how well
their machine learning algorithms perform. This paper presents the Rainforest
Automation Energy (RAE) dataset to help smart grid researchers test their
algorithms which make use of smart meter data. This initial release of RAE
contains 1Hz data (mains and sub-meters) from two a residential house. In
addition to power data, environmental and sensor data from the house's
thermostat is included. Sub-meter data from one of the houses includes heat
pump and rental suite captures which is of interest to power utilities. We also
show and energy breakdown of each house and show (by example) how RAE can be
used to test non-intrusive load monitoring (NILM) algorithms.
"
1485,"An Overview of Data Mining Applications in Oil and Gas Exploration:
  Structural Geology and Reservoir Property-Issues","  Low oil prices have motivated energy executives to look into cost reduction
in their supply chains more seriously. To this end, a new technology that is
experimentally considered in hydrocarbon exploration is data mining. There are
two major categories of geoscientific problems in which data mining is applied:
structural geology and reservoir property-issues. This research overviews these
categories by considering a variety of interesting works in each of them. The
result is an understanding of the specific geoscientific problems studied in
the literature, along with the relative data mining methods. This way, this
work tries to lay the ground for a mutual understanding on oil and gas
exploration between the data miners and the geoscientists.
"
1486,"A data-driven workflow for predicting horizontal well production using
  vertical well logs","  In recent work, data-driven sweet spotting technique for shale plays
previously explored with vertical wells has been proposed. Here, we extend this
technique to multiple formations and formalize a general data-driven workflow
to facilitate feature extraction from vertical well logs and predictive
modeling of horizontal well production. We also develop an experimental
framework that facilitates model selection and validation in a realistic
drilling scenario. We present some experimental results using this methodology
in a field with 90 vertical wells and 98 horizontal wells, showing that it can
achieve better results in terms of predictive ability than kriging of known
production values.
"
1487,Optimal placement of mix zones in road networks,"  The road networks, vehicle users could enjoy numerous kind of services such
as location based service in vehicle users can connected to Internet and
communication of different users. Therefore, in order to acquire adequate
privacy level and quality of service, one must have to wisely place mix zones
to connect vehicle users to internet or some other internetwork. According to
this research, we have analyzed the problem of optimal placement mix zones over
road network. To enhance the coverage capacity of vehicles, in order to reduce
the cost and communication delay. Further, it has also been discovered to
minimize the cost of mix zone placement. Moreover, it has also been shown that,
as the best deployment mix zones get minimized cost while at the same time the
average capacity of mix zone can be maximized also privacy level increased
because of optimal placement and high traffic environment.
"
1488,"The Role of Data Analysis in the Development of Intelligent Energy
  Networks","  Data analysis plays an important role in the development of intelligent
energy networks (IENs). This article reviews and discusses the application of
data analysis methods for energy big data. The installation of smart energy
meters has provided a huge volume of data at different time resolutions,
suggesting data analysis is required for clustering, demand forecasting, energy
generation optimization, energy pricing, monitoring and diagnostics. The
currently adopted data analysis technologies for IENs include pattern
recognition, machine learning, data mining, statistics methods, etc. However,
existing methods for data analysis cannot fully meet the requirements for
processing the big data produced by the IENs and, therefore, more comprehensive
data analysis methods are needed to handle the increasing amount of data and to
mine more valuable information.
"
1489,"Reservoir Computing for Detection of Steady State in Performance Tests
  of Compressors","  Fabrication of devices in industrial plants often includes undergoing quality
assurance tests or tests that seek to determine some attributes or capacities
of the device. For instance, in testing refrigeration compressors, we want to
find the true refrigeration capacity of the compressor being tested. Such test
(also called an episode) may take up to four hours, being an actual hindrance
to applying it to the total number of compressors produced. This work seeks to
reduce the time spent on such industrial trials by employing Recurrent Neural
Networks (RNNs) as dynamical models for detecting when a test is entering the
so-called steady-state region. Specifically, we use Reservoir Computing (RC)
networks which simplify the learning of RNNs by speeding up training time and
showing convergence to a global optimum. Also, this work proposes a
self-organized subspace projection method for RC networks which uses
information from the beginning of the episode to define a cluster to which the
episode belongs to. This assigned cluster defines a particular binary input
that shifts the operating point of the reservoir to a subspace of trajectories
for the duration of the episode. This new method is shown to turn the RC model
robust in performance with respect to varying combination of reservoir
parameters, such as spectral radius and leak rate, when compared to a standard
RC network.
"
1490,"JetsonLEAP: a Framework to Measure Power on a Heterogeneous
  System-on-a-Chip Device","  Computer science marches towards energy-aware practices. This trend impacts
not only the design of computer architectures, but also the design of programs.
However, developers still lack affordable and accurate technology to measure
energy consumption in computing systems. The goal of this paper is to mitigate
such problem. To this end, we introduce JetsonLEAP, a framework that supports
the implementation of energy-aware programs. JetsonLEAP consists of an embedded
hardware, in our case, the Nvidia Tegra TK1 System-on-a-chip device, a circuit
to control the flow of energy, of our own design, plus a library to instrument
program parts. We discuss two different circuit setups. The most precise setup
lets us reliably measure the energy spent by 225,000 instructions, the least
precise, although more affordable setup, gives us a window of 975,000
instructions. To probe the precision of our system, we use it in tandem with a
high-precision, high-cost acquisition system, and show that results do not
differ in any significant way from those that we get using our simpler
apparatus. Our entire infrastructure - board, power meter and both circuits -
can be reproduced with about $500.00. To demonstrate the efficacy of our
framework, we have used it to measure the energy consumed by programs running
on ARM cores, on the GPU, and on a remote server. Furthermore, we have studied
the impact of OpenACC directives on the energy efficiency of high-performance
applications.
"
1491,Computational Anatomy in Theano,"  To model deformation of anatomical shapes, non-linear statistics are required
to take into account the non-linear structure of the data space. Computer
implementations of non-linear statistics and differential geometry algorithms
often lead to long and complex code sequences. The aim of the paper is to show
how the Theano framework can be used for simple and concise implementation of
complex differential geometry algorithms while being able to handle complex and
high-dimensional data structures. We show how the Theano framework meets both
of these requirements. The framework provides a symbolic language that allows
mathematical equations to be directly translated into Theano code, and it is
able to perform both fast CPU and GPU computations on high-dimensional data. We
show how different concepts from non-linear statistics and differential
geometry can be implemented in Theano, and give examples of the implemented
theory visualized on landmark representations of Corpus Callosum shapes.
"
1492,Smart Asset Management for Electric Utilities: Big Data and Future,"  This paper discusses about future challenges in terms of big data and new
technologies. Utilities have been collecting data in large amounts but they are
hardly utilized because they are huge in amount and also there is uncertainty
associated with it. Condition monitoring of assets collects large amounts of
data during daily operations. The question arises ""How to extract information
from large chunk of data?"" The concept of ""rich data and poor information"" is
being challenged by big data analytics with advent of machine learning
techniques. Along with technological advancements like Internet of Things
(IoT), big data analytics will play an important role for electric utilities.
In this paper, challenges are answered by pathways and guidelines to make the
current asset management practices smarter for the future.
"
1493,Control Flow Information Analysis in Process Model Matching Techniques,"  Online Appendix to: ""Analyzing Control Flow Information to Improve the
Effectiveness of Process Model Matching Techniques"" by the same authors.
"
1494,An Online Development Environment for Answer Set Programming,"  Recent progress in logic programming (e.g., the development of the Answer Set
Programming paradigm) has made it possible to teach it to general undergraduate
and even high school students. Given the limited exposure of these students to
computer science, the complexity of downloading, installing and using tools for
writing logic programs could be a major barrier for logic programming to reach
a much wider audience. We developed an online answer set programming
environment with a self contained file system and a simple interface, allowing
users to write logic programs and perform several tasks over the programs.
"
1495,"Project Makespan Estimation: Computational Load of Interval and Point
  Estimates","  The estimation of project completion time is to be repeated several times in
the project planning phase to reach the optimal tradeoff between time, cost,
and quality. Estimation procedures provide either an interval or a point
estimate. The computational load of several estimation procedures is reviewed.
A multiple polynomial regression model is provided for major interval
estimation procedures and shows that the accuracy in the probability model for
activities is the most influential factor. The computational time does not
appear to be an impeding factor, though it is larger for MonteCarlo simulation,
so that the computational time can be traded off in search of a simpler
estimation procedure.
"
1496,Skin Temperature Measurement,"  This report represents the design and implementation of a skin temperature
measurement system. The system aims to measure the skin temperature from a
sensor and send it to the PC using a USB cable to display on screen. The data
needs to be updated every second. The PIC18F4550 microcontroller has been used
in this project to obtain data from the sensor and send it to the PC using USB
2.0 that has been built into the microcontroller. The microcontroller has a
10-bit Analog Digital Converting accuracy that is one of the important criteria
for this design as it is going to be used for medical purposes. As the project
concentrates more on designing software than hardware, the EasyPIC4 development
board was used which comes with all hardware required for this project. The
Jackson diagram method was used to design and implement the coding program for
the microcontroller software part of the system. The MikroC IDE has been used
to compile and load the program into PIC18F4550 microcontroller. The program
for the microcontroller uses C language that aims to keep the USB link alive by
using interrupt function. A sensor collects data from sensor as 4 bits and send
it to the PC every second using a USB cable. The data received from sensor,
will be sent by microcontroller to the PC. The Visual Basic software was used
in the PC side of device to catch and output the data on the screen. A template
file for the Visual Basic program was generated by Easy HID wizard to make
software programming part easier for designer. The USBTrace analyzer has been
used in the scenario any problems occur during or after the design and
construction of the software. The software enables a user to monitor the data
on the USB bus that sends the data to the PC from microcontroller.
"
1497,Duty to Delete on Non-Volatile Memory,"  We firstly suggest new cache policy applying the duty to delete invalid cache
data on Non-volatile Memory (NVM). This cache policy includes generating random
data and overwriting the random data into invalid cache data. Proposed cache
policy is more economical and effective regarding perfect deletion of data. It
is ensure that the invalid cache data in NVM is secure against malicious
hackers.
"
1498,Ethics of autonomous information systems towards an artificial thinking,"  Many projects relies on cognitives sciences, neurosciences, computer sciences
and robotics. They concerned today the building of autonomous artificial beings
able to think. This paper shows a model to compare the human thinking with an
hypothetic numerical way of thinking based on four hierarchies : the
information system classification, the cognitive pyramid, the linguistic
pyramid and the digital information hierarchy. After a state of art on the
nature of human thinking, feasibility of autonomous multi-agent systems
provided with artificial consciousness which are able to think is discussed.
The ethical aspects and consequences for humanity of such systems is evaluated.
These systems lead the scientific community to react.
"
1499,The Limits to Machine Consciousness,"  It is generally accepted that machines can replicate cognitive tasks
performed by conscious agents as long as they are not based on the capacity of
awareness. We consider several views on the nature of subjective awareness,
which is fundamental for self-reflection and review, and present reasons why
this property is not computable. We argue that consciousness is more than an
epiphenomenon and assuming it to be a separate category is consistent with both
quantum mechanics and cognitive science. We speak of two kinds of
consciousness, little-C and big-C, and discuss the significance of this
classification in analyzing the current academic debates in the field. The
interaction between the system and the measuring apparatus of the experimenter
is examined both from the perspectives of decoherence and the quantum Zeno
effect. These ideas are used as context to address the question of limits to
machine consciousness.
"
1500,"Meaningless comparisons lead to false optimism in medical machine
  learning","  A new trend in medicine is the use of algorithms to analyze big datasets,
e.g. using everything your phone measures about you for diagnostics or
monitoring. However, these algorithms are commonly compared against weak
baselines, which may contribute to excessive optimism. To assess how well an
algorithm works, scientists typically ask how well its output correlates with
medically assigned scores. Here we perform a meta-analysis to quantify how the
literature evaluates their algorithms for monitoring mental wellbeing. We find
that the bulk of the literature ($\sim$77%) uses meaningless comparisons that
ignore patient baseline state. For example, having an algorithm that uses phone
data to diagnose mood disorders would be useful. However, it is possible to
over 80% of the variance of some mood measures in the population by simply
guessing that each patient has their own average mood - the patient-specific
baseline. Thus, an algorithm that just predicts that our mood is like it
usually is can explain the majority of variance, but is, obviously, entirely
useless. Comparing to the wrong (population) baseline has a massive effect on
the perceived quality of algorithms and produces baseless optimism in the
field. To solve this problem we propose ""user lift"" that reduces these
systematic errors in the evaluation of personalized medical monitoring.
"
1501,Modular AWG-based Optical Shuffle Network,"  This paper proposes an arrayed-waveguide grating (AWG) based
wavelength-division-multiplexing (WDM) shuffle network. Compared with previous
optical shuffle networks, our proposal is compact, easy to implement, highly
scalable, and cost effective.
"
1502,"Z-checker: A Framework for Assessing Lossy Compression of Scientific
  Data","  Because of vast volume of data being produced by today's scientific
simulations and experiments, lossy data compressor allowing user-controlled
loss of accuracy during the compression is a relevant solution for
significantly reducing the data size. However, lossy compressor developers and
users are missing a tool to explore the features of scientific datasets and
understand the data alteration after compression in a systematic and reliable
way. To address this gap, we have designed and implemented a generic framework
called Z-checker. On the one hand, Z-checker combines a battery of data
analysis components for data compression. On the other hand, Z-checker is
implemented as an open-source community tool to which users and developers can
contribute and add new analysis components based on their additional analysis
demands. In this paper, we present a survey of existing lossy compressors. Then
we describe the design framework of Z-checker, in which we integrated
evaluation metrics proposed in prior work as well as other analysis tools.
Specifically, for lossy compressor developers, Z-checker can be used to
characterize critical properties of any dataset to improve compression
strategies. For lossy compression users, Z-checker can detect the compression
quality, provide various global distortion analysis comparing the original data
with the decompressed data and statistical analysis of the compression error.
Z-checker can perform the analysis with either coarse granularity or fine
granularity, such that the users and developers can select the best-fit,
adaptive compressors for different parts of the dataset. Z-checker features a
visualization interface displaying all analysis results in addition to some
basic views of the datasets such as time series. To the best of our knowledge,
Z-checker is the first tool designed to assess lossy compression
comprehensively for scientific datasets.
"
1503,"Relat\'orio T\'ecnico: Controle Distribu\'ido de Tr\'afego Baseado em
  Ve\'iculos Conectados e Comunica\c{c}\~oes Veiculares Centradas em Interesses","  Although advanced traffic management systems can deal with the heterogeneous
traffic flows approaching of intersections, their performances are compromised,
when the traffic volume is not distributed uniformly. To evenly distribute the
traffic flow, an advanced driver information system should be aware of the
traffic control operations. However, such requirement can not ultimately be
satisfied due to the gaps in state of the art in advanced traffic management
systems. Therefore, this study proposes a distributed traffic control system,
in which agents embedded in connected vehicles, traffic signals, urban elements
and a traffic control center interact with each other to provide a greater
traffic fluidity. Therefore, the agents depend strongly on a heterogeneous
vehicular network. In this sense, this study also proposes a heterogeneous
vehicular network whose communication protocol can satisfy the communication
requirements of intelligent transportation systems service applications.
According to the results obtained from simulations, the distributed traffic
control system was able to maximize the flow of vehicles and the mean speed of
the vehicles, and minimize the wait time, travel time, fuel consume and
emissions (CO, CO$_2$, HC, NOx and PMx).
"
1504,"Implementation of the Logistic Map with FPGA using 32 bits fixed point
  standard","  This article presents a design of the logistic map by means of FPGA (Field
Programmable Gate Ar-ray) under fixed-point standard and 32-bits of precision.
The design was carried out with Altera Quartus platform. The hardware
description language VHDL-93 has been adopted and the results were simulated by
means of Altera ModelSim package. The main of the project was to produce a
cha-otic system with a low energy and time cost. Using the VHDL, it was
possible to use only 1439 logical gates from 114480 available. The Lyapunov
exponent has been calculated with good agreement with literature reference,
which shows the effectiveness the proposed method.
"
1505,"Enhanced power grid evaluation through efficient stochastic model-based
  analysis","  Electrical infrastructures provide services at the basis of a number of
application sectors, several of which are critical from the perspective of
human life, environment or financials. Following the increasing trend in
electricity generation from renewable sources, pushed by the need to meet
sustainable energy goals in many countries, more sophisticated control
strategies are being adopted to regulate the operation of the electric power
system, driving electrical infrastructures towards the so called Smart Grid
scenario. It is therefore paramount to be assisted by technologies able to
analyze the Smart Grid behavior in critical scenarios, e.g. where cyber
malfunctions or grid disruptions occur. In this context, stochastic model-based
analysis are well suited to assess dependability and quality of service related
indicators, and continuous improvements in modeling strategies and system
models design are required. Thus, my PhD work addresses this topic by
contributing to study new Smart Grid scenarios, concerning the advanced
interplay between ICT and electrical infrastructures in presence of cyber
faults/attacks, define a new modeling approach, based on modularity and
composition, and start to study how to improve the electrical grid dynamics
representation. In this article these studies are briefly presented and
discussed.
"
1506,"Optimizing Google Shopping Campaigns Structures With Query-Level
  Matching","  How to bid on a Google shopping account (set of shopping campaigns) with
query-level matching like in Google Adwords.
"
1507,Neville's algorithm revisited,"  Neville's algorithm is known to provide an efficient and numerically stable
solution for polynomial interpolations. In this paper, an extension of this
algorithm is presented which includes the derivatives of the interpolating
polynomial.
"
1508,"SigViewer: Visualizing Multimodal Signals Stored in XDF (Extensible Data
  Format) Files","  Multimodal biosignal acquisition is facilitated by recently introduced
software solutions such as LabStreaming Layer (LSL) and its associated data
format XDF (Extensible Data Format). However, there are no stand-alone
applications that can visualize multimodal time series stored in XDF files. We
extended SigViewer, an open source cross-platform Qt C++ application with the
capability of loading, resampling, annotating, and visualizing signals stored
in XDF files and successfully applied the tool for post-hoc visual verification
of the accuracy of a system that aims to predict the phase of alpha
oscillations within the electroencephalogram in real-time.
"
1509,"A Method with Feedback for Aggregation of Group Incomplete Pair-Wise
  Comparisons","  A method for aggregation of expert estimates in small groups is proposed. The
method is based on combinatorial approach to decomposition of pair-wise
comparison matrices and to processing of expert data. It also uses the basic
principles of Analytic Hierarchy/Network Process approaches, such as building
of criteria hierarchy to decompose and describe the problem, and evaluation of
objects by means of pair-wise comparisons. It allows to derive priorities based
on group incomplete pair-wise comparisons and to organize feedback with experts
in order to achieve sufficient agreement of their estimates. Double entropy
inter-rater index is suggested for usage as agreement measure. Every expert is
given an opportunity to use the scale, in which the degree of detail (number of
points/grades) most adequately reflects this expert's competence in the issue
under consideration, for every single pair comparison. The method takes all
conceptual levels of individual expert competence (subject domain, specific
problem, individual pair-wise comparison matrix, separate pair-wise comparison)
into consideration. The method is intended to be used in the process of
strategic planning in weakly-structured subject domains.
"
1510,"D3NOC: Dynamic Data-Driven Network On Chip in Photonic Electronic
  Hybrids","  In this paper, we present a reconfigurable hybrid Photonic-Plasmonic
Network-on-Chip (NoC) based on the Dynamic Data Driven Application System
(DDDAS) paradigm. In DDDAS computations and measurements form a dynamic closed
feedback loop in which they tune one another in response to changes in the
environment. Our proposed system enables dynamic augmentation of a base
electrical mesh topology with an optical express bus during the run-time. In
addition, the measurement process itself adjusts to the environment. In order
to achieve lower latencies, lower dynamic power, and higher throughput, we take
advantage of a Configurable Hybrid Photonic Plasmonic Interconnect (CHyPPI) for
our reconfigurable connections. We evaluate the performance and power of our
system against kernels from NAS Parallel Benchmark (NPB) in addition to some
synthetically generated traffic. In comparison to a 16x16 base electrical mesh,
D3NOC shows up to 89% latency and 67% dynamic power net improvements beyond
overhead-corrected performance. It should be noted that the design-space of NoC
reconfiguration is vast and the goal of this study is not design-space
exploration. Our goal is to show the potentials of adaptive dynamic
measurements when coupled with other reconfiguration techniques in the NoC
context.
"
1511,"Computational prediction and analysis of protein-protein interaction
  networks","  Biological networks provide insight into the complex organization of
biological processes in a cell at the system level. They are an effective tool
for understanding the comprehensive map of functional interactions, finding the
functional modules and pathways. Reconstruction and comparative analysis of
these networks provide useful information to identify functional modules,
prioritization of disease causing genes and also identification of drug
targets. The talk will consist of two parts. I will discuss several methods for
protein-protein interaction network alignment and investigate their preferences
to other existing methods. Further, I briefly talk about reconstruction of
protein-protein interaction networks by using deep learning.
"
1512,A HelloWord \textsc{Bib}\negthinspace\TeX~stile file .\textbf{bst},"  A HelloWord \textsc{Bib}\negthinspace\TeX~stile file .\textbf{bst} is
described
"
1513,From Logic to Biology via Physics: a survey,"  This short text summarizes the work in biology proposed in our book,
Perspectives on Organisms, where we analyse the unity proper to organisms by
looking at it from different viewpoints. We discuss the theoretical roles of
biological time, complexity, theoretical symmetries, singularities and critical
transitions. We explicitly borrow from the conclusions in some key chapters and
introduce them by a reflection on ""incompleteness"", also proposed in the book.
We consider that incompleteness is a fundamental notion to understand the way
in which we construct knowledge. Then we will introduce an approach to
biological dynamics where randomness is central to the theoretical
determination: randomness does not oppose biological stability but contributes
to it by variability, adaptation, and diversity. Then, evolutionary and
ontogenetic trajectories are continual changes of coherence structures
involving symmetry changes within an ever-changing global stability.
"
1514,TikZ-network manual,"  TikZ-network is an open source software project for visualizing graphs and
networks in LaTeX. It aims to provide a simple and easy tool to create,
visualize and modify complex networks. The packaged is based on the PGF/TikZ
languages for producing vector graphics from a geometric/algebraic description.
Particular focus is made on the software usability and interoperability with
other tools. Simple networks can be directly created within LaTeX, while more
complex networks can be imported from external sources (e.g. igraph, networkx,
QGIS, ...). Additionally, tikz-network supports visualization of multilayer
networks in two and three dimensions. The software is available at:
https://github.com/hackl/tikz-network.
"
1515,Simple Signal Extension Method for Discrete Wavelet Transform,"  Discrete wavelet transform of finite-length signals must necessarily handle
the signal boundaries. The state-of-the-art approaches treat such boundaries in
a complicated and inflexible way, using special prolog or epilog phases. This
holds true in particular for images decomposed into a number of scales,
exemplary in JPEG 2000 coding system. In this paper, the state-of-the-art
approaches are extended to perform the treatment using a compact streaming
core, possibly in multi-scale fashion. We present the core focused on CDF 5/3
wavelet and the symmetric border extension method, both employed in the JPEG
2000. As a result of our work, every input sample is visited only once, while
the results are produced immediately, i.e. without buffering.
"
1516,"Stackable vs Autonomous Cars for Shared Mobility Systems: a Preliminary
  Performance Evaluation","  Car sharing is one of the key elements of a Mobility-on-Demand system, but it
still suffers from several shortcomings, the most significant of which is the
fleet unbalance during the day. What is typically observed in car sharing
systems, in fact, is a vehicle shortage in so-called hot spots (i.e., areas
with high demand) and vehicle accumulation in cold spots, due to the patterns
in people flows during the day. In this work, we overview the main approaches
to vehicle redistribution based on the type of vehicles the car sharing fleet
is composed of, and we evaluate their performance using a realistic car sharing
demand derived for a suburban area around Lyon, France. The main result of this
paper is that stackable vehicles can achieve a relocation performance close to
that of autonomous vehicles, significantly improving over the no-relocation
approach and over traditional relocation with standard cars.
"
1517,On Vague Computers,"  Vagueness is something everyone is familiar with. In fact, most people think
that vagueness is closely related to language and exists only there. However,
vagueness is a property of the physical world. Quantum computers harness
superposition and entanglement to perform their computational tasks. Both
superposition and entanglement are vague processes. Thus quantum computers,
which process exact data without ""exploiting"" vagueness, are actually vague
computers.
"
1518,"Technical Note: Towards Virtual Monitors for Image Guided Interventions
  - Real-time Streaming to Optical See-Through Head-Mounted Displays","  Purpose: Image guidance is crucial for the success of many interventions.
Images are displayed on designated monitors that cannot be positioned optimally
due to sterility and spatial constraints. This indirect visualization causes
potential occlusion, hinders hand-eye coordination, leads to increased
procedure duration and surgeon load. Methods: We propose a virtual monitor
system that displays medical images in a mixed reality visualization using
optical see-through head-mounted displays. The system streams high-resolution
medical images from any modality to the head-mounted display in real-time that
are blended with the surgical site. It allows for mixed reality visualization
of images in head-, world-, or body-anchored mode and can thus be adapted to
specific procedural needs. Results: For typical image sizes, the proposed
system exhibits an average end-to-end delay and refresh rate of 214 +- 30 ms
and 41:4 +- 32:0 Hz, respectively. Conclusions: The proposed virtual monitor
system is capable of real-time mixed reality visualization of medical images.
In future, we seek to conduct first pre-clinical studies to quantitatively
assess the impact of the system on standard image guided procedures.
"
1519,"Improving Compression Based Dissimilarity Measure for Music Score
  Analysis","  In this paper, we propose a way to improve the compression based
dissimilarity measure, CDM. We propose to use a modified value of the file
size, where the original CDM uses an unmodified file size. Our application is a
music score analysis. We have chosen piano pieces from five different
composers. We have selected 75 famous pieces (15 pieces for each composer). We
computed the distances among all pieces by using the modified CDM. We use the
K-nearest neighbor method when we estimate the composer of each piece of music.
The modified CDM shows improved accuracy. The difference is statistically
significant.
"
1520,Sensor Fusion for Public Space Utilization Monitoring in a Smart City,"  Public space utilization is crucial for urban developers to understand how
efficient a place is being occupied in order to improve existing or future
infrastructures. In a smart cities approach, implementing public space
monitoring with Internet-of-Things (IoT) sensors appear to be a viable
solution. However, choice of sensors often is a challenging problem and often
linked with scalability, coverage, energy consumption, accuracy, and privacy.
To get the most from low cost sensor with aforementioned design in mind, we
proposed data processing modules for capturing public space utilization with
Renewable Wireless Sensor Network (RWSN) platform using pyroelectric infrared
(PIR) and analog sound sensor. We first proposed a calibration process to
remove false alarm of PIR sensor due to the impact of weather and environment.
We then demonstrate how the sounds sensor can be processed to provide various
insight of a public space. Lastly, we fused both sensors and study a particular
public space utilization based on one month data to unveil its usage.
"
1521,"Beating the bookies with their own numbers - and how the online sports
  betting market is rigged","  The online sports gambling industry employs teams of data analysts to build
forecast models that turn the odds at sports games in their favour. While
several betting strategies have been proposed to beat bookmakers, from expert
prediction models and arbitrage strategies to odds bias exploitation, their
returns have been inconsistent and it remains to be shown that a betting
strategy can outperform the online sports betting market. We designed a
strategy to beat football bookmakers with their own numbers. Instead of
building a forecasting model to compete with bookmakers predictions, we
exploited the probability information implicit in the odds publicly available
in the marketplace to find bets with mispriced odds. Our strategy proved
profitable in a 10-year historical simulation using closing odds, a 6-month
historical simulation using minute to minute odds, and a 5-month period during
which we staked real money with the bookmakers (we made code, data and models
publicly available). Our results demonstrate that the football betting market
is inefficient - bookmakers can be consistently beaten across thousands of
games in both simulated environments and real-life betting. We provide a
detailed description of our betting experience to illustrate how the sports
gambling industry compensates these market inefficiencies with discriminatory
practices against successful clients.
"
1522,Hotspot-aware DSA Grouping and Mask Assignment,"  In Directed Self Assembly (DSA), poor printing of guiding templates can cause
misassembly resulting in high defect probability. Therefore, hotspots should be
avoided in the choice of the DSA groups. Accordingly, Directed Self-Assembly
(DSA) technologies which use Multiple Patterning (MP) to print the guiding
templates need to be aware of hotspots during the DSA grouping and MP
Decomposition. In this paper, we present a hotspot-aware heuristic for DSA
grouping and MP decomposition. Results show that that the proposed heuristic
eliminates 78% of the hotspots and conflicts that result from using a
hotspot-unaware grouping and decomposition algorithm. In comparison to the
optimal solution using Integer Linear Programming, the proposed heuristic
results in ~24% more violations.
"
1523,"An Extension of Deep Pathway Analysis: A Pathway Route Analysis
  Framework Incorporating Multi-dimensional Cancer Genomics Data","  Recent breakthroughs in cancer research have come via the up-and-coming field
of pathway analysis. By applying statistical methods to prior known gene and
protein regulatory information, pathway analysis provides a meaningful way to
interpret genomic data. While many gene/protein regulatory relationships have
been studied, never before has such a significant amount data been made
available in organized forms of gene/protein regulatory networks and pathways.
However, pathway analysis research is still in its infancy, especially when
applying it to solve practical problems.
  In this paper we propose a new method of studying biological pathways, one
that cross analyzes mutation information, transcriptome and proteomics data.
Using this outcome, we identify routes of aberrant pathways potentially
responsible for the etiology of disease. Each pathway route is encoded as a
bayesian network which is initialized with a sequence of conditional
probabilities specifically designed to encode directionality of regulatory
relationships encoded in the pathways. Far more complex interactions, such as
phosphorylation and methylation, among others, in the pathways can be modeled
using this approach. The effectiveness of our model is demonstrated through its
ability to distinguish real pathways from decoys on TCGA mRNA-seq, mutation,
Copy Number Variation and phosphorylation data for both Breast cancer and
Ovarian cancer study. The majority of pathways distinguished can be confirmed
by biological literature. Moreover, the proportion of correctly indentified
pathways is \% higher than previous work where only mRNA-seq mutation data is
incorporated for breast cancer patients. Consequently, such an in-depth pathway
analysis incorporating more diverse data can give rise to the accuracy of
perturbed pathway detection.
"
1524,The ALICE O2 common driver for the C-RORC and CRU read-out cards,"  ALICE (A Large Ion Collider Experiment) is the heavy-ion detector designed to
study the strongly interacting state of matter realized in relativistic
heavy-ion collisions at the CERN Large Hadron Collider (LHC). A major upgrade
of the experiment is planned during the 2019-2020 long shutdown. In order to
cope with a data rate 100 times higher than during LHC Run 1 and with the
continuous read-out of the Time Projection Chamber (TPC), it is necessary to
upgrade the Online and Offline Computing to a new common system called O2 . The
O2 read- out chain will use commodity x86 Linux servers equipped with custom
PCIe FPGA-based read- out cards. This paper discusses the driver architecture
for the cards that will be used in O2 : the PCIe v2 x8, Xilinx Virtex 6 based
C-RORC (Common Readout Receiver Card) and the PCIe v3 x16, Intel Arria 10 based
CRU (Common Readout Unit). Access to the PCIe cards is provided via three
layers of software. Firstly, the low-level PCIe (PCI Express) layer responsible
for the userspace interface for low-level operations such as memory mapping the
PCIe BAR (Base Address Registers) and creating scatter-gather lists, which is
provided by the PDA (Portable Driver Architecture) library developed by the
Frankfurt Institute for Advanced Studies (FIAS). Above that sits our userspace
driver which implements synchronization, controls the read-out card -- e.g.
resetting and configuring the card, providing it with bus addresses to transfer
data to and checking for data arrival -- and presents a uniform, high-level C++
interface that abstracts over the differences between the C-RORC and CRU. This
interface -- of which direct usage is principally intended for high-performance
read-out processes -- allows users to configure and use the various aspects of
the read-out cards, such as configuration, DMA transfers and commands to the
front-end. [...]
"
1525,Adapting Engineering Education to Industrie 4.0 Vision,"  Industrie 4.0 is originally a future vision described in the high-tech
strategy of the German government that is conceived upon the information and
communication technologies like Cyber-Physical Systems, Internet of Things,
Physical Internet and Internet of Services to achieve a high degree of
flexibility in production, higher productivity rates through real-time
monitoring and diagnosis, and a lower wastage rate of material in production.
An important part of the tasks in the preparation for Industrie 4.0 is the
adaption of the higher education to the requirements of this vision, in
particular the engineering education. In this work, we introduce a road map
consisting of three pillars describing the changes/enhancements to be conducted
in the areas of curriculum development, lab concept, and student club
activities. We also report our current application of this road map at the
Turkish-German University, Istanbul.
"
1526,"A software framework for pipelined arithmetic algorithms in field
  programmable gate arrays","  Pipelined algorithms implemented in field programmable gate arrays are being
extensively used for hardware triggers in the modern experimental high energy
physics field and the complexity of such algorithms are increases rapidly. For
development of such hardware triggers, algorithms are developed in
$\texttt{C++}$, ported to hardware description language for synthesizing
firmware, and then ported back to $\texttt{C++}$ for simulating the firmware
response down to the single bit level. We present a $\texttt{C++}$ software
framework which automatically simulates and generates hardware description
language code for pipelined arithmetic algorithms.
"
1527,"Do two parties represent the US? Clustering analysis of US public
  ideology survey","  Recent surveys have shown that an increasing portion of the US public
believes the two major US parties adequately represent the US public opinion
and think additional parties are needed. However, there are high barriers for
third parties in political elections. In this paper, we aim to address two
questions: ""How well do the two major US parties represent the public's
ideology?"" and ""Does a more-than-two-party system better represent the ideology
of the public?"". To address these questions, we utilize the American National
Election Studies Time series dataset. We perform unsupervised clustering with
Gaussian Mixture Model method on this dataset. When clustered into two
clusters, we find a large centrist cluster and a small right-wing cluster. The
Democratic Party's position (estimated using the mean position of the
individuals self-identified with the parties) is similar to that of the
centrist cluster, and the Republican Party's position is between the two
clusters. We investigate if more than two parties represent the population
better by comparing the Akaike Information Criteria for clustering results of
the various number of clusters. We find that additional clusters give a better
representation of the data, even after penalizing for the additional
parameters. This suggests a multiparty system represents of the ideology of the
public better.
"
1528,"Modeling and Real-Time Scheduling of DC Platform Supply Vessel for Fuel
  Efficient Operation","  DC marine architecture integrated with variable speed diesel generators (DGs)
has garnered the attention of the researchers primarily because of its ability
to deliver fuel efficient operation. This paper aims in modeling and to
autonomously perform real-time load scheduling of dc platform supply vessel
(PSV) with an objective to minimize specific fuel oil consumption (SFOC) for
better fuel efficiency. Focus has been on the modeling of various components
and control routines, which are envisaged to be an integral part of dc PSVs.
Integration with photovoltaic-based energy storage system (ESS) has been
considered as an option to cater for the short time load transients. In this
context, this paper proposes a real-time transient simulation scheme, which
comprises of optimized generation scheduling of generators and ESS using dc
optimal power flow algorithm. This framework considers real dynamics of dc PSV
during various marine operations with possible contingency scenarios, such as
outage of generation systems, abrupt load changes, and unavailability of ESS.
The proposed modeling and control routines with real-time transient simulation
scheme have been validated utilizing the real-time marine simulation platform.
The results indicate that the coordinated treatment of renewable based ESS with
DGs operating with optimized speed yields better fuel savings. This has been
observed in improved SFOC operating trajectory for critical marine missions.
Furthermore, SFOC minimization at multiple suboptimal points with its treatment
in the real-time marine system is also highlighted.
"
1529,Customized Routing Optimization Based on Gradient Boost Regressor Model,"  In this paper, we discussed limitation of current
electronic-design-automoation (EDA) tool and proposed a machine learning
framework to overcome the limitations and achieve better design quality. We
explored how to efficiently extract relevant features and leverage gradient
boost regressor (GBR) model to predict underestimated risky net (URN).
Customized routing optimizations are applied to the URNs and results show clear
timing improvement and trend to converge toward timing closure.
"
1530,"An Experimental Analysis of the Power Consumption of Convolutional
  Neural Networks for Keyword Spotting","  Nearly all previous work on small-footprint keyword spotting with neural
networks quantify model footprint in terms of the number of parameters and
multiply operations for a feedforward inference pass. These values are,
however, proxy measures since empirical performance in actual deployments is
determined by many factors. In this paper, we study the power consumption of a
family of convolutional neural networks for keyword spotting on a Raspberry Pi.
We find that both proxies are good predictors of energy usage, although the
number of multiplies is more predictive than the number of model parameters. We
also confirm that models with the highest accuracies are, unsurprisingly, the
most power hungry.
"
1531,Detecting Disguised Plagiarism,"  Source code plagiarism detection is a problem that has been addressed several
times before; and several tools have been developed for that purpose. In this
research project we investigated a set of possible disguises that can be
mechanically applied to plagiarized source code to defeat plagiarism detection
tools. We propose a preprocessor to be used with existing plagiarism detection
tools to ""normalize"" source code before checking it, thus making such disguises
ineffective.
"
1532,Machine Learning Based Fast Power Integrity Classifier,"  In this paper, we proposed a new machine learning based fast power integrity
classifier that quickly flags the EM/IR hotspots. We discussed the features to
extract to describe the power grid, cell power density, routing impact and
controlled collapse chip connection (C4) bumps, etc. The continuous and
discontinuous cases are identified and treated using different machine learning
models. Nearest neighbors, random forest and neural network models are compared
to select the best performance candidates. Experiments are run on open source
benchmark, and result is showing promising prediction accuracy.
"
1533,How Long Will My Phone Battery Last?,"  Mobile devices are only as useful as their battery lasts. Unfortunately, the
operation and life of a mobile device's battery degrade over time and usage.
The state-of-health (SoH) of batteries quantifies their degradation, but mobile
devices are unable to support its accurate estimation -- despite its importance
-- due mainly to their limited hardware and dynamic usage patterns, causing
various problems such as unexpected device shutoffs or even fire/explosion. To
remedy this lack of support, we design, implement and evaluate V-Health, a
low-cost user-level SoH estimation service for mobile devices based only on
their battery voltage, which is commonly available on all commodity mobile
devices. V-Health also enables four novel use-cases that improve mobile users'
experience from different perspectives. The design of V-Health is inspired by
our empirical finding that the relaxing voltages of a device battery
fingerprint its SoH, and is steered by extensive measurements with 15 batteries
used for various commodity mobile devices, such as Nexus 6P, Galaxy S3, iPhone
6 Plus, etc. These measurements consist of 13,377 battery
discharging/charging/resting cycles and have been conducted over 72 months
cumulatively. V-Health has been evaluated via both laboratory experiments and
field tests over 4-6 months, showing <5% error in SoH estimation.
"
1534,"(geo)graphs - Complex Networks as a shapefile of nodes and a shapefile
  of edges for different applications","  Spatial dependency and spatial embedding are basic physical properties of
many phenomena modeled by networks. The most indicated computational
environment to deal with spatial information is to use Georeferenced
Information System (GIS) and Geographical Database Management Systems (GDBMS).
Several models have been proposed in this direction, however there is a gap in
the literature in generic frameworks for working with Complex Networks in
GIS/GDBMS environments. Here we introduce the concept of (geo)graphs: graphs in
which the nodes have a known geographical location and the edges have spatial
dependence. We present case studies and two open source softwares (GIS4GRAPH
and GeoCNet) that indicate how to retrieve networks from GIS data and how to
represent networks over GIS data by using (geo)graphs.
"
1535,The process of 3D-printed skull models for the anatomy education,"  Objective The 3D printed medical models can come from virtual digital
resources, like CT scanning. Nevertheless, the accuracy of CT scanning
technology is limited, which is 1mm. In this situation, the collected data is
not exactly the same as the real structure and there might be some errors
causing the print to fail. This study presents a common and practical way to
process the skull data to make the structures correctly. And then we make a
skull model through 3D printing technology, which is useful for medical
students to understand the complex structure of skull. Materials and Methods
The skull data is collected by the CT scan. To get a corrected medical model,
the computer-assisted image processing goes with the combination of five 3D
manipulation tools: Mimics, 3ds Max, Geomagic, Mudbox and Meshmixer, to
reconstruct the digital model and repair it. Subsequently, we utilize a
low-cost desktop 3D printer, Ultimaker2, with polylactide filament (PLA)
material to print the model and paint it based on the atlas. Result After the
restoration and repairing, we eliminate the errors and repair the model by
adding the missing parts of the uploaded data within 6 hours. Then we print it
and compare the model with the cadaveric skull from frontal, left, right and
anterior views respectively. The printed model can show the same structures and
also the details of the skull clearly and is a good alternative of the
cadaveric skull.
"
1536,"Solution of network localization problem with noisy distances and its
  convergence","  The network localization problem with convex and non-convex distance
constraints may be modeled as a nonlinear optimization problem. The existing
localization techniques are mainly based on convex optimization. In those
techniques, the non-convex distance constraints are either ignored or relaxed
into convex constraints for using the convex optimization methods like SDP,
least square approximation, etc.. We propose a method to solve the nonlinear
non-convex network localization problem with noisy distance measurements
without any modification of constraints in the general model. We use the
nonlinear Lagrangian technique for non-convex optimization to convert the
problem to a root finding problem of a single variable continuous function.
This problem is then solved using an iterative method. However, in each step of
the iteration the computation of the functional value involves a finite
mini-max problem (FMX). We use smoothing gradient method to fix the FMX
problem. We also prove that the solution obtained from the proposed iterative
method converges to the actual solution of the general localization problem.
The proposed method obtains the solutions with a desired label of accuracy in
real time.
"
1537,"Obtaining the coefficients of a Vector Autoregression Model through
  minimization of parameter criteria","  VAR models are a type of multi-equation model that have been widely applied
in econometrics. With the arrival of Big Data, huge amounts of data are being
collected in numerous fields, making feasible the application of these kind of
statistical models. Tools exist to tackle this problem, but the large amount of
data, along with the availability of computational techniques and high
performance systems, advise an in-depth analysis of the computational aspects
of VAR, so large models can be solved efficiently with today's computational
systems.
  This work aims to solve a VAR model by obtaining the coefficients through
heuristic and metaheuristic algorithms, minimizing one parameter criterion, and
also to compare with those coefficients obtained by OLS. Furthermore, we
consider different approaches to reduce the time required to find the model
like using matrix decompositions (QR or LQ), exploiting matrix structure, using
high performance linear algebra subroutines (BLAS and LAPACK) or parallel
metaheuristics.
"
1538,Treatment of Unicode canoncal decomposition among operating systems,"  This article shows how the text characters that have multiple representations
under the Unicode standard are treated by popular operating systems. Whilst
most characters have a unique representation in Unicode, some characters such
as the accented European letters, can have multiple representations due to a
feature of Unicode called normalization. These characters are treated
differently by popular operating systems, leading to additional challenges
during interoperability of computer programs.
"
1539,Rule based End-to-End Learning Framework for Urban Growth Prediction,"  Due to the rapid growth of urban areas in the past decades, it has become
increasingly important to model and monitor urban growth in mega cities.
Although several researchers have proposed models for simulating urban growth,
they have been primarily dependent on various manually selected spatial and
nonspatial explanatory features for building models. A practical difficulty
with this approach is manual selection procedure, which tends to make model
design process laborious and non-generic. Despite the fact that explanatory
features provide us with the understanding of a complex process, there has been
no standard set of features in urban growth prediction over which scholars have
consensus. Hence, design and deploying of systems for urban growth prediction
have remained challenging tasks. In order to reduce the dependency on human
devised features, we have proposed a novel End-to-End prediction framework to
represent remotely sensed satellite data in terms of rules of a cellular
automata model in order to improve the performance of urban growth prediction.
Using our End-to-End framework, we have achieved superior performance in Figure
of Merit, Producer's accuracy, User's accuracy, and Overall accuracy metrics
respectively over existing learning based methods.
"
1540,"Explanation of an Invisible Common Constraint of Mind, Mathematics and
  Computational Complexity","  There is a cognitive limit in Human Mind. This cognitive limit has played a
decisive role in almost all fields including computer sciences. The cognitive
limit replicated in computer sciences is responsible for inherent Computational
Complexity. The complexity starts decreasing if certain conditions are met,
even sometime it does not appears at all. Very simple Mechanical computing
systems are designed and implemented to demonstrate this idea and it is further
supported by Electrical systems. These verifiable and consistent systems
demonstrate the idea of computational complexity reduction. This work explains
a very important but invisible connection from Mind to Mathematical axioms
(Peano Axioms etc.) and Mathematical axioms to computational complexity. This
study gives a completely new perspective that goes well beyond Cognitive
Science, Mathematics, Physics, Computer Sciences and Philosophy. Based on this
new insight some important predictions are made.
"
1541,An optical solution for the set splitting problem,"  We describe here an optical device, based on time-delays, for solving the set
splitting problem which is well-known NP-complete problem. The device has a
graph-like structure and the light is traversing it from a start node to a
destination node. All possible (potential) paths in the graph are generated and
at the destination we will check which one satisfies completely the problem's
constrains.
"
1542,"Design Automation for Binarized Neural Networks: A Quantum Leap
  Opportunity?","  Design automation in general, and in particular logic synthesis, can play a
key role in enabling the design of application-specific Binarized Neural
Networks (BNN). This paper presents the hardware design and synthesis of a
purely combinational BNN for ultra-low power near-sensor processing. We
leverage the major opportunities raised by BNN models, which consist mostly of
logical bit-wise operations and integer counting and comparisons, for pushing
ultra-low power deep learning circuits close to the sensor and coupling it with
binarized mixed-signal image sensor data. We analyze area, power and energy
metrics of BNNs synthesized as combinational networks. Our synthesis results in
GlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for
implementing a combinational BNN with 32x32 binary input sensor receptive field
and weight parameters fixed at design time. This is 2.2x smaller than a
synthesized network with re-configurable parameters. With respect to other
comparable techniques for deep learning near-sensor processing, our approach
features a 10x higher energy efficiency.
"
1543,Applications of Biological Cell Models in Robotics,"  In this paper I present some of the most representative biological models
applied to robotics. In particular, this work represents a survey of some
models inspired, or making use of concepts, by gene regulatory networks (GRNs):
these networks describe the complex interactions that affect gene expression
and, consequently, cell behaviour.
"
1544,OpenSEA: Semi-Formal Methods for Soft Error Analysis,"  Alpha-particles and cosmic rays cause bit flips in chips. Protection circuits
ease the problem, but cost chip area and power, and so designers try hard to
optimize them. This leads to bugs: an undetected fault can bring
miscalculations, the checker that alarms about harmless faults incurs
performance penalty. Such bugs are hard to find: circuit simulation with tests
is inefficient since it enumerates the huge fault time-location space, and
formal methods do not scale since they explore the whole inputs. In this paper,
we use formal methods on designer's input tests, while keeping time-location
open. This idea is at the core of the tool OpenSEA. OpenSEA can (i) find
latches vulnerable to and protected against faults, (ii) find tests that
exhibit checker false alarms, (iii) use fixed and open inputs, and (iv) use
environment assumptions. Evaluation on a number of industrial designs shows
that OpenSEA produces valuable results.
"
1545,"automan: a simple, Python-based, automation framework for numerical
  computing","  We present an easy-to-use, Python-based framework that allows a researcher to
automate their computational simulations. In particular the framework
facilitates assembling several long-running computations and producing various
plots from the data produced by these computations. The framework makes it
possible to reproduce every figure made for a publication with a single
command. It also allows one to distribute the computations across a network of
computers. The framework has been used to write research papers in numerical
computing. This paper discusses the design of the framework, and the benefits
of using it. The ideas presented are general and should help researchers
organize their computations for better reproducibility.
"
1546,Understanding Career Progression in Baseball Through Machine Learning,"  Professional baseball players are increasingly guaranteed expensive long-term
contracts, with over 70 deals signed in excess of \$90 million, mostly in the
last decade. These are substantial sums compared to a typical franchise
valuation of \$1-2 billion. Hence, the players to whom a team chooses to give
such a contract can have an enormous impact on both competitiveness and profit.
Despite this, most published approaches examining career progression in
baseball are fairly simplistic. We applied four machine learning algorithms to
the problem and soundly improved upon existing approaches, particularly for
batting data.
"
1547,"The View from the Other Side: The Border Between Controversial Speech
  and Harassment on Kotaku in Action","  In this paper, we use mixed methods to study a controversial Internet site:
The Kotaku in Action (KiA) subreddit. Members of KiA are part of GamerGate, a
distributed social movement. We present an emic account of what takes place on
KiA who are they, what are their goals and beliefs, and what rules do they
follow. Members of GamerGate in general and KiA in particular have often been
accused of harassment. However, KiA site policies explicitly prohibit such
behavior, and members insist that they have been falsely accused. Underlying
the controversy over whether KiA supports harassment is a complex disagreement
about what ""harassment"" is, and where to draw the line between freedom of
expression and censorship. We propose a model that characterizes perceptions of
controversial speech, dividing it into four categories: criticism, insult,
public shaming, and harassment. We also discuss design solutions that address
the challenges of moderating harassment without impinging on free speech, and
communicating across different ideologies.
"
1548,"""Oh Tanenbaum, oh Tanenbaum..."": Technical Foundations of Xmas 4.0
  Research","  Andrew Tanenbaum and his textbooks -- e.g. on Operating Systems, Computer
Networks, Structured Computer Organization and Distributed Systems, to name but
a few -- have had a tremendous impact on generations of computer science
students (and teachers at the same time). Given this, it is striking to observe
that this comprehensive body of work apparently does not provide a single line
on a research topic that seems to be intimately related with his name (at least
in German), i.e. Xmas Research (XR). Hence, the goal of this paper is to fill
this gap and provide insight into a number of paradigmatic XR research
questions, for instance: Can we today still count on Santa Claus? Or at least
on Xmas trees? And does this depend on basic tree structures, or can we rather
find solutions on the level of programming languages? By addressing such basic
open issues, we aim at providing a solid technical foundation for future steps
towards the imminent evolution of Xmas 4.0.
"
1549,"Effect of NBTI/PBTI Aging and Process Variations on Write Failures in
  MOSFET and FinFET Flip-Flops","  The assessment of noise margins and the related probability of failure in
digital cells has growingly become essential, as nano-scale CMOS and FinFET
technologies are confronting reliability issues caused by aging mechanisms,
such as NBTI, and variability in process parameters. The influence of such
phenomena is particularly associated to the Write Noise Margins (WNM) in memory
elements, since a wrong stored logic value can result in an upset of the system
state. In this work, we calculated and compared the effect of process
variations and NBTI aging over the years on the actual WNM of various CMOS and
FinFET based flip-flop cells. The massive transistor-level Monte Carlo
simulations produced both nominal (i.e. mean) values and associated standard
deviations of the WNM of the chosen flip-flops. This allowed calculating the
consequent write failure probability as a function of an input voltage shift on
the flip-flop cells, and assessing a comparison for robustness among different
circuit topologies and technologies.
"
1550,"Methodological Framework for Determining the Land Eligibility of
  Renewable Energy Sources","  The quantity and distribution of land which is eligible for renewable energy
sources is fundamental to the role these technologies will play in future
energy systems. As it stands, however, the current state of land eligibility
investigation is found to be insufficient to meet the demands of the future
energy modelling community. Three key areas are identified as the predominate
causes of this; inconsistent criteria definitions, inconsistent or unclear
methodologies, and inconsistent dataset usage. To combat these issues, a land
eligibility framework is developed and described in detail. The validity of
this framework is then shown via the recreation of land eligibility results
found in the literature, showing strong agreement in the majority of cases.
Following this, the framework is used to perform an evaluation of land
eligibility criteria within the European context whereby the relative
importance of commonly considered criteria are compared.
"
1551,Limits for Rumor Spreading in stochastic populations,"  Biological systems can share and collectively process information to yield
emergent effects, despite inherent noise in communication. While man-made
systems often employ intricate structural solutions to overcome noise, the
structure of many biological systems is more amorphous. It is not well
understood how communication noise may affect the computational repertoire of
such groups. To approach this question we consider the basic collective task of
rumor spreading, in which information from few knowledgeable sources must
reliably flow into the rest of the population.
  In order to study the effect of communication noise on the ability of groups
that lack stable structures to efficiently solve this task, we consider a noisy
version of the uniform PULL model. We prove a lower bound which implies that,
in the presence of even moderate levels of noise that affect all facets of the
communication, no scheme can significantly outperform the trivial one in which
agents have to wait until directly interacting with the sources. Our results
thus show an exponential separation between the uniform PUSH and PULL
communication models in the presence of noise. Such separation may be
interpreted as suggesting that, in order to achieve efficient rumor spreading,
a system must exhibit either some degree of structural stability or,
alternatively, some facet of the communication which is immune to noise.
  We corroborate our theoretical findings with a new analysis of experimental
data regarding recruitment in Cataglyphis niger desert ants.
"
1552,DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car,"  We present DeepPicar, a low-cost deep neural network based autonomous car
platform. DeepPicar is a small scale replication of a real self-driving car
called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),
which takes images from a front-facing camera as input and produces car
steering angles as output. DeepPicar uses the same network architecture---9
layers, 27 million connections and 250K parameters---and can drive itself in
real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using
DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end
deep learning based real-time control of autonomous vehicles. We also
systematically compare other contemporary embedded computing platforms using
the DeepPicar's CNN-based real-time control workload. We find that all tested
platforms, including the Pi 3, are capable of supporting the CNN-based
real-time control, from 20 Hz up to 100 Hz, depending on hardware platform.
However, we find that shared resource contention remains an important issue
that must be considered in applying CNN models on shared memory based embedded
computing platforms; we observe up to 11.6X execution time increase in the CNN
based control loop due to shared resource contention. To protect the CNN
workload, we also evaluate state-of-the-art cache partitioning and memory
bandwidth throttling techniques on the Pi 3. We find that cache partitioning is
ineffective, while memory bandwidth throttling is an effective solution.
"
1553,Analytical Inverter Delay Modeling Using Matlab's Curve Fitting Toolbox,"  This paper presents a new analytical propagation delay model for deep
submicron CMOS inverters. The model is inspired by the key observation that the
inverter delay is a complicated function of several process parameters as well
as load capacitance. These relationships are considered by fitting functions
for each parameter derived from the Curve Fitting Toolbox in Matlab. Compared
to SPICE simulations based on the BSIM4 transistor model, the analytical delay
model shows very good accuracy with an average error less than 2% over a wide
range of process parameters and output loads. Hence, the proposed model can be
efficiently used for different technology nodes as well as statistical gate
delay characterisation.
"
1554,"Step Detection Algorithm For Accurate Distance Estimation Using Dynamic
  Step Length","  In this paper, a new Smartphone sensor based algorithm is proposed to detect
accurate distance estimation. The algorithm consists of two phases, the first
phase is for detecting the peaks from the Smartphone accelerometer sensor. The
other one is for detecting the step length which varies from step to step. The
proposed algorithm is tested and implemented in real environment and it showed
promising results. Unlike the conventional approaches, the error of the
proposed algorithm is fixed and is not affected by the long distance.
  Keywords distance estimation, peaks, step length, accelerometer.
"
1555,The Socket Store: An App Model for the Application-Network Interaction,"  A developer of mobile or desktop applications is responsible for implementing
the network logic of his software. Nonetheless: i) Developers are not network
specialists, while pressure for emphasis on the visible application parts
places the network logic out of the coding focus. Moreover, computer networks
undergo evolution at paces that developers may not follow. ii) From the network
resource provider point of view, marketing novel services and involving a broad
audience is also challenge for the same reason. Moreover, the objectives of
end-user networking logic are neither clear nor uniform. This constitutes the
central optimization of network resources an additional challenge. As a
solution to these problems, we propose the Socket Store. The Store is a
marketplace containing end-user network logic in modular form. The Store
modules act as intelligent mediators between the end-user and the network
resources. Each module has a clear, specialized objective, such as connecting
two clients over the Internet while avoiding transit networks suspicious for
eavesdropping. The Store is populated and peer-reviewed by network specialists,
whose motive is the visibility, practical applicability and monetization
potential of their work. A developer first purchases access to a given socket
module. Subsequently, he incorporates it to his applications under development,
obtaining state-of-the-art performance with trivial coding burden. A full Store
prototype is implemented and a critical data streaming module is evaluated as a
driving case.
"
1556,"StreetGen : In base city scale procedural generation of streets: road
  network, road surface and street objects","  Streets are large, diverse, and used for several (and possibly conflicting)
transport modalities as well as social and cultural activities. Proper planning
is essential and requires data. Manually fabricating data that represent
streets (street reconstruction) is error-prone and time consuming. Automatising
street reconstruction is a challenge because of the diversity, size, and scale
of the details (few centimetres for cornerstone) required. The state-of-the-art
focuses on roads (no context, no urban features) and is strongly determined by
each application (simulation, visualisation, planning). We propose a unified
framework that works on real Geographic Information System (GIS) data and uses
a strong, yet simple modelling hypothesis when possible to robustly model
streets at the city level or street level. Our method produces a coherent
street-network model containing topological traffic information, road surface
and street objects. We demonstrate the robustness and genericity of our method
by reconstructing the entire city of Paris streets and exploring other similar
reconstruction (airport driveway).
"
1557,"Mobility Based Routing Protocol with MAC Collision Improvement in
  Vehicular Ad Hoc Networks","  Intelligent transportation system attracts a great deal of research attention
because it helps enhance traffic safety, improve driving experiences, and
transportation efficiency. Vehicular Ad Hoc Network (VANET) supports wireless
connections among vehicles and offers information exchange, thus significantly
facilitating intelligent transportation systems. Since the vehicles move fast
and often change lanes unpredictably, the network topology evolves rapidly in a
random fashion, which imposes diverse challenges in routing protocol design
over VANET. When it comes to the 5G era, the fulfilment of ultra low end-to-end
delay and ultra high reliability becomes more crucial than ever. In this paper,
we propose a novel routing protocol that incorporates mobility status and MAC
layer channel contention information. The proposed routing protocol determines
next hop by applying mobility information and MAC contention information which
differs from existing greedy perimeter stateless routing (GPSR) protocol.
Simulation results of the proposed routing protocol show its performance
superiority over the existing approach.
"
1558,Approximability in the GPAC,"  Most of the physical processes arising in nature are modeled by either
ordinary or partial differential equations. From the point of view of analog
computability, the existence of an effective way to obtain solutions of these
systems is essential. A pioneering model of analog computation is the General
Purpose Analog Computer (GPAC), introduced by Shannon as a model of the
Differential Analyzer and improved by Pour-El, Lipshitz and Rubel, Costa and
Gra\c{c}a and others. Its power is known to be characterized by the class of
differentially algebraic functions, which includes the solutions of initial
value problems for ordinary differential equations. We address one of the
limitations of this model, concerning the notion of approximability, a
desirable property in computation over continuous spaces that is however absent
in the GPAC. In particular, the Shannon GPAC cannot be used to generate
non-differentially algebraic functions which can be approximately computed in
other models of computation. We extend the class of data types using networks
with channels which carry information on a general complete metric space $X$;
for example $X=C(R,R)$, the class of continuous functions of one real (spatial)
variable. We consider the original modules in Shannon's construction
(constants, adders, multipliers, integrators) and we add \emph{(continuous or
discrete) limit} modules which have one input and one output. We then define an
L-GPAC to be a network built with $X$-stream channels and the above-mentioned
modules. This leads us to a framework in which the specifications of such
analog systems are given by fixed points of certain operators on continuous
data streams. We study these analog systems and their associated operators, and
show how some classically non-generable functions, such as the gamma function
and the zeta function, can be captured with the L-GPAC.
"
1559,"Hardware implementation of auto-mutual information function for
  condition monitoring","  This study is aimed at showing applicability of mutual information, namely
auto-mutual information function for condition monitoring in electrical motors,
through age detection in accelerated motor aging. Vibration data collected in
artificial induction motor experiment is used for verification of both the
original auto-mutual information function algorithm and its hardware
implementation in Verilog, produced from an initial version made with Matlab
HDL (Hardware Description Language) Coder. A conceptual model for industry and
education based on a field programmable logic array development board is
developed and demonstrated on the auto-mutual information function example,
while suggesting other applications as well. It has also been shown that
attractor reconstruction for the vibration data cannot be straightforward.
"
1560,TikZ-FeynHand: Basic User Guide,"  This is a userguide for the LaTex package Tikz-FeynHand at
https://ctan.org/pkg/tikz-feynhand which let's you draw Feynman diagrams using
TikZ. It contains many examples and a 5-minute introduction to TikZ.
  The package is a low-end modification of the package TikZ-Feynman at
https://ctan.org/pkg/tikz-feynman, one of whose principal advantages is the
automatic generation of diagrams, for which it needs LuaTex. FeynHand only
provides the manual mode and hence runs in LaTex without any reference to
LuaTex.
  In addition it provides some NEW STYLES for vertices and propagators,
alternative SHORTER KEYWORDS in addition to TikZ-Feynman's longer ones, some
shortcut commands for QUICKLY CUSTOMIZING the diagrams' look, and the new
feature to put one propagator ""ON TOP"" of another.
"
1561,"A cost effective and reliable environment monitoring system for HPC
  applications","  We present a slow control system to gather all relevant environment
information necessary to effectively and reliably run an HPC (High Performance
Computing) system at a high value over price ratio. The scalable and reliable
overall concept is presented as well as a newly developed hardware device for
sensor read out. This device incorporates a Raspberry Pi, an Arduino and PoE
(Power over Ethernet) functionality in a compact form factor. The system is in
use at the 2 PFLOPS cluster of the Johannes Gutenberg-University and
Helmholtz-Institute in Mainz.
"
1562,Road Network Fusion for Incremental Map Updates,"  In the recent years a number of novel, automatic map-inference techniques
have been proposed, which derive road-network from a cohort of GPS traces
collected by a fleet of vehicles. In spite of considerable attention, these
maps are imperfect in many ways: they create an abundance of spurious
connections, have poor coverage, and are visually confusing. Hence, commercial
and crowd-sourced mapping services heavily use human annotation to minimize the
mapping errors. Consequently, their response to changes in the road network is
inevitably slow. In this paper we describe \mapfuse, a system which fuses a
human-annotated map (e.g., OpenStreetMap) with any automatically inferred map,
thus effectively enabling quick map updates. In addition to new road creation,
we study in depth road closure, which have not been examined in the past. By
leveraging solid, human-annotated maps with minor corrections, we derive maps
which minimize the trajectory matching errors due to both road network change
and imperfect map inference of fully-automatic approaches.
"
1563,"A veracity preserving model for synthesizing scalable electricity load
  profiles","  Electricity users are the major players of the electric systems, and
electricity consumption is growing at an extraordinary rate. The research on
electricity consumption behaviors is becoming increasingly important to design
and deployment of the electric systems. Unfortunately, electricity load
profiles are difficult to acquire. Data synthesis is one of the best approaches
to solving the lack of data, and the key is the model that preserves the real
electricity consumption behaviors. In this paper, we propose a hierarchical
multi-matrices Markov Chain (HMMC) model to synthesize scalable electricity
load profiles that preserve the real consumption behavior on three time scales:
per day, per week, and per year. To promote the research on the electricity
consumption behavior, we use the HMMC approach to model two distinctive raw
electricity load profiles. One is collected from the resident sector, and the
other is collected from the non-resident sectors, including different
industries such as education, finance, and manufacturing. The experiments show
our model performs much better than the classical Markov Chain model. We
publish two trained models online, and researchers can directly use these
trained models to synthesize scalable electricity load profiles for further
researches.
"
1564,"On the economics of electrical storage for variable renewable energy
  sources","  The use of renewable energy sources is a major strategy to mitigate climate
change. Yet Sinn (2017) argues that excessive electrical storage requirements
limit the further expansion of variable wind and solar energy. We question, and
alter, strong implicit assumptions of Sinn's approach and find that storage
needs are considerably lower, up to two orders of magnitude. First, we move
away from corner solutions by allowing for combinations of storage and
renewable curtailment. Second, we specify a parsimonious optimization model
that explicitly considers an economic efficiency perspective. We conclude that
electrical storage is unlikely to limit the transition to renewable energy.
"
1565,"Predicting Transportation Modes of GPS Trajectories using Feature
  Engineering and Noise Removal","  Understanding transportation mode from GPS (Global Positioning System) traces
is an essential topic in the data mobility domain. In this paper, a framework
is proposed to predict transportation modes. This framework follows a sequence
of five steps: (i) data preparation, where GPS points are grouped in trajectory
samples; (ii) point features generation; (iii) trajectory features extraction;
(iv) noise removal; (v) normalization. We show that the extraction of the new
point features: bearing rate, the rate of rate of change of the bearing rate
and the global and local trajectory features, like medians and percentiles
enables many classifiers to achieve high accuracy (96.5%) and f1 (96.3%)
scores. We also show that the noise removal task affects the performance of all
the models tested. Finally, the empirical tests where we compare this work
against state-of-art transportation mode prediction strategies show that our
framework is competitive and outperforms most of them.
"
1566,"A Generative Model for Non-Intrusive Load Monitoring in Commercial
  Buildings","  In the recent years, there has been an increasing academic and industrial
interest for analyzing the electrical consumption of commercial buildings.
Whilst having similarities with the Non Intrusive Load Monitoring (NILM) tasks
for residential buildings, the nature of the signals that are collected from
large commercial buildings introduces additional difficulties to the NILM
research causing existing NILM approaches to fail. On the other hand, the
amount of publicly available datasets collected from commercial buildings is
very limited, which makes the NILM research even more challenging for this type
of large buildings. In this study, we aim at addressing these issues. We first
present an extensive statistical analysis of both commercial and residential
measurements from public and private datasets and show important differences.
Secondly, we develop an algorithm for generating synthetic current waveforms.
We then demonstrate using real measurement and quantitative metrics that both
our device model and our simulations are realistic and can be used to evaluate
NILM algorithms. Finally, to encourage research on commercial buildings we
release a synthesized dataset.
"
1567,Analysing the Potential of BLE to Support Dynamic Broadcasting Scenarios,"  In this paper, we present a novel approach for broadcasting information based
on a Bluetooth Low Energy (BLE) ibeacon technology. We propose a dynamic method
that uses a combination of Wi-Fi and BLE technology where every technology
plays a part in a user discovery and broadcasting process. In such system, a
specific ibeacon device broadcasts the information when a user is in proximity.
Using experiments, we conduct a scenario where the system discovers users,
disseminates information, and later we use collected data to examine the system
performance and capability. The results show that our proposed approach has a
promising potential to become a powerful tool in the discovery and broadcasting
concept that can be easily implemented and used in business environments.
"
1568,"A state of the art of urban reconstruction: street, street network,
  vegetation, urban feature","  World population is raising, especially the part of people living in cities.
With increased population and complex roles regarding their inhabitants and
their surroundings, cities concentrate difficulties for design, planning and
analysis. These tasks require a way to reconstruct/model a city. Traditionally,
much attention has been given to buildings reconstruction, yet an essential
part of city were neglected: streets. Streets reconstruction has been seldom
researched. Streets are also complex compositions of urban features, and have a
unique role for transportation (as they comprise roads). We aim at completing
the recent state of the art for building reconstruction (Musialski2012) by
considering all other aspect of urban reconstruction. We introduce the need for
city models. Because reconstruction always necessitates data, we first analyse
which data are available. We then expose a state of the art of street
reconstruction, street network reconstruction, urban features
reconstruction/modelling, vegetation , and urban objects
reconstruction/modelling.
  Although reconstruction strategies vary widely, we can order them by the role
the model plays, from data driven approach, to model-based approach, to inverse
procedural modelling and model catalogue matching. The main challenges seems to
come from the complex nature of urban environment and from the limitations of
the available data. Urban features have strong relationships, between them, and
to their surrounding, as well as in hierarchical relations. Procedural
modelling has the power to express these relations, and could be applied to the
reconstruction of urban features via the Inverse Procedural Modelling paradigm.
"
1569,"CANA: A python package for quantifying control and canalization in
  Boolean Networks","  Logical models offer a simple but powerful means to understand the complex
dynamics of biochemical regulation, without the need to estimate kinetic
parameters. However, even simple automata components can lead to collective
dynamics that are computationally intractable when aggregated into networks. In
previous work we demonstrated that automata network models of biochemical
regulation are highly canalizing, whereby many variable states and their
groupings are redundant (Marques-Pita and Rocha, 2013). The precise charting
and measurement of such canalization simplifies these models, making even very
large networks amenable to analysis. Moreover, canalization plays an important
role in the control, robustness, modularity and criticality of Boolean network
dynamics, especially those used to model biochemical regulation (Gates and
Rocha, 2016; Gates et al., 2016; Manicka, 2017). Here we describe a new
publicly-available Python package that provides the necessary tools to extract,
measure, and visualize canalizing redundancy present in Boolean network models.
It extracts the pathways most effective in controlling dynamics in these
models, including their effective graph and dynamics canalizing map, as well as
other tools to uncover minimum sets of control variables.
"
1570,"A System for the Generation of Synthetic Wide Area Aerial Surveillance
  Imagery","  The development, benchmarking and validation of aerial Persistent
Surveillance (PS) algorithms requires access to specialist Wide Area Aerial
Surveillance (WAAS) datasets. Such datasets are difficult to obtain and are
often extremely large both in spatial resolution and temporal duration. This
paper outlines an approach to the simulation of complex urban environments and
demonstrates the viability of using this approach for the generation of
simulated sensor data, corresponding to the use of wide area imaging systems
for surveillance and reconnaissance applications. This provides a
cost-effective method to generate datasets for vehicle tracking algorithms and
anomaly detection methods. The system fuses the Simulation of Urban Mobility
(SUMO) traffic simulator with a MATLAB controller and an image generator to
create scenes containing uninterrupted door-to-door journeys across large areas
of the urban environment. This `pattern-of-life' approach provides
three-dimensional visual information with natural movement and traffic flows.
This can then be used to provide simulated sensor measurements (e.g. visual
band and infrared video imagery) and automatic access to ground-truth data for
the evaluation of multi-target tracking systems.
"
1571,On the Constituent Attributes of Software and Organisational Resilience,"  Our societies are increasingly dependent on services supplied by computers &
their software. New technology only exacerbates this dependence by increasing
the number, performance, and degree of autonomy and inter-connectivity of
software-empowered computers and cyber-physical ""things"", which translates into
unprecedented scenarios of interdependence. As a consequence, guaranteeing the
persistence-of-identity of individual & collective software systems and
software-backed organisations becomes an important prerequisite toward
sustaining the safety, security, & quality of the computer services supporting
human societies. Resilience is the term used to refer to the ability of a
system to retain its functional and non-functional identity. In this article we
conjecture that a better understanding of resilience may be reached by
decomposing it into ancillary constituent properties, the same way as a better
insight in system dependability was obtained by breaking it down into
sub-properties. 3 of the main sub-properties of resilience proposed here refer
respectively to the ability to perceive environmental changes; understand the
implications introduced by those changes; and plan & enact adjustments intended
to improve the system-environment fit. A fourth property characterises the way
the above abilities manifest themselves in computer systems. The 4 properties
are then analyzed in 3 families of case studies, each consisting of 3 software
systems that embed different resilience methods. Our major conclusion is that
reasoning in terms of resilience sub-properties may help revealing the
characteristics and limitations of classic methods and tools meant to achieve
system and organisational resilience. We conclude by suggesting that our method
may prelude to meta-resilient systems -- systems, that is, able to adjust
optimally their own resilience with respect to changing environmental
conditions.
"
1572,"A Novel Approach for Fast and Accurate Mean Error Distance Computation
  in Approximate Adders","  In error-tolerant applications, approximate adders have been exploited
extensively to achieve energy efficient system designs. Mean error distance is
one of the important error metrics used as a performance measure of approximate
adders. In this work, a fast and efficient methodology is proposed to determine
the exact mean error distance in approximate lower significant bit adders. A
detailed description of the proposed algorithm along with an example has been
demonstrated in this paper. Experimental analysis shows that the proposed
method performs better than existing Monte Carlo simulation approach both in
terms of accuracy and execution time.
"
1573,Automation of Processor Verification Using Recurrent Neural Networks,"  When considering simulation-based verification of processors, the current
trend is to generate stimuli using pseudorandom generators (PRGs), apply them
to the processor inputs and monitor the achieved coverage of its functionality
in order to determine verification completeness. Stimuli can have different
forms, for example, they can be represented by bit vectors applied to the input
ports of the processor or by programs that are loaded directly into the program
memory. In this paper, we propose a new technique dynamically altering
constraints for PRG via recurrent neural network, which receives a coverage
feedback from the simulation of design under verification. For the
demonstration purposes we used processors provided by Codasip as their coverage
state space is reasonably big and differs for various kinds of processors.
Nevertheless, techniques presented in this paper are widely applicable. The
results of experiments show that not only the coverage closure is achieved much
sooner, but we are able to isolate a small set of stimuli with high coverage
that can be used for running regression tests.
"
1574,"Prediction-Based Fast Thermoelectric Generator Reconfiguration for
  Energy Harvesting from Vehicle Radiators","  Thermoelectric generation (TEG) has increasingly drawn attention for being
environmentally friendly. A few researches have focused on improving TEG
efficiency at the system level on vehicle radiators. The most recent
reconfiguration algorithm shows improvement in performance but suffers from
major drawback on computational time and energy overhead, and non-scalability
in terms of array size and processing frequency. In this paper, we propose a
novel TEG array reconfiguration algorithm that determines near-optimal
configuration with an acceptable computational time. More precisely, with
$O(N)$ time complexity, our prediction-based fast TEG reconfiguration algorithm
enables all modules to work at or near their maximum power points (MPP).
Additionally, we incorporate prediction methods to further reduce the runtime
and switching overhead during the reconfiguration process. Experimental results
present $30\%$ performance improvement, almost $100\times$ reduction on
switching overhead and $13\times$ enhancement on computational speed compared
to the baseline and prior work. The scalability of our algorithm makes it
applicable to larger scale systems such as industrial boilers and heat
exchangers.
"
1575,"On the Energy Consumption Forecasting of Data Centers Based on Weather
  Conditions: Remote Sensing and Machine Learning Approach","  The energy consumption of Data Centers (DCs) is a very important figure for
the telecommunications operators, not only in terms of cost, but also in terms
of operational reliability. A relation between the energy consumption and the
weather conditions would indicate that weather forecast models could be used
for predicting energy consumption of DCs. A reliable forecast would result in a
more efficient management of the available energy and would make it easier to
take advantage of the modern types of power-grid based on renewable energy
resources. In this ,paper, we exploit the capabilities provided by the
FIESTA-IoT platform in order to investigate the correlation between the weather
conditions and the energy consumption in DCs. Then, by using multi-variable
linear regression process, we model this correlation between the energy
consumption and the dominant weather conditions parameters in order to
effectively forecast the energy consumption based on the weather forecast. We
have validated our results through live measurements from the RealDC testbed.
Results from our proposed approach indicate that forecasting of energy
consumption based on weather conditions could help not only DC operators in
managing their cooling systems and power usage, but also electricity companies
in optimizing their power distribution systems.
"
1576,"Angular and Temporal Correlation of V2X Channels Across Sub-6 GHz and
  mmWave Bands","  5G millimeter wave (mmWave) technology is envisioned to be an integral part
of next-generation vehicle-to-everything (V2X) networks and autonomous vehicles
due to its broad bandwidth, wide field of view sensing, and precise
localization capabilities. The reliability of mmWave links may be compromised
due to difficulties in beam alignment for mobile channels and due to blocking
effects between a mmWave transmitter and a receiver. To address such
challenges, out-of-band information from sub-6 GHz channels can be utilized for
predicting the temporal and angular channel characteristics in mmWave bands,
which necessitates a good understanding of how propagation characteristics are
coupled across different bands. In this paper, we use ray tracing simulations
to characterize the angular and temporal correlation across a wide range of
propagation frequencies for V2X channels ranging from 900 MHz up to 73 GHz, for
a vehicle maintaining line-of-sight (LOS) and non-LOS (NLOS) beams with a
transmitter in an urban environment. Our results shed light on increasing
sparsity behavior of propagation channels with increasing frequency and
highlight the strong temporal/angular correlation among 5.9 GHz and 28 GHz
bands especially for LOS channels.
"
1577,"High-resolution computer meshes of the lower body bones of an adult
  human female derived from CT images","  Background Computer-based geometrical meshes of bones are important for
applications in computational biomechanics as well as clinical software. There
is however a lack of freely available detailed bone meshes, especially related
to the human female morphology.
  Methods & Results We provide high resolution bone meshes of the lower body,
derived from CT images of a 59 year old female cadaver that were sourced from
the Visible Human Data Set, Visible Human Project (NIH, USA). Important bone
landmarks and joint rotation axes are identified from the extracted meshes. A
script-based framework is developed to provide a graphical user interface that
can visualize, resample and modify the meshes to fit different subject scales.
  Conclusion This open-data resource fills a gap in available data and is
provided for free usage in research and other applications. The associated
scripts allows users to easily transform the meshes to different laboratory and
software setups. This resource may be accessed through the following web link:
https://github.com/manishsreenivasa/BMFToolkit
  This document is the author's version of this article.
"
1578,A Review of Augmented Reality Applications for Building Evacuation,"  Evacuation is one of the main disaster management solutions to reduce the
impact of man-made and natural threats on building occupants. To date, several
modern technologies and gamification concepts, e.g. immersive virtual reality
and serious games, have been used to enhance building evacuation preparedness
and effectiveness. Those tools have been used both to investigate human
behavior during building emergencies and to train building occupants on how to
cope with building evacuations.
  Augmented Reality (AR) is novel technology that can enhance this process
providing building occupants with virtual contents to improve their evacuation
performance. This work aims at reviewing existing AR applications developed for
building evacuation. This review identifies the disasters and types of building
those tools have been applied for. Moreover, the application goals, hardware
and evacuation stages affected by AR are also investigated in the review.
Finally, this review aims at identifying the challenges to face for further
development of AR evacuation tools.
"
1579,"Increased Prediction Accuracy in the Game of Cricket using Machine
  Learning","  Player selection is one the most important tasks for any sport and cricket is
no exception. The performance of the players depends on various factors such as
the opposition team, the venue, his current form etc. The team management, the
coach and the captain select 11 players for each match from a squad of 15 to 20
players. They analyze different characteristics and the statistics of the
players to select the best playing 11 for each match. Each batsman contributes
by scoring maximum runs possible and each bowler contributes by taking maximum
wickets and conceding minimum runs. This paper attempts to predict the
performance of players as how many runs will each batsman score and how many
wickets will each bowler take for both the teams. Both the problems are
targeted as classification problems where number of runs and number of wickets
are classified in different ranges. We used na\""ive bayes, random forest,
multiclass SVM and decision tree classifiers to generate the prediction models
for both the problems. Random Forest classifier was found to be the most
accurate for both the problems.
"
1580,Aesthetical Attributes for Segmenting Arabic Word,"  The connected allograph representing calligraphic Arabic word does not appear
individually in any calligraphic resource but in association with other letters
all adapted to each other. The graphic segmentation of the word by respecting
aesthetical attributes indicating the grapheme of every letter is far from
being an obvious task. The question consists in discovering every letter
constituting the word, points of cutting which separate its grapheme from other
constituents of word's shape. The obtained segment must be a complete drawing
of the represented letter. This segmentation according to contextual graphic
and qualitative criteria connecting the attached allograph will have to satisfy
typographic constraints varying in conformity with the possibilities offered by
the wanted technology. In this paper, we develop an approach for segmenting
Arabic word from which the purpose is to extract graphemes respecting the
design of Arabic letters such as it is in the calligraphic literature. The
procedure bases itself on the principle that the Arabic connected letters have
a common part included in the cursive area, which must not be lost during the
process of cutting.
"
1581,Problem of Multiple Diacritics Design for Arabic Script,"  This study focuses on the design of multiple Arabic diacritical marks and to
developing a model that generates the stacking of multiples Arabic diacritics
in order to integrate it into a system of Arabic composition. The problem
concerns the presence of multiple diacritics on a single basic letter. This
model is based on the layering composition. The combination of diacritics with
letters requires a basic layering to combine any diacritics in the word with
their base letter, without having to deal individually and separately each pair
of base letter and diacritics.
"
1582,SMT Solving for Vesicle Traffic Systems in Cells,"  In biology, there are several questions that translate to combinatorial
search. For example, vesicle traffic systems that move cargo within eukaryotic
cells have been proposed to exhibit several graph properties such as three
connectivity. These properties are consequences of underlying biophysical
constraints. A natural question for biologists is: what are the possible
networks for various combinations of those properties? In this paper, we
present novel SMT based encodings of the properties over vesicle traffic
systems and a tool that searches for the networks that satisfies the properties
using SMT solvers. In our experiments, we show that our tool can search for
networks of sizes that are considered to be relevant by biologists.
"
1583,"Automatic Detection of Indoor and Outdoor Scenarios using NMEA Message
  Data from GPS Receivers","  Detection of indoor and outdoor scenarios is an important resource for many
types of activities such as multisensor navigation and location-based services.
This research presents the use of NMEA data provided by GPS receivers to
characterize different types of scenarios automatically. A set of static tests
was performed to evaluate metrics such as number of satellites, positioning
solution geometry and carrier-to-receiver noise-density ratio values to detect
possible patterns to determine indoor and outdoor scenarios. Subsequently,
validation tests are applied to verify that parameters obtained are adequate.
"
1584,Random Tilings with the GPU,"  We present GPU accelerated implementations of Markov chain algorithms to
sample random tilings, dimers, and the six-vertex model.
"
1585,"Designing a cost-time-quality-efficient grinding process using MODM
  methods","  In this paper a multi-objective mathematical model has been used to optimize
grinding parameters include workpiece speed, depth of cut and wheel speed which
highly affect the final surface quality. The mathematical model of the
optimization problem consists of three conflict objective functions subject to
wheel wear and production rate constraints. Exact methods can solve the NLP
model in few seconds, therefore using Meta-heuristic algorithms which provide
near optimal solutions in not suitable. Considering this, five Multi-Objective
Decision Making methods have been used to solve the multi-objective
mathematical model using GAMS software to achieve the optimal parameters of the
grinding process. The Multi-Objective Decision Making methods provide different
effective solutions where the decision maker can choose each solution in
different situations. Different criteria have been considered to evaluate the
performance of the five Multi-Objective Decision Making methods. Also,
Technique for Order of Preference by Similarity to Ideal Solution method has
been used to obtain the priority of each method and determine which
Multi-Objective Decision Making method performs better considering all criteria
simultaneously. The results indicated that Weighted Sum Method and Goal
programming method are the best Multi-Objective Decision Making methods. The
Weighted Sum Method and Goal programming provided solutions which are
competitive to each other. In addition, these methods obtained solutions which
have minimum grinding time, cost and surface roughness among other
Multi-Objective Decision Making methods.
"
1586,A Cyberinfrastructure for BigData Transportation Engineering,"  Big Data-driven transportation engineering has the potential to improve
utilization of road infrastructure, decrease traffic fatalities, improve fuel
consumption, decrease construction worker injuries, among others. Despite these
benefits, research on Big Data-driven transportation engineering is difficult
today due to the computational expertise required to get started. This work
proposes BoaT, a transportation-specific programming language, and it's Big
Data infrastructure that is aimed at decreasing this barrier to entry. Our
evaluation that uses over two dozen research questions from six categories show
that research is easier to realize as a BoaT computer program, an order of
magnitude faster when this program is run, and exhibits 12-14x decrease in
storage requirements.
"
1587,Investigating Power Outage Effects on Reliability of Solid-State Drives,"  Solid-State Drives (SSDs) are recently employed in enterprise servers and
high-end storage systems in order to enhance performance of storage subsystem.
Although employing high speed SSDs in the storage subsystems can significantly
improve system performance, it comes with significant reliability threat for
write operations upon power failures. In this paper, we present a comprehensive
analysis investigating the impact of workload dependent parameters on the
reliability of SSDs under power failure for variety of SSDs (from top
manufacturers). To this end, we first develop a platform to perform two
important features required for study: a) a realistic fault injection into the
SSD in the computing systems and b) data loss detection mechanism on the SSD
upon power failure. In the proposed physical fault injection platform, SSDs
experience a real discharge phase of Power Supply Unit (PSU) that occurs during
power failure in data centers which was neglected in previous studies. The
impact of workload dependent parameters such as workload Working Set Size
(WSS), request size, request type, access pattern, and sequence of accesses on
the failure of SSDs is carefully studied in the presence of realistic power
failures. Experimental results over thousands number of fault injections show
that data loss occurs even after completion of the request (up to 700ms) where
the failure rate is influenced by the type, size, access pattern, and sequence
of IO accesses while other parameters such as workload WSS has no impact on the
failure of SSDs.
"
1588,PURE: Scalable Phase Unwrapping with Spatial Redundant Arcs,"  Phase unwrapping is a key problem in many coherent imaging systems, such as
synthetic aperture radar (SAR) interferometry. A general formulation for
redundant integration of finite differences for phase unwrapping (Costantini et
al., 2010) was shown to produce a more reliable solution by exploiting
redundant differential estimates. However, this technique requires a commercial
linear programming solver for large-scale problems. For a linear cost function,
we propose a method based on Dual Decomposition that breaks the given problem
defined over a non-planar graph into tractable sub-problems over planar
subgraphs. We also propose a decomposition technique that exploits the
underlying graph structure for solving the sub-problems efficiently and
guarantees asymptotic convergence to the globally optimal solution. The
experimental results demonstrate that the proposed approach is comparable to
the existing state-of-the-art methods in terms of the estimate with a better
runtime and memory footprint.
"
1589,"Data-Driven Exploration of Factors Affecting Federal Student Loan
  Repayment","  Student loans occupy a significant portion of the federal budget, as well as,
the largest financial burden in terms of debt for graduates. This paper
explores data-driven approaches towards understanding the repayment of such
loans. Using statistical and machine learning models on the College Scorecard
Data, this research focuses on extracting and identifying key factors affecting
the repayment of a student loan. The specific factors can be used to develop
models which provide predictive capability towards repayment rate, detect
irregularities/non-repayment, and help understand the intricacies of student
loans.
"
1590,A Computational Framework for Modelling and Analyzing Ice Storms,"  Ice storms are extreme weather events that can have devastating implications
for the sustainability of natural ecosystems as well as man made
infrastructure. Ice storms are caused by a complex mix of atmospheric
conditions and are among the least understood of severe weather events. Our
ability to model ice storms and characterize storm features will go a long way
towards both enabling support systems that offset storm impacts and increasing
our understanding of ice storms. In this paper, we present a holistic
computational framework to answer key questions of interest about ice storms.
We model ice storms as a function of relevant surface and atmospheric
variables. We learn these models by adapting and applying supervised and
unsupervised machine learning algorithms on data with missing or incorrect
labels. We also include a knowledge representation module that reasons with
domain knowledge to revise the output of the learned models. Our models are
trained using reanalysis data and historical records of storm events. We
evaluate these models on reanalyis data as well as Global Climate Model (GCM)
data for historical and future climate change scenarios. Furthermore, we
discuss the use of appropriate bias correction approaches to run such modeling
frameworks with GCM data.
"
1591,"Characterizing the Temporal Dynamics of Information in Visually Guided
  Predictive Control Using LSTM Recurrent Neural Networks","  Theories for visually guided action account for online control in the
presence of reliable sources of visual information, and predictive control to
compensate for visuomotor delay and temporary occlusion. In this study, we
characterize the temporal relationship between information integration window
and prediction distance using computational models. Subjects were immersed in a
simulated environment and attempted to catch virtual balls that were
transiently ""blanked"" during flight. Recurrent neural networks were trained to
reproduce subject's gaze and hand movements during blank. The models
successfully predict gaze behavior within 3 degrees, and hand movements within
8.5 cm as far as 500 ms in time, with integration window as short as 27 ms.
Furthermore, we quantified the contribution of each input source of information
to motor output through an ablation study. The model is a proof of concept for
prediction as a discrete mapping between information integrated over time and a
temporally distant motor output.
"
1592,"Autonomous Vehicle Scheduling At Intersections Based On Production Line
  Technique","  This thesis considers the problem of scheduling autonomous vehicles at
intersections. A new system is proposed which is more efficient and could
replace the recently introduced Autonomous Intersection Management (AIM) model.
The proposed system is based on the production line technique. The environment
of the intersection, vehicles position, speeds, and turning are specified and
determined in advance. The goal of the proposed system is to eliminate vehicle
collision and reduce the waiting time to cross the intersection. Three
different patterns of traffic flow towards the intersection have been tested.
The system requires less waiting time, compared to the other models, including
the random case where the flow is unpredictable. The K-Nearest Neighbors (KNN)
algorithm has been used to predict vehicles making a right turn at the
intersection. The experimental results show there is no chance of collision
inside the intersection using the proposed model; however, the system might
require more space in the traffic lane for some specific traffic patterns.
"
1593,"Energy Efficiency and Emission Testing for Connected and Automated
  Vehicles Using Real-World Driving Data","  By using the onboard sensing and external connectivity technology, connected
and automated vehicles (CAV) could lead to improved energy efficiency, better
routing, and lower traffic congestion. With the rapid development of the
technology and adaptation of CAV, it is more critical to develop the universal
evaluation method and the testing standard which could evaluate the impacts on
energy consumption and environmental pollution of CAV fairly, especially under
the various traffic conditions. In this paper, we proposed a new method and
framework to evaluate the energy efficiency and emission of the vehicle based
on the unsupervised learning methods. Both the real-world driving data of the
evaluated vehicle and the large naturalistic driving dataset are used to
perform the driving primitive analysis and coupling. Then the linear weighted
estimation method could be used to calculate the testing result of the
evaluated vehicle. The results show that this method can successfully identify
the typical driving primitives. The couples of the driving primitives from the
evaluated vehicle and the typical driving primitives from the large real-world
driving dataset coincide with each other very well. This new method could
enhance the standard development of the energy efficiency and emission testing
of CAV and other off-cycle credits.
"
1594,"Cost-Benefit Analysis of Data Intelligence -- Its Broader
  Interpretations","  The core of data science is our fundamental understanding about data
intelligence processes for transforming data to decisions. One aspect of this
understanding is how to analyze the cost-benefit of data intelligence
workflows. This work is built on the information-theoretic metric proposed by
Chen and Golan for this purpose and several recent studies and applications of
the metric. We present a set of extended interpretations of the metric by
relating the metric to encryption, compression, model development, perception,
cognition, languages, and news media.
"
1595,"Constraining the Synopsys Pin Access Checker Utility for Improved
  Standard Cells Library Verification Flow","  While standard cell layouts are drawn with minimum design rules for maximum
benefit of design area shrinkage, the complicated design rules begin to cause
difficulties with signal routes accessing the pins in standard cell layouts.
Multiple design iterations are required to resolve routing issues, thus
increasing the runtime and the overall chip area. To optimize the chip
performance, power and area (PPA) and improve the routability, it is necessary
to consider the pin accessibility during standard cell development phase so
that each cell is designed to maximize the number of feasible pin-access
solutions available to the router. As part of the Synopsys IC Compiler Library
Preparation Reference Methodology, the Synopsys Pin Access Checker (PAC)
reports DRC violations associated with the standard cell. Based on Synopsys
PAC's methodology, we demonstrate several methods to improve the probability of
detecting pin accessibility issues, such as reducing the number of cells
required for each Synopsys 'testcell', increasing the complexity of the pin
connectivity assignment and recommending the router constraints.
"
1596,In Design DFM Rule Scoring and Fixing Method using ICV,"  As compared to DRC rules, DFM rules are a list of selected recommended rules
which aim to improve the design margins for better manufacturability. In
GLOBALFOUNDRIES, we use DFM scoring methodology as an effective technique to
analyze design quality in terms of manufacturability. Physical design engineers
can perform our Manufacturability Check Deck (MCD) to asset their design
quality during the sign-off stage. In the past, Synopsys users have to convert
their design though milkyway database to GDSII format and execute the
verification through the third party EDA tools. This method is costly and
time-consuming for our Synopsys users. Today, we propose a new and easy-to-use
integrated flow which leverages on the ICV engine to provide DFM scoring and
in-design fixing techniques. The new methodology address DFM violations early
in the design flow and achieve DFM compliance design during sign-off phase.
"
1597,"Advanced In-Design Auto-Fixing Flow for Cell Abutment Pattern Matching
  Weakpoints","  Pattern matching design verification has gained noticeable attention in
semiconductor technologies as it can precisely identify more localized
problematic areas (weakpoints) in the layout. To address these weakpoints,
engineers adopt 'Rip-up and Reroute' methodology to reroute the nets and avoid
these weakpoints. However, the technique is unable to address weakpoints due to
the cell placement. The only present approach is to manually shift or flip the
standard cells to eradicate the weakpoint. To overcome the challenge in going
from a manual and laborious process to a fully automated fixing, we have
proposed an in-design auto-fixing feature, tested with the commercial design
tool, Synopsys IC Compiler. Our experimental result has demonstrated close to
one hundred percent lithography weakpoints fixing on all of our 14nm designs.
"
1598,IoT for Green Building Management,"  Buildings consume 60% of global electricity. However, current building
management systems (BMSs) are highly expensive and difficult to justify for
small to medium-sized buildings. As such, the Internet of Things (IoT), which
can monitor and collect a large amount of data on different contexts of a
building and feed the data to the processor of the BMS, provides a new
opportunity to integrate intelligence into the BMS to monitor and manage the
energy consumption of the building in a cost-effective manner. Although an
extensive literature is available on IoT based BMS and applications of signal
processing techniques for some aspects of building energy management
separately, detailed study on their integration to address the overall BMS is
quite limited. As such, the proposed paper will address this gap by providing
an overview of an IoT based BMS leveraging signal processing and machine
learning techniques. It is demonstrated how to extract high-level building
occupancy information through simple and low-cost IoT sensors and studied the
impact of human activities on energy usage of a building, which can be
exploited to design energy conservation measures to reduce the building's
energy consumption.
"
1599,"Multiple-Lithography-Compliant Verification for Standard Cell Library
  Development Flow","  Starting from 22-nm, a standard cell must be designed to be full
lithography-compliant, which includes Design Rule Check,
Design-for-Manufacturability and Double-Patterning compliant. It has become a
great challenge for physical layout designers to provide a full
lithography-compliant standard cell layout that is optimized for area, power,
timing, signal integrity, and yield. This challenge is further exacerbated with
abutted single- and multiple-height standard cells. At present, different
foundries and library vendors have different approaches for full
lithography-compliant library preparation and validation. To the best of our
knowledge, there is no single tool integrates all types of
lithography-compliant check in standard cell libraries validation flow. In this
work, we will demonstrate multiple lithography-compliant verification for
standard cell library development flow. Validation flow and detailed algorithm
implementation will be explained to assist engineers to achieve full
lithography-compliant standard cell libraries. An area-efficient standard cell
placement methodology will also be discussed to validate the issues arises from
standard cell abutment.
"
1600,"Standard Cell Library Evaluation with Multiple lithography-compliant
  verification and Improved Synopsys Pin Access Checking Utility","  While standard cell layouts are drawn with minimum design rules to maximize
the benefit of design area shrinkage, the complicated design rules have caused
difficulties with signal routes accessing the pins in standard cell layouts. As
a result, it has become a great challenge for physical layout designers to
design a standard cell layout that is optimized for area, power, timing, signal
integrity, and printability. Multiple design iterations are required to
consider pin accessibility during standard cells layout to increase the number
of feasible solutions available to the router. In this work, we will
demonstrate several improvements with the Synopsys PAC methodology, such as
reducing the number of cells required for each Synopsys 'testcell' with the
same cell abutment condition, increasing the complexity of the pin connection
for better pin accessibility evaluation. We also recommend additional
constraints to improve the probability of detecting pin accessibility issues.
We also integrate other physical verification methods to access the design rule
compliance and the printability of standard cells. We hope that the easy to use
utility enables layout engineers to perform the verification, simplifying the
verification methodology.
"
1601,Quantum Adiabatic Evolution for Global Optimization in Big Data,"  Big Data is characterized by Volume, Velocity, Veracity and Complexity. The
interaction between this huge data is complex with an associated free will
having dynamic and non linear nature. We reduced big data based on its
characteristics, conceptually driven by quantum field theory and utilizing the
physics of condensed matter theory in a complex nonlinear dynamic system:
Quantum Topological Field Theory of Data. The model is formulated from the
dynamics and evolution of single datum, eventually defining the global
properties and evolution of collective data space via action, partition
function, green propagators in almost polynomially solvable O(nlogn)
complexity. The simulated results show that the time complexity of our
algorithm for global optimization via quantum adiabatic evolution is almost in
O(logn) Our algorithm first mines the space via greedy approach and makes a
list of all ground state Hamiltonians, then utilizing the tunnelling property
of quantum mechanics optimizes the algorithm unlike up hill and iterative
techniques and doesnot let algorithm to get localized in local minima or sharp
valley due to adiabatic evolution of the system. The loss in quantumness, non
realizable, no clone, noise, decoherence, splitting of energy states due to
electric and magnetic fields, variant to perturbations and less lifetime makes
it inefficient for practical implementation. The inefficiencies of qubit can be
overcome via property that remains invariant to perturbation and Cartesian
independent having well defined mathematical structure. It can be well
addressed via topological field theory of data.
"
1602,DATA:SEARCH'18 -- Searching Data on the Web,"  This half day workshop explores challenges in data search, with a particular
focus on data on the web. We want to stimulate an interdisciplinary discussion
around how to improve the description, discovery, ranking and presentation of
structured and semi-structured data, across data formats and domain
applications. We welcome contributions describing algorithms and systems, as
well as frameworks and studies in human data interaction. The workshop aims to
bring together communities interested in making the web of data more
discoverable, easier to search and more user friendly.
"
1603,A Guide to the SPHERE 100 Homes Study Dataset,"  The SPHERE project has developed a multi-modal sensor platform for health and
behavior monitoring in residential environments. So far, the SPHERE platform
has been deployed for data collection in approximately 50 homes for duration up
to one year. This technical document describes the format and the expected
content of the SPHERE dataset(s) under preparation. It includes a list of some
data quality problems (both known to exist in the dataset(s) and potential
ones), their workarounds, and other information important to people working
with the SPHERE data, software, and hardware. This document does not aim to be
an exhaustive descriptor of the SPHERE dataset(s); it also does not aim to
discuss or validate the potential scientific uses of the SPHERE data.
"
1604,An Integrated View on the Future of Logistics and Information Technology,"  In this position paper, we present our vision on the future of the logistics
business domain and the use of information technology (IT) in this domain. The
vision is based on extensive experience with Dutch and European logistics in
various contexts and from various perspectives. We expect that the vision also
holds for logistics outside Europe. We build our vision in a number of steps.
First, we make an inventory of the most important trends in the logistics
domain - we call these mega-trends. Next, we do the same for the information
technology domain, restricted to technologies that have relevance for
logistics. Then, we introduce logistics meta-concepts that we use to describe
our vision and relate them to business engineering. We use these three
ingredients to analyze leading concepts that we currently observe in the
logistics domain. Next, we consolidate all elements into a model that
represents our vision of the integrated future of logistics and IT. We
elaborate on the role of data platforms and open standards in this integrated
vision.
"
1605,"Neural Network-based exploration of construct validity for Russian
  version of the 10-item Big Five Inventory","  This study aims to present a new method of exploring construct validity of
questionnaires based on neural network. Using this test we further explore
convergent validity for Russian adaptation of TIPI (Ten-Item Personality
Inventory by Gosling, Rentfrow, and Swann). Due to small number of questions
TIPI-RU can be used as an express-method for surveying large number of people,
especially in the Internet-studies. It can be also used with other translations
of the same questionnaire in the intercultural studies. The neural network test
for construct validity can be used as more convenient substitute for path
model.
"
1606,Gamorithm,"  Examining games from a fresh perspective we present the idea of game-inspired
and game-based algorithms, dubbed ""gamorithms"".
"
1607,"Distributed Optimization Strategy for Multi Area Economic Dispatch Based
  on Electro Search Optimization Algorithm","  A new adopted evolutionary algorithm is presented in this paper to solve the
non-smooth, non-convex and non-linear multi-area economic dispatch (MAED). MAED
includes some areas which contains its own power generation and loads. By
transmitting the power from the area with lower cost to the area with higher
cost, the total cost function can be minimized greatly. The tie line capacity,
multi-fuel generator and the prohibited operating zones are satisfied in this
study. In addition, a new algorithm based on electro search optimization
algorithm (ESOA) is proposed to solve the MAED optimization problem with
considering all the constraints. In ESOA algorithm all probable moving states
for individuals to get away from or move towards the worst or best solution
needs to be considered. To evaluate the performance of the ESOA algorithm, the
algorithm is applied to both the original economic dispatch with 40 generator
systems and the multi-area economic dispatch with 3 different systems such as:
6 generators in 2 areas; and 40 generators in 4 areas. It can be concluded
that, ESOA algorithm is more accurate and robust in comparison with other
methods.
"
1608,Design and Application of Data Aquistion Interface Circuit,"  A commitment to condition monitoring involves the operators of plant in the
conduct of a range of activities. These activities may be compli-cated in
nature and indeed may often be performed automatically under computer control.
They can, however, always be down into a rela-tively small number of easily
identifiable functional tasks. This makes it much easier to identify the common
elements of machine condition monitoring schemes. A proposed interface circuit
design and application will be further explain in this paper, the implemented
monitoring unit circuit also illustrated, see appendix A. Two scenarios
presented in this paper, first ten turns assume to be shorted, and in the
second thirty turns shorted to show the difference in the amplitude of
frequencies at each case. This paper present. An improvement in three-phase
squirrel-cage induction motor stator inter-turn fault detection and diagnosis
based on a neural network approach is presented.
"
1609,Design of TDC ASIC based on Temperature Compensation,"  .On the basis of requirement of CSNS, we designed a TDC chip with temperature
compensation function in this paper, which employed TSMC 180nm process. Using
delay unit bufx8 as the major method, delay lines in each level delayed input
signal line through the bufx8 unit to realize fundamental measurement function.
The time intervals of two fixed delay standard pulses did not change with
temperature variation via intra-chip phase-locked loop. After that, the two
standard pulses were sent to TDC internal delay line and measured their values.
Then the measured values and standard values were compared. According to the
result of comparing and decision switch, the structure of delay lines was
reconstructed and their levels were recorded at the same time. We could ensure
that the total length of the effective delay line were close to clock cycle as
much as possible under the current temperature. The chip was tested after the
completion of design. It was found that the time resolution of TDC ASIC was
73ps under 1.8V power supply at room temperature while the time resolutions
were 103ps and 62ps at 85$^\circ$ and 0$^\circ$, respectively.
"
1610,"Research on Artificial Intelligence Ethics Based on the Evolution of
  Population Knowledge Base","  The unclear development direction of human society is a deep reason for that
it is difficult to form a uniform ethical standard for human society and
artificial intelligence. Since the 21st century, the latest advances in the
Internet, brain science and artificial intelligence have brought new
inspiration to the research on the development direction of human society.
Through the study of the Internet brain model, AI IQ evaluation, and the
evolution of the brain, this paper proposes that the evolution of population
knowledge base is the key for judging the development direction of human
society, thereby discussing the standards and norms for the construction of
artificial intelligence ethics.
"
1611,"Automatic streetlights that glow on detecting night and object using
  Arduino","  Our manuscript aims to develop a system which will lead to energy
conservation and by doing so, we would be able to lighten few more homes. The
proposed work is accomplished by using Arduino microcontroller and sensors that
will control the electricity based on night and object's detection. Meanwhile,
a counter is set that will count the number of objects passed through the road.
The beauty of the proposed work is that the wastage of unused electricity can
be reduced, lifetime of the streetlights gets enhance because the lights do not
stay ON during the whole night, and helps to increase safety measurements. We
are confident that the proposed idea will be beneficial in the future
applications of microcontrollers and sensors etc.
"
1612,"A Framework for Detecting and Translating User Behavior from Smart Meter
  Data","  The European adoption of smart electricity meters triggers the developments
of new value-added service for smart energy and optimal consumption. Recently,
several algorithms and tools have been built to analyze smart meter's data.
This paper introduces an open framework and prototypes for detecting and
presenting user behavior from its smart meter power consumption data. The
framework aims at presenting the detected user behavior in natural language
reports. In order to validate the proposed framework, an experiment has been
performed and the results have been presented.
"
1613,Towards a Circular Economy via Intelligent Metamaterials,"  The present study proposes the use of intelligent metasurfaces in the design
of products, as enforcers of circular economy principles. Intelligent
metasurfaces can tune their physical properties (electromagnetic, acoustic,
mechanical) by receiving software commands. When incorporated within products
and spaces they can mitigate the resource waste caused by inefficient,
partially optimized designs and security concerns. Thus, circular economy and
fast-paced product design become compatible. The study begins by considering
electromagnetic metamaterials, and proposes a complete methodology for their
deployment. Finally, it is shown that the same principles can be extended to
the control of mechanical properties of objects, exemplary enabling the
micro-management of vibrations and heat, with unprecedented circular economy
potential.
"
1614,"FluidDyn: a Python open-source framework for research and teaching in
  fluid dynamics","  FluidDyn is a project to foster open-science and open-source in the fluid
dynamics community. It is thought of as a research project to channel
open-source dynamics, methods and tools to do science. We propose a set of
Python packages forming a framework to study fluid dynamics with different
methods, in particular laboratory experiments (package fluidlab), simulations
(packages fluidfft, fluidsim and fluidfoam) and data processing (package
fluidimage). In the present article, we give an overview of the specialized
packages of the project and then focus on the base package called fluiddyn,
which contains common code used in the specialized packages. Packages fluidfft
and fluidsim are described with greater detail in two companion papers, Mohanan
et al. (2018a,b). With the project FluidDyn, we demonstrate that specialized
scientific code can be written with methods and good practices of the
open-source community. The Mercurial repositories are available in Bitbucket
(https://bitbucket.org/fluiddyn/). All codes are documented using Sphinx and
Read the Docs, and tested with continuous integration run on Bitbucket,
Pipelines and Travis. To improve the reuse potential, the codes are as modular
as possible, leveraging the simple object-oriented programming model of Python.
All codes are also written to be highly efficient, using C++, Cython and
Pythran to speedup the performance of critical functions.
"
1615,Graph Compact Orthogonal Layout Algorithm,"  There exist many orthogonal graph drawing algorithms that minimize edge
crossings or edge bends, however they produce unsatisfactory drawings in many
practical cases. In this paper we present a grid-based algorithm for drawing
orthogonal graphs with nodes of prescribed size. It distinguishes by creating
pleasant and compact drawings in relatively small running time. The main idea
is to minimize the total edge length that implicitly minimizes crossings and
makes the drawing easy to comprehend. The algorithm is based on combining local
and global improvements. Local improvements are moving each node to a new place
and swapping of nodes. Global improvement is based on constrained quadratic
programming approach that minimizes the total edge length while keeping node
relative positions.
"
1616,"A Localization Method Avoiding Flip Ambiguities for micro-UAVs with
  Bounded Distance Measurement Errors","  Localization is a fundamental function in cooperative control of micro
unmanned aerial vehicles (UAVs), but is easily affected by flip ambiguities
because of measurement errors and flying motions. This study proposes a
localization method that can avoid the occurrence of flip ambiguities in
bounded distance measurement errors and constrained flying motions; to
demonstrate its efficacy, the method is implemented on bilateration and
trilateration. For bilateration, an improved bi-boundary model based on the
unit disk graph model is created to compensate for the shortage of distance
constraints, and two boundaries are estimated as the communication range
constraint. The characteristic of the intersections of the communication range
and distance constraints is studied to present a unique localization criterion
which can avoid the occurrence of flip ambiguities. Similarly, for
trilateration, another unique localization criterion for avoiding flip
ambiguities is proposed according to the characteristic of the intersections of
three distance constraints. The theoretical proof shows that these proposed
criteria are correct. A localization algorithm is constructed based on these
two criteria. The algorithm is validated using simulations for different
scenarios and parameters, and the proposed method is shown to provide excellent
localization performance in terms of average estimated error. Our code can be
found at: https://github.com/QingbeiGuo/AFALA.git.
"
1617,Smart Grids Data Analysis: A Systematic Mapping Study,"  Data analytics and data science play a significant role in nowadays society.
In the context of Smart Grids (SG), the collection of vast amounts of data has
seen the emergence of a plethora of data analysis approaches. In this paper, we
conduct a Systematic Mapping Study (SMS) aimed at getting insights about
different facets of SG data analysis: application sub-domains (e.g., power load
control), aspects covered (e.g., forecasting), used techniques (e.g.,
clustering), tool-support, research methods (e.g., experiments/simulations),
replicability/reproducibility of research. The final goal is to provide a view
of the current status of research. Overall, we found that each sub-domain has
its peculiarities in terms of techniques, approaches and research methodologies
applied. Simulations and experiments play a crucial role in many areas. The
replicability of studies is limited concerning the provided implemented
algorithms, and to a lower extent due to the usage of private datasets.
"
1618,Sunlight Enabled Vehicle Detection by LED Street Lights,"  We propose and demonstrate a preliminary traffic sensing system based on the
widely distributed LED street lights. The system utilizes and discriminates the
photoelectric responses of the LEDs to sunlight when a vehicle moves through
the LEDs' field of view aiming at the road. A data vector is constructed from
the consecutively collected time samples of a moving observation window, and a
support vector machine (SVM) based learning algorithm is subsequently developed
to classify the presence of a vehicle. Finally, we build a simulated platform
to experimentally evaluate the performance of the vehicle detection algorithm.
"
1619,"The system of cloud oriented learning tools as an element of educational
  and scientific environment of high school","  The aim of this research is to design and implementation of cloud based
learning environment for separate division of the university. The analysis of
existing approaches to the construction of cloud based learning environments,
the formation of requirements cloud based learning tools, the selection on the
basis of these requirements, cloud ICT training and pilot their use for
building cloud based learning environment for separate division of the
university with the use of open source software and resources its own IT
infrastructure of the institution. Results of the study is planned to
generalize to develop recommendations for the design of cloud based environment
of high school.
"
1620,"Data Likelihood of Active Fires Satellite Detection and Applications to
  Ignition Estimation and Data Assimilation","  Data likelihood of fire detection is the probability of the observed
detection outcome given the state of the fire spread model. We derive fire
detection likelihood of satellite data as a function of the fire arrival time
on the model grid. The data likelihood is constructed by a combination of the
burn model, the logistic regression of the active fires detections, and the
Gaussian distribution of the geolocation error. The use of the data likelihood
is then demonstrated by an estimation of the ignition point of a wildland fire
by the maximization of the likelihood of MODIS and VIIRS data over multiple
possible ignition points.
"
1621,"The alternative bases of Boolean functions as a means of improving the
  structure of digital blocks","  This paper analyzes three forms of representation of Boolean functions, such
as Classical, Algebraic and Reed-Muller. The concept of intersection and
subsets of representation forms have been introduced, moreover suitable
criteria for creating these subsets have been established. Later, these subsets
have been quantitatively compared by the number of parameters, in order to
assess the effectiveness of using each of the forms of representations proposed
in the work. Definitions of the specific weight of subsets of priority forms of
the representation of Boolean functions showed that the classical form is the
least optimal, in comparison with the parameters of other forms Also, it has
been shown that the use of alternative forms of representation of Boolean
functions, in some cases, allows to reduce twice the number of incoming PLA
buses. Estimating the average loss from the exclusive use of the Classical Form
Representation also shows that the use of alternatives yields significant
benefits in some parameters, this can be used to optimize devices in the logic
design process and reduce the chip area, what also contributes to reductions in
the cost of such devices.
"
1622,Creation and Fixing of Lithography Hotspots with Synopsys Tools,"  At advanced process nodes, pattern matching techniques have been used in the
detection of lithography hotspots, which can affect yields of manufactured
integrated circuits. Although commercial pattern matching and in-design hotspot
fixing tools have been developed, engineers still need to verify that specific
hotspot patterns in routed designs can indeed be detected or even repaired by
software tools. Therefore, there is the need to create test cases with which
targeted hotspot patterns can be generated in routed layouts by using an APR
(automatic placement and routing) tool. In this paper, we propose a methodology
of creating hotspot patterns in the routing space by using Synopsys tools.
Also, methods for repairing hotspots during the physical design phase are
presented. With the use of the proposed hotspot creation methodology, we can
generate routed designs containing targeted hotspot patterns. As a result, the
effectiveness of hotspot detection rules, hotspot fixing guidance rules, and
relevant software tool functions can be verified.
"
1623,Context-Aware DFM Rule Analysis and Scoring Using Machine Learning,"  To evaluate the quality of physical layout designs in terms of
manufacturability, DFM rule scoring techniques have been widely used in
physical design and physical verification phases. However, one major drawback
of conventional DFM rule scoring methodologies is that resultant DFM rule
scores may not accurate since the scores may not highly correspond to
lithography simulation results. For instance, conventional DFM rule scoring
methodologies usually use rule-based techniques to compute scores without
considering neighboring geometric scenarios of targeted layout shapes. That can
lead to inaccurate scoring results since computed DFM rule scores can be either
too optimistic or too pessimistic. Therefore, in this paper, we propose a novel
approach with the use of machine learning technology to analyze the context of
targeted layouts and predict their lithography impacts on manufacturability.
"
1624,"PhaseMAC: A 14 TOPS/W 8bit GRO based Phase Domain MAC Circuit for
  In-Sensor-Computed Deep Learning Accelerators","  PhaseMAC (PMAC), a phase domain Gated-Ring-Oscillator (GRO) based 8bit MAC
circuit, is proposed to minimize both area and power consumption of deep
learning accelerators. PMAC composes of only digital cells and consumes
significantly smaller power than standard digital designs, owing to its
efficient analog accumulation nature. It occupies 26.6 times smaller area than
conventional analog designs, which is competitive to digital MAC circuits. PMAC
achieves a peak efficiency of 14 TOPS/W, which is best reported and 48% higher
than conventional arts. Results in anomaly detection tasks are demonstrated,
which is the hottest application in the industrial IoT scene.
"
1625,MMDF2018 Workshop Report,"  Driven by the recent advances in smart, miniaturized, and mass produced
sensors, networked systems, and high-speed data communication and computing,
the ability to collect and process larger volumes of higher veracity real-time
data from a variety of modalities is expanding. However, despite research
thrusts explored since the late 1990's, to date no standard, generalizable
solutions have emerged for effectively integrating and processing multimodal
data, and consequently practitioners across a wide variety of disciplines must
still follow a trial-and-error process to identify the optimum procedure for
each individual application and data sources. A deeper understanding of the
utility and capabilities (as well as the shortcomings and challenges) of
existing multimodal data fusion methods as a function of data and challenge
characteristics has the potential to deliver better data analysis tools across
all sectors, therein enabling more efficient and effective automated
manufacturing, patient care, infrastructure maintenance, environmental
understanding, transportation networks, energy systems, etc. There is therefore
an urgent need to identify the underlying patterns that can be used to
determine a priori which techniques will be most useful for any specific
dataset or application. This next stage of understanding and discovery (i.e.,
the development of generalized solutions) can only be achieved via a high level
cross-disciplinary aggregation of learnings, and this workshop was proposed at
an opportune time as many domains have already started exploring use of
multimodal data fusion techniques in a wide range of application-specific
contexts.
"
1626,PlayNPort: A Portable Wireless Music Player and Text Reader System,"  Portable Consumer Electronics has made a mark in the industry. With the ease
of use at an accessible price range, they have experienced significant growth
in the market. Our idea is to develop a portable wireless music player and text
reader using a Cortex-M series microcontroller and bare-metal programming
techniques. We chose to use an SD card as the storage device. The resulting
electronic device is similar to a consumer grade music player available in a
car. The system comprises an MCU, an MP3 encoder/decoder, an LCD, an audio
output jack, an SD card and a remote control. We also present various
challenges involved in developing the system and solutions we used to overcome
the challenges. The intricacy of the work lies in the fact that the system was
developed to be consumer-centric by providing a rich User Experience. It can be
used as a personal entertainment system in a car.
"
1627,"Theoretical analysis and propositions for ""ontology citation""","  Ontology citation, the practice of referring the ontology in a similar
fashion the scientific community routinely follows in providing the
bibliographic references to other scholarly works, has not received enough
attention it supposed to. Interestingly, so far none of the existing standard
citation styles (e.g., APA, CMOS, and IEEE) have included ontology as a citable
information source in the list of citable information sources such as journal
article, book, website, etc. Also, not much work can be found in the literature
on this topic though there are various issues and aspects of it that demand a
thorough study. For instance, what to cite? Is it the publication that
describes the ontology, or the ontology itself? The citation format, style,
illustration of motivations of ontology citation, the citation principles,
ontology impact factor, citation analysis, and so forth. In this work, we
primarily analyse the current ontology citation practices and the related
issues. We illustrate the various motivations and the basic principles of
ontology citation. We also propose a template for referring the source of
ontologies.
"
1628,Web Based Information System for Heat Supply Monitoring,"  The paper presents web based information system for heat supply monitoring.
The proposed model and information system for gathering data from heating
station heat-flow meters and regulators is software realized. The novel system
with proved functionality can be commercialized at the cost of minimal
investments, finding wildly use on Bulgarian market as cheap and quality
alternative of the western systems.
"
1629,"Current potentials and challenges using Sentinel-1 for broadacre field
  remote sensing","  ESA operates the Sentinel-1 satellites, which provides Synthetic Aperture
Radar (SAR) data of Earth. Recorded Sentinel-1 data have shown a potential for
remotely observing and monitoring local conditions on broad acre fields. Remote
sensing using Sentinel-1 have the potential to provide daily updates on the
current conditions in the individual fields and at the same time give an
overview of the agricultural areas in the region. Research depends on the
ability of independent validation of the presented results. In the case of the
Sentinel-1 satellites, every researcher has access to the same base dataset,
and therefore independent validation is possible. Well documented research
performed with Sentinel-1 allow other research the ability to redo the
experiments and either validate or falsify presented findings. Based on current
state-of-art research we have chosen to provide a service for researchers in
the agricultural domain. The service allows researchers the ability to monitor
local conditions by using the Sentinel-1 information combined with a priori
knowledge from broad acre fields. Correlating processed Sentinel-1 to the
actual conditions is still a task the individual researchers must perform to
benefit from the service. In this paper, we presented our methodology in
translating sentinel-1 data to a level that is more accessible to researchers
in the agricultural field. The goal here was to make the data more easily
available, so the primary focus can be on correlating and comparing to
measurements collected in the broadacre fields. We illustrate the value of the
service with three examples of the possible application areas. The presented
application examples are all based on Denmark, where we have processed all
sentinel-1 scan from since 2016.
"
1630,"CrowdExpress: A Probabilistic Framework for On-Time Crowdsourced Package
  Deliveries","  Speed and cost of logistics are two major concerns to on-line shoppers, but
they generally conflict with each other in nature. To alleviate the
contradiction, we propose to exploit existing taxis that are transporting
passengers on the street to relay packages collaboratively, which can
simultaneously lower the cost and accelerate the speed. Specifically, we
propose a probabilistic framework containing two phases called CrowdExpress for
the on-time package express deliveries. In the first phase, we mine the
historical taxi GPS trajectory data offline to build the package transport
network. In the second phase, we develop an online adaptive taxi scheduling
algorithm to find the path with the maximum arriving-on-time probability
""on-the-fly"" upon real- time requests, and direct the package routing
accordingly. Finally, we evaluate the system using the real-world taxi data
generated by over 19,000 taxis in a month in the city of New York, US. Results
show that around 9,500 packages can be delivered successfully on time per day
with the success rate over 94%, moreover, the average computation time is
within 25 milliseconds.
"
1631,"UAV Aided Aerial-Ground IoT for Air Quality Sensing in Smart City:
  Architecture, Technologies and Implementation","  As air pollution is becoming the largest environmental health risk, the
monitoring of air quality has drawn much attention in both theoretical studies
and practical implementations. In this article, we present a real-time,
fine-grained and power-efficient air quality monitoring system based on aerial
and ground sensing. The architecture of this system consists of four layers:
the sensing layer to collect data, the transmission layer to enable
bidirectional communications, the processing layer to analyze and process the
data, and the presentation layer to provide graphic interface for users. Three
major techniques are investigated in our implementation, given by the data
processing, the deployment strategy and the power control. For data processing,
spacial fitting and short-term prediction are performed to eliminate the
influences of the incomplete measurement and the latency of data uploading. The
deployment strategies of ground sensing and aerial sensing are investigated to
improve the quality of the collected data. The power control is further
considered to balance between power consumption and data accuracy. Our
implementation has been deployed in Peking University and Xidian University
since February 2018, and has collected about 100 thousand effective data
samples by June 2018.
"
1632,"A Conceptual Approach to Complex Model Management with Generalized
  Modelling Patterns and Evolutionary Identification","  Complex systems' modeling and simulation are powerful ways to investigate a
multitude of natural phenomena providing extended knowledge on their structure
and behavior. However, enhanced modeling and simulation require integration of
various data and knowledge sources, models of various kinds (data-driven
models, numerical models, simulation models, etc.), intelligent components in
one composite solution. Growing complexity of such composite model leads to the
need of specific approaches for management of such model. This need extends
where the model itself becomes a complex system. One of the important aspects
of complex model management is dealing with the uncertainty of various kinds
(context, parametric, structural, input/output) to control the model. In the
situation where a system being modeled, or modeling requirements change over
time, specific methods and tools are needed to make modeling and application
procedures (meta-modeling operations) in an automatic manner. To support
automatic building and management of complex models we propose a general
evolutionary computation approach which enables managing of complexity and
uncertainty of various kinds. The approach is based on an evolutionary
investigation of model phase space to identify the best model's structure and
parameters. Examples of different areas (healthcare, hydrometeorology, social
network analysis) were elaborated with the proposed approach and solutions.
"
1633,OpenMPL: An Open Source Layout Decomposer,"  Multiple patterning lithography has been widely adopted in advanced
technology nodes of VLSI manufacturing. As a key step in the design flow,
multiple patterning layout decomposition (MPLD) is critical to design closure.
Due to the NP-hardness of the general decomposition problem, various efficient
algorithms have been proposed with high quality solutions. However, with
increasingly complicated design flow and peripheral processing steps,
developing a high-quality layout decomposer becomes more and more difficult,
slowing down the further advancement in this field. This paper presents OpenMPL
[1], an open-source layout decomposition framework, with well-separated
peripheral processing and the core solving steps. We demonstrate the
flexibility of the framework with efficient implementations of various
state-of-the-art algorithms, which enable us to reproduce most of the recent
results on widely-recognized benchmarks. We believe OpenMPL can pave the road
for developing layout decomposition engines and stimulate further researches on
this problem.
"
1634,"UVM Based Reusable Verification IP for Wishbone Compliant SPI Master
  Core","  The System on Chip design industry relies heavily on functional verification
to ensure that the designs are bug-free. As design engineers are coming up with
increasingly dense chips with much functionality, the functional verification
field has advanced to provide modern verification techniques. In this paper, we
present verification of a wishbone compliant Serial Peripheral Interface (SPI)
Master core using a System Verilog based standard verification methodology, the
Universal Verification Methodology (UVM). The reason for using UVM factory
pattern with parameterized classes is to develop a robust and reusable
verification IP. SPI is a full duplex communication protocol used to interface
components most likely in embedded systems. We have verified an SPI Master IP
core design that is wishbone compliant and compatible with SPI protocol and bus
and furnished the results of our verification. We have used QuestaSim for
simulation and analysis of waveforms, Integrated Metrics Center, Cadence for
coverage analysis. We also propose interesting future directions for this work
in developing reliable systems.
"
1635,DATC RDF: An Open Design Flow from Logic Synthesis to Detailed Routing,"  In this paper, we present DATC Robust Design Flow (RDF) from logic synthesis
to detailed routing. Our goals are 1) to provide an open-source academic design
flow from logic synthesis to detailed routing based on existing contest
results, 2) to construct a database for design benchmarks and point tool
libraries, and 3) to interact with industrial designs by using industrial
standard design input/output formats. We also demonstrate RDF in a scalable
cloud infrastructure. Design methodology and cross-stage optimization research
can be conducted via RDF.
"
1636,"An Automated System for Checking Lithography Friendliness of Standard
  Cells","  At advanced process nodes, lithography weakpoints can exist in physical
layouts of integrated circuit designs even if the layouts pass design rule
checking (DRC). Existence of lithography weakpoints in a physical layout can
cause manufacturability issues, which in turn can result in yield losses. In
our experiments, we have found that specific standard cells have tendencies to
create lithography weakpoints after their cell instances are placed and routed,
even though each of these cells does not contain any lithography weakpoint
before performing placement and routing. In addition, our experiments have
shown that abutted standard cell instances can induce lithography weakpoints.
Therefore, in this paper, we propose methodologies that are used in a novel
software system for checking standard cells in terms of the aforementioned
lithography issues. Specifically, the software system is capable of detecting
and sorting problematic standard cells which are prone to generate lithography
weakpoints, as well as reporting standard cells that should not be abutted.
Methodologies proposed in this paper allow us to reduce or even prevent the
generation of undesirable lithography weakpoints during the physical synthesis
phase of designing a digital integrated circuit.
"
1637,FSS++ Workshop Report: Handling Uncertainty for Data Quality Management,"  This report describes the results of the eSCF Awareness Workshop on Handling
Uncertainty for Data Quality Management - Challenges from Transport and Supply
Chain Management that was held on June 5, 2018 in Heeze, The Netherlands. The
goal of this workshop was to create and enhance awareness into data quality
management issues that are encountered in practice, for business organizations
that aim to integrate a data-analytical mind set into their operations.
"
1638,Community Detection Across Emerging Quantum Architectures,"  One of the roadmap plans for quantum computers is an integration within HPC
ecosystems assigning them a role of accelerators for a variety of
computationally hard tasks. However, in the near term, quantum hardware will be
in a constant state of change. Heading towards solving real-world problems, we
advocate development of portable, architecture-agnostic hybrid
quantum-classical frameworks and demonstrate one for the community detection
problem evaluated using quantum annealing and gate-based universal quantum
computation paradigms.
"
1639,Ten Simple Rules for Reproducible Research in Jupyter Notebooks,"  Reproducibility of computational studies is a hallmark of scientific
methodology. It enables researchers to build with confidence on the methods and
findings of others, reuse and extend computational pipelines, and thereby drive
scientific progress. Since many experimental studies rely on computational
analyses, biologists need guidance on how to set up and document reproducible
data analyses or simulations.
  In this paper, we address several questions about reproducibility. For
example, what are the technical and non-technical barriers to reproducible
computational studies? What opportunities and challenges do computational
notebooks offer to overcome some of these barriers? What tools are available
and how can they be used effectively?
  We have developed a set of rules to serve as a guide to scientists with a
specific focus on computational notebook systems, such as Jupyter Notebooks,
which have become a tool of choice for many applications. Notebooks combine
detailed workflows with narrative text and visualization of results. Combined
with software repositories and open source licensing, notebooks are powerful
tools for transparent, collaborative, reproducible, and reusable data analyses.
"
1640,"Real-Time Fine-Grained Air Quality Sensing Networks in Smart City:
  Design, Implementation and Optimization","  Driven by the increasingly serious air pollution problem, the monitoring of
air quality has gained much attention in both theoretical studies and practical
implementations. In this paper, we present the architecture, implementation and
optimization of our own air quality sensing system, which provides real-time
and fine-grained air quality map of the monitored area. As the major component,
the optimization problem of our system is studied in detail. Our objective is
to minimize the average joint error of the established real-time air quality
map, which involves data inference for the unmeasured data values. A deep
Q-learning solution has been proposed for the power control problem to
reasonably plan the sensing tasks of the power-limited sensing devices online.
A genetic algorithm has been designed for the location selection problem to
efficiently find the suitable locations to deploy limited number of sensing
devices. The performance of the proposed solutions are evaluated by
simulations, showing a significant performance gain when adopting both
strategies.
"
1641,Human-Competitive Awards 2018,"  Report on Humies competition at GECCO 2018 in Japan
"
1642,"STAIRoute: Early Global Routing using Monotone Staircases for Congestion
  Reduction","  With aggressively shrinking process nodes, physical design methods face
severe challenges due to poor convergence and uncertainty in getting an optimal
solution. An early detection of potential failures is thus mandated. This has
encouraged to devise a feedback mechanism from a lower abstraction level of the
design flow to the higher ones, such as placement driven synthesis, routability
(timing) driven placement etc.
  Motivated by this, we propose an early global routing framework using pattern
routing following the floorplanning stage. We assess feasibility of a floorplan
topology of a given design by estimating routability, routed wirelength and
vias count while addressing the global congestion scenario across the layout.
Different capacity profiles for the routing regions, such as uniform or
non-uniform different cases of metal pitch variation across the metals layers
ensures adaptability to technology scaling. The proposed algorithm STAIRoute
takes $O(n^2kt)$ time for a given design with $n$ blocks and $k$ nets having at
most $t$ terminals. Experimental results on a set of floorplanning benchmark
circuits show $100\%$ routing completion, with no over-congestion in the
routing regions reported. The wirelength for the $t$-terminal ($t\geq$ 2) nets
is comparable with the Steiner length computed by FLUTE. An estimation on the
number of vias for different capacity profiles is also presented, along with
congestion and runtime results.
"
1643,"Waveform Signal Entropy and Compression Study of Whole-Building Energy
  Datasets","  Electrical energy consumption has been an ongoing research area since the
coming of smart homes and Internet of Things devices. Consumption
characteristics and usages profiles are directly influenced by building
occupants and their interaction with electrical appliances. Extracted
information from these data can be used to conserve energy and increase user
comfort levels. Data analysis together with machine learning models can be
utilized to extract valuable information for the benefit of occupants
themselves, power plants, and grid operators. Public energy datasets provide a
scientific foundation to develop and benchmark these algorithms and techniques.
With datasets exceeding tens of terabytes, we present a novel study of five
whole-building energy datasets with high sampling rates, their signal entropy,
and how a well-calibrated measurement can have a significant effect on the
overall storage requirements. We show that some datasets do not fully utilize
the available measurement precision, therefore leaving potential accuracy and
space savings untapped. We benchmark a comprehensive list of 365 file formats,
transparent data transformations, and lossless compression algorithms. The
primary goal is to reduce the overall dataset size while maintaining an
easy-to-use file format and access API. We show that with careful selection of
file format and encoding scheme, we can reduce the size of some datasets by up
to 73%.
"
1644,"Estimating Traffic Conditions At Metropolitan Scale Using Traffic Flow
  Theory","  The rapid urbanization and increasing traffic have serious social, economic,
and environmental impact on metropolitan areas worldwide. It is of a great
importance to understand the complex interplay of road networks and traffic
conditions. The authors propose a novel framework to estimate traffic
conditions at the metropolitan scale using GPS traces. Their approach begins
with an initial estimation of network travel times by solving a convex
optimization program based on traffic flow theory. Then, they iteratively
refine the estimated network travel times and vehicle traversed paths. Last,
the authors perform a bilevel optimization process to estimate traffic
conditions on road segments that are not covered by GPS data. The evaluation
and comparison of the authors' approach over two state-of-the-art methods show
up to 96.57% relative improvements. The authors have further conducted field
tests by coupling road networks of San Francisco and Beijing with real-world
GIS data, which involve 128,701 nodes, 148,899 road segments, and over 26
million GPS traces.
"
1645,"Early Routability Assessment in VLSI Floorplans: A Generalized Routing
  Model","  Multiple design iterations are inevitable in nanometer Integrated Circuit
(IC) design flow until desired printability and performance metrics are
achieved. This starts with placement optimization aimed at improving
routability, wirelength, congestion and timing in the design. Contrarily, no
such practice exists on a floorplanned layout, during the early stage of the
design flow. Recently, STAIRoute \cite{karb2} aimed to address that by
identifying the shortest routing path of a net through a set of routing regions
in the floorplan in multiple metal layers. Since the blocks in hierarchical
ASIC/SoC designs do not use all the permissible routing layers for the internal
routing corresponding to standard cell connectivity, the proposed STAIRoute
framework is not an effective for early global routability assessment. This
leads to improper utilization of routing area, specifically in higher routing
layers with fewer routing blockages, as the lack of placement of standard cells
does not facilitates any routing of their interconnections.
  This paper presents a generalized model for early global routability
assessment, HGR, by utilizing the free regions over the blocks beyond certain
metal layers. The proposed (hybrid) routing model comprises of (a) the junction
graph model in STAIRoute routing through the block boundary regions in lower
routing layers, and (ii) the grid graph model for routing in higher layers over
the free regions of the blocks.
  Experiment with the latest floorplanning benchmarks exhibit an average
reduction of $4\%$, $54\%$ and $70\%$ in netlength, via count, and congestion
respectively when HGR is used over STAIRoute. Further, we conducted another
experiment on an industrial design flow targeted for $45nm$ process, and the
results are encouraging with $~3$X runtime boost when early global routing is
used in conjunction with the existing physical design flow.
"
1646,"Exploring the Scope of Unconstrained Via Minimization by Recursive
  Floorplan Bipartitioning","  Random via failure is a major concern for post-fabrication reliability and
poor manufacturing yield. A demanding solution to this problem is redundant via
insertion during post-routing optimization. It becomes very critical when a
multi-layer routing solution already incurs a large number of vias. Very few
global routers addressed unconstrained via minimization (UVM) problem, while
using minimal pattern routing and layer assignment of nets. It also includes a
recent floorplan based early global routability assessment tool STAIRoute
\cite{karb2}.
  This work addresses an early version of unconstrained via minimization
problem during early global routing by identifying a set of minimal bend
routing regions in any floorplan, by a new recursive bipartitioning framework.
These regions facilitate monotone pattern routing of a set of nets in the
floorplan by STAIRoute. The area/number balanced floorplan bipartitionining is
a multi-objective optimization problem and known to be NP-hard \cite{majum2}.
No existing approaches considered bend minimization as an objective and some of
them incurred higher runtime overhead. In this paper, we present a Greedy as
well as randomized neighbor search based staircase wave-front propagation
methods for obtaining optimal bipartitioning results for minimal bend routing
through multiple routing layers, for a balanced trade-off between routability,
wirelength and congestion.
  Experiments were conducted on MCNC/GSRC floorplanning benchmarks for studying
the variation of early via count obtained by STAIRoute for different values of
the trade-off parameters ($\gamma, \beta$) in this multi-objective optimization
problem, using $8$ metal layers. We studied the impact of ($\gamma, \beta$)
values on each of the objectives as well as their linear combination function
$Gain$ of these objectives.
"
1647,Future Perspectives of Co-Simulation in the Smart Grid Domain,"  The recent attention towards research and development in cyber-physical
energy systems has introduced the necessity of emerging multi-domain
co-simulation tools. Different educational, research and industrial efforts
have been set to tackle the co-simulation topic from several perspectives. The
majority of previous works has addressed the standardization of models and
interfaces for data exchange, automation of simulation, as well as improving
performance and accuracy of co-simulation setups. Furthermore, the domains of
interest so far have involved communication, control, markets and the
environment in addition to physical energy systems. However, the current
characteristics and state of co-simulation testbeds need to be re-evaluated for
future research demands. These demands vary from new domains of interest, such
as human and social behavior models, to new applications of co-simulation, such
as holistic prognosis and system planning. This paper aims to formulate these
research demands that can then be used as a road map and guideline for future
development of co-simulation in cyber-physical energy systems.
"
1648,Smart Grid Co-Simulation with MOSAIK and HLA: A Comparison Study,"  Evaluating new technological developments for energy systems is becoming more
and more complex. The overall application environment is a continuously growing
and interconnected cyber-physical system so that analytical assessment is
practically impossible to realize. Consequently, new solutions must be
evaluated in simulation studies. Due to the interdisciplinarity of the
simulation scenarios, various heterogeneous tools must be connected. This
approach is known as co-simulation. During the last years, different approaches
have been developed or adapted for applications in energy systems. In this
paper, two co-simulation approaches are compared that follow generic, versatile
concepts. The tool mosaik, which has been explicitly developed for the purpose
of co-simulation in complex energy systems, is compared to the High Level
Architecture (HLA), which possesses a domain-independent scope but is often
employed in the energy domain. The comparison is twofold, considering the
tools' conceptual architectures as well as results from the simulation of
representative test cases. It suggests that mosaik may be the better choice for
entry-level, prototypical co-simulation while HLA is more suited for complex
and extensive studies.
"
1649,ACTT: Automotive CAN Tokenization and Translation,"  Modern vehicles contain scores of Electrical Control Units (ECUs) that
broadcast messages over a Controller Area Network (CAN). Vehicle manufacturers
rely on security through obscurity by concealing their unique mapping of CAN
messages to vehicle functions which differs for each make, model, year, and
even trim. This poses a major obstacle for after-market modifications notably
performance tuning and in-vehicle network security measures. We present ACTT:
Automotive CAN Tokenization and Translation, a novel, vehicle-agnostic,
algorithm that leverages available diagnostic information to parse CAN data
into meaningful messages, simultaneously cutting binary messages into tokens,
and learning the translation to map these contiguous bits to the value of the
vehicle function communicated.
"
1650,Design paradigms of intelligent control systems on a chip,"  This paper focuses on the Field Programmable Gate Array (FPGA) design and
implementation of intelligent control system applications on a chip,
specifically fuzzy logic and genetic algorithm processing units. Initially, an
overview of the FPGA technology is presented, followed by design methodologies,
development tools and the use of hardware description languages (HDL). Two FPGA
design examples with the use of Hardware Description Languages (HDLs) of
parameterized fuzzy logic controller cores are discussed. Thereinafter, a
System-on-a-Chip (SoC) designed by the authors in previous work and realized on
FPGA featuring a Digital Fuzzy Logic Controller (DFLC) and a soft processor
core for the path tracking problem of mobile robots is discussed. Finally a
Genetic Algorithm implementation (previously published by the authors) in FPGA
chip for the Traveling Salesman Problem (TSP) is also discussed.
"
1651,"Solving High Volume Capacitated Vehicle Routing Problem with Time
  Windows using Recursive-DBSCAN clustering algorithm","  This paper introduces a new approach to improve the performance of the
Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) solvers for a
high number of nodes. It proposes to cluster nodes together using
Recursive-DBSCAN - an algorithm that recursively applies DBSCAN until clusters
below the preset maximum number of nodes are obtained. That approach leads to
61% decrease in runtimes of the CVRPTW solver as benchmarked against Google
Optimization Tools, while the difference of total distance and number of
vehicles used by found solutions is below 7%. The improvement of runtimes with
the Recursive-DBSCAN method is because of splitting the node-set into
constituent clusters, which limits the number of solutions checked by the
solver, consequently reducing the runtime. The proposed method consumes less
memory and is able to find solutions for problems up to 5000 nodes, while the
baseline Google Optimisation Tools solves problems up to 2000 nodes.
"
1652,"Climate Anomalies vs Air Pollution: Carbon Emissions and Anomaly
  Networks","  This project aims to shed light on how man-made carbon emissions are
affecting global wind patterns by looking for temporal and geographical
correlations between carbon emissions, surface temperatures anomalies, and wind
speed anomalies at high altitude. We use a networks-based approach and daily
data from 1950 to 2010 [1-3] to model and draw correlations between disparate
regions of the globe.
"
1653,"An Interactive, Graphical CPU Scheduling Simulator for Teaching
  Operating Systems","  We present a graphical simulation tool for visually and interactively
exploring the processing of various events handled by an operating system when
running a program. Our graphical simulator is available for use on the web and
locally by both instructors and students for purposes of pedagogy. Instructors
can use it for live demonstrations of course concepts in class, while students
can use it outside of class to explore the concepts. The graphical simulation
tool is implemented using the React library for the fancy ui elements of the
Node.js framework and is available as a single page web application at
https://cpudemo.azurewebsites.net. Assigning the development of the underling
text-based simulation engine, on which the graphical simulator runs, to
students as a course project is also an effective approach to teach students
the concepts. The goals of this paper are to showcase the demonstrative
capabilities of the tool for instruction, share student experiences in
developing the engine underlying the simulation, and to inspire its use by
other educators.
"
1654,"Schr\""odinger's Man","  What if someone built a ""box"" that applies quantum superposition not just to
quantum bits in the microscopic but also to macroscopic everyday ""objects"",
such as Schr\""odinger's cat or a human being? If that were possible, and if the
different ""copies"" of a man could exploit quantum interference to synchronize
and collapse into their preferred state, then one (or they?) could in a sense
choose their future, win the lottery, break codes and other security devices,
and become king of the world, or actually of the many-worlds. We set up the
plot-line of a new episode of Black Mirror to reflect on what might await us if
one were able to build such a technology.
"
1655,Towards Modernising Data Collection and Archive for the Tor Network,"  CollecTor is developed by Tor Project's Metrics Team for the purpose of
archiving data relating to the public Tor network and applications developed by
Tor Project. This report distills the requirements for a prototype modernized
replacement of the CollecTor service, and evaluates frameworks and libraries
that are available to reduce code maintenance costs for the CollecTor service.
"
1656,Markov chain aggregation and its application to rule-based modelling,"  Rule-based modelling allows to represent molecular interactions in a compact
and natural way. The underlying molecular dynamics, by the laws of stochastic
chemical kinetics, behaves as a continuous-time Markov chain. However, this
Markov chain enumerates all possible reaction mixtures, rendering the analysis
of the chain computationally demanding and often prohibitive in practice. We
here describe how it is possible to efficiently find a smaller, aggregate
chain, which preserves certain properties of the original one. Formal methods
and lumpability notions are used to define algorithms for automated and
efficient construction of such smaller chains (without ever constructing the
original ones). We here illustrate the method on an example and we discuss the
applicability of the method in the context of modelling large signalling
pathways.
"
1657,"An intelligent household greenhouse system design based on Internet of
  Things","  In order to combine indoor greenery conservation with Internet of Things
(IOT) Technologies, this paper designs an intelligent household greenhouse
project with the features of comprehensive sensing, reliable transmission and
intelligent processing. Through the analysis of functional requirements of the
intelligent household greenhouse system, an intelligent household greenhouse
system is designed with the functions of greenhouse environmental data
detection, greenhouse environmental control regulation, data remote
transmission and human-computer interaction. Its sensor layer collects
environmental data in real time based on the ZigBee wireless sensor network.
The network layer STM32 intelligent gateway coordinates with network server, so
as to exchange data from sensor layer to application layer, and solve the
problems of non-blocking of data sending and receiving as well as concurrent
requests of multiple mobile terminals. The application layer is designed into
two types. One is a desktop management system as a data storage and analysis
center, and the other is a mobile terminal APP. At the same time, we design a
communication protocol that is applicable to the interaction of the three-layer
structure of the Internet of Things, with the characteristics of simplicity,
stability, readability, and scalability. It can avoid the mutual influence of
multi-level data exchange and ensure the correctness of data circulation. In
the design, the system sensor layer ensures stable transmission of various data
and instructions, and the network layer has a high degree of concurrency and
real time. And various measurement and control data of the sensor layer can
interact with the data of mobile-terminal equipment of the application layer.
The desktop management system and mobile terminal APP can monitor greenhouse
data in real time and control various actuators in the greenhouse.
"
1658,Open Source Software Opportunities and Risks,"  Open Source Software (OSS) history is traced to initial efforts in 1971 at
Massachusetts Institute of Technology (MIT) Artificial Intelligence (AI) Lab,
the initial goals of OSS around Free vs. Freedom, and its evolution and impact
on commercial and custom applications. Through OSS history, much of the
research and has been around contributors (suppliers) to OSS projects, the
commercialization, and overall success of OSS as a development process. In
conjunction with OSS growth, intellectual property issues and licensing issues
still remain. The consumers of OSS, application architects, in developing
commercial or internal applications based upon OSS should consider license risk
as they compose their applications using Component Based Software Development
(CBSD) approaches, either through source code, binary, or standard protocols
such as HTTP.
"
1659,"Real-time Structural Health Monitoring System Using Internet of Things
  and Cloud Computing","  Real-time monitoring of various structural behaviors, particularly
displacement and acceleration, serves important and valuable information for
people; for example, they can be used for active control or damage warning.
With recent advancement of the Internet of Things and client-side web
technologies, wireless integrated sensor devices nowadays can process real-time
raw sensor signal data into target measurements, such as displacement, and then
send the results through a standard protocol to the servers on the Internet.
The monitoring results are further processed for visualization purpose in the
servers and the computed results are pushed to connected clients like browsers
or mobile applications in real-time. We build a real-time cloud-based system
that can receive heterogeneous IoT data, allow users to create a
three-dimensional model online according to the real world structure, and the
monitoring results can be visualized in that model. In this paper, we
illustrate the software architecture of the proposed system and focus on the
technologies that are used, like client-side scripting, NoSql database, and
socket communication. We also present the challenges of displaying the overall
movement and shape transformation of the 3D structural model. Thus, each
internal-connected element's rotations and translations are obtained by
converting the monitoring results of each sensor device measured in the global
coordinate system. To overcome this, we create an inverted movement calculation
method. A simple 3D two-level structural model and simulated sensor
displacements are used to demonstrate system function and validate the inverted
movement calculation method.
"
1660,"RoboCup Junior in the Hunter Region: Driving the Future of Robotic STEM
  Education","  RoboCup Junior is a project-oriented educational initiative that sponsors
regional, national and international robotic events for young students in
primary and secondary school. It leads children to the fundamentals of teamwork
and complex problem solving through step-by-step logical thinking using
computers and robots. The Faculty of Engineering and Built Environment at the
University of Newcastle in Australia has hosted and organized the Hunter
regional tournament since 2012. This paper presents an analysis of data
collected from RoboCup Junior in the Hunter Region, New South Wales, Australia,
for a period of six years 2012-2017 inclusive. Our study evaluates the
effectiveness of the competition in terms of geographical spread, participation
numbers, and gender balance. We also present a case study about current
university students who have previously participated in RoboCup Junior.
"
1661,Putting Natural Time into Science,"  This contribution argues that the notion of time used in the scientific
modeling of reality deprives time of its real nature. Difficulties from logic
paradoxes to mathematical incompleteness and numerical uncertainty ensue. How
can the emergence of novelty in the Universe be explained? How can the
creativity of the evolutionary process leading to ever more complex forms of
life be captured in our models of reality? These questions are deeply related
to our understanding of time. We argue here for a computational framework of
modeling that seems to us the only currently known type of modeling available
in Science able to capture aspects of the nature of time required to better
model and understand real phenomena.
"
1662,Towards Approximate Mobile Computing,"  Mobile computing is one of the main drivers of innovation, yet the future
growth of mobile computing capabilities remains critically threatened by
hardware constraints, such as the already extremely dense transistor packing
and limited battery capacity. The breakdown of Dennard scaling and stagnating
energy storage improvements further amplify these threats. However, the
computational burden we put on our mobile devices is not always justified. In a
myriad of situations the result of a computation is further manipulated,
interpreted, and finally acted upon. This allows for the computation to be
relaxed, so that the result is calculated with ""good enough"", not perfect
accuracy. For example, results of a Web search may be perfectly acceptable even
if the order of the last few listed items is shuffled, as an end user decides
which of the available links to follow. Similarly, the quality of a
voice-over-IP call may be acceptable, despite being imperfect, as long as the
two involved parties can clearly understand each other. This novel way of
thinking about computation is termed Approximate Computing (AC) and promises to
reduce resource usage, while ensuring that satisfactory performance is
delivered to end-users. AC is already experimented with on various levels of
desktop computer architecture, from the hardware level where incorrect adders
have been designed to sacrifice result correctness for reduced energy
consumption, to compiler-level optimisations that omit certain lines of code to
speed up video encoding. AC is yet to be attempted on mobile devices and in
this article we examine the potential benefits of mobile AC and present an
overview of AC techniques applicable in the mobile domain.
"
1663,"Detecting Multiple Communities Using Quantum Annealing on the D-Wave
  System","  A very important problem in combinatorial optimization is partitioning a
network into communities of densely connected nodes; where the connectivity
between nodes inside a particular community is large compared to the
connectivity between nodes belonging to different ones. This problem is known
as community detection, and has become very important in various fields of
science including chemistry, biology and social sciences. The problem of
community detection is a twofold problem that consists of determining the
number of communities and, at the same time, finding those communities. This
drastically increases the solution space for heuristics to work on, compared to
traditional graph partitioning problems. In many of the scientific domains in
which graphs are used, there is the need to have the ability to partition a
graph into communities with the ``highest quality'' possible since the presence
of even small isolated communities can become crucial to explain a particular
phenomenon. We have explored community detection using the power of quantum
annealers, and in particular the D-Wave 2X and 2000Q machines. It turns out
that the problem of detecting at most two communities naturally fits into the
architecture of a quantum annealer with almost no need of reformulation. This
paper addresses a systematic study of detecting two or more communities in a
network using a quantum annealer.
"
1664,"Information Operations Recognition: from Nonlinear Analysis to
  Decision-making","  The book is dedicated to the issues of information operations recognition
based on analysis of information space, particularly, web-resources, social
networks, and blogs. In this context, open source intelligence technology
(OSINT) solves the problem of initial analysis of modern-time information
flows. The book provides a detailed description of mathematical principles of
information operations recognition, based on mathematical statistics, nonlinear
dynamics, complex networks theory, information and mathematical modeling,
sociology. A separate chapter covers the applications of approaches from expert
estimation theory and decision-making support to information operation
recognition. The book is addressed to a broad circle of specialists from
information technology and security domains.
"
1665,On Double-Sided QR-Codes,"  Due to the widespread adoption of the smart mobile devices, QR codes have
become one of the most-known types of 2D codes around the world. However, the
data capacity properties of modern QR codes are still not perfect. To address
this issue, in this paper, we propose a novel approach to make double-sided QR
codes, which could carry two different messages in a straight and mirrored
position. To facilitate the process of creation of such codes we propose two
methods of their construction: the brute-force method and the analytic
solution.
"
1666,"Realize special instructions on clustering VLIW DSP:
  multiplication-accumulation instruction","  BWDSP is a 32bit static scalar digital signal processor with VLIW and SIMD
features, which is designed for high-performance computing. Associated special
instructions are designed for its special architecture and application
scenarios. However, the existing compilation framework doesn't meet these
special instructions. Therefore, in the context of traditional Open64 compiler,
proposed a special instruction algorithm. Through this algorithm implements the
multiplication-accumulation operation with BWDSP structure, to improve the
performance of algorithms with multiply-accumulate requirements. Experimental
results show that the algorithm, which can make an maximum of 8.85 speedup on
BWDSP.
"
1667,Monitorology the art of observing the world,"  In the age of ever increasing demand for big data and data analytics, a
question of collecting the data becomes fundamental. What and how to collect
the data is essential as it has direct impact on decision making, system
operation and control. Specifically, we focus on the art of observing the world
by electronic devices such as sensors and meters that, in general, we call
monitors. We define five challenges to ensure effective and efficient
monitoring that still need a lot of research. Additionally, we illustrate each
challenge by example. Since reliance on big data and data analytics is
continuously increasing, these challenges will become ever more relevant to
save the world from flood of meaningless, dumb data, leading frequently to
false conclusions and wrong decisions whose impact may range from a minor
inconvenience to major disasters and even loss of lives.
"
1668,"Optimal Clustering of Energy Consumers based on Entropy of the
  Correlation Matrix between Clusters","  Increased deployment of residential smart meters has made it possible to
record energy consumption data on short intervals. These data, if used
efficiently, carry valuable information for managing power demand and
increasing energy consumption efficiency. However, analyzing smart meter data
of millions of customers in a timely manner is quite challenging. An efficient
way to analyze these data is to first identify clusters of customers, and then
focus on analyzing these clusters. Deciding on the optimal number of clusters
is a challenging task. In this manuscript, we propose a metric to efficiently
find the optimal number of clusters. A genetic algorithm based feature
selection is used to reduce the number of features, which are then fed into
self-organizing maps for clustering. We apply the proposed clustering technique
on two electricity consumption datasets from Victoria, Australia and Ireland.
The numerical simulations reveal effectiveness of the proposed method in
finding the optimal clusters.
"
1669,"Research on the pixel-based and object-oriented methods of urban feature
  extraction with GF-2 remote-sensing images","  During the rapid urbanization construction of China, acquisition of urban
geographic information and timely data updating are important and fundamental
tasks for the refined management of cities. With the development of domestic
remote sensing technology, the application of Gaofen-2 (GF-2) high-resolution
remote sensing images can greatly improve the accuracy of information
extraction. This paper introduces an approach using object-oriented
classification methods for urban feature extraction based on GF-2 satellite
data. A combination of spectral, spatial attributes and membership functions
was employed for mapping the urban features of Qinhuai District, Nanjing. The
data preprocessing is carried out by ENVI software, and the subsequent data is
exported into the eCognition software for object-oriented classification and
extraction of urban feature information. Finally, the obtained raster image
classification results are vectorized using the ARCGIS software, and the vector
graphics are stored in the library, which can be used for further analysis and
modeling. Accuracy assessment was performed using ground truth data acquired by
visual interpretation and from other reliable secondary data sources. Compared
with the result of pixel-based supervised (neural net) classification, the
developed object-oriented method can significantly improve extraction accuracy,
and after manual interpretation, an overall accuracy of 95.44% can be achieved,
with a Kappa coefficient of 0.9405, which objectively confirmed the superiority
of the object-oriented method and the feasibility of the utilization of GF-2
satellite data.
"
1670,"Pragmatic inference and visual abstraction enable contextual flexibility
  during visual communication","  Visual modes of communication are ubiquitous in modern life --- from maps to
data plots to political cartoons. Here we investigate drawing, the most basic
form of visual communication. Participants were paired in an online environment
to play a drawing-based reference game. On each trial, both participants were
shown the same four objects, but in different locations. The sketcher's goal
was to draw one of these objects so that the viewer could select it from the
array. On `close' trials, objects belonged to the same basic-level category,
whereas on `far' trials objects belonged to different categories. We found that
people exploited shared information to efficiently communicate about the target
object: on far trials, sketchers achieved high recognition accuracy while
applying fewer strokes, using less ink, and spending less time on their
drawings than on close trials. We hypothesized that humans succeed in this task
by recruiting two core faculties: visual abstraction, the ability to perceive
the correspondence between an object and a drawing of it; and pragmatic
inference, the ability to judge what information would help a viewer
distinguish the target from distractors. To evaluate this hypothesis, we
developed a computational model of the sketcher that embodied both faculties,
instantiated as a deep convolutional neural network nested within a
probabilistic program. We found that this model fit human data well and
outperformed lesioned variants. Together, this work provides the first
algorithmically explicit theory of how visual perception and social cognition
jointly support contextual flexibility in visual communication.
"
1671,Hyperspectral Calibration of Art: Acquisition and Calibration Workflows,"  Hyperspectral imaging has become an increasingly used tool in the analysis of
works of art. However, the quality of the acquired data and the processing of
that data to produce accurate and reproducible spectral image cubes can be a
challenge to many cultural heritage users. The calibration of data that is both
spectrally and spatially accurate is an essential step in order to obtain
useful and relevant results from hyperspectral imaging. Data that is too noisy
or inaccurate will produce sub-optimal results when used for pigment mapping,
the detection of hidden features, change detection or for quantitative spectral
documentation. To help address this, therefore, we will examine the specific
acquisition and calibration workflows necessary for works of art. These
workflows includes the key parameters that must be addressed during acquisition
and the essential steps and issues at each of the stages required during
post-processing in order to fully calibrate hyperspectral data. In addition we
will look in detail at the key issues that affect data quality and propose
practical solutions that can make significant differences to overall
hyperspectral image quality.
"
1672,"TinBiNN: Tiny Binarized Neural Network Overlay in about 5,000 4-LUTs and
  5mW","  Reduced-precision arithmetic improves the size, cost, power and performance
of neural networks in digital logic. In convolutional neural networks, the use
of 1b weights can achieve state-of-the-art error rates while eliminating
multiplication, reducing storage and improving power efficiency. The
BinaryConnect binary-weighted system, for example, achieves 9.9% error using
floating-point activations on the CIFAR-10 dataset. In this paper, we introduce
TinBiNN, a lightweight vector processor overlay for accelerating inference
computations with 1b weights and 8b activations. The overlay is very small --
it uses about 5,000 4-input LUTs and fits into a low cost iCE40 UltraPlus FPGA
from Lattice Semiconductor. To show this can be useful, we build two embedded
'person detector' systems by shrinking the original BinaryConnect network. The
first is a 10-category classifier with a 89% smaller network that runs in
1,315ms and achieves 13.6% error. The other is a 1-category classifier that is
even smaller, runs in 195ms, and has only 0.4% error. In both classifiers, the
error can be attributed entirely to training and not reduced precision.
"
1673,"On the Spectrum of Finite, Rooted Homogeneous Trees","  In this paper we study the adjacency spectrum of families of finite rooted
trees with regular branching properties. In particular, we show that in the
case of constant branching, the eigenvalues are realized as the roots of a
family of generalized Fibonacci polynomials and produce a limiting distribution
for the eigenvalues as the tree depth goes to infinity. We indicate how these
results can be extended to periodic branching patterns and also provide a
generalization to higher order simplicial complexes.
"
1674,"Numerical Algorithmic Science and Engineering within Computer Science:
  Rationale, Foundations and Organization","  A re-calibration is proposed for ""numerical analysis"" as it arises
specifically within the broader, embracing field of modern computer science
(CS). This would facilitate research into theoretical and practicable models of
real-number computation at the foundations of CS, and it would also advance the
instructional objectives of the CS field. Our approach is premised on the key
observation that the great ""watershed"" in numerical computation is much more
between finite- and infinite-dimensional numerical problems than it is between
discrete and continuous numerical problems. A revitalized discipline for
numerical computation within modern CS can more accurately be defined as
""numerical algorithmic science & engineering (NAS&E), or more compactly, as
""numerical algorithmics,"" its focus being the algorithmic solution of numerical
problems that are either discrete, or continuous over a space of finite
dimension, or a combination of the two. It is the counterpart within modern CS
of the numerical analysis discipline, whose primary focus is the algorithmic
solution of continuous, infinite-dimensional numerical problems and their
finite-dimensional approximates, and whose specialists today have largely been
repatriated to departments of mathematics. Our detailed overview of NAS&E from
the viewpoints of rationale, foundations, and organization is preceded by a
recounting of the role played by numerical analysts in the evolution of
academic departments of computer science, in order to provide background for
NAS&E and place the newly-emerging discipline within its larger historical
context.
"
1675,Substation One-Line Diagram Automatic Generation and Visualization,"  In Energy Management System (EMS) applications and many other off-line
planning and study tools, one-line diagram (OLND) of the whole system and
stations is a straightforward view for planners and operators to design,
monitor, analyze, and control the power system. Large-scale power system OLND
is usually manually developed and maintained. The work is tedious,
time-consuming and ease to make mistake. Meanwhile, the manually created
diagrams are hard to be shared among the on-line and off-line systems. To save
the time and efforts to draw and maintain OLNDs, and provide the capability to
share the OLNDs, a tool to automatically develop substation based upon Common
Information Model (CIM) standard is needed. Currently, there is no standard
rule to draw the substation OLND. Besides, the substation layouts can be
altered from the typical formats in textbooks based on factors of economy,
efficiency, engineering practice, etc. This paper presents a tool on substation
OLND automatic generation and visualization. This tool takes the substation
CIM/E model as input, then automatically computes the coordinates of all
components and generates the substation OLND based on its components attributes
and connectivity relations. Evaluation of the proposed approach is presented
using a real provincial power system. Over 95\% of substation OLNDs are
decently presented and the rest are corner cases, needing extra effort to do
specific reconfiguration.
"
1676,Was ist eine Professur fuer Kuenstliche Intelligenz?,"  The Federal Government of Germany aims to boost the research in the field of
Artificial Intelligence (AI). For instance, 100 new professorships are said to
be established. However, the white paper of the government does not answer what
an AI professorship is at all. In order to give colleagues, politicians, and
citizens an idea, we present a view that is often followed when appointing
professors for AI at German and international universities. We hope that it
will help to establish a guideline with internationally accepted measures and
thus make the public debate more informed.
"
1677,Trial of an AI: Empowering people to explore law and science challenges,"  Artificial Intelligence represents many things: a new market to conquer or a
quality label for tech companies, a threat for traditional industries, a menace
for democracy, or a blessing for our busy everyday life. The press abounds in
examples illustrating these aspects, but one should draw not hasty and
premature conclusions. The first successes in AI have been a surprise for
society at large-including researchers in the field. Today, after the initial
stupefaction, we have examples of the system reactions: traditional companies
are heavily investing in AI, social platforms are monitored during elections,
data collection is more and more regulated, etc. The resilience of an
organization (i.e. its capacity to resist to a shock) relies deeply on the
perception of its environment. Future problems have to be anticipated, while
unforeseen events occurring have to be quickly identified in order to be
mitigated as fast as possible. The author states that this clear perception
starts with a common definition of AI in terms of capacities and limits. AI
practitioners should make notions and concepts accessible to the general public
and the impacted fields (e.g. industries, law, education). It is a truism that
only law experts would have the potential to estimate IA impacts on judicial
system. However, questions remain on how to connect different kind of expertise
and what is the appropriate level of detail required for the knowledge
exchanges. And the same consideration is true for dissemination towards
society. Ultimately, society will live with decisions made by the ""experts"". It
sounds wise to involve society in the decision process rather than risking to
pay consequences later. Therefore, society also needs the key concepts to
understand AI impact on their life. This was the purpose of the trial of an IA
that took place in October 2018 at the Court of Appeal of Paris: gathering
experts from various fields to expose challenges in law and science towards a
general public.
"
1678,Anti-Turing Machine,"  The invention of CPU-centric computing paradigm was incredible breakthrough
of computer science that revolutionized our everyday life dramatically.
However, the CPU- centric paradigm is based on the Turing machine concept and,
as a result, expensive and power-hungry data transferring between the memory
and CPU core is inevitable operation. Anti-Turing machine paradigm can be based
on two fundamental principles: (1) data-centric computing, and (2)
decentralized computing. Anti-Turing machine is able to execute a special type
of programs. The commands of such program have to be addressed to the 2D or 3D
persistent memory space is able to process data in-place. This program should
not define the position or structure of data but it has to define the goal of
data processing activity. Generally speaking, it needs to consider the whole
memory space like the data transformation space. But the data placement,
particular algorithm implementation, and strategy of algorithm execution are
out of scope of the program.
"
1679,"Simulation-Based Analytics for Fabrication Quality-Associated Decision
  Support","  Automated, data-driven quality management systems, which facilitate the
transformation of data into useable information, are desired to enhance
decision-making processes. Integration of accurate, reliable, and
straightforward approaches that measure uncertainty of inspection processes are
instrumental for the successful implementation of automated, data-driven
quality management systems. This research has addressed these needs by
exploring and adapting Bayesian statistics-based approaches for fraction
nonconforming posterior distribution derivation purposes. Using these accurate
and reliable inputs, this research further develops novel, analytically-based
approaches to improve the practical function of traditional construction
fabrication quality management systems. Multiple descriptive and predictive
analytical functionalities are developed to support and augment
quality-associated decision-making processes. Multi-relational databases (e.g.,
quality management system, engineering design system, and cost management
system) from an industrial company in Edmonton, Canada, are investigated and
mapped to implement the novel system proposed. This research has contributed to
academic literature and practice by: (1) advancing decision-support systems for
construction management by developing a dynamic simulation environment that
uses real-time data to enhance simulation predictability; (2) developing
integrated analytical methods for improved modeling in fabrication
quality-associated decision making; and (3) creating reliable and interpretable
decision-support metrics for quality performance measurement, complexity
analysis, and rework cost management to reduce the data interpretation load of
practitioners and to uncover valuable knowledge and information from available
data sources.
"
1680,Trustworthy Experimentation Under Telemetry Loss,"  Failure to accurately measure the outcomes of an experiment can lead to bias
and incorrect conclusions. Online controlled experiments (aka AB tests) are
increasingly being used to make decisions to improve websites as well as mobile
and desktop applications. We argue that loss of telemetry data (during upload
or post-processing) can skew the results of experiments, leading to loss of
statistical power and inaccurate or erroneous conclusions. By systematically
investigating the causes of telemetry loss, we argue that it is not practical
to entirely eliminate it. Consequently, experimentation systems need to be
robust to its effects. Furthermore, we note that it is nontrivial to measure
the absolute level of telemetry loss in an experimentation system. In this
paper, we take a top-down approach towards solving this problem. We motivate
the impact of loss qualitatively using experiments in real applications
deployed at scale, and formalize the problem by presenting a theoretical
breakdown of the bias introduced by loss. Based on this foundation, we present
a general framework for quantitatively evaluating the impact of telemetry loss,
and present two solutions to measure the absolute levels of loss. This
framework is used by well-known applications at Microsoft, with millions of
users and billions of sessions. These general principles can be adopted by any
application to improve the overall trustworthiness of experimentation and
data-driven decision making.
"
1681,"Methodology for Designing Decision Support Systems for Visualising and
  Mitigating Supply Chain Cyber Risk from IoT Technologies","  This paper proposes a methodology for designing decision support systems for
visualising and mitigating the Internet of Things cyber risks. Digital
technologies present new cyber risk in the supply chain which are often not
visible to companies participating in the supply chains. This study
investigates how the Internet of Things cyber risks can be visualised and
mitigated in the process of designing business and supply chain strategies. The
emerging DSS methodology present new findings on how digital technologies
affect business and supply chain systems. Through epistemological analysis, the
article derives with a decision support system for visualising supply chain
cyber risk from Internet of Things digital technologies. Such methods do not
exist at present and this represents the first attempt to devise a decision
support system that would enable practitioners to develop a step by step
process for visualising, assessing and mitigating the emerging cyber risk from
IoT technologies on shared infrastructure in legacy supply chain systems.
"
1682,A Blockchain-based Educational Record Repository,"  The Blockchain technology was initially adopted to implement various
cryptocurrencies. Currently, Blockchain is foreseen as a general purpose
technology with a huge potential in many areas. Blockchain-based applications
have inherent characteristics like authenticity, immutability and consensus.
Beyond that, records stored on Blockchain ledger can be accessed any time and
from any location. Blockchain has a great potential for managing and
maintaining educational records. This paper presents a Blockchain-based
Educational Record Repository (BcER2) that manages and distributes educational
assets for academic and industry professionals. The BcER2 system allows
educational records like e-diplomas and e-certificates to be securely and
seamless transferred, shared and distributed by parties.
"
1683,Algorithms Clearly Beat Gamers at Quantum Moves. A Verification,"  The paper [S{\o}rensen et al., Nature 532] considers how human players
compare to algorithms for solving the Quantum Moves game BringHomeWater and
design new algorithms based on the intuition extracted from players. The claim
by [S{\o}rensen et al., Nature 532] is that players outperform widely used
algorithms, in particular the KASS algorithm, based on the Krotov algorithm,
and that player intuition is crucial to develop improved methods. However, as
initially discussed by D. Sels [D. Sels, Phys. Rev. A 97], a standard
Coordinate Ascent algorithm outperforms all players by a large margin. Albeit
D. Sels only compare to player solutions, the simple algorithm outperforms all
algorithms based on player solutions and Krotov, and it does so using much less
time and iterations. In this paper we elaborate on the methods discussed by D.
Sels and verify that the presented algorithm, solves the problem better than
all players and algorithms derived from player solutions in [S{\o}rensen et
al., Nature 532]. We also verify the theoretical analysis presented by D. Sels,
that gives a theoretically derived protocol that outperforms all players. We
add a comparison with gradient ascent or GRAPE. Starting from uniform random
values, GRAPE outperforms all players by a large margin. GRAPE works at least
as well as the methods from [S{\o}rensen et al., Nature 532] initialized with
player solutions. A standard analysis of the results from GRAPE provides a
starting point for GRAPE, that outperform all algorithms from [S{\o}rensen et
al., Nature 532]. We compare with a basic Krotov algorithm, and get results
similar to GRAPE, clearly outperforming players and the KASS algorithm. These
experiments verify and underline the result in [D. Sels, Phys. Rev. A 97] that
the conclusions from [S{\o}rensen et al., Nature 532] regarding algorithms are
untenable. In fact the opposite conclusions are true.
"
1684,"Grounds for trust: Essential Epistemic Opacity and Computational
  Reliabilism","  Several philosophical issues in connection with computer simulations rely on
the assumption that results of simulations are trustworthy. Examples of these
include the debate on the experimental role of computer simulations
\cite{Parker2009, Morrison2009}, the nature of computer data
\cite{Barberousse2013, Humphreys2013}, and the explanatory power of computer
simulations \cite{Krohs2008, Duran2017}. The aim of this article is to show
that these authors are right in assuming that the results of computer
simulations are to be trusted when computer simulations are reliable processes.
After a short reconstruction of the problem of \textit{epistemic opacity}, the
article elaborates extensively on \textit{computational reliabilism}, a
specified form of process reliabilism with computer simulations located at the
center. The article ends with a discussion of four sources for computational
reliabilism, namely, verification and validation, robustness analysis for
computer simulations, a history of (un)successful implementations, and the role
of expert knowledge in simulations.
"
1685,"Computer simulations in science and engineering - Concepts - Practices -
  Perspectives","  The ubiquitous presence of computer simulations in all kinds of research
areas evidence their role as the new driving force for the advancement of
science and engineering research. Nothing seems to escape the image of success
that computer simulations project onto the research community and the general
public. One simple way to illustrate this consists of asking ourselves how
would contemporary science and engineering look like without the use of
computer simulations. The answer would certainly diverge from the current image
we have of scientific and engineering research.
  As much as computer simulations are successful, they are also methods that
fail in their purpose of inquiring about the world; and as much as researchers
make use of them, computer simulations raise important questions that are at
the heart of contemporary science and engineering practice. In this respect,
computer simulations make a fantastic subject of research for the natural
sciences, the social sciences, engineering and, as in our case, also for
philosophy. Studies on computer simulations touch upon many different facets of
scientific and engineering research and evoke philosophically inclined
questions of interpretation with close ties to problems in experimental
settings and engineering applications (...)
"
1686,"Varying the explanatory span: scientific explanation for computer
  simulations","  This article aims to develop a new account of scientific explanation for
computer simulations. To this end, two questions are answered: what is the
explanatory relation for computer simulations? and what kind of epistemic gain
should be expected? For several reasons tailored to the benefits and needs of
computer simulations, these questions are better answered within the
unificationist model of scientific explanation. Unlike previous efforts in the
literature, I submit that the explanatory relation is between the simulation
model and the results of the simulation. I also argue that our epistemic gain
goes beyond the unificationist account, encompassing a practical dimension as
well.
"
1687,Teleporting digital images,"  During the last 25 years the scientific community has coexisted with the most
fascinating protocol due to Quantum Physics: quantum teleportation (QTele),
which would have been impossible if quantum entanglement, so questioned by
Einstein, did not exist. In this work, a complete architecture for the
teleportation of Computational Basis States (CBS) is presented. Such CBS will
represent each of the possible 24 classical bits commonly used to encode every
pixel of a 3-color-channel-image (red-green-blue, or cyan-yellow-magenta). For
this purpose, a couple of interfaces: classical-to-quantum (Cl2Qu) and
quantum-to-classical (Qu2Cl) are presented with two versions of the
teleportation protocol: standard and simplified.
"
1688,"MoA Interpretation of the Iterative Conjugate Gradient Method with Psi
  Reduction - A Tutorial to teach the Mathematically literate in Linear and
  Tensor Algebra: Part I","  It is often difficult to learn new mathematics semantically and
syntactically, even when there are similarities in the words and meaning when
discussed aloud. The goal of this document is to facilitate learning through
explanations and definitions relating our common mathematical knowledge and
highlighting what is new. It is meant to be a working document that will evolve
based on feedback from target audiences, those mathematically literate in
linear and tensor algebra, those that want to learn MoA, Psi Calculus, and its
uses, those that want and need the ability to prove a design, either in
hardware or software through the ONF, Operational Normal Form, and those
wanting to exploit all resources optimally, especially when Tensor Algebra,
i.e. algorithms foundational to their application,are needed: Knowledge
Representation, Machine Learning, Signal Processing, AI, HPC, etc.
"
1689,"Combining Conformance Checking and Classification of XES Log Data for
  the Manufacturing Domain","  Currently, data collection on the shop floor is based on individual resources
such as machines, robots, and Autonomous Guided Vehicles (AGVs). There is a gap
between this approach and manufacturing orchestration software that supervises
the process of creating single products and controls the ressources'
interactions. This creates the need to save resource-based data streams in
databases, clean it, and then re-contextualize it, i.e., by connecting it to
orders, batches, and single products. Looking at this data from a
process-oriented analysis point of view enables new analysis prospects. This
paper utilises these prospects in an experimental way by creating BPMN models
for the manufacturing of two real-world products: (1) a low volume, high
complexity lower-housing for a gas-turbine and (2) a high volume, low
complexity, small tolerance valve lifter for a gas turbine. In contrast to the
resource-based data collection, 30+ values are modeled into the BPMN models and
enacted by a workflow engine, creating execution logs in the XES standard
format. Conformance checks are carried out and interpreted for both scenarios
and it is shown how existing classification and clustering techniques can be
applied on the collected data in order to predict good and bad parts, ex-post
and potentially at run-time.
"
1690,"Evaluation of IoT-Based Computational Intelligence Tools for DNA
  Sequence Analysis in Bioinformatics","  In contemporary age, Computational Intelligence (CI) performs an essential
role in the interpretation of big biological data considering that it could
provide all of the molecular biology and DNA sequencing computations. For this
purpose, many researchers have attempted to implement different tools in this
field and have competed aggressively. Hence, determining the best of them among
the enormous number of available tools is not an easy task, selecting the one
which accomplishes big data in the concise time and with no error can
significantly improve the scientist's contribution in the bioinformatics field.
This study uses different analysis and methods such as Fuzzy, Dempster-Shafer,
Murphy and Entropy Shannon to provide the most significant and reliable
evaluation of IoT-based computational intelligence tools for DNA sequence
analysis. The outcomes of this study can be advantageous to the bioinformatics
community, researchers and experts in big biological data.
"
1691,"The Rise of Internet of Things (IoT) in Big Healthcare Data: Review and
  Open research Issues","  Health is one of the sustainable development areas in all of the countries.
Internet of Things has a variety of use in this sector which was not studied
yet. The aim of this research is to prioritize IoT usage in the healthcare
sector to achieve sustainable development. The study is an applied descriptive
research according to data collection. As per the research methodology which is
FAHP, it is a single cross sectional survey research. After data collection,
the agreed paired comparison matrices, allocated to weighted criteria and the
priority of IoT usage were determined. Based on the research findings, the two
criteria of Economic Prosperity and Quality of Life achieved the highest
priority for IoT sustainable development in the healthcare sector. Moreover,
the top priorities for IoT in the area of health, according to the usage, were
identified as Ultraviolet Radiation, Dental Health and Fall Detection.
"
1692,"Power and Thermal Analysis of Commercial Mobile Platforms: Experiments
  and Case Studies","  State-of-the-art mobile processors can deliver fast response time and high
throughput to maximize the user experience. However, high performance comes at
the expense of larger power density, which leads to higher skin temperatures.
Since this can degrade the user experience, there is a strong need for power
consumption and thermal analysis in mobile processors. In this paper, we first
perform experiments on the Nexus 6P phone to study the power, performance and
thermal behavior of modern smartphones. Using the insight from these
experiments, we propose a control algorithm that throttles select applications
without affecting other apps. We demonstrate our governor on the Exynos 5422
processor employed in the Odroid-XU3 board.
"
1693,"A Review on Energy Consumption Optimization Techniques in IoT Based
  Smart Building Environments","  In recent years, due to the unnecessary wastage of electrical energy in
residential buildings, the requirement of energy optimization and user comfort
has gained vital importance. In the literature, various techniques have been
proposed addressing the energy optimization problem. The goal of each technique
was to maintain a balance between user comfort and energy requirements such
that the user can achieve the desired comfort level with the minimum amount of
energy consumption. Researchers have addressed the issue with the help of
different optimization algorithms and variations in the parameters to reduce
energy consumption. To the best of our knowledge, this problem is not solved
yet due to its challenging nature. The gap in the literature is due to the
advancements in the technology and drawbacks of the optimization algorithms and
the introduction of different new optimization algorithms. Further, many newly
proposed optimization algorithms which have produced better accuracy on the
benchmark instances but have not been applied yet for the optimization of
energy consumption in smart homes. In this paper, we have carried out a
detailed literature review of the techniques used for the optimization of
energy consumption and scheduling in smart homes. The detailed discussion has
been carried out on different factors contributing towards thermal comfort,
visual comfort, and air quality comfort. We have also reviewed the fog and edge
computing techniques used in smart homes.
"
1694,"Clifford algebras, Spin groups and qubit trees","  Representations of Spin groups and Clifford algebras derived from structure
of qubit trees are introduced in this work. For ternary trees the construction
is more general and reduction to binary trees is formally defined by deleting
of superfluous branches. Usual Jordan-Wigner construction also may be formally
obtained in such approach by bringing the process up to trivial qubit chain
(""trunk""). The methods can be also used for effective simulations of some
quantum circuits corresponding to the binary tree structure. Modeling of more
general qubit trees and relation with mapping used in Bravyi-Kitaev
transformation are also briefly outlined.
"
1695,"New-Generation Design-Technology Co-Optimization (DTCO):
  Machine-Learning Assisted Modeling Framework","  In this paper, we propose a machine-learning assisted modeling framework in
design-technology co-optimization (DTCO) flow. Neural network (NN) based
surrogate model is used as an alternative of compact model of new devices
without prior knowledge of device physics to predict device and circuit
electrical characteristics. This modeling framework is demonstrated and
verified in FinFET with high predicted accuracy in device and circuit level.
Details about the data handling and prediction results are discussed. Moreover,
same framework is applied to new mechanism device tunnel FET (TFET) to predict
device and circuit characteristics. This work provides new modeling method for
DTCO flow.
"
1696,"Appliance Event Detection -- A Multivariate, Supervised Classification
  Approach","  Non-intrusive load monitoring (NILM) is a modern and still expanding
technique, helping to understand fundamental energy consumption patterns and
appliance characteristics. Appliance event detection is an elementary step in
the NILM pipeline. Unfortunately, several types of appliances (e.g., switching
mode power supply (SMPS) or multi-state) are known to challenge
state-of-the-art event detection systems due to their noisy consumption
profiles. Classical rule-based event detection system become infeasible and
complex for these appliances. By stepping away from distinct event definitions,
we can learn from a consumer-configured event model to differentiate between
relevant and irrelevant event transients.
  We introduce a boosting oriented adaptive training, that uses false positives
from the initial training area to reduce the number of false positives on the
test area substantially. The results show a false positive decrease by more
than a factor of eight on a dataset that has a strong focus on SMPS-driven
appliances. To obtain a stable event detection system, we applied several
experiments on different parameters to measure its performance. These
experiments include the evaluation of six event features from the spectral and
time domain, different types of feature space normalization to eliminate
undesired feature weighting, the conventional and adaptive training, and two
common classifiers with its optimal parameter settings. The evaluations are
performed on two publicly available energy datasets with high sampling rates:
BLUED and BLOND-50.
"
1697,"A Method for Expressing and Displaying the Vehicle Behavior Distribution
  in Maintenance Work Zones","  Maintenance work zones on the road network have impacts on the normal
travelling of vehicles, which increase the risk of traffic accidents. The
traffic characteristic analysis in maintenance work zones is a basis for
maintenance work zone related research such as layout design, traffic control
and safety assessment. Due to the difficulty in vehicle microscopic behaviour
data acquisition, traditional traffic characteristic analysis mainly focuses on
macroscopic characteristics. With the development of data acquisition
technology, it becomes much easier to obtain a large amount of microscopic
behaviour data nowadays, which lays a good foundation for analysing the traffic
characteristics from a new point of view. This paper puts forward a method for
expressing and displaying the vehicle behaviour distribution in maintenance
work zones. Using portable vehicle microscopic behaviour data acquisition
devices, lots of data can be obtained. Based on this data, an endpoint
detection technology is used to automatically extract the segments in behaviour
data with violent fluctuations, which are segments where vehicles take
behaviours such as acceleration or turning. Using the support vector machine
classification method, the specific types of behaviours of the segments
extracted can be identified, and together with a data combination method, a
total of ten types of behaviours can be identified. Then the kernel density
analysis is used to cluster different types of behaviours of all passing
vehicles to show the distribution on maps. By this method, how vehicles travel
through maintenance work zones, and how different vehicle behaviours distribute
in maintenance work zones can be displayed intuitively on maps, which is a
novel traffic characteristic and can shed light to maintenance work zone
related researches such as safety assessment and design method.
"
1698,Smart Laptop Bag with Machine Learning for Activity Recognition,"  In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.
"
1699,PowerNet: Neural Power Demand Forecasting in Smart Grid,"  Power demand forecasting is a critical task for achieving efficiency and
reliability in power grid operation. Accurate forecasting allows grid operators
to better maintain the balance of supply and demand as well as to optimize
operational cost for generation and transmission. This article proposes a novel
neural network architecture PowerNet, which can incorporate multiple
heterogeneous features, such as historical energy consumption data, weather
data, and calendar information, for the power demand forecasting task. Compared
to two recent works based on Gradient Boosting Tree (GBT) and Support Vector
Regression (SVR), PowerNet demonstrates a decrease of 33.3% and 14.3% in
forecasting error, respectively. We further provide empirical results the two
operational considerations that are crucial when using PowerNet in practice,
i.e., how far in the future the model can forecast with a decent accuracy and
how often we should re-train the forecasting model to retain its modeling
capability. Finally, we briefly discuss a multilayer anomaly detection approach
based on PowerNet.
"
1700,Galaxy Learning -- A Position Paper,"  The recent rapid development of artificial intelligence (AI, mainly driven by
machine learning research, especially deep learning) has achieved phenomenal
success in various applications. However, to further apply AI technologies in
real-world context, several significant issues regarding the AI ecosystem
should be addressed. We identify the main issues as data privacy, ownership,
and exchange, which are difficult to be solved with the current centralized
paradigm of machine learning training methodology. As a result, we propose a
novel model training paradigm based on blockchain, named Galaxy Learning, which
aims to train a model with distributed data and to reserve the data ownership
for their owners. In this new paradigm, encrypted models are moved around
instead, and are federated once trained. Model training, as well as the
communication, is achieved with blockchain and its smart contracts. Pricing of
training data is determined by its contribution, and therefore it is not about
the exchange of data ownership. In this position paper, we describe the
motivation, paradigm, design, and challenges as well as opportunities of Galaxy
Learning.
"
1701,"Charging control of electric vehicles using contextual bandits
  considering the electrical distribution grid","  With the proliferation of electric vehicles, the electrical distribution
grids are more prone to overloads. In this paper, we study an intelligent
pricing and power control mechanism based on contextual bandits to provide
incentives for distributing charging load and preventing network failure. The
presented work combines the microscopic mobility simulator SUMO with electric
network simulator SIMONA and thus produces reliable electrical distribution
load values. Our experiments are carefully conducted under realistic conditions
and reveal that conditional bandit learning outperforms context-free
reinforcement learning algorithms and our approach is suitable for the given
problem. As reinforcement learning algorithms can be adapted rapidly to include
new information we assume these to be suitable as part of a holistic traffic
control scenario.
"
1702,Programmable Logic Arrays,"  Programmable logic arrays (PLAs) are traditional digital electronic devices.
A PLA is a simple programmable logic device (SPLD) used to implement
combinational logic circuits. A PLA has a set of programmable AND gates, which
link to a set of programmable OR gates to produce an output. The AND-OR layout
of a PLA allows for implementing logic functions that are in a sum-of-products
form. PLAs are available in the market in different types. PLAs could be stand
alone chips, or parts of bigger processing systems. Stand alone PLAs are
available as mask programmable (MPLAs) and field programmable (FPLAs) devices.
The attractions of PLAs that brought them to mainstream engineers include their
simplicity, relatively small circuit area, predictable propagation delay, and
ease of development. The powerful-but-simple property brought PLAs to rapid
prototyping, synthesis, design optimization techniques, embedded systems,
traditional computer systems, hybrid high-performance computing systems, etc.
Indeed, there has been renewable interests in working with the simple AND-to-OR
PLAs.
"
1703,Logic Design,"  Electronic circuits can be separated into two groups, digital and analog
circuits. Analog circuits operate on analog quantities that are continuous in
value, whereas digital circuits operate on digital quantities that are discrete
in value and limited in precision. In practice, most digital systems contain
combinational circuits along with memory; these systems are known as sequential
circuits. Sequential circuits are of two types: synchronous and asynchronous.
In a synchronous sequential circuit, a clock signal is used at discrete
instants of time to synchronize desired operations. Asynchronous sequential
circuits do not require synchronizing clock pulses; however, the completion of
an operation signals the start of the next operation in sequence. The basic
logic design steps are generally identical for sequential and combinational
circuits; these are specification, formulation, optimization, and the
implementation of the optimized equations using a suitable hardware technology.
The differences between sequential and combinational design steps appear in the
details of each step. The minimization (optimization) techniques used in logic
design range from simple (manual) to complex (automated). An example of manual
optimization methods is the Karnough map (K-map). Indeed, hardware
implementation technology has been growing faster than the ability of designers
to produce hardware designs. Hence, there has been a growing interest in
developing techniques and tools that facilitate the process of logic design.
"
1704,High-level Synthesis,"  Hardware synthesis is a general term used to refer to the processes involved
in automatically generating a hardware design from its specification.
High-level synthesis (HLS) could be defined as the translation from a
behavioral description of the intended hardware circuit into a structural
description similar to the compilation of programming languages (such as C and
Pascal into assembly language. The chained synthesis tasks at each level of the
design process include system synthesis, register-transfer synthesis, logic
synthesis, and circuit synthesis. The development of hardware solutions for
complex applications is no more a complicated task with the emergence of
various HLS tools. Many areas of application have benefited from the modern
advances in hardware design, such as automotive and aerospace industries,
computer graphics, signal and image processing, security, complex simulations
like molecular modeling, and DND matching. The field of HLS is continuing its
rapid growth to facilitate the creation of hardware and to blur more and more
the border separating the processes of designing hardware and software.
"
1705,Analytical review of medical mobile diagnostic systems,"  This article analyzes the mobile medical diagnostic systems and compare them
with the proposed HealthTracker system based on smartwatch Apple Watch. Before
the development of the system HealthTracker, there was conducted a review and
analysis of existing similar systems to identify common and distinctive
features of the future system. This analysis will improve HealthTracker system,
based on the strengths and weaknesses of existing systems and help identify and
justify the key benefits and unique system HealthTracker. The main goal is to
provide a system HealthTracker convenient way to interact with the patient the
doctor based on the vital signs of the patient. Apple Watch is an excellent
watch presented in 2014 that has the capacity to collect and compile data on
the health of the user and can be used for medical purposes.
"
1706,Image-based reconstruction for the impact problems by using DPNNs,"  With the improvement of the pattern recognition and feature extraction of
Deep Neural Networks (DPNNs), image-based design and optimization have been
widely used in multidisciplinary researches. Recently, a Reconstructive Neural
Network (ReConNN) has been proposed to obtain an image-based model from an
analysis-based model [1, 2], and a steady-state heat transfer of a heat sink
has been successfully reconstructed. Commonly, this method is suitable to
handle stable-state problems. However, it has difficulties handling nonlinear
transient impact problems, due to the bottlenecks of the Deep Neural Network
(DPNN). For example, nonlinear transient problems make it difficult for the
Generative Adversarial Network (GAN) to generate various reasonable images.
Therefore, in this study, an improved ReConNN method is proposed to address the
mentioned weaknesses. Time-dependent ordered images can be generated.
Furthermore, the improved method is successfully applied in impact simulation
case and engineering experiment. Through the experiments, comparisons and
analyses, the improved method is demonstrated to outperform the former one in
terms of its accuracy, efficiency and costs.
"
1707,"Governance by Glass-Box: Implementing Transparent Moral Bounds for AI
  Behaviour","  Artificial Intelligence (AI) applications are being used to predict and
assess behaviour in multiple domains, such as criminal justice and consumer
finance, which directly affect human well-being. However, if AI is to improve
people's lives, then people must be able to trust AI, which means being able to
understand what the system is doing and why. Even though transparency is often
seen as the requirement in this case, realistically it might not always be
possible or desirable, whereas the need to ensure that the system operates
within set moral bounds remains. In this paper, we present an approach to
evaluate the moral bounds of an AI system based on the monitoring of its inputs
and outputs. We place a ""glass box"" around the system by mapping moral values
into explicit verifiable norms that constrain inputs and outputs, in such a way
that if these remain within the box we can guarantee that the system adheres to
the value. The focus on inputs and outputs allows for the verification and
comparison of vastly different intelligent systems; from deep neural networks
to agent-based systems. The explicit transformation of abstract moral values
into concrete norms brings great benefits in terms of explainability;
stakeholders know exactly how the system is interpreting and employing relevant
abstract moral human values and calibrate their trust accordingly. Moreover, by
operating at a higher level we can check the compliance of the system with
different interpretations of the same value. These advantages will have an
impact on the well-being of AI systems users at large, building their trust and
providing them with concrete knowledge on how systems adhere to moral values.
"
1708,Simulation Typology and Termination Risks,"  The goal of the article is to explore what is the most probable type of
simulation in which humanity lives (if any) and how this affects simulation
termination risks. We firstly explore the question of what kind of simulation
in which humanity is most likely located based on pure theoretical reasoning.
We suggest a new patch to the classical simulation argument, showing that we
are likely simulated not by our own descendants, but by alien civilizations.
Based on this, we provide classification of different possible simulations and
we find that simpler, less expensive and one-person-centered simulations,
resurrectional simulations, or simulations of the first artificial general
intelligence's (AGI's) origin (singularity simulations) should dominate. Also,
simulations which simulate the 21st century and global catastrophic risks are
probable. We then explore whether the simulation could collapse or be
terminated. Most simulations must be terminated after they model the
singularity or after they model a global catastrophe before the singularity.
Undeniably observed glitches, but not philosophical speculations could result
in simulation termination. The simulation could collapse if it is overwhelmed
by glitches. The Doomsday Argument in simulations implies termination soon. We
conclude that all types of the most probable simulations except resurrectional
simulations are prone to termination risks in a relatively short time frame of
hundreds of years or less from now.
"
1709,"Inquiry of P-reduction in Cook's 1971 Paper -- from Oracle machine to
  Turing machine","  In this paper, we inquire the key concept P-reduction in Cook's theorem and
reveal that there exists the fallacy of definition in P-reduction caused by the
disguised displacement of NDTM from Oracle machine to Turing machine. The
definition or derivation of P-reduction is essentially equivalent to Turing's
computability. Whether NP problems might been reduced to logical forms
(tautology or SAT) or NP problems might been reduced each other, they have not
been really proven in Cook's 1971 paper.
"
1710,"Transfer and Online Reinforcement Learning in STT-MRAM Based Embedded
  Systems for Autonomous Drones","  In this paper we present an algorithm-hardware codesign for camera-based
autonomous flight in small drones. We show that the large write-latency and
write-energy for nonvolatile memory (NVM) based embedded systems makes them
unsuitable for real-time reinforcement learning (RL). We address this by
performing transfer learning (TL) on metaenvironments and RL on the last few
layers of a deep convolutional network. While the NVM stores the meta-model
from TL, an on-die SRAM stores the weights of the last few layers. Thus all the
real-time updates via RL are carried out on the SRAM arrays. This provides us
with a practical platform with comparable performance as end-to-end RL and
83.4% lower energy per image frame
"
1711,"Quartierstrom -- Implementation of a real world prosumer centric local
  energy market in Walenstadt, Switzerland","  Prosumers in many regions are facing reduced feed-in tariffs and currently
have no possibility to influence the level of remuneration for the locally
produced solar energy. Peer-to-peer communities may offer an alternative to the
feed-in tariff model by enabling prosumers to directly sell their solar energy
to local consumers (possibly at a rate that is beneficial for both consumer and
prosumer). The Quartierstrom project investigates a transactional energy system
that manages the exchange and remuneration of electricity between consumers,
prosumers and the local electric grid provider in the absence of
intermediaries. This whitepaper describes the prototypical real-world system
being implemented in the town of Walenstadt, Switzerland, with 37 participating
households. The community members of this pilot project pay a reduced tariff
for grid usage if the electricity produced by a prosumer is sold to another
community member, which is located on the same voltage or grid level downstream
a substation1. Such a tariff structure incentivizes local balancing, i.e.
locally produced energy can be consumed locally whenever possible to avoid
costs from higher grid levels. The blockchain is a novel technology suitable to
log the produced and consumed units of energy within a community, making it
possible to implement market places. In those marketplaces, both prosumers and
consumers can indicate a price at which they are willing to sell / buy locally
produced solar energy without the intermediation of a utility. The key goals of
this project are the assessment of A) the technical, economical and ecological
feasibility of a blockchain-based community energy system regarding local
utilization of solar energy, grid quality and energy efficiency and B)
resulting dynamics regarding local market prices and user acceptance.
"
1712,Quantifying Complexity,"  Complexity remains one of the central challenges in science and technology.
Although several approaches at defining and/or quantifying complexity have been
proposed, at some point each of them seems to run into intrinsic limitations.
Two are the main objectives of the present work: (i) to review some of the main
approaches to complexity; and (ii) to suggest a cost-based approach that, to a
great extent, can be understood as an integration of the several facets of
complexity. More specifically, it is poised that complexity, an inherently
relative and subjective concept, can be summarized as the cost of developing a
model, plus the cost of its respective operation. The proposal is illustrated
respectively to several applications examples, including a real-data base
situation.
"
1713,"Digital Availability of Product Information for Collaborative
  Engineering of Spacecraft","  In this paper, we introduce a system to collect product information from
manufacturers and make it available in tools that are used for concurrent
design of spacecraft. The planning of a spacecraft needs experts from different
disciplines, like propulsion, power, and thermal. Since these different
disciplines rely on each other there is a high need for communication between
them, which is often realized by a Model-Based Systems Engineering (MBSE)
process and corresponding tools. We show by comparison that the product
information provided by manufacturers often does not match the information
needed by MBSE tools on a syntactic or semantic level. The information from
manufacturers is also currently not available in machine-readable formats.
Afterwards, we present a prototype of a system that makes product information
from manufacturers directly available in MBSE tools, in a machine-readable way.
"
1714,Definitively Identifying an Inherent Limitation to Actual Cognition,"  A century ago, discoveries of a serious kind of logical error made separately
by several leading mathematicians led to acceptance of a sharply enhanced
standard for rigor within what ultimately became the foundation for Computer
Science. By 1931, Godel had obtained a definitive and remarkable result: an
inherent limitation to that foundation. The resulting limitation is not
applicable to actual human cognition, to even the smallest extent, unless both
of these extremely brittle assumptions hold: humans are infallible reasoners
and reason solely via formal inference rules. Both assumptions are contradicted
by empirical data from well-known Cognitive Science experiments. This article
investigates how a novel multi-part methodology recasts computability theory
within Computer Science to obtain a definitive limitation whose application to
human cognition avoids assumptions contradicting empirical data. The limitation
applies to individual humans, to finite sets of humans, and more generally to
any real-world entity.
"
1715,"Customizing Pareto Simulated Annealing for Multi-objective Optimization
  of Control Cabinet Layout","  Determining the optimal location of control cabinet components requires the
exploration of a large configuration space. For real-world control cabinets it
is impractical to evaluate all possible cabinet configurations. Therefore, we
need to apply methods for intelligent exploration of cabinet configuration
space that enable to find a near-optimal configuration without evaluation of
all possible configurations. In this paper, we describe an approach for
multi-objective optimization of control cabinet layout that is based on Pareto
Simulated Annealing. Optimization aims at minimizing the total wire length used
for interconnection of components and the heat convection within the cabinet.
We simulate heat convection to study the warm air flow within the control
cabinet and determine the optimal position of components that generate heat
during the operation. We evaluate and demonstrate the effectiveness of our
approach empirically for various control cabinet sizes and usage scenarios.
"
1716,"FPScreen: A Rapid Similarity Search Tool for Massive Molecular Library
  Based on Molecular Fingerprint Comparison","  We designed a fast similarity search engine for large molecular libraries:
FPScreen. We downloaded 100 million molecules' structure files in PubChem with
SDF extension, then applied a computational chemistry tool RDKit to convert
each structure file into one line of text in MACCS format and stored them in a
text file as our molecule library. The similarity search engine compares the
similarity while traversing the 166-bit strings in the library file line by
line. FPScreen can complete similarity search through 100 million entries in
our molecule library within one hour. That is very fast as a biology
computation tool. Additionally, we divided our library into several strides for
parallel processing. FPScreen was developed in WEB mode.
"
1717,"Assessment of Urban Ecological Service value used in Urban Rail Transit
  Project","  Ecosystem services refer to the ones human beings often obtain from the
natural environment ecosystem. In order to solve the problem of environmental
degradation, based on the Integrated Valuation of Ecosystem Services and
Trade-offs (InVEST model), this paper makes innovation by adding the urban
module that was not in the previous models, which can better deal with the
evaluation of ecosystem services in urban scenarios.
"
1718,Deep Fuzzy Systems,"  An investigation of deep fuzzy systems is presented in this paper. A deep
fuzzy system is represented by recursive fuzzy systems from an input terminal
to output terminal. Recursive fuzzy systems are sequences of fuzzy grade
memberships obtained using fuzzy transmition functions and recursive calls to
fuzzy systems. A recursive fuzzy system which calls a fuzzy system n times
includes fuzzy chains to evaluate the final grade membership of this recursive
system. A connection matrix which includes recursive calls are used to
represent recursive fuzzy systems.
"
1719,Inverse reinforcement learning conditioned on brain scan,"  We outline a way for an agent to learn the dispositions of a particular
individual through inverse reinforcement learning where the state space at time
t includes an fMRI scan of the individual, to represent his brain state at that
time. The fundamental assumption being that the information shown on an fMRI
scan of an individual is conditioned on his thoughts and thought processes. The
system models both long and short term memory as well any internal dynamics we
may not be aware of that are in the human brain. The human expert will put on a
suit for a set duration with sensors whose information will be used to train a
policy network, while a generative model will be trained to produce the next
fMRI scan image conditioned on the present one and the state of the
environment. During operation the humanoid robots actions will be conditioned
on this evolving fMRI and the environment it is in.
"
1720,Characterizing IoT Data and its Quality for Use,"  The Internet of Things (IoT) is a cyber physical social system that
encompasses science, enterprise and societal domains. Data is the most
important commodity in IoT, enabling the ""smarts"" through analytics and
decision making. IoT environments can generate and consume vast amounts of
data. But managing this data effectively and gaining meaningful insights from
it requires us to understand its characteristics. Traditional scientific,
enterprise and big data management approaches may not be adequate, and have to
evolve. Further, these characteristics and the physical deployment environments
also impact the quality of the data for use. In this paper, we offer a taxonomy
of IoT data characteristics, along with data quality considerations, that are
constructed from the ground-up based on the diverse IoT domains and
applications we review. We emphasize on the essential features, rather than a
vast array of attributes. We also indicate factors that influence the data
quality. Such a review is of value to IoT managers, data handlers and
application composers in managing and making meaningful use of data, and for
big data platform developers to offer meaningful solutions to address these
considerations.
"
1721,"Wise Data: A Novel Approach in Data Science from a Network Science
  Perspective","  Human beings have been generating data since very long times ago. We ask the
following common-sense and wise questions (WizQuestions):
  1. Why do we refer to some pieces of data more often than referring to other
pieces? 2. What does make those commonly-referred pieces of data so unique and
different? 3. What are the characteristics of data that sometimes make the data
so unique and different?
  In this article, we introduce a novel approach (model) that helps us answer
these questions from data science and network science perspectives. WizWordily
speaking, our proposed approach enables us to model the data (as a network),
measure the quality of data, and study the network of data deeply and
thoroughly.
"
1722,Algorithmic measurement procedures,"  Measurements are shown to be processes designed to return figures: they are
effective. This effectivity allows for a formalization as Turing machines,
which can be described employing computation theory. Inspired in the halting
problem we draw some limitations for measurement procedures: procedures that
verify if a quantity is measured cannot work in every case.
"
1723,Autonomous CPS mobility securely designed,"  In the last years the interconnection and ongoing development of physical
systems combined with cyber resources has led to increasing automation. Through
this progress in technology, autonomous vehicles, especially autonomous trains
are getting more attention from industry and are already under test. The use of
autonomous trains is known for increasing operation efficiency and reduction of
personnel and infrastructure costs, which is mostly considered for main tracks.
However, for less-used secondary lines, autonomous trains and their underlying
sensor infrastructure are not yet considered. Thus, a system needs to be
developed, which is less expensive for installation and operation of these
trains and underlying infrastructure for secondary lines. Therefore, this
position paper describes the process of how to derive an approach to help
develop a digital interlocking system at design time for the use with secondary
railway lines. In this work, we motivate the necessary research by
investigating gaps in existing work as well as presenting a possible solution
for this problem, a meta-model. The model considers safety, security as well as
interoperability like 5G and socio-technical aspects to provide a holistic
modeling approach for the development of the interlocking system for industrial
secondary line use cases.
"
1724,Fashion Retail: Forecasting Demand for New Items,"  Fashion merchandising is one of the most complicated problems in forecasting,
given the transient nature of trends in colours, prints, cuts, patterns, and
materials in fashion, the economies of scale achievable only in bulk
production, as well as geographical variations in consumption. Retailers that
serve a large customer base spend a lot of money and resources to stay prepared
for meeting changing fashion demands, and incur huge losses in unsold inventory
and liquidation costs [2]. This problem has been addressed by analysts and
statisticians as well as ML researchers in a conventional fashion - of building
models that forecast for future demand given a particular item of fashion with
historical data on its sales. To our knowledge, none of these models have
generalized well to predict future demand at an abstracted level for a new
design/style of fashion article. To address this problem, we present a study of
large scale fashion sales data and directly infer which clothing/footwear
attributes and merchandising factors drove demand for those items. We then
build generalised models to forecast demand given new item attributes, and
demonstrate robust performance by experimenting with different neural
architectures, ML methods, and loss functions.
"
1725,Using AI for Economic Upliftment of Handicraft Industry,"  The handicraft industry is a strong pillar of Indian economy which provides
large-scale employment opportunities to artisans in rural and underprivileged
communities. However, in this era of globalization, diverse modern designs have
rendered traditional designs old and monotonous, causing an alarming decline of
handicraft sales. For this age-old industry to survive the global competition,
it is imperative to integrate contemporary designs with Indian handicrafts. In
this paper, we use novel AI techniques to generate contemporary designs for two
popular Indian handicrafts - Ikat and Block Print. These techniques were
successfully employed by communities across India to manufacture and sell
products with greater appeal and revenue. The designs are evaluated to be
significantly more likeable and marketable than the current designs used by
artisans.
"
1726,"Earthquake Prediction With Artificial Neural Network Method: The
  Application Of West Anatolian Fault In Turkey","  A method that exactly knows the earthquakes beforehand and can generalize
them cannot still been developed. However, earthquakes are tried to be
predicted through numerous methods. One of these methods, artificial neural
networks give appropriate outputs to different patterns by learning the
relationship between the determined inputs and outputs. In this study, a
feedforward back propagation artificial neural network that is connected to
Gutenberg-Richter relationship and that bases on b value used in earthquake
predictions was developed. The artificial neural network was trained employing
earthquake data belonging to four different regions which have intensive
seismic activity in the west of Turkey. After the training process, the
earthquake data belonging to later dates of the same regions were used for
testing and the performance of the network was put forward. When the prediction
results of the developed network are examined, the prediction results that the
network predicts that an earthquake is not going to occur are quite high in all
regions. Furthermore, the earthquake prediction results that the network
predicts that an earthquake is going to occur are different to some extent for
the studied regions.
"
1727,TPM: A GPS-based Trajectory Pattern Mining System,"  With the development of big data and artificial intelligence, the technology
of urban computing becomes more mature and widely used. In urban computing,
using GPS-based trajectory data to discover urban dense areas, extract similar
urban trajectories, predict urban traffic, and solve traffic congestion
problems are all important issues. This paper presents a GPS-based trajectory
pattern mining system called TPM. Firstly, the TPM can mine urban dense areas
via clustering the spatial-temporal data, and automatically generate
trajectories after the timing trajectory identification. Mainly, we propose a
method for trajectory similarity matching, and similar trajectories can be
extracted via the trajectory similarity matching in this system. The TPM can be
applied to the trajectory system equipped with the GPS device, such as the
vehicle trajectory, the bicycle trajectory, the electronic bracelet trajectory,
etc., to provide services for traffic navigation and journey recommendation.
Meantime, the system can provide support in the decision for urban resource
allocation, urban functional region identification, traffic congestion and so
on.
"
1728,Emergency Management Systems and Algorithms: a Comprehensive Survey,"  Owing to the increasing frequency and destruction of natural and manmade
disasters to modern highly-populated societies, emergency management, which
provides solutions to prevent or address disasters, have drawn considerable
research over the last few decades and become a multidisciplinary area. Because
of its open and inclusive nature, new technologies always tend to influence,
change or even revolutionise this research area. Hence, it is imperative to
consolidate the state-of-the-art studies and knowledge to meet the research
needs and identify the future research directions. The paper presents a
comprehensive and systemic review of the existing research in the field of
emergency management from both the system design aspect and algorithm
engineering aspect. We begin with the history and evolution of the emergency
management research. Then the two main research topics of this area, ""emergency
navigation"" and ""emergency search and rescue planning"", are introduced and
discussed. Finally, we suggest the emerging challenges and opportunities from
system optimisation, evacuee behaviour modelling and optimisation, computing
patterns, data analysis, energy and cyber security aspects.
"
1729,EU H2020 Gauss project. Geo-Fencing Software System,"  The Geofencing system is the key to operate the Unmanned Aerial Vehicle (UAV)
within the safe and appropriate zone to avoid public concerns and other privacy
issues. The system is designed to keep the UAV away from geofenced obstacles
using the onboard GNSS and IMU location. The Geofencing system is part of the
H2020 GAUSS project and facilities other subsystems, for instance, to support
the command and control link, which is the security measure to secure the UAV
from hijacking and signal spoofing. The regulatory authorities expressed the
concern of having UAVs flying in the no-fly zone and causing troubles from
offending private privacy to hazards at airport airspace. Hence the geofence
system shall provide guidance message, which enables the UAV to evacuate from
no-fly-zone, based on real-time updated location. This thesis aims to first
illustrate the generation of geofence and then apply the geofence system on UAV
operation. This application enables UAV to fly in the designated area without
human intervention. The project is built with JAVA using GIS-enabled Database
Management System and Open Soured Map data powered by OpenStreetMap and OS map.
This method has been tested by simulations which had results of high accuracy.
"
1730,"A Theoretical Model For Artificial Learning, Memory Management And
  Decision Making System","  Human beings are considered as the most intelligent species on Earth. The
ability to think, to create, to innovate, are the key elements which make
humans superior over other existing species on Earth. Machines lack all those
elements, although machines are faster than human in aspects like computing,
equating etc. But humans are still more valuable than machines, due to all
those previously discussed elements. Various models have been developed in last
few years to create models that can think like human beings, but are not
completely successful. This paper presents a new theoretical system for
learning, memory management and decision making that can be used to develop
highly complex systems, and shows the potential to be used for development of
systems that can be used to provide the essential features to the machines to
act like human beings.
"
1731,"Inferring Accurate Bus Trajectories from Noisy Estimated Arrival Time
  Records","  Urban commuting data has long been a vital source of understanding population
mobility behaviour and has been widely adopted for various applications such as
transport infrastructure planning and urban anomaly detection. While
individual-specific transaction records (such as smart card (tap-in, tap-out)
data or taxi trip records) hold a wealth of information, these are often
private data available only to the service provider (e.g., taxicab operator).
In this work, we explore the utility in harnessing publicly available, albeit
noisy, transportation datasets, such as noisy ""Estimated Time of Arrival"" (ETA)
records (commonly available to commuters through transit Apps or electronic
signages). We first propose a framework to extract accurate individual bus
trajectories from such ETA records, and present results from both a primary
city (Singapore) and a secondary city (London) to validate the techniques.
Finally, we quantify the upper bound on the spatiotemporal resolution, of the
reconstructed trajectory outputs, achieved by our proposed technique.
"
1732,"Modeling and analysis of alternative distribution and Physical Internet
  schemes in urban area","  Urban logistics is becoming more complicated and costlier due to new
challenges in recent years. Since the main problem lies on congestion, the
clean vehicle is not necessarily the most effective solution. There is thus a
need to redesign the logistics networks in the city. This paper proposes a
methodology to evaluate different distribution schemes in the city among which
we find the most efficient and sustainable one. External impacts are added to
the analysis of schemes, including accident, air pollution, climate change,
noise, and congestion. An optimization model based on an analytical model is
developed to optimize transportation means and distribution schemes. Results
based on Bordeaux city show that PI scheme improves the performances of
distribution.
"
1733,SysMART Outdoor Services: A System of Connected and Smart Supermarkets,"  Smart cities are today's modern trend. Many high-tech industrial firms are
exploring different approaches to implement smart cities. Various projects aim
at internet-of-things and smart solutions. Current implementations are mostly
localized to a specific building or area; however, the growth is crossing space
and geographic location limits. Shopping is a central activity that is frequent
and typically a time-consuming task. SysMART is system of connected and smart
supermarkets. SysMART enables a plausible shopping experience for customers.
The aim of SysMART is to provide an advanced lifestyle with its ease of use
functionality. SysMART outdoor services support distant parking availability,
traffic status, and remote inventory checks for supermarkets in a chain.
SysMART implementation relies on cutting edge technologies that support rapid
prototyping and precision data acquisition, such as, National Instrument
devices. The selected development environment is LabView with its world-class
interfacing libraries. The paper comprises a detailed system description,
development strategy, interface design, software engineering, and a thorough
analysis and evaluation.
"
1734,Prototype Software Monitoring Sarana dan Prasarana Perguruan Tinggi,"  This study aims to facilitate the management system of monitoring
infrastructure program of university facilities and infrastructure, through
software engineering technology approach as an effort to improve productivity
and quality of monitoring process become more efficient and effective. The
software in this research is built in a systematic and organized approach to
monitoring infrastructure of facilities and infrastructure using appropriate
tools and techniques. Through this research, universities are expected to be
able to develop the necessary quality measures to support the process of
planning and controlling infrastructure infrastructure monitoring. The research
was conducted using survey method, development of monitoring management and
software. Up to the design stage of this program prototype, research has
produced a special picture of the software requirements to be built in the next
year. Software development process starting from the analysis phase of system
and software requirements, designing data structures up to the architecture
stage of the program has produced a list of needs/requirements, the design of
program prototype contained in the design of input/output for the monitoring
process facilities and infrastructure.
"
1735,An adaptive architecture for portability of greenhouse models,"  This work deals with the portability of greenhouse models, as we believe that
this is a challenge to their practical usage in control strategies under
production conditions. We address this task by means of adaptive neural
networks, which re-adjust their weights when transferred to new conditions.
Such an adaptive account for computational models is typical of the field of
developmental robotics, which investigates learning of motor control in
artificial systems inspired on infants development. Similarly to robots,
greenhouses are complex systems comprising technical and biological elements,
whose state can be measured and modified through control actions. We present an
adaptive model architecture to perform online learning on greenhouse models.
This learning process makes use of an episodic memory and of online
re-training. This allows for adaptation without the need for a complete new
training, which might be prohibitive if the data under the new conditions is
scarce. Current experiments focus on how a model of tomato photosynthesis,
developed in a research facility, can adapt itself to a new environment in a
production greenhouse. Further research will focus on model plasticity by means
of adaptive learning rates and management of the episodic memory described in
this paper. The models presented as a proof-of-concept estimate the
transpiration and photosynthesis of a hydroponic tomato crop by using
measurements of the climate as inputs. The models are trained and tested using
data from a greenhouse in Berlin, Germany. Thereafter, the adaptive
architecture is fed with data from a production greenhouse in southern Germany,
where other tomato varieties were grown under different irrigation and climate
strategies. The proposed adaptive architecture represents a promising tool for
spreading the use of models produced by high-tech research centers to the
greenhouse production sector.
"
1736,"Received Signal Strength Based Wireless Source Localization with Inexact
  Anchor Position","  Received signal strength(RSS)-based approach of wireless localization is easy
to implement at low cost. In practice, exact positions of anchors may not be
available. This paper focuses on determining the location of the source in the
presence of inexact position of anchors based on RSS directly. This study at
first uses Taylor expansion and a min-max approach to get the approximate
maximum likelihood estimator of the source coordinates. Then this paper
proposes a relaxed semi-definite programming model to circumvent the
non-convexity. This paper also proposes a rounding algorithm concerning both
the inexact source location and the inaccurate anchor location. Experimental
results together with analysis are presented to validate the proposed method
"
1737,"Simulation and Learning for Urban Mobility: City-scale Traffic
  Reconstruction and Autonomous Driving","  Traffic congestion has become one of the most critical issues worldwide. The
costs due to traffic gridlock and jams are approximately $160 billion in the
United States, more than {\pounds}13 billion in the United Kingdom, and over
one trillion dollars across the globe annually. As more metropolitan areas will
experience increasingly severe traffic conditions, the ability to analyze,
understand, and improve traffic dynamics becomes critical. This dissertation is
an effort towards achieving such an ability. I propose various techniques
combining simulation and machine learning to tackle the problem of traffic from
two perspectives: city-scale traffic reconstruction and autonomous driving.
"
1738,DNA based Network Model and Blockchain,"  Biological cells can transmit, process and receive chemically encoded data in
the same way as network devices transmit, process, and receive digitally
encoded data. Communication protocols have led to the rapid development of
computer networks. Therefore, we need to develop communication protocols for
biological cell networks, which will lead to significant development,
especially in medical applications where surgery or delivery of drugs can be
performed using nanoscale devices. Blockchain is a peer-to-peer network that
contains a series of clusters to make a valid and secure transaction. Blockhain
technology is used in many areas such as e-commerce, public services, security,
finance, Internet stuff, etc. Although blockchain has a major impact on
Internet technology, it suffers from time problems and scalability. DNA
computing is the execution of computations using natural molecules, especially
DNA. DNA gaps above silicon because of massive parallelism, size and storage
density. In this paper, biological cells and DNA are used to create the
necessary protocols for the networks to be used in the performance of the
cell-based communication system. The proposed hybrid solution involves DNA as
well as calculated on an enzymatic basis, where each contributes to the
function of a given protocol. Also a correspondence between blockchain and DNA
is proposed that can be utilized to create DNA based blockchain.
"
1739,How Downwards Causation Occurs in Digital Computers,"  Digital computers carry out algorithms coded in high level programs. These
abstract entities determine what happens at the physical level: they control
whether electrons flow through specific transistors at specific times or not,
entailing downward causation in both the logical and implementation
hierarchies. This paper explores how this is possible in the light of the
alleged causal completeness of physics at the bottom level, and highlights the
mechanism that enables strong emergence (the manifest causal effectiveness of
application programs) to occur. Although synchronic emergence of higher levels
from lower levels is manifestly true, diachronic emergence is generically not
the case; indeed we give specific examples where it cannot occur because of the
causal effectiveness of higher level variables.
"
1740,"Modelagem de um Problema de Dimensionamento de Lotes com Demanda
  Variavel e Deterministica e Efeitos de Learning e Forgetting","  The main goal of this paper was to analyze the importance that the effects of
learning and forgetting might have in a lot-sizing problem. It assumes that the
learning curve and the economies of scale are present in several industries yet
are, in most cases, not considered when dealing with a lot-sizing problem. The
importance of the effects was demonstrated and quantified, showing that there
is still space for developments in this field. However, as the problem becomes
quadratic, there is a possibility that the current algorithms are not able to
solve the problem to optimality. Thus, future improvements in the algorithms
may further improve the results. However, the overall results found with
current algorithms show that the contribution of a discount from a learning
curve can be very considerable, even if it is a minimal amount.
"
1741,Real-time stock analysis for blending recipes in industrial plants,"  Many companies use Excel spreadsheets to keep stock records and to calculate
process-specific data. These spreadsheets are often hard to understand and
track. And if the user does not protect them, there is a risk that the user
randomly changes or erase formulas. The paper focuses on the stocks of products
used in a blending process with a known recipe. Developing an application that
can bring this data in a centralized form and that can assist the operator in
decide is a necessity. When a programmer implements an application that uses
data from plants he needs to consider one fundamental aspect as reading
real-time data from the process. The real-time stock analysis application takes
into account all the above elements. The application is easy to use by an
operator in the command room of installation because of the planning algorithms
integrated into it. The algorithms proposed and implemented in this paper have
well-defined goals: identifying the ingredients needed to achieve the blending
process for required quantities, determine the quantities of the finished
product that can be made with the existing ingredients and determine the
optimum quantities of the finished product. The application implemented in C#
intensively uses these algorithms and gives the user the ability to build the
result step by step.
"
1742,Stochastic model of business process decomposition,"  Decomposition is the basis of works dedicated to business process modelling
at the stage of information and management systems analysis and design. The
article shows that the business process decomposition can be represented as a
Galton Watson branching stochastic process. This representation allows
estimating the decomposition tree depth and the total amount of its elements,
as well as explaining the empirical requirement for the business function
decomposition (not more than 7 elements). The problem is deemed relevant as the
obtained results allow objectively estimating the labor input in business
process modelling.
"
1743,BinarySDG: binary sensor data generation with R,"  The scarcity of Smart Home data is still a pretty big problem, and in a world
where the size of a dataset can often make the difference between a poor
performance and a good performance for problems related to machine learning
projects, this needs to be resolved. But whereas the problem of retrieving real
data can't really be resolved, as most of the time the process of installing
sensors and retrieving data can be found to be really expensive and
time-consuming, we need to find a faster and easier solution, which is where
synthetic data comes in. Here we propose BinarySDG (Binary Synthetic Data
Generator) as a flexible and easy way to generate synthetic data for binary
sensors.
"
1744,Completely uniformly distributed sequences based on de Bruijn sequences,"  We study a construction published by Donald Knuth in 1965 yielding a
completely uniformly distributed sequence of real numbers. Knuth's work is
based on de Bruijn sequences of increasing orders and alphabet sizes, which
grow exponentially in each of the successive segments composing the generated
sequence. In this work we present a similar albeit simpler construction using
linearly increasing alphabet sizes, and give an elementary proof showing that
the sequence it yields is also completely uniformly distributed. In addition,
we present an alternative proof of the same result based on Weyl's criterion.
"
1745,Exact Calculation of Expected Values for Splitting Pairs in Blackjack,"  Computer calculations for most exact expected values in blackjack have been
available since the 1960's, but exact results for pair splitting and
resplitting have previously been too computer intensive. This paper describes a
new algorithm for exact pair-splitting. By using dealer probability caching
methods and revising the method for recursively generating possible player
hands, the estimated calculation time compared to standard methods was reduced
by five orders of magnitude. The resulting algorithm was used to calculate the
first exact and complete pair splitting results for a single deck game. The
exact results were compared to prior approximate theories for resplitting. The
prior theories are accurate for many calculations, but inaccurate for
resplitting tens. A new approximation method was developed that is accurate for
all resplitting calculations.
"
1746,"A Self-Healing Hardware Architecture for Safety-Critical Digital
  Embedded Devices","  Digital Embedded Devices of next-generation safety-critical industrial
automation systems require high levels of survivability and resilience against
the hardware and software failure. One of the concepts for achieving this
requirement is the design of resilient and survivable digital embedded systems.
In the last two decades, development of self-healing digital systems based on
molecular and cellular biology have received attention for the design of robust
digital systems. However, many of these approaches have not been architected
from the outset with safety in mind, nor have they been targeted for the
applications of automation community where a significant need exists. This
paper presents a new self-healing hardware architecture, inspired from the way
nature responds, defends and heals: the stem cells in the immune system of
living organisms, the life cycle of the living cell, and the pathway from
Deoxyribonucleic acid (DNA) to protein. The proposed architecture is
integrating cellular-based biological concepts, traditional fault tolerance
techniques, and operational schematics for the international standard IEC
61131-3 to facilitate adoption in the automation industry and safety-critical
applications. To date, two industrial applications have been mapped on the
proposed architecture, which are capable of tolerating a significant number of
faults that can stem from harsh environmental changes and external disturbances
and we believe the nexus of its concepts can positively impact the next
generation of critical systems in the automation industry
"
1747,"Conjure Documentation, Release 2.3.0","  Conjure is an automated modelling tool for Constraint Programming. In this
documentation, you will find the following: A brief introduction to Conjure,
installation instructions, a description of how to use Conjure through its
command line user interface, a list of Conjure's features, a description of
Conjure's input language Essence, and a collection of simple demonstrations of
Conjure's use.
"
1748,Overview of Fault Tolerant Techniques in Underwater Sensor Networks,"  Sensor networks provide services to a broad range of applications ranging
from intelligence service surveillance to weather forecasting. Most of the
sensor networks are terrestrial, however much of our planet is covered by water
and Underwater Sensor Networks (USN) are an emerging research area. One of the
unavoidable increasing challenge for modern technology is tolerating faults -
accepting that hardware is imperfect and cope with it. Fault tolerance may have
more impact underwater than in terrestrial environment as terrestrial
environment is more forgiving, reaching the malfunctioning devices for
replacement underwater is harder and may be more costly. Current paper is the
first to investigate fault tolerance, particularly cross layer fault tolerance,
in USN-s.
"
1749,Authentication Modeling with Five Generic Processes,"  Conceptual modeling is an essential tool in many fields of study, including
security specification in information technology systems. As a model, it
restricts access to resources and identifies possible threats to the system. We
claim that current modeling languages (e.g., Unified Modeling Language,
Business Process Model and Notation) lack the notion of genericity, which
refers to a limited set of elementary processes. This paper proposes five
generic processes for modeling the structural behavior of a system: creating,
releasing, transferring, receiving, and processing. The paper demonstrates
these processes within the context of public key infrastructure, biometric, and
multifactor authentication. The results indicate that the proposed generic
processes are sufficient to represent these authentication schemes.
"
1750,"A Survey of Benchmarks to Evaluate Data Analytics for Smart-*
  Applications","  The growth of ubiquitous sensor networks at an accelerating pace cuts across
many areas of modern day life. They enable measuring, inferring, understanding
and acting upon a wide variety of indicators, in fields ranging from
agriculture to healthcare or to complex urban environments. The applications
devoted to this task are designated as Smart-* Applications. They hide a
staggering complexity, relying on multiple layers of data collection,
transmission, aggregation, analysis and also storage, both at the network edge
and on the cloud. Furthermore, Smart-* Applications raise additional specific
challenges, such as the need to process and extract knowledge from diverse
data, which is flowing at high velocity in near real-time or in the heavily
distributed environment they rely on. How to assess the performance of such a
complex stack, when faced with the specifics of \mbox{Smart-*} Applications,
remains an open research question. In this article, the key specific
characteristics and requirements of Smart-* Applications are initially
detailed. Afterwards, for each of these requirements, there is a description of
the benchmarks one can use to precisely evaluate the performance of the
underlying systems and technologies. Finally, an identification of future
research directions related to identified open issues for benchmarking Smart-*
Applications is performed.
"
1751,Information collection for fraud detection in P2P financial market,"  Fintech companies have been facing challenges from fraudulent behavior for a
long time. Fraud rate in Chinese P2P financial market could go as high as 10%.
It is crucial to collect sufficient information of the user as input to the
anti-fraud process. Data collection framework for Fintech companies are
different fro m conventional internet firms. With individual-based crawling
request , we need to deal with new challenges negligible elsewhere . In this
paper , we give an outline of how we collect data from the web to facilitate
our anti-fraud process. We also overview the challenges and solutions to our
problems. Our team at HC Financial Service Group is one of the few companies
that are capable of developing full-fledged crawlers on our own.
"
1752,Detection of fraudulent users in P2P financial market,"  Financial fraud detection is one of the core technological assets of Fintech
companies. It saves tens of millions of money fro m Chinese Fintech companies
since the bad loan rate is more than 10%. HC Financial Service Group is the 3rd
largest company in the Chinese P2P financial market. In this paper we
illustrate how we tackle the fraud detection problem at HC Financial. We
utilize two powerful workhorses in the machine learning field - random forest
and gradient boosting decision tree to detect fraudulent users . We demonstrate
that by carefully select features and tune model parameters , we could
effectively filter out fraudulent users in the P2P market.
"
1753,A note on 'Collaborative hub location problem under cost uncertainty',"  Three models were presented in M.K. Khakim Habibi, Hamid Allaoui, Gilles
Goncalves, Collaborative hub location problem under cost uncertainty, Computers
& Industrial Engineering Volume 124, October 2018, Pages 393-410 as models for
collaborative Capacitated Multiple Allocation Hub Location Problem. In this
note, we point out a few flaws in modeling. In particular, we elaborate and
explain that none of the those models incorporates any element of a
collaborative activity.
"
1754,"A Method of EV Detour-to-Recharge Behavior Modeling and Charging Station
  Deployment","  Electric vehicles (EVs) are increasingly used in transportation. Worldwide
use of EVs, for their limited battery capacity, calls for effective planning of
EVs charging stations to enhance the efficiency of using EVs. This paper
provides a methodology of describing EV detouring behavior for recharging, and
based on this, we adopt the extra driving length caused by detouring and the
length of uncompleted route as the indicators of evaluating an EV charging
station deployment plan. In this way, we can simulate EV behavior based on
travel data (demand). Then, a genetic algorithm (GA) based EV charging station
sitting optimization method is developed to obtain an effective plan. A
detailed case study based on a 100-node 203-branch transportation network
within a 30 km * 30 km region is included to test the effectiveness of our
method. Insights from our method may be applicable for charging station
planning in various transportation networks.
"
1755,Simulation Reproducibility of a Chaotic Circuit,"  An evergreen scientific feature is the ability for scientific works to be
reproduced. This feature allows researchers to understand, enhance, or even
question works that have been developed by other scientists. In control theory
the importance of modeling and simulation of systems is widely recognized.
Despite this recognition, less attention is paid to the effects of finite
precision of computers on the simulation reproducibility of nonlinear dynamic
systems. In this work, a case study of reproducibility is presented in the
simulation of a chaotic Jerk circuit, using the software LtSpice. In order to
do so, we performed simulations of the circuit in the same version of the
software on different computers, in order to collect the data and compare them
with experimental results. The comparison was made with the NRMSE (Normalized
Root Mean Square Error), in order to identify the computer with the highest
prediction horizon. Tests performed in 4 different configurations showed the
difficulties of simulation reproducibility in LtSpice. The methodology
developed was efficient in identifying the computer with better performance,
which allows applying it to other cases in the literature.
"
1756,"What's My Process Model Composed of? A Systematic Literature Review of
  Meta-Models in BPM","  Business process modelling languages typically enable the representation of
business process models by employing (graphical) symbols. These symbols can
vary depending upon the verbosity of the language, the modeling paradigm, the
focus of the language, and so on. To make explicit the different constructs and
rules employed by a specific language as well as bridge the gap across
different languages, meta-models have been proposed in literature. These
meta-models are a crucial source of knowledge on what state-of-the-art
literature considers relevant to describe business processes. Moreover, the
rapid growth of techniques and tools that aim at supporting all dimensions of
business processes and not only its control flow perspective, as for instance
data and organisational aspects, makes even more important to have a clear
idea, already at the conceptual level, of the key process constructs. The goal
of this work is to provide the first extensive systematic literature review
(SLR) of business process meta-models. This SLR aims at answering research
questions concerning: (i) the kind of meta-models proposed in literature; (ii)
the recurring constructs they contain; (iii) their purposes; and (iv) their
evaluations. Thirty-six papers were selected and evaluated against four
research questions. The results indicate the existence of a reasonable body of
work conducted in this specific area, but not a full maturity. In particular,
while traditional paradigms towards business process modelling, and aspects
related to the business process control flow seem to be well present, novel
paradigms and aspects related to the organisational, data and goal-oriented
aspects of business processes seem to be still under-investigated.
"
1757,The Open Porous Media Flow Reservoir Simulator,"  The Open Porous Media (OPM) initiative is a community effort that encourages
open innovation and reproducible research for simulation of porous media
processes. OPM coordinates collaborative software development, maintains and
distributes open-source software and open data sets, and seeks to ensure that
these are available under a free license in a long-term perspective.
  In this paper, we present OPM Flow, which is a reservoir simulator developed
for industrial use, as well as some of the individual components used to make
OPM Flow. The descriptions apply to the 2019.10 release of OPM.
"
1758,Scalability of TTool's AMS extensions: a case study,"  Embedded cyber-physical systems (CPS) are commonly built upon heterogeneous
digital and analog integrated circuits, including sensors and actuators. Less
common is their deployment on parallel, NoC based designs based on general
purpose processor cores of a Multi-processor System-on-chip (MPSoC).
Application code has to be run on the MPSoC for the digital part, and interact
with the analog sensors. We recently proposed a major extension to the design
and exploration tool named TTool, now allowing the design of CPS on a high
level of abstraction and the generation of cycle-bit accurate simulations. We
explore the scalability of our approach with an automotive case study.
"
1759,GeoSES -- um \'Indice Socioecon\^omico para Estudos de Sa\'ude no Brasil,"  Objective: to define an index that summarizes the main dimensions of the
socioeconomic context for research purposes, evaluation and monitoring health
inequalities. Methods: the index was created from the 2010 Brazilian
Demographic Census, whose variables selection was guided by theoretical
references for health studies, including seven socioeconomic dimensions:
education, mobility, poverty, wealth, income, segregation and deprivation of
resources and services. The index was developed using principal component
analysis, and was evaluated for its construct, content and applicability
components. Results: GeoSES-BR dimensions showed good association with HDI-M
(above 0.85). The model with the poverty dimension best explained the relative
risk of avoidable cause mortality in Brazil. In the intraurban scale, the model
with GeoSES-IM was the one that best explained the relative risk of mortality
from circulatory system diseases. Conclusion: GeoSES showed significant
explanatory potential in the studied scales.
"
1760,"Data-driven charging strategies for grid-beneficial, customer-oriented
  and battery-preserving electric mobility","  Electric Vehicle (EV) penetration and renewable energies enables synergies
between energy supply, vehicle users, and the mobility sector. However, also
new issues arise for car manufacturers: During charging and discharging of EV
batteries a degradation (battery aging) occurs that correlates with a value
depreciation of the entire EV. As EV users' satisfaction depends on reliable
and value-stable products, car manufacturers offer charging assistants for
simplified and sustainable EV usage by considering individual customer needs
and battery aging. Hitherto models to quantify battery aging have limited
practicability due to a complex execution. Data-driven methods hold feasible
alternatives for SOH estimation. However, the existing approaches barely use
user-related data. By means of a linear and a neural network regression model,
we first estimate the energy consumption for driving considering individual
driving styles and environmental conditions. In following work, the consumption
model trained on data from batteries without degradation can be used to
estimate the energy consumption for EVs with aged batteries. A discrepancy
between the estimation and the real consumption indicates a battery aging
caused by increased internal losses. We then target to evaluate the influence
of charging strategies on battery degradation.
"
1761,"Smart Monitoring: remote-monitoring technology of power, gas, and water
  consumption in Smart Cities","  This paper describes the remote-collection technology of detailed data (Smart
Monitoring) on the consumption and quality of energy resources in public
services. In this article, under ""energy resources"" (hereinafter referred to as
resources) we outline electrical power, water (hot and cold), heat, and gas.
Data on resource quality refer to the parameters characterizing the consumed
resource. We also present an option of the data-acquisition system structure
based on Smart Monitoring technology. Particular attention is paid to security
in the system and the centralized management of its elements. The data flow in
such system carries information about the behavior of energy consumers and the
household equipment they use. Data on energy consumption for billing purposes
in such a system is just one of many kinds, and not the most important feature.
The development of Smart Monitoring technology is aimed at developing the
market of IT services and mass services based on analysis of collected detailed
data on energy-resource consumption.
"
1762,"Benchmark Dataset for Timetable Optimization of Bus Routes in the City
  of New Delhi","  Public transport is one of the major forms of transportation in the world.
This makes it vital to ensure that public transport is efficient. This research
presents a novel real-time GPS bus transit data for over 500 routes of buses
operating in New Delhi. The data can be used for modeling various timetable
optimization tasks as well as in other domains such as traffic management,
travel time estimation, etc. The paper also presents an approach to reduce the
waiting time of Delhi buses by analyzing the traffic behavior and proposing a
timetable. This algorithm serves as a benchmark for the dataset. The algorithm
uses a constrained clustering algorithm for classification of trips. It further
analyses the data statistically to provide a timetable which is efficient in
learning the inter- and intra-month variations.
"
1763,"Gene expression and pathway bioinformatics analysis detect a potential
  predictive value of MAP3K8 in thyroid cancer progression","  Thyroid cancer is the commonest endocrine malignancy. Mutation in the BRAF
serine/threonine kinase is the most frequent genetic alteration in thyroid
cancer. Target therapy for advanced and poorly differentiated thyroid
carcinomas include BRAF pathway inhibitors. Here, we evaluated the role of
MAP3K8 expression as a potential driver of resistance to BRAF inhibition in
thyroid cancer. By analyzing Gene Expression Omnibus data repository, across
all thyroid cancer histotypes, we found that MAP3K8 is up-regulated in poorly
differentiated thyroid carcinomas and its expression is related to a stem cell
like phenotype and a poorer prognosis and survival. Taken together these data
unravel a novel mechanism for thyroid cancer progression and chemo-resistance
and confirm previous results obtained in cultured thyroid cancer stem cells
"
1764,"Fast Safety Assessment and Correction Framework for Maintenance Work
  Zones","  A framework is proposed to assess the safety of maintenance work zones in a
timely manner, show whether there are safety hazards, whether adjustments need
to be made and how to adjust it. By means of advanced data acquisition
technologies such as multi video detection and portable device based
naturalistic driving, the microscopic vehicle behaviour data can be collected.
Based on this data, a method for expressing and displaying the distribution of
unsafe vehicle behaviour is used to show whether safety hazards exist. Using
Vissim, the impacts of the length and speed limit of the warning area, the
length and type of the upstream transition area and the length of the work area
of the maintenance work zone on the distribution of unsafe vehicle behaviour
are simulated to establish the safety correction matrix, which can tell
maintenance departments the direction of adjustment when safety hazards exist
in maintenance work zones.
"
1765,Priority Quality Attributes for Engineering AI-enabled Systems,"  Deploying successful software-reliant systems that address their mission
goals and user needs within cost, resource, and expected quality constraints
require design trade-offs. These trade-offs dictate how systems are structured
and how they behave and consequently can effectively be evolved and sustained.
Software engineering practices address this challenge by centering system
design and evolution around delivering key quality attributes, such as
security, privacy, data centricity, sustainability, and explainability. These
concerns are more urgent requirements for software-reliant systems that also
include AI components due to the uncertainty introduced by data elements.
Moreover, systems employed by the public sector exhibit unique design time and
runtime challenges due to the regulatory nature of the domains. We assert that
the quality attributes of security, privacy, data centricity, sustainability,
and explainability pose new challenges to AI engineering and will drive the
success of AI-enabled systems in the public sector. In this position paper, we
enumerate with examples from healthcare domain concerns related to these
requirements to mitigate barriers to architecting and fielding AI-enabled
systems in the public sector.
"
1766,Business Process Variant Analysis: Survey and Classification,"  Process variant analysis aims at identifying and addressing the differences
existing in a set of process executions enacted by the same process model. A
process model can be executed differently in different situations for various
reasons, e.g., the process could run in different locations or seasons, which
gives rise to different behaviors. Having intuitions about the discrepancies in
process behaviors, though challenging, is beneficial for managers and process
analysts since they can improve their process models efficiently, e.g., via
interactive learning or adapting mechanisms. Several methods have been proposed
to tackle the problem of uncovering discrepancies in process executions.
However, because of the interdisciplinary nature of the challenge, the methods
and sorts of analysis in the literature are very heterogeneous. This article
not only presents a systematic literature review and taxonomy of methods for
variant analysis of business processes but also provides a methodology
including the required steps to apply this type of analysis for the
identification of variants in business process executions.
"
1767,Value-Added Chemical Discovery Using Reinforcement Learning,"  Computer-assisted synthesis planning aims to help chemists find better
reaction pathways faster. Finding viable and short pathways from sugar
molecules to value-added chemicals can be modeled as a retrosynthesis planning
problem with a catalyst allowed. This is a crucial step in efficient biomass
conversion. The traditional computational chemistry approach to identifying
possible reaction pathways involves computing the reaction energies of hundreds
of intermediates, which is a critical bottleneck in silico reaction discovery.
Deep reinforcement learning has shown in other domains that a well-trained
agent with little or no prior human knowledge can surpass human performance.
While some effort has been made to adapt machine learning techniques to the
retrosynthesis planning problem, value-added chemical discovery presents unique
challenges. Specifically, the reaction can occur in several different sites in
a molecule, a subtle case that has never been treated in previous works. With a
more versatile formulation of the problem as a Markov decision process, we
address the problem using deep reinforcement learning techniques and present
promising preliminary results.
"
1768,Improvements of the REDCRAFT Software Package,"  Traditional approaches to elucidation of protein structures by NMR
spectroscopy rely on distance restraints also known as nuclear Overhauser
effects (NOEs). The use of NOEs as the primary source of structure
determination by NMR spectroscopy is time consuming and expensive. Residual
Dipolar Couplings (RDCs) have become an alternate approach for structure
calculation by NMR spectroscopy. In previous works, the software package
REDCRAFT has been presented as a means of harnessing the information containing
in RDCs for structure calculation of proteins. In this work, we present
significant improvements to the REDCRAFT package including: refinement of the
decimation procedure, the inclusion of graphical user interface, adoption of
NEF standards, and addition of scripts for enhanced protein modeling options.
The improvements to REDCRAFT have resulted in the ability to fold proteins that
the previous versions were unable to fold. For instance, we report the results
of folding of the protein 1A1Z in the presence of highly erroneous data.
"
1769,Inflationary Constant Factors and Why Python is Faster Than C++,"  Constant-factor differences are frequently ignored when analyzing the
complexity of algorithms and implementations, as they appear to be
insignificant in practice. In this paper, we demonstrate that this assumption
can in fact have far more profound implications on time complexity than is
obvious at first glance, and that a poor consideration of trade-offs can result
in polynomially slower algorithms whose roots can be deeply and fundamentally
ingrained into a programming language itself. While the general observation may
not be novel from a theoretical standpoint, it is rarely (if ever) presented in
traditional computer science curricula or other settings, and appears to be far
from common knowledge in practical software engineering. We thus hope bring
awareness to this issue and urge careful consideration of significant
trade-offs that can result from trivial decisions made while programming.
"
1770,Context Adaptivity as Enabler for Meaningful Pervasive Advertising,"  Socio-demographic user profiles are currently regarded as the most convenient
base for successful personalized advertising. However, signs point to the
dormant power of context recognition. While technologies that can sense the
environment are increasingly advanced, questions such as what to sense and how
to adapt to a consumer's context are largely unanswered. Research in the field
is scattered and frequently prototype-driven. What the community lacks is a
thorough methodology to provide the basis for any context-adaptive system:
conceptualizing context. This position paper describes our current research of
conceptualizing context for pervasive advertising. It summarizes findings from
literature analysis and proposes a methodology for context conceptualization,
which is currently work-in-progress.
"
1771,Nonintrusive Load Monitoring for Machines used in Manufacturing,"  In order to increase the electric energy efficiency of production machines,
it is necessary to determine the energy demand of the constituent electric
loads. Therefore, a new measurement system based on nonintrusive load
monitoring is proposed in this paper. It only measures the voltage and current
of the aggregate load and then uses automatic disaggregation methods to
estimate the energy demand of the constituent loads. In two case studies, the
energy demand of most loads could be determined with an accuracy of 85~\% or
more in this way.
"
1772,"High-Freedom Inverse Design with Deep Neural Network for Metasurface
  Filter in the Visible","  In order to obtain a metasurface structure capable of filtering the light of
a specific wavelength in the visible band, traditional method usually traverses
the space consisting of possible designs, searching for a potentially
satisfying device by performing iterative calculations to solve Maxwell's
equations. In this paper, we propose a neural network that can complete an
inverse design process to solve the problem. Compared with the traditional
method, our method is much faster while competent of generating better devices
with the desired spectrum. One of the most significant advantages is that it
can handle a real spectrum as well as an artificial one. Besides, our method
encompasses a high degree of freedom to generate devices, ensuring their
generated spectra resemble desired ones and meeting the accuracy requirements
without losing practicability in the manufacturing process.
"
1773,"Non-linearity identification for construction workers'
  personality-safety behaviour predictive relationship using neural network and
  linear regression modelling","  The prediction of workers' safety behaviour can help identify vulnerable
workers who intend to undertake unsafe behaviours and be useful in the design
of management practices to minimise the occurrence of accidents. The latest
literature has evidenced that there is within-population diversity that leads
people's intended safety behaviours in the workplace, which are found to vary
among individuals as a function of their personality traits. In this study, an
innovative forecasting model, which employs neural network algorithms, is
developed to numerically simulate the predictive relationship between
construction workers' personality traits and their intended safety behaviour.
The data-driven nature of neural network enabled a reliable estimate of the
relationship, which allowed this research to find that a nonlinear effect
exists in the relationship. This research has practical implications. The
neural network developed is shown to have highly satisfactory prediction
accuracy and is thereby potentially useful for assisting project
decision-makers to assess how prone workers are to carry out unsafe behaviours
in the workplace.
"
1774,CAD Tool Design Space Exploration via Bayesian Optimization,"  The design complexity is increasing as the technology node keeps scaling
down. As a result, the electronic design automation (EDA) tools also become
more and more complex. There are lots of parameters involved in EDA tools,
which results in a huge design space. What's worse, the runtime cost of the EDA
flow also goes up as the complexity increases, thus exhaustive exploration is
prohibitive for modern designs. Therefore, an efficient design space
exploration methodology is of great importance in advanced designs. In this
paper we target at an automatic flow for reducing manual tuning efforts to
achieve high quality circuits synthesis outcomes. It is based on Bayesian
optimization which is a promising technique for optimizing black-box functions
that are expensive to evaluate. Gaussian process regression is leveraged as the
surrogate model in Bayesian optimization framework. In this work, we use 64-bit
prefix adder design as a case study. We demonstrate that the Bayesian
optimization is efficient and effective for performing design space exploration
on EDA tool parameters, which has great potential for accelerating the design
flow in advanced technology nodes.
"
1775,"BiEntropy, TriEntropy and Primality","  The order and disorder of binary representations of the natural numbers < 2^8
is measured using the BiEntropy function. Significant differences are detected
between the primes and the non primes. The BiEntropic prime density is shown to
be quadratic with a very small Gaussian distributed error. The work is repeated
in binary using a monte carlo simulation for a sample of the natural numbers <
2^32 and in trinary for all natural numbers < 3^9 with similar but cubic
results. We find a significant relationship between BiEntropy and TriEntropy
such that we can discriminate between the primes and numbers divisible by six.
We discuss the theoretical underpinnings of these results and show how they
generalise to give a tight bound on the variance of Pi(x) - Li(x) for all x.
This bound is much tighter than the bound given by Von Koch in 1901 as an
equivalence for proof of the Riemann Hypothesis. Since the primes are Gaussian
due to a simple induction on the binary derivative, this implies that the twin
primes conjecture is true. We also provide absolutely convergent asymptotes for
the numbers of Fermat and Mersenne primes in the appendices.
"
1776,"Estudo comparativo de meta-heur\'isticas para problemas de
  colora\c{c}\~oes de grafos","  A classic graph coloring problem is to assign colors to vertices of any graph
so that distinct colors are assigned to adjacent vertices. Optimal graph
coloring colors a graph with a minimum number of colors, which is its chromatic
number. Finding out the chromatic number is a combinatorial optimization
problem proven to be computationally intractable, which implies that no
algorithm that computes large instances of the problem in a reasonable time is
known. For this reason, approximate methods and metaheuristics form a set of
techniques that do not guarantee optimality but obtain good solutions in a
reasonable time. This paper reports a comparative study of the Hill-Climbing,
Simulated Annealing, Tabu Search, and Iterated Local Search metaheuristics for
the classic graph coloring problem considering its time efficiency for
processing the DSJC125 and DSJC250 instances of the DIMACS benchmark.
"
1777,"Particle tracking manifestation of compressible flow and mixing induced
  by Rayleigh-Taylor instability","  Rayleigh-Taylor-instability induced flow and mixing are of great importance
both in nature and engineering scenarios. To capture the underpinning physics,
one-way coupled particles are introduced to make a Lagrangian tracking
supplement to the discrete Boltzmann simulation. Detailed morphological
information of the flow field and thermodynamic nonequilibrium behavior around
the interfaces in a two-miscible-fluid system are delineated. Via a defined
local mixedness, from densities of two kinds of particles, we quantitatively
captured the appearance position and indicated the intensity of
Kelvin-Helmholtz instability. The effects of compressibility and viscosity on
mixing are investigated separately. Both of them show two-stage effects on
mixing process. The underlying mechanism of these two-stage effects is
interpreted as that a ""hard"" system with low compressibility and/or high
viscosity is beneficial to the development of large structures at the initial
stage and a ""soft"" system with high compressibility and/or low viscosity is
favorable for the generation of small structures at the late stage. At the late
stage, for a fixed time, the field averaged mixedness exponentially decreases
with the viscosity, which indicates the existence of a characteristic viscosity
$\mu_0$. The tracer particles make possible to observe finer structures with
clear interfaces in the late mixing stage. One can clearly observe the mixing
process and distinguish tracer particles from the heavy and light fluids.
Results in this paper are helpful for understanding the mechanism of complex
compressible flow with Rayleigh-Taylor instability and present references for
related particle tracking experiments.
"
1778,"An Approach Towards Intelligent Accident Detection, Location Tracking
  and Notification System","  Advancement in transportation system has boosted speed of our lives.
Meantime, road traffic accident is a major global health issue resulting huge
loss of lives, properties and valuable time. It is considered as one of the
reasons of highest rate of death nowadays. Accident creates catastrophic
situation for victims, especially accident occurs in highways imposes great
adverse impact on large numbers of victims. In this paper, we develop an
intelligent accident detection, location tracking and notification system that
detects an accident immediately when it takes place. Global Positioning System
(GPS) device finds the exact location of accident. Global System for Mobile
(GSM) module sends a notification message including the link of location in the
google map to the nearest police control room and hospital so that they can
visit the link, find out the shortest route of the accident spot and take
initiatives to speed up the rescue process.
"
1779,A Noxious Market for Personal Data,"  Many policymakers, academics and governments have advocated for exchangeable
property rights over information as it presents a market solution to what could
be considered a market failure. Particularly in jurisdictions such as Africa,
Asia or South America, where weaker legal protections and fleeting regulatory
enforcement leaves data subjects vulnerable or exploited regardless of the
outcome. We argue that whether we could achieve this personal data economy in
which individuals have ownership rights akin to property rights over their data
should be approached with caution as a solution to ensuring individuals have
agency over their data across different legal landscapes. We present an
objection to the use of property rights, a market solution, due to the
\textit{noxious} nature of personal data - which is founded on Satz and
Sandel's objection to markets.
"
1780,A Promise Theoretic Account of the Boeing 737 Max MCAS Algorithm Affair,"  Many public controversies involve the assessment of statements about which we
have imperfect information. Without a structured approach, it is quite
difficult to develop an approach to reasoning which is not based on ad hoc
choices. Forms of logic have been used in the past to try to bring such
clarity, but these fail for a variety of reasons. We demonstrate a simple
approach to bringing a standardized approach to semantics, in certain
discourse, using Promise Theory. As a case, we use Promise Theory (PT) to
collect and structure publicly available information about the case of the MCAS
software component for the Boeing 737 Max flight control system.
"
1781,"Cybernetical Concepts for Cellular Automaton and Artificial Neural
  Network Modelling and Implementation","  As a discipline cybernetics has a long and rich history. In its first
generation it not only had a worldwide span, in the area of computer modelling,
for example, its proponents such as John von Neumann, Stanislaw Ulam, Warren
McCulloch and Walter Pitts, also came up with models and methods such as
cellular automata and artificial neural networks, which are still the
foundation of most modern modelling approaches. At the same time, cybernetics
also got the attention of philosophers, such as the Frenchman Gilbert Simondon,
who made use of cybernetical concepts in order to establish a metaphysics and a
natural philosophy of individuation, giving cybernetics thereby a philosophical
interpretation, which he baptised allagmatic. In this paper, we emphasise this
allagmatic theory by showing how Simondon's philosophical concepts can be used
to formulate a generic computer model or metamodel for complex systems
modelling and its implementation in program code, according to generic
programming. We also present how the developed allagmatic metamodel is capable
of building simple cellular automata and artificial neural networks.
"
1782,Roof Age Determination for the Automated Site-Selection of Rooftop Solar,"  Rooftop solar is one of the most promising tools for drawing down greenhouse
gas (GHG) emissions and is cost-competitive with fossil fuels in many areas of
the world today. One of the most important criteria for determining the
suitability of a building for rooftop solar is the current age of its roof. The
reason for this is simple -- because rooftop solar installations are
long-lived, the roof needs to be new enough to last for the lifetime of the
solar array or old enough to justify being replaced. In this paper we present a
data-driven method for determining the age of a roof from historical satellite
imagery, which removes one of the last obstacles to a fully automated pipeline
for rooftop solar site selection. We estimate that a full solution to this
problem would reduce customer acquisition costs for rooftop solar by
$\sim$20\%, leading to an additional $\sim$750 megatons of CO$_2$ displaced
between 2020 and 2050.
"
1783,"Efficient Programmable Random Variate Generation Accelerator from Sensor
  Noise","  We introduce a method for non-uniform random number generation based on
sampling a physical process in a controlled environment. We demonstrate one
proof-of-concept implementation of the method that reduces the error of Monte
Carlo integration of a univariate Gaussian by 1068 times while doubling the
speed of the Monte Carlo simulation. We show that the supply voltage and
temperature of the physical process must be controlled to prevent the mean and
standard deviation of the random number generator from drifting.
"
1784,Integrating data science ethics into an undergraduate major,"  We present a programmatic approach to incorporating ethics into an
undergraduate major in statistical and data sciences. We discuss
departmental-level initiatives designed to meet the National Academy of
Sciences recommendation for weaving ethics into the curriculum from
top-to-bottom as our majors progress from our introductory courses to our
senior capstone course, as well as from side-to-side through co-curricular
programming. We also provide six examples of data science ethics modules used
in five different courses at our liberal arts college, each focusing on a
different ethical consideration. The modules are designed to be portable such
that they can be flexibly incorporated into existing courses at different
levels of instruction with minimal disruption to syllabi. We conclude with next
steps and preliminary assessments.
"
1785,The Epistemic Landscape: a Computability Perspective,"  By nature, transmissible human knowledge is enumerable: every sentence,
movie, audio record can be encoded in a sufficiently long string of 0's and
1's. The works of G\""odel, Turing and others showed that there are inherent
limits and properties associated with the fact that language technology is
enumerable. G\""odel's numbering technique is universal for enumerable
structures and shows strong limits of the language technology. Computability
theory is a particular example: programs can be numbered and all sorts of
limits can be studied from there. Computability is also at the heart of science
since any experimental validation of a theory supposes that theoretical results
have been computed, then checked against concrete experiments. It implies that
limitations on what is computable ultimately are also limits of what we
understand as ""scientific theory"", and more generally to all the transmissible
knowledge. We argue that it is fruitful to look a epistemology from a
computability perspective. We show that it allows to precisely define different
kinds of knowledge acquisition techniques, and helps the study of how they are
related to one another.
"
1786,"Proposal of a standard of Knowledge Management and Technological
  Innovation for Mexico","  The purpose of this work is to offer a methodology that allows to construct a
standard in Knowledge Management and Technological Innovation which may be used
in various organizations in M\'exico to improve the operation of their
resources and productivity. Based on the review of the existing literature, a
model is offered including several elements to enable organizations to
establish their position in relation to both concepts. The following proposal
is based on a systematic effort to understand and integrate models of Knowledge
Management and Innovation published in recent years as well as the results of
the experiences to propose standards of Knowledge Management and Technological
Innovation. In order to elaborate the proposal, factors and their associated
components have been analyzed through a review of the literature in order to
build and validate a standard proposal. To test the research study, a six-stage
research model has been constructed. For this purpose, an in-depth exploratory
research study has been carried out in a public sector organization, in an area
that allows the replicability of the model. The results have been analyzed to
construct and empirically validate the Mexican Standard of Knowledge Management
an Technological Innovation. Finally, after the statistical analysis, results
obtained from the application of the validated instrument are shown , which
supports the definition of the model.
"
1787,"On the loss of learning capability inside an arrangement of neural
  networks","  We analyze the loss of information and the loss of learning capability inside
an arrangement of neural networks. Our method is new and based on the
formulation of non-unitary Bogoliubov transformations in order to connect the
information between different points of the arrangement. This can be done after
expanding the activation function in a Fourier series and then assuming that
its information is stored inside a Quantum scalar field.
"
1788,"The Separator, a Two-Phase Oil and Water Gravity CPS Separator Testbed","  Industrial Control Systems (ICS) are evolving with advances in new
technology. The addition of wireless sensors and actuators and new control
techniques means that engineering practices from communication systems are
being integrated into those used for control systems. The two are engineered in
very different ways. Neither engineering approach is capable of accounting for
the subtle interactions and interdependence that occur when the two are
combined. This paper describes our first steps to bridge this gap, and push the
boundaries of both computer communication system and control system design. We
present The Separator testbed, a Cyber-Physical testbed enabling our search for
a suitable way to engineer systems that combine both computer networks and
control systems.
"
1789,"Proceedings of the X International Workshop on Locational Analysis and
  Related Problems","  The International Workshop on Locational Analysis and Related Problems will
take place during January 23-24, 2020 in Seville (Spain). It is organized by
the Spanish Location Network and the Location Group GELOCA from the Spanish
Society of Statistics and Operations Research(SEIO). The Spanish Location
Network is a group of more than 140 researchers from several Spanish
universities organized into 7 thematic groups. The Network has been funded by
the Spanish Government since 2003.
  One of the main activities of the Network is a yearly meeting aimed at
promoting the communication among its members and between them and other
researchers, and to contribute to the development of the location field and
related problems. The last meetings have taken place in C\'adiz (January
20-February 1, 2019), Segovia (September 27-29, 2017), M\'alaga (September
14-16, 2016), Barcelona (November 25-28, 2015), Sevilla (October 1-3, 2014),
Torremolinos (M\'alaga, June 19-21, 2013), Granada (May 10-12, 2012), Las
Palmas de Gran Canaria (February 2-5, 2011) and Sevilla (February 1-3, 2010).
  The topics of interest are location analysis and related problems. This
includes location models, networks, transportation, logistics, exact and
heuristic solution methods, and computational geometry, among others.
"
1790,"On Orthogonal Projections on the Space of Consistent Pairwise
  Comparisons Matrices","  In this study, the orthogonalization process for different inner products is
applied to pairwise comparisons. Properties of consistent approximations of a
given inconsistent pairwise comparisons matrix are examined. A method of a
derivation of a priority vector induced by a pairwise comparison matrix for a
given inner product has been introduced. The mathematical elegance of
orthogonalization and its universal use in most applied sciences has been the
motivating factor for this study. However, the finding of this study that
approximations depend on the inner product assumed, is of considerable
importance.
"
1791,"Proceedings of the VI International Workshop on Locational Analysis and
  Related Problems","  The International Workshop on Locational Analysis and Related Problems will
take place during November 25-27, 2015 in Barcelona (Spain). It is organized by
the Spanish Location Network and Location Group GELOCA (SEIO). GELOCA is a
working group on location belonging to the Statistics and Operations Research
Spanish Society. The Spanish Location Network is a group of more than 140
researchers distributed into 16 nodes corresponding to several Spanish
universities. The Network has been funded by the Spanish Government. Every
year, the Network organizes a meeting to promote the communication among its
members and between them and other researchers, and to contribute to the
development of the location field and related problems. Previous meetings took
place in Sevilla (October 1-3, 2014), Torremolinos (M\'alaga, June 19-21,
2013), Granada (May 10-12, 2012), Las Palmas de Gran Canaria (February 2-5,
2011) and Sevilla (February 1-3, 2010). The topics of interest are location
analysis and related problems. This includes location, routing, networks,
transportation and logistics models; exact and heuristic solution methods, and
computational geometry, among others.
"
1792,"Proceedings of the VIII International Workshop on Locational Analysis
  and Related Problems","  The International Workshop on Locational Analysis and Related Problems will
take place during September 27-29, 2017 in Segovia (Spain). It is organized by
the Spanish Location Network and Location Group GELOCA (SEIO). GELOCA is a
working group on location belonging to the Statistics and Operations Research
Spanish Society. The Spanish Location Network is a group of more than 100
researchers distributed into 16 nodes corresponding to several Spanish
universities. The Network has been funded by the Spanish Government. Every
year, the Network organizes a meeting to promote the communication between its
members and between them and other researchers, and to contribute to the
development of the location field and related problems. Previous meetings took
place in M\'alaga (September 14-16, 2016), Barcelona (November 25-28, 2015),
Sevilla (October 1-3, 2014), Torremolinos (M\'alaga, June 19-21, 2013), Granada
(May 10-12, 2012), Las Palmas de Gran Canaria (February 2-5, 2011) and Sevilla
(February 1-3, 2010). The topics of interest are location analysis and related
problems. It includes location, networks, transportation, routing, logistics
models, as well as, exact and heuristic solution methods, and computational
geometry, among others.
"
1793,"Proceedings of the IX International Workshop on Locational Analysis and
  Related Problems","  The International Workshop on Locational Analysis and Related Problems will
take place during January 30-February 1, 2019 in C\'adiz (Spain). It is
organized by the Spanish Location Network and Location Group GELOCA (SEIO).
GELOCA is a working group on location belonging to the Statistics and
Operations Research Spanish Society. The Spanish Location Network is a group of
more than 140 researchers distributed into 16 nodes corresponding to several
Spanish universities. The Network has been funded by the Spanish Government.
Every year, the Network organizes a meeting to promote the communication
between its members and between them and other researchers, and to contribute
to the development of the location field and related problems. Previous
meetings took place in Segovia (September 27-29, 2017), M\'alaga (September
14-16, 2016), Barcelona (November 25-28, 2015), Sevilla (October 1-3, 2014),
Torremolinos (M\'alaga, June 19-21, 2013), Granada (May 10-12, 2012), Las
Palmas de Gran Canaria (February 2-5, 2011) and Sevilla (February 1-3, 2010).
The topics of interest are location analysis and related problems. It includes
location models, networks, transportation, logistics, exact and heuristic
solution methods, and computational geometry, among others.
"
1794,Toward Predicting Success and Failure in CS2: A Mixed-Method Analysis,"  Factors driving success and failure in CS1 are the subject of much study but
less so for CS2. This paper investigates the transition from CS1 to CS2 in
search of leading indicators of success in CS2. Both CS1 and CS2 at the
University of North Carolina Wilmington (UNCW) are taught in Python with annual
enrollments of 300 and 150 respectively. In this paper, we report on the
following research questions: 1) Are CS1 grades indicators of CS2 grades? 2)
Does a quantitative relationship exist between CS2 course grade and a modified
version of the SCS1 concept inventory? 3) What are the most challenging aspects
of CS2, and how well does CS1 prepare students for CS2 from the student's
perspective? We provide a quantitative analysis of 2300 CS1 and CS2 course
grades from 2013--2019. In Spring 2019, we administered a modified version of
the SCS1 concept inventory to 44 students in the first week of CS2. Further, 69
students completed an exit questionnaire at the conclusion of CS2 to gain
qualitative student feedback on their challenges in CS2 and on how well CS1
prepared them for CS2. We find that 56% of students' grades were lower in CS2
than CS1, 18% improved their grades, and 26% earned the same grade. Of the
changes, 62% were within one grade point. We find a statistically significant
correlation between the modified SCS1 score and CS2 grade points. Students
identify linked lists and class/object concepts among the most challenging.
Student feedback on CS2 challenges and the adequacy of their CS1 preparations
identify possible avenues for improving the CS1-CS2 transition.
"
1795,"Predicting Memory Compiler Performance Outputs using Feed-Forward Neural
  Networks","  Typical semiconductor chips include thousands of mostly small memories. As
memories contribute an estimated 25% to 40% to the overall power, performance,
and area (PPA) of a chip, memories must be designed carefully to meet the
system's requirements. Memory arrays are highly uniform and can be described by
approximately 10 parameters depending mostly on the complexity of the
periphery. Thus, to improve PPA utilization, memories are typically generated
by memory compilers. A key task in the design flow of a chip is to find optimal
memory compiler parametrizations which on the one hand fulfill system
requirements while on the other hand optimize PPA. Although most compiler
vendors also provide optimizers for this task, these are often slow or
inaccurate. To enable efficient optimization in spite of long compiler run
times, we propose training fully connected feed-forward neural networks to
predict PPA outputs given a memory compiler parametrization. Using an
exhaustive search-based optimizer framework which obtains neural network
predictions, PPA-optimal parametrizations are found within seconds after chip
designers have specified their requirements. Average model prediction errors of
less than 3%, a decision reliability of over 99% and productive usage of the
optimizer for successful, large volume chip design projects illustrate the
effectiveness of the approach.
"
1796,Engaging Users through Social Media in Public Libraries,"  The participatory library is an emerging concept which refers to the idea
that an integrated library system must allow users to take part in core
functions of the library rather than engaging on the periphery. To embrace the
participatory idea, libraries have employed many technologies, such as social
media to help them build participatory services and engage users. To help
librarians understand the impact of emerging technologies on a participatory
service building, this paper takes social media as an example to explore how to
use different engagement strategies that social media provides to engage more
users. This paper provides three major contributions to the library system. The
libraries can use the resultant engagement strategies to engage its users.
Additionally, the best-fit strategy can be inferred and designed based on the
preferences of users. Lastly, the preferences of users can be understood based
on data analysis of social media. Three such contributions put together to
fully address the proposed research question of how to use different engagement
strategies on social media to build participatory library services and better
engage more users visiting the library?
"
1797,"Toward a Wearable RFID System for Real-Time Activity Recognition Using
  Radio Patterns","  Elderly care is one of the many applications supported by real-time activity
recognition systems. Traditional approaches use cameras, body sensor networks,
or radio patterns from various sources for activity recognition. However, these
approaches are limited due to ease-of-use, coverage, or privacy preserving
issues. In this paper, we present a novel wearable Radio Frequency
Identification (RFID) system aims at providing an easy-to-use solution with
high detection coverage. Our system uses passive tags which are
maintenance-free and can be embedded into the clothes to reduce the wearing and
maintenance efforts. A small RFID reader is also worn on the user's body to
extend the detection coverage as the user moves. We exploit RFID radio patterns
and extract both spatial and temporal features to characterize various
activities. We also address the issues of false negative of tag readings and
tag/antenna calibration, and design a fast online recognition system. Antenna
and tag selection is done automatically to explore the minimum number of
devices required to achieve target accuracy. We develop a prototype system
which consists of a wearable RFID system and a smartphone to demonstrate the
working principles, and conduct experimental studies with four subjects over
two weeks. The results show that our system achieves a high recognition
accuracy of 93.6 percent with a latency of 5 seconds. Additionally, we show
that the system only requires two antennas and four tagged body parts to
achieve a high recognition accuracy of 85 percent.
"
1798,NDE 4.0 From Design Thinking to Strategy,"  Cyber technologies are offering new horizons for quality control in
manufacturing and safety assurance in-service of physical assets. The line
between non-destructive evaluation (NDE) and Industry 4.0 is getting blurred
since both are sensory data-driven domains. This multidisciplinary approach has
led to the emergence of a new capability: NDE 4.0. The NDT community is coming
together once again to define the purpose, chart the process, and address the
adoption of emerging technologies. In this paper, the authors have taken a
design thinking approach to spotlight proper objectives for research on this
subject. It begins with qualitative research on twenty different perceptions of
stakeholders and misconceptions around the current state of NDE. The
interpretation is used to define ten value propositions or use cases under ""NDE
for Industry 4.0"" and ""Industry 4.0 for NDE"" leading up to the clarity of
purpose for NDE 4.0, enhanced safety and economic value for stakeholders. To
pursue this worthy cause, the paper delves into some of the top adoption
challenges, and proposes a journey of managed innovation, conscious skills
development, and a new form of leadership required to succeed in the
cyber-physical world.
"
1799,Modeling and solving a vehicle-sharing problem,"  Motivated by the change in mobility patterns, we present a new modeling
approach for the vehicle-sharing problem. We aim at assigning vehicles to
user-trips so as to maximize savings compared to other modes of transport. We
base our formulations on the minimum-cost and the multi-commodity flow problem.
These formulations make the problem applicable in daily operations. In the
analysis we discuss an optimal composition of a shared fleet, restricted sets
of modes of transport, and variations of the objective function.
"
1800,"TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production
  Systems","  Digital twin is a virtual replica of a real-world object that lives
simultaneously with its physical counterpart. Since its first introduction in
2003 by Grieves, digital twin has gained momentum in a wide range of
applications such as industrial manufacturing, automotive and artificial
intelligence. However, many digital-twin-related approaches, found in
industries as well as literature, mainly focus on modelling individual physical
things with high-fidelity methods with limited scalability. In this paper, we
introduce a digital-twin architecture called TiLA (Twin-in-the-Loop
Architecture). TiLA employs heterogeneous models and online data to create a
digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS)
model of computation. It facilitates the creation of a scalable digital twin
with different levels of modelling abstraction as well as giving GALS formalism
for execution strategy. Furthermore, TiLA provides facilities to develop
applications around the twin as well as an interface to synchronise the twin
with the physical system through an industrial communication protocol. A
digital twin for a manufacturing line has been developed as a case study using
TiLA. It demonstrates the use of digital twin models together with online data
for monitoring and analysing failures in the physical system.
"
1801,"Reconfigurable Computing Applied to Latency Reduction for the Tactile
  Internet","  Tactile internet applications allow robotic devices to be remotely controlled
over a communication medium with an unnoticeable time delay. In a bilateral
communication, the acceptable round trip latency is usually in the order of 1ms
up to 10ms depending on the application requirements. It is estimated that 70%
of the total latency is generated by the communication network, and the
remaining 30% is produced by master and slave devices. Thus, this paper aims to
propose a strategy to reduce 30% of the total latency that is produced by such
devices. The strategy is to apply reconfigurable computation using FPGAs to
minimize the execution time of device-associated algorithms. With this in mind,
this work presents a hardware reference model for modules that implement
nonlinear positioning and force calculations as well as a tactile system formed
by two robotic manipulators. In addition to presenting the implementation
details, simulations and experimental tests are performed in order to validate
the proposed model. Results associated with the FPGA sampling rate, throughput,
latency, and post-synthesis occupancy area are analyzed.
"
1802,"Objective Multi-variable Classification and Inference of Biological
  Neuronal Networks","  Classification of biological neuron types and networks poses challenges to
the full understanding of the brain's organisation and functioning. In this
paper, we develop a novel objective classification model of biological neuronal
types and networks based on the communication metrics of neurons. This presents
advantages against the existing approaches since the mutual information or the
delay between neurons obtained from spike trains are more abundant data compare
to conventional morphological data. We firstly designed two open-access
supporting computational platforms of various neuronal circuits from the Blue
Brain Project realistic models, named Neurpy and Neurgen. Then we investigate
how the concept of network tomography could be achieved with cortical neuronal
circuits for morphological, topological and electrical classification of
neurons. We extract the simulated data to many different classifiers (including
SVM, Decision Trees, Random Forest, and Artificial Neuron Networks) classifying
the specific cell type (and sub-group types) achieving accuracies of up to
70\%. Inference of biological network structures using network tomography
reached up to 65\% of accuracy. We also analysed recall, precision and F1score
of the classification of five layers, 25 cell m-types, and 14 cell e-types. Our
research not only contributes to existing classification efforts but sets the
road-map for future usage of cellular-scaled brain-machine interfaces for
in-vivo objective classification of neurons as a sensing mechanism of the
brain's structure.
"
1803,"Current Practices in the Information Collection for Enterprise
  Architecture Management","  The digital transformation influences business models, processes, and
enterprise IT landscape as a whole. Therefore, business-IT alignment is
becoming more important than ever before. Enterprise architecture management
(EAM) is designed to support and improve this business-IT alignment. The
success of EAM crucially depends on the information available about a company's
enterprise architecture, such as infrastructure components, applications, and
business processes. This paper discusses the results of a qualitative expert
survey with 26 experts in the field of EAM. The goal of this survey was to
highlight current practices in the information collection for EAM and identify
relevant information from enterprise-external data sources. The results provide
a comprehensive overview of collected and utilized information in the industry,
including an assessment of the relevance of such information. Furthermore, the
results highlight challenges in practice and point out investments that
organizations plan in the field of EAM.
"
1804,"NDE 4.0: Digital Twin, Semantics, Interfaces, Networking, Feedback, New
  Markets and Integration into the Industrial Internet of Things","  The industrial revolution is divided into three phases by historians: The
invention of the steam engine (mechanization), electricity (mass production)
and the microelectric revolution (automation). There was a similar development
in non-destructive evaluation: tools such as lenses or stethoscopes allowed the
human senses to be sharpened, the conversion of waves makes the invisible
visible and thus offers a ""look"" into the components and finally automation,
digitization and reconstruction. During the entire industrial development NDE
was decisively responsible for the quality and thus for the success of the
manufactured goods. Industry is now talking about a fourth revolution: The
informatization, digitization and networking of industrial production. As
always, NDE will be critical to the success of this fourth revolution by
providing the database needed for feedback in a networked production
environment. For NDE, this will lead to change. The test results must be made
available to a networked production environment in such a way that they can be
evaluated for feedback loops, the testability must be considered in the design
and the reliability of the test statements will become increasingly important.
This publication presents first an orientation to NDE 4.0, including the
development of Industry and NDE, a definition of its revolutions, a collection
of several current-day challenges of NDE, and a discussion whether and how
those can be solved with NDE 4.0. Second this publication presents concepts on
how NDE can be integrated into Industry 4.0 landscapes: The Reference
Architecture Model Industry 4.0 (RAMI 4.0) shows the complete Industry 4.0
space and allows every Industry 4.0 standard and interface to be located. The
Industry 4.0 Asset Administration Shell (AAS) implements the digital twin and
is the interface between Industry 4.0 communication and the physical device.
The ...
"
1805,"Software-Based Monitoring and Analysis of a USB Host Controller Subject
  to Electrostatic Discharge","  Observing, understanding, and mitigating the effects of failure in embedded
systems is essential for building dependable control systems. We develop a
software-based monitoring methodology to further this goal. This methodology
can be applied to any embedded system peripheral and allows the system to
operate normally while the monitoring software is running. We use software to
instrument the operating system kernel and record indicators of system
behavior. By comparing those indicators against baseline indicators of normal
system operation, faults can be detected and appropriate action can be taken.
  We implement this methodology to detect faults caused by electrostatic
discharge in a USB host controller. As indicators, we select specific control
registers that provide a manifestation of the internal execution of the host
controller. Analysis of the recorded register values reveals differences in
system execution when the system is subject to interference. %We also develop a
classifier capable of predicting whether or not the system's behavior is being
affected by such shocks. This improved understanding of system behavior may
lead to better hardware and software mitigation of electrostatic discharge and
assist in root-cause analysis and repair of failures.
"
1806,Dataset for anomalies detection in 3D printing,"  Nowadays, Internet of Things plays a significant role in many domains.
Especially, Industry 4.0 is making a great usage of concepts like smart sensors
and big data analysis. IoT devices are commonly used to monitor industry
machines and detect anomalies in their work. In this paper we present and
describe a set of data streams coming from working 3D printer. Among others, it
contains accelerometer data of printer head, intrusion power and temperatures
of the printer elements. In order to gain data we lead to several printing
malfunctions applied to the 3D model. Resulting dataset can therefore be used
for anomalies detection research.
"
1807,Knowledge Management Systems Requirements Specifications,"  In recent years, Knowledge Management Systems (KMS) have drawn remarkable
attention. However, there is no common understanding of how a knowledge
management system should look like or where the corresponding research should
be directed at. Based on a number of essential requirements that a KMS should
satisfy, this report introduces some possible requirements for the
commonwealth's KMS components forming the KMS architecture. Also, these
requirements will be analysed through evaluating and measuring there
functionality to produce a tangible outcome.
"
1808,"Distributed Resources for the Earth System Grid Advanced Management
  (DREAM)","  The DREAM project was funded more than 3 years ago to design and implement a
next-generation ESGF (Earth System Grid Federation [1]) architecture which
would be suitable for managing and accessing data and services resources on a
distributed and scalable environment. In particular, the project intended to
focus on the computing and visualization capabilities of the stack, which at
the time were rather primitive. At the beginning, the team had the general
notion that a better ESGF architecture could be built by modularizing each
component, and redefining its interaction with other components by defining and
exposing a well defined API. Although this was still the high level principle
that guided the work, the DREAM project was able to accomplish its goals by
leveraging new practices in IT that started just about 3 or 4 years ago: the
advent of containerization technologies (specifically, Docker), the development
of frameworks to manage containers at scale (Docker Swarm and Kubernetes), and
their application to the commercial Cloud. Thanks to these new technologies,
DREAM was able to improve the ESGF architecture (including its computing and
visualization services) to a level of deployability and scalability beyond the
original expectations.
"
1809,"Business Process Re-engineering in Supply Chains Examining the case of
  the expanding Halal industry","  Due to several issues arising in the rapidly-expanding Halal industry, among
them the production of non-genuine or contaminated products and meats, there is
a need to develop effective solutions for ensuring authenticity and quality.
This paper proposes the specification of a formalized supply chain framework
for the production and monitoring of food and products. The latter enforces
high-level quality of automated monitoring as well as shorter production cycles
through enhanced coordination between the actors and organizations involved.
Our proposal is guided by business process support to ensure quality and
efficiency of product development and delivery. It moreover meets the
requirements of industrial standards by adopting the Capability Maturity Model
Integration highest process maturity level through establishing quantitative
process-improvement objectives, proposing the integrated support of engineering
processes, enforcing synchronization and coordination, drastic monitoring and
exception handling. We then delve into some of the important technologies from
the implementation point-of-view and align it with the formalized Halal
framework. An Information Technology support instantiation is proposed leading
to a use case scenario with technology identification.
"
1810,Correlating Unlabeled Events at Runtime,"  Process mining is of great importance for both data-centric and
process-centric systems. Process mining receives so-called process logs which
are collections of partially-ordered events. An event has to possess at least
three attributes, case ID, task ID and a timestamp for mining approaches to
work. When a case ID is unknown, the event is called unlabeled. Traditionally,
process mining is an offline task, where events are collected from different
sources are usually manually correlated. That is, events belonging to the same
instance are assigned the same case ID. With today's high-volume/high-speed
nature of, e.g., IoT applications, process mining shifts to be an online task.
For this, event correlation has to be automated and has to occur as the data is
generated. In this paper, we introduce an approach that correlates unlabeled
events at runtime. Given a process model, a stream of unlabeled events and
other information about task duration, our approach can induce a case
identifier to a set of unlabeled events with a trust percentage. It can also
check the conformance of the identified cases with the process model. A
prototype of the proposed approach was implemented and evaluated against
real-life and synthetic logs.
"
1811,ROOT I/O compression improvements for HEP analysis,"  We overview recent changes in the ROOT I/O system, increasing performance and
enhancing it and improving its interaction with other data analysis ecosystems.
Both the newly introduced compression algorithms, the much faster bulk I/O data
path, and a few additional techniques have the potential to significantly to
improve experiment's software performance. The need for efficient lossless data
compression has grown significantly as the amount of HEP data collected,
transmitted, and stored has dramatically increased during the LHC era. While
compression reduces storage space and, potentially, I/O bandwidth usage, it
should not be applied blindly: there are significant trade-offs between the
increased CPU cost for reading and writing files and the reduce storage space.
"
1812,Quantum Approximation for Wireless Scheduling,"  This paper proposes a quantum approximate optimization algorithm (QAOA)
method for wireless scheduling problems. The QAOA is one of the promising
hybrid quantum-classical algorithms for many applications and it provides
highly accurate optimization solutions in NP-hard problems. QAOA maps the given
problems into Hilbert spaces, and then it generates Hamiltonian for the given
objectives and constraints. Then, QAOA finds proper parameters from classical
optimization approaches in order to optimize the expectation value of generated
Hamiltonian. Based on the parameters, the optimal solution to the given problem
can be obtained from the optimum of the expectation value of Hamiltonian.
Inspired by QAOA, a quantum approximate optimization for scheduling (QAOS)
algorithm is proposed. First of all, this paper formulates a wireless
scheduling problem using maximum weight independent set (MWIS). Then, for the
given MWIS, the proposed QAOS designs the Hamiltonian of the problem. After
that, the iterative QAOS sequence solves the wireless scheduling problem. This
paper verifies the novelty of the proposed QAOS via simulations implemented by
Cirq and TensorFlow-Quantum.
"
1813,"Convolutional Neural Networks vs. Deformable Image Registration For
  Medical Slice Interpolation","  Medical image slice interpolation is an active field of research. The methods
for this task can be categorized into two broad groups: intensity-based and
object-based interpolation methods. While intensity-based methods are generally
easier to perform and less computationally expensive, object-based methods are
capable of producing more accurate results and account for deformable changes
in the objects within the slices. In this paper, performance of two well-known
object-based interpolation methods is analyzed and compared. Here, a deformable
registration-based method specifically designed for medical applications and a
learning-based method, trained for video frame interpolation, are considered.
While the deformable registration-based technique is capable of accurate
modeling of the changes in the shapes of the objects within slices, the
learning-based method is able to produce results with similar accuracy, but
with a much sharper appearance in a fraction of the time. This is despite the
fact that the learning-based approach is not trained on medical images and
rather is trained using regular video footage. However, experiments show that
the method is capable of accurate slice interpolation results.
"
1814,"Interactive distributed cloud-based web-server systems for the smart
  healthcare industry","  The work aims to investigate the possible contemporary interactive cloud
based solutions in the fields of the applied medicine for the smart Healthcare
as the data visualization open-source free system distributed under the MIT
license. A comparative study of a number of the well-known implementations of
the Ray Casting algorithms was studied. A new method of numerical calculus is
proposed for calculating the volume -- the method of spheres, as well as a
proposal for paralleling the algorithm on graphic accelerators in a linearly
homogeneous computing environment using the block decomposition methods. For
the artifacts control -- algorithm of the cubic interpolation was used. The
cloud server architecture was proposed.
"
1815,AI in society and culture: decision making and values,"  With the increased expectation of artificial intelligence, academic research
face complex questions of human-centred, responsible and trustworthy technology
embedded into society and culture. Several academic debates, social
consultations and impact studies are available to reveal the key aspects of the
changing human-machine ecosystem. To contribute to these studies, hundreds of
related academic sources are summarized below regarding AI-driven decisions and
valuable AI. In details, sociocultural filters, taxonomy of human-machine
decisions and perspectives of value-based AI are in the focus of this
literature review. For better understanding, it is proposed to invite
stakeholders in the prepared large-scale survey about the next generation AI
that investigates issues that go beyond the technology.
"
1816,"High Performance Interference Suppression in Multi-User Massive MIMO
  Detector","  In this paper, we propose a new nonlinear detector with improved interference
suppression in Multi-User Multiple Input, Multiple Output (MU-MIMO) system. The
proposed detector is a combination of the following parts: QR decomposition
(QRD), low complexity users sorting before QRD, sorting-reduced (SR) K-best
method and minimum mean square error (MMSE) pre-processing. Our method
outperforms a linear interference rejection combining (IRC, i.e. MMSE
naturally) method significantly in both strong interference and additive white
noise scenarios with both ideal and real channel estimations. This result has
wide application importance for scenarios with strong interference, i.e. when
co-located users utilize the internet in stadium, highway, shopping center,
etc. Simulation results are presented for the non-line of sight 3D-UMa model of
5G QuaDRiGa 2.0 channel for 16 highly correlated single-antenna users with
QAM16 modulation in 64 antennas of Massive MIMO system. The performance was
compared with MMSE and other detection approaches.
"
1817,Neighbourhood Evaluation Criteria for Vertex Cover Problem,"  Neighbourhood Evaluation Criteria is a heuristical approximate algorithm that
attempts to solve the Minimum Vertex Cover. degree count is kept in check for
each vertex and the highest count based vertex is included in our cover set. In
the case of multiple equivalent vertices, the one with the lowest neighbourhood
influence is selected. In the case of still existing multiple equivalent
vertices, the one with the lowest remaining active vertex count (the highest
Independent Set enabling count) is selected as a tie-breaker.
"
1818,White Paper on Business of 6G,"  Developing products, services and vertical applications for the future
digitized society in the 6G era requires a multidisciplinary approach and a
re-definition of how we create, deliver and consume network resources, data and
services for both communications and sensing purposes. This development will
change and disrupt the traditional business models and ecosystem roles of
digital service providers, as well as open the market for key stakeholders in
the 6G era like digital service operators, cloud operators and resource
brokers. White paper discusses unprecedented opportunities of enabling and
empowering multiple stakeholders to have a more active participation in the
future 6G ecosystem via novel sustainable open ecosystemic business models with
flexible integration of long tail services with tailored performance
attributes. This research adopts a qualitative scenario planning method and
portrays three scenario themes resulting in a total of 12 scenarios for the
futures of the 6G business. By focusing on key trends, their interactions, and
irreducible uncertainties, scenario building generates perspectives for the
futures within which alternative 6G business strategies were developed and
assessed for a traditional incumbent mobile network operator and a novel 6G
digital service provider stemming from redefined sustainable economics.
Value-capture in the 6G era requires understanding the dynamics of platforms
and ecosystems. Results indicate that, to reach some of the preferred futures,
we should pay attention to the privacy and security issues related to business
and regulation needs; public/governmental, corporate, community and user(s)
perspectives to and aims of governance; ecosystem configuration related to
users, decentralized business models and platforms; user empowerment; and the
role of location-specificity of services.
"
1819,Dyslexia and Dysgraphia prediction: A new machine learning approach,"  Learning disabilities like dysgraphia, dyslexia, dyspraxia, etc. interfere
with academic achievements but have also long terms consequences beyond the
academic time. It is widely admitted that between 5% to 10% of the world
population is subject to this kind of disabilities. For assessing such
disabilities in early childhood, children have to solve a battery of tests.
Human experts score these tests, and decide whether the children require
specific education strategy on the basis of their marks. The assessment can be
lengthy, costly and emotionally painful. In this paper, we investigate how
Artificial Intelligence can help in automating this assessment. Gathering a
dataset of handwritten text pictures and audio recordings, both from standard
children and from dyslexic and/or dysgraphic children, we apply machine
learning techniques for classification in order to analyze the differences
between dyslexic/dysgraphic and standard readers/writers and to build a model.
The model is trained on simple features obtained by analysing the pictures and
the audio files. Our preliminary implementation shows relatively high
performances on the dataset we have used. This suggests the possibility to
screen dyslexia and dysgraphia via non-invasive methods in an accurate way as
soon as enough data are available.
"
1820,"System of Computer Modeling and Features of their use in the Educational
  Process of General Secondary Eeducation","  The article analyzes the historical aspect of the formation of computer
modeling as one of the perspective directions of educational process
development. The notion of ""system of computer modeling"", conceptual model of
system of computer modeling (SCMod), its components (mathematical, animation,
graphic, strategic), functions, principles and purposes of use are grounded.
The features of the organization of students work using SCMod, individual and
group work, the formation of subject competencies are described; the aspect of
students' motivation to learning is considered. It is established that
educational institutions can use SCMod at different levels and stages of
training and in different contexts, which consist of interrelated physical,
social, cultural and technological aspects. It is determined that the use of
SCMod in general secondary school would increase the capacity of teachers to
improve the training of students in natural and mathematical subjects and
contribute to the individualization of the learning process, in order to meet
the pace, educational interests and capabilities of each particular student. It
is substantiated that the use of SCMod in the study of natural-mathematical
subjects contributes to the formation of subject competencies, develops the
skills of analysis and decision-making, increases the level of digital
communication, develops vigilance, raises the level of knowledge, increases the
duration of attention of students. Further research requires the justification
of the process of forming students' competencies in natural-mathematical
subjects and designing cognitive tasks using SCMod.
"
1821,"AGI and the Knight-Darwin Law: why idealized AGI reproduction requires
  collaboration","  Can an AGI create a more intelligent AGI? Under idealized assumptions, for a
certain theoretical type of intelligence, our answer is: ""Not without outside
help"". This is a paper on the mathematical structure of AGI populations when
parent AGIs create child AGIs. We argue that such populations satisfy a certain
biological law. Motivated by observations of sexual reproduction in
seemingly-asexual species, the Knight-Darwin Law states that it is impossible
for one organism to asexually produce another, which asexually produces
another, and so on forever: that any sequence of organisms (each one a child of
the previous) must contain occasional multi-parent organisms, or must
terminate. By proving that a certain measure (arguably an intelligence measure)
decreases when an idealized parent AGI single-handedly creates a child AGI, we
argue that a similar Law holds for AGIs.
"
1822,"SciANN: A Keras/Tensorflow wrapper for scientific computations and
  physics-informed deep learning using artificial neural networks","  In this paper, we introduce SciANN, a Python package for scientific computing
and physics-informed deep learning using artificial neural networks. SciANN
uses the widely used deep-learning packages Tensorflow and Keras to build deep
neural networks and optimization models, thus inheriting many of Keras's
functionalities, such as batch optimization and model reuse for transfer
learning. SciANN is designed to abstract neural network construction for
scientific computations and solution and discovery of partial differential
equations (PDE) using the physics-informed neural networks (PINN) architecture,
therefore providing the flexibility to set up complex functional forms. We
illustrate, in a series of examples, how the framework can be used for curve
fitting on discrete data, and for solution and discovery of PDEs in strong and
weak forms. We summarize the features currently available in SciANN, and also
outline ongoing and future developments.
"
1823,Acceptance of e-procurement in organisations,"  This research is concerned with the development of a realistic model for
e-procurement adoption by organisations and groups observing the Rules of
Islamic Sharia (RIS). This model is intended to be based on the behavioural
control, subjective norms, and the recognition of the benefits and risks of e
procurement adoption. The developed model,(E-PAM), combined and extended two
existing models previously used for information technology adoption. Central to
the design of the E-PAM is the principle that a realistic model should consider
all relevant psychological, social, cultural, demography, and religious
factors. .
"
1824,"Optimizing Visual Cortex Parameterization with Error-Tolerant
  Teichmuller Map in Retinotopic Mapping","  The mapping between the visual input on the retina to the cortical surface,
i.e., retinotopic mapping, is an important topic in vision science and
neuroscience. Human retinotopic mapping can be revealed by analyzing cortex
functional magnetic resonance imaging (fMRI) signals when the subject is under
specific visual stimuli. Conventional methods process, smooth, and analyze the
retinotopic mapping based on the parametrization of the (partial) cortical
surface. However, the retinotopic maps generated by this approach frequently
contradict neuropsychology results. To address this problem, we propose an
integrated approach that parameterizes the cortical surface, such that the
parametric coordinates linearly relates the visual coordinate. The proposed
method helps the smoothing of noisy retinotopic maps and obtains
neurophysiological insights in human vision systems. One key element of the
approach is the Error-Tolerant Teichmuller Map, which uniforms the angle
distortion and maximizes the alignments to self-contradicting landmarks. We
validated our overall approach with synthetic and real retinotopic mapping
datasets. The experimental results show the proposed approach is superior in
accuracy and compatibility. Although we focus on retinotopic mapping, the
proposed framework is general and can be applied to process other human sensory
maps.
"
1825,"Chook -- A comprehensive suite for generating binary optimization
  problems with planted solutions","  We present Chook, an open-source Python-based tool to generate discrete
optimization problems of tunable complexity with a priori known solutions.
Chook provides a cross-platform unified environment for solution planting using
a number of techniques, such as tile planting, Wishart planting, equation
planting, and deceptive cluster loop planting. Chook also incorporates planted
solutions for higher-order (beyond quadratic) binary optimization problems. The
support for various planting schemes and the tunable hardness allows the user
to generate problems with a wide range of complexity on different graph
topologies ranging from hypercubic lattices to fully-connected graphs.
"
1826,COVID-19 Epidemic Study II: Phased Emergence From the Lockdown in Mumbai,"  The nation-wide lockdown starting 25 March 2020, aimed at suppressing the
spread of the COVID-19 disease, was extended until 31 May 2020 in three
subsequent orders by the Government of India. The extended lockdown has had
significant social and economic consequences and `lockdown fatigue' has likely
set in. Phased reopening began from 01 June 2020 onwards. Mumbai, one of the
most crowded cities in the world, has witnessed both the largest number of
cases and deaths among all the cities in India (41986 positive cases and 1368
deaths as of 02 June 2020). Many tough decisions are going to be made on
re-opening in the next few days. In an earlier IISc-TIFR Report, we presented
an agent-based city-scale simulator(ABCS) to model the progression and spread
of the infection in large metropolises like Mumbai and Bengaluru. As discussed
in IISc-TIFR Report 1, ABCS is a useful tool to model interactions of city
residents at an individual level and to capture the impact of
non-pharmaceutical interventions on the infection spread. In this report we
focus on Mumbai. Using our simulator, we consider some plausible scenarios for
phased emergence of Mumbai from the lockdown, 01 June 2020 onwards. These
include phased and gradual opening of the industry, partial opening of public
transportation (modelling of infection spread in suburban trains), impact of
containment zones on controlling infections, and the role of compliance with
respect to various intervention measures including use of masks, case
isolation, home quarantine, etc. The main takeaway of our simulation results is
that a phased opening of workplaces, say at a conservative attendance level of
20 to 33\%, is a good way to restart economic activity while ensuring that the
city's medical care capacity remains adequate to handle the possible rise in
the number of COVID-19 patients in June and July.
"
1827,"Generation of Complex Road Networks Using a Simplified Logical
  Description for the Validation of Automated Vehicles","  Simulation is a valuable building block for the verification and validation
of automated driving functions (ADF). When simulating urban driving scenarios,
simulation maps are one important component. Often, the generation of those
road networks is a time consuming and manual effort. Furthermore, typically
many variations of a distinct junction or road section are demanded to ensure
that an ADF can be validated in the process of releasing those functions to the
public. Therefore, in this paper, we present a prototypical solution for a
logical road network description which is easy to maintain and modify. The
concept aims to be non-redundant so that changes of distinct quantities do not
affect other places in the code and thus the variation of maps is
straightforward. In addition, the simple definition of junctions is a focus of
the work. Intersecting roads are defined separately, are then set in relation
and the junction is finally generated automatically. The idea is to derive the
description from a commonly used, standardized format for simulation maps in
order to generate this format from the introduced logical description.
Consequently, we developed a command-line tool that generates the standardized
simulation map format OpenDRIVE.
"
1828,"CAN-D: A Modular Four-Step Pipeline for Comprehensively Decoding
  Controller Area Network Data","  Controller area networks (CANs) are a broadcast protocol for real-time
communication of critical vehicle subsystems. Manufacturers of passenger
vehicles hold secret their mappings of CAN data to vehicle signals, and these
definitions vary per make, model, and year. Without these mappings, the wealth
of real-time vehicle information hidden in CAN packets is uninterpretable--
severely impeding vehicle-related research including CAN cybersecurity,
after-market tuning, efficiency and performance monitoring, and fault
diagnosis. Guided by the four-part CAN signal definition, we present CAN-D (CAN
Decoder), a modular, four-step pipeline for identifying each signal's
boundaries (start bit and length), endianness (byte ordering), signedness
(bit-to-integer encoding), and meaningful, physical interpretation (label,
unit, scaling factors). En route to CAN-D, we provide a comprehensive review of
the CAN signal reverse engineering research. All previous methods ignore
endianness and signedness, rendering them simply incapable of decoding many
standard CAN signal definitions. We formulate and provide an efficient solution
to an optimization problem, allowing identification of the optimal set of
signal boundaries and byte orderings. In addition, we provide two novel,
state-of-the-art signal boundary classifiers (both superior to previous
approaches in precision and recall) and the first signedness classification
algorithm, which exhibits > 97% F-score. Overall, CAN-D is the only solution
with the potential to extract any CAN signal and is the state of the art. In
evaluation on ten vehicles of different makes, CAN-D's average $\ell^1$ error
is 5 times better than all preceding methods and exhibits lower average error
even when considering only signals that meet prior methods' assumptions.
Finally, CAN-D is implemented in lightweight hardware allowing OBD-II plugin
for real-time in-vehicle CAN decoding.
"
1829,Smart Motion Detection System using Raspberry Pi,"  This paper throws light on the security issues that modern day homes and
businesses face and describes the implementation of a motion detection system
using Raspberry Pi which could be an effective solution to address the security
concerns. The goal of the solution is to provide an implementation that uses
PIR motion sensors for motion detection and sends notifications to users via
emails. Furthermore, the system is verified using a verification tool named
UPPAAL.
"
1830,The Metastable Behavior of a Schmitt-Trigger,"  Schmitt-Trigger circuits are the method of choice for converting general
signal shapes into clean, well-behaved digital ones. In this context these
circuits are often used for metastability handling, as well. However, like any
other positive feedback circuit, a Schmitt-Trigger can become metastable
itself. Therefore, its own metastable behavior must be well understood; in
particular the conditions that may cause its metastability. In this paper we
will build on existing results from Marino to show that (a) a monotonic input
signal can cause late transitions but never leads to a non-digital voltage at
the Schmitt-Trigger output, and (b) a non-monotonic input can pin the
Schmitt-Trigger output to a constant voltage at any desired (also non-digital)
level for an arbitrary duration. In fact, the output can even be driven to any
waveform within the dynamic limits of the system. We will base our analysis on
a mathematical model of a Schmitt-Trigger's dynamic behavior and perform SPICE
simulations to support our theory and confirm its validity for modern CMOS
implementations. Furthermore, we will discuss several use cases of a
Schmitt-Trigger in the light of our results.
"
1831,Does Cascading Schmitt-Trigger Stages Improve the Metastable Behavior?,"  Schmitt-Trigger stages are the method of choice for robust discretization of
input voltages with excessive transition times or significant noise. However,
they may suffer from metastability. Based on the experience that the cascading
of flip-flop stages yields a dramatic improvement of their overall
metastability hardness, in this paper we elaborate on the question whether the
cascading of Schmitt-Trigger stages can obtain a similar gain. We perform a
theoretic analysis that is backed up by an existing metastability model for a
single Schmitt-Trigger stage and elaborate some claims about the behavior of a
Schmitt-Trigger cascade. These claims suggest that the occurrence of
metastability is indeed reduced from the first stage to the second which
suggests an improvement. On the downside, however, it becomes clear that
metastability can still not be completely ruled out, and in some cases the
behavior of the cascade may be less beneficial for a given application, e.g. by
introducing seemingly acausal transitions. We validate our findings by
extensive HSPICE simulations in which we directly cover our most important
claims.
"
1832,A Faithful Binary Circuit Model with Adversarial Noise,"  Accurate delay models are important for static and dynamic timing analysis of
digital circuits, and mandatory for formal verification. However, F\""ugger et
al. [IEEE TC 2016] proved that pure and inertial delays, which are employed for
dynamic timing analysis in state-of-the-art tools like ModelSim, NC-Sim and
VCS, do not yield faithful digital circuit models. Involution delays, which are
based on delay functions that are mathematical involutions depending on the
previous-output-to-input time offset, were introduced by F\""ugger et al.
[DATE'15] as a faithful alternative (that can easily be used with existing
tools). Although involution delays were shown to predict real signal traces
reasonably accurately, any model with a deterministic delay function is
naturally limited in its modeling power. In this paper, we thus extend the
involution model, by adding non-deterministic delay variations (random or even
adversarial), and prove analytically that faithfulness is not impaired by this
generalization. Albeit the amount of non-determinism must be considerably
restricted to ensure this property, the result is surprising: the involution
model differs from non-faithful models mainly in handling fast glitch trains,
where small delay shifts have large effects. This originally suggested that
adding even small variations should break the faithfulness of the model, which
turned out not to be the case. Moreover, the results of our simulations also
confirm that this generalized involution model has larger modeling power and,
hence, applicability.
"
1833,Converting Biomechanical Models from OpenSim to MuJoCo,"  OpenSim is a widely used biomechanics simulator with several anatomically
accurate human musculo-skeletal models. While OpenSim provides useful tools to
analyse human movement, it is not fast enough to be routinely used for emerging
research directions, e.g., learning and simulating motor control through deep
neural networks and Reinforcement Learning (RL). We propose a framework for
converting OpenSim models to MuJoCo, the de facto simulator in machine learning
research, which itself lacks accurate musculo-skeletal human models. We show
that with a few simple approximations of anatomical details, an OpenSim model
can be automatically converted to a MuJoCo version that runs up to 600 times
faster. We also demonstrate an approach to computationally optimize MuJoCo
model parameters so that forward simulations of both simulators produce similar
results.
"
1834,"Artificial Buildings: Safety, Complexity and a Quantifiable Measure of
  Beauty","  A place to live is one of the most crucial necessities for all living
organisms since the advent of life on planet Earth. The nature of homes has
changed considerably over time. At the very early stages, human begins lived in
natural places such as caves. Later on, they started to use their intelligence
to build places with special purposes. Nowadays, modern technologies such as
robotics and artificial intelligence have made their ways into the construction
process and opened up a whole new area of opportunities and concerns that may
be of interest to both technologists and philosophers. In this article, I
review the evolution of buildings from fully natural to fully artificial and
discuss philosophical thoughts that a fully automated construction technology
may raise. I elaborate on the safety concerns of a fully automated
architectural process. Then, I'll borrow Kolmogorov complexity from algorithmic
information theory to define a complexity measure for buildings. The proposed
measure is then used to provide a quantifiable measure of beauty.
"
1835,"Run-Time Power Modelling in Embedded GPUs with Dynamic Voltage and
  Frequency Scaling","  This paper investigates the application of a robust CPU-based power modelling
methodology that performs an automatic search of explanatory events derived
from performance counters to embedded GPUs. A 64-bit Tegra TX1 SoC is
configured with DVFS enabled and multiple CUDA benchmarks are used to train and
test models optimized for each frequency and voltage point. These optimized
models are then compared with a simpler unified model that uses a single set of
model coefficients for all frequency and voltage points of interest. To obtain
this unified model, a number of experiments are conducted to extract
information on idle, clock and static power to derive power usage from a single
reference equation. The results show that the unified model offers competitive
accuracy with an average 5\% error with four explanatory variables on the test
data set and it is capable to correctly predict the impact of voltage,
frequency and temperature on power consumption. This model could be used to
replace direct power measurements when these are not available due to hardware
limitations or worst-case analysis in emulation platforms.
"
1836,"Environmental Impact of Bundling Transport Deliveries Using SUMO:
  Analysis of a cooperative approach in Austria","  Urban Traffic is recognized as one of the major CO2 contributors that puts a
high burden on the environment. Different attempts have been made for reducing
the impacts ranging from traffic management actions to shared-vehicle concepts
to simply reducing the number of vehicles on the streets. By relying on
cooperative approaches between different logistics companies, such as sharing
and pooling resources for bundling deliveries in the same zone, an increased
environmental benefit can be attained. To quantify this benefit we compare the
CO2 emissions, fuel consumption and total delivery time resulting from
deliveries performed by one cargo truck with two trailers versus by two
single-trailer cargo trucks under real conditions in a simulation scenario in
the city of Linz in Austria. Results showed a fuel consumption and CO2
emissions reduction of 28% and 34% respectively in the scenario in which
resources were bundled in one single truck.
"
1837,"C-Wars: The Unfolding Argument Strikes Back -- A Reply to 'Falsification
  & Consciousness'","  The 'unfolding argument' was presented by Doerig et.al. [1] as an argument to
show that causal structure theories (CST) like IIT are either falsified or
outside the realm of science. In their recent paper [2],[3], the authors
mathematically formalized the process of generating observable data from
experiments and using that data to generate inferences and predictions onto an
experience space. The resulting `substitution argument built on this formal
framework was used to show that all existing theories of consciousness were
'pre-falsified' if the inference reports are valid. If this argument is indeed
correct, it would have a profound effect on the field of consciousness as a
whole indicating extremely fundamental problems that would require radical
changes to how consciousness science is performed. However in this note the
author identifies the shortcomings in the formulation of the substitution
argument and explains why it's claims about functionalist theories are wrong.
"
1838,Efficient Metastability Characterization for Schmitt-Triggers,"  Despite their attractiveness as metastability filters, Schmitt-Triggers can
suffer from metastability themselves. Therefore, in the selection or
construction of a suitable Schmitt-Trigger implementation, it is a necessity to
accurately determine the metastable behavior. Only then one is able to compare
different designs and thus guide proper optimizations, and only then one can
assess the potential for residual metastable upsets. However, while the state
of the art provides a lot of research and practical characterization approaches
for flip-flops, comparatively little is known about Schmitt-Trigger
characterization. Unlike the flip-flop with its single metastable point, the
Schmitt-Trigger exhibits a whole range of metastable points depending on the
input voltage. Thus the task of characterization gets much more challenging.
  In this paper we present different approaches to determine the metastable
behavior of Schmitt-Triggers using novel methods and mechanisms. We compare
their accuracy and runtime by applying them to three common circuit
implementations. The achieved results are then used to reason about the
metastable behavior of the chosen designs which turns out to be problematic in
some cases. Overall the approaches proposed in this paper are generic and can
be extended beyond the Schmitt-Trigger, i.e., to efficiently characterize
metastable states in other circuits as well.
"
1839,"SAXSDOG: open software for real-time azimuthal integration of 2D
  scattering images","  In-situ small- and wide-angle scattering experiments at synchrotrons often
result in massive amounts of data within seconds only. Especially during such
beamtimes, processing of the acquired data online, so without mentionable
delay, is key to obtain feedback on failure or success of the experiment. We
thus developed SAXSDOG, a python based environment for real-time azimuthal
integration of large-area scattering-images. The software is primarily designed
for dedicated data-pipelines: once a scattering image is transferred from the
detector onto the storage-unit, it is automatically integrated and
pre-evaluated using integral parameters within milliseconds. The control and
configuration of the underlying server-based processes is done via a graphical
user interface SAXSLEASH, which visualizes the resulting 1D data together with
integral classifiers in real time. SAXSDOG further includes a portable
'take-home' version for users that runs on standalone computers, enabling its
use in labs or at the preferred workspace.
"
1840,Business Email Compromise (BEC) and Cyberpsychology,"  The paper gives a brief introduction about what BEC (Business Email
Compromise) is and why we should be concerned about. In addition, it presents 2
examples, Ubiquity and Peebles Media Group, which have been chosen to analyse
the phenomena of BEC and underpin how universal BEC threat is for all
companies. The psychology behind this scam has been, then, studied. In
particular, the Big Five Framework has been analysed to understand how
personality traits play an important role in Social Engineering-based attacks.
Furthermore, the 6 basic principles of influence, by Cialdini, have been
presented to show which strategies are adopted in such scam. The paper follows
with the analysis of the BEC impacts, the incidents evaluation and, finally,
with the description of some precautions, that companies should undertake in
order to mitigate the likelihood of a Business Email Compromise.
"
1841,"A Research Agenda on Pediatric Chest X-Ray: Is Deep Learning Still in
  Childhood?","  Several reasons explain the significant role that chest X-rays play on
supporting clinical analysis and early disease detection in pediatric patients,
such as low cost, high resolution, low radiation levels, and high availability.
In the last decade, Deep Learning (DL) has been given special attention from
the computer-aided diagnosis research community, outperforming the state of the
art of many techniques, including those applied to pediatric chest X-rays
(PCXR). Due to this increasing interest, much high-quality secondary research
has also arisen, overviewing machine learning and DL algorithms on medical
imaging and PCXR, in particular. However, these secondary studies follow
different guidelines, hampering their reproduction or improvement by
third-parties regarding the identified trends and gaps. This paper proposes a
""deep radiography"" of primary research on DL techniques applied in PCXR images.
We elaborated on a Systematic Literature Mapping (SLM) protocol, including
automatic search on six sources for studies published from January 1, 2010, to
May 20, 2020, and selection criteria utilized on a hundred research papers. As
a result, this paper categorizes twenty-six relevant studies and provides a
research agenda highlighting limitations, gaps, and trends for further
investigations on DL usage in PCXR images. Besides the fact that there is no
systematic mapping study on this research topic, to the best of authors'
knowledge, this work organizes the process of finding and selecting relevant
studies and data gathering and synthesis in a reproducible way.
"
1842,"Deep Learning-Based FPGA Function Block Detection Method using an
  Image-Coded Representation of Bitstream","  Examining field-programmable gate array (FPGA) bitstream is found to help
detect known function blocks, which offers assistance and insight to analyze
the circuit's system function. Our goal is to detect one or more than one
function block in FPGA design from a complete bitstream by utilizing the latest
deep learning techniques, which do not require manually designing features. To
this end, in this paper, we propose a deep learning-based FPGA function block
detection method by transforming the bitstream into a three-channel color
image. In specific, we first analyze the format of the bitstream to find the
mapping relationship between the configuration bits and configurable logic
blocks. Next, an image-coded representation of bitstream is proposed suitable
for deep learning processing. This bitstream-to-image transformation takes into
account of the adjacency nature of the programmable logic as well as high
degree of redundancy of configuration information. With the color images
transformed from bitstreams as the training dataset, a deep learning-based
object detection algorithm is applied for generating the function block
detection results. The effects of EDA tools, input size of the deep neural
network, and the data arrangement of representation on the detection accuracy
are explored. The Xilinx Zynq-7000 SoCs and Xilinx Zynq UltraScale+ MPSoCs are
adopted to verify the proposed method, and the results show that the mean
Average Precision (IoU=0.5) for 10 function blocks is as high as 97.72% for
YOLOv3 detector.
"
1843,"Observing the Invisible: Live Cache Inspection for High-Performance
  Embedded Systems","  The vast majority of high-performance embedded systems implement multi-level
CPU cache hierarchies. But the exact behavior of these CPU caches has
historically been opaque to system designers. Absent expensive hardware
debuggers, an understanding of cache makeup remains tenuous at best. This
enduring opacity further obscures the complex interplay among applications and
OS-level components, particularly as they compete for the allocation of cache
resources. Notwithstanding the relegation of cache comprehension to proxies
such as static cache analysis, performance counter-based profiling, and cache
hierarchy simulations, the underpinnings of cache structure and evolution
continue to elude software-centric solutions. In this paper, we explore a novel
method of studying cache contents and their evolution via snapshotting. Our
method complements extant approaches for cache profiling to better formulate,
validate, and refine hypotheses on the behavior of modern caches. We leverage
cache introspection interfaces provided by vendors to perform live cache
inspections without the need for external hardware. We present CacheFlow, a
proof-of-concept Linux kernel module which snapshots cache contents on an
NVIDIA Tegra TX1 SoC (system on chip).
"
1844,Privacy vs National Security,"  There are growing concerns and anxiety about privacy among the general public
especially after the revelations of former NSA contractor and whistleblowers
like Edward Snowden and others. While privacy is the fundamental concept of
being human, the growing tug-of-war between an individuals privacy and freedom
vs national security has renewed the concerns about where the fine balance
should lie between the two. For the first time in history the technological
advancement has made the mass data gathering, analysis, and storage a
financially and technologically feasible option for the governments and private
businesses. This has led to the growing interest of governments and security
agencies around the globe to develop sophisticated algorithms using the power
of Big-Data, Machine-Learning and Artificial Intelligence. The technology has
enabled governments and private businesses to collect and store thousands of
data points on every individual, which has put an individuals privacy under
constant threat. This article analyses the individual's privacy concepts and
its perceived link with national security. The article will also discuss the
various aspects of privacy and national-security, arguments of both sides and
where a boundary should be drawn between privacy and national security.
"
1845,"Genetic Algorithm: Reviews, Implementations, and Applications","  Nowadays genetic algorithm (GA) is greatly used in engineering pedagogy as an
adaptive technique to learn and solve complex problems and issues. It is a
meta-heuristic approach that is used to solve hybrid computation challenges. GA
utilizes selection, crossover, and mutation operators to effectively manage the
searching system strategy. This algorithm is derived from natural selection and
genetics concepts. GA is an intelligent use of random search supported with
historical data to contribute the search in an area of the improved outcome
within a coverage framework. Such algorithms are widely used for maintaining
high-quality reactions to optimize issues and problems investigation. These
techniques are recognized to be somewhat of a statistical investigation process
to search for a suitable solution or prevent an accurate strategy for
challenges in optimization or searches. These techniques have been produced
from natural selection or genetics principles. For random testing, historical
information is provided with intelligent enslavement to continue moving the
search out from the area of improved features for processing of the outcomes.
It is a category of heuristics of evolutionary history using behavioral
science-influenced methods like an annuity, gene, preference, or combination
(sometimes refers to as hybridization). This method seemed to be a valuable
tool to find solutions for problems optimization. In this paper, the author has
explored the GAs, its role in engineering pedagogies, and the emerging areas
where it is using, and its implementation.
"
1846,Automatic Parking in Smart Cities,"  The objective behind this project is to maximize the efficiency of land
space, to decrease the driver stress and frustration, along with a considerable
reduction in air pollution. Our contribution is in the form of an automatic
parking system that is controlled by cellular phones. The structure is a
hexagon shape that uses conveyor belts, to transport the vehicles from the
entrance into the parking spaces over an elevating platform. The entrance gate
includes length-measuring sensors to determine whether the approaching vehicle
is eligible to enter. Our system is controlled through a microcontroller, and
using cellular communications to connect to the customer. The project can be
applied to different locations and is capable of capacity extensions.
"
1847,"BIDEAL: A Toolbox for Bicluster Analysis -- Generation, Visualization
  and Validation","  This paper introduces a novel toolbox named BIDEAL for the generation of
biclusters, their analysis, visualization, and validation. The objective is to
facilitate researchers to use forefront biclustering algorithms embedded on a
single platform. A single toolbox comprising various biclustering algorithms
play a vital role to extract meaningful patterns from the data for detecting
diseases, biomarkers, gene-drug association, etc. BIDEAL consists of seventeen
biclustering algorithms, three biclusters visualization techniques, and six
validation indices. The toolbox can analyze several types of data, including
biological data through a graphical user interface. It also facilitates data
preprocessing techniques i.e., binarization, discretization, normalization,
elimination of null and missing values. The effectiveness of the developed
toolbox has been presented through testing and validations on Saccharomyces
cerevisiae cell cycle, Leukemia cancer, Mammary tissue profile, and Ligand
screen in B-cells datasets. The biclusters of these datasets have been
generated using BIDEAL and evaluated in terms of coherency, differential
co-expression ranking, and similarity measure. The visualization of generated
biclusters has also been provided through a heat map and gene plot.
"
1848,3D city models for urban farming site identification in buildings,"  Studies have suggested that there is farming potential in urban residential
buildings. However, these studies are limited in scope, require field visits
and time-consuming measurements. Furthermore, they have not suggested ways to
identify suitable sites on a larger scale let alone means of surveying numerous
micro-locations across the same building. Using a case study area focused on
high-rise buildings in Singapore, this paper examines a novel application of 3D
city models to identify suitable farming micro-locations in residential
buildings. We specifically investigate whether the vertical spaces of these
buildings comprising outdoor corridors, facades and windows receive sufficient
photosynthetically active radiation (PAR) for growing food crops and do so at a
high resolution. We also analyze the spatio-temporal characteristics of PAR,
and the impact of shadows and different weather conditions on PAR in the
building. Environmental simulations on the 3D model of the study area indicated
that the cumulative daily PAR or Daily Light Integral (DLI) at a location in
the building was dependent on its orientation and shape, sun's diurnal and
annual motion, weather conditions, and shadowing effects of the building's
facades and surrounding buildings. The DLI in the study area generally
increased with building's levels and, depending on the particular
micro-location, was found suitable for growing moderately light-demanding crops
such as lettuce and sweet pepper. These variations in DLI at different
locations of the same building affirmed the need for such simulations. The
simulations were validated with field measurements of PAR, and correlation
coefficients between them exceeded 0.5 in most cases thus, making a case that
3D city models offer a promising practical solution to identifying suitable
farming locations in residential buildings, and have the potential for
urban-scale applications.
"
1849,"Towards Leveraging End-of-Life Tools as an Asset: Value Co-Creation
  based on Deep Learning in the Machining Industry","  Sustainability is the key concept in the management of products that reached
their end-of-life. We propose that end-of-life products have -- besides their
value as recyclable assets -- additional value for producer and consumer. We
argue this is especially true for the machining industry, where we illustrate
an automatic characterization of worn cutting tools to foster value co-creation
between tool manufacturer and tool user (customer) in the future. In the work
at hand, we present a deep-learning-based computer vision system for the
automatic classification of worn tools regarding flank wear and chipping. The
resulting Matthews Correlation Coefficient of 0.878 and 0.644 confirms the
feasibility of our system based on the VGG-16 network and Gradient Boosting.
Based on these first results we derive a research agenda which addresses the
need for a more holistic tool characterization by semantic segmentation and
assesses the perceived business impact and usability by different user groups.
"
1850,Value driven Analysis Framework of Service Ecosystem Evolution Mechanism,"  With the development of cloud computing, service computing, IoT(Internet of
Things) and mobile Internet, the diversity and sociality of services are
increasingly apparent. To meet the customized user demands, Service Ecosystem
is emerging as a complex social-technology system, which is formed with various
IT services through cross-border integration. However, how to analyze and
promote the evolution mechanism of service ecosystem is still a serious
challenge in the field, which is of great significance to achieve the expected
system evolution trends. Based on this, this paper proposes a value-driven
analysis framework of service ecosystem, including value creation, value
operation, value realization and value distribution. In addition, a
computational experiment system is established to verify the effectiveness of
the analysis framework, which stimulates the effect of different operation
strategies on the value network in the service ecosystem. The result shows that
our analysis framework can provide new means and ideas for the analysis of
service ecosystem evolution, and can also support the design of operation
strategies. Index
"
1851,"No Cross-Validation Required: An Analytical Framework for Regularized
  Mixed-Integer Problems (Extended Version)","  This paper develops a method to obtain the optimal value for the
regularization coefficient in a general mixed-integer problem (MIP). This
approach eliminates the cross-validation performed in the existing penalty
techniques to obtain a proper value for the regularization coefficient. We
obtain this goal by proposing an alternating method to solve MIPs. First, via
regularization, we convert the MIP into a more mathematically tractable form.
Then, we develop an iterative algorithm to update the solution along with the
regularization (penalty) coefficient. We show that our update procedure
guarantees the convergence of the algorithm. Moreover, assuming the objective
function is continuously differentiable, we derive the convergence rate, a
lower bound on the value of regularization coefficient, and an upper bound on
the number of iterations required for the convergence. We use a radio access
technology (RAT) selection problem in a heterogeneous network to benchmark the
performance of our method. Simulation results demonstrate near-optimality of
the solution and consistency of the convergence behavior with obtained
theoretical bounds.
"
1852,Value Entropy Model: Metric Method of Service Ecosystem Evolution,"  With the development of cloud computing, service computing, IoT(Internet of
Things) and mobile Internet, the diversity and sociality of services are
increasingly apparent. To meet the customized user demands, service ecosystems
begins to emerge with the formation of various IT services collaboration
network. However, service ecosystem is a complex social-technology system with
the characteristics of natural ecosystems, economic systems and complex
networks. Hence, how to realize the multi-dimensional evaluation of service
ecosystem is of great significance to promote its sound development. Based on
this, this paper proposes a value entropy model to analyze the performance of
service ecosystem, which is conducive to integrate evaluation indicators of
different dimensions. In addition, a computational experiment system is
constructed to verify the effectiveness of value entropy model. The result
shows that our model can provide new means and ideas for the analysis of
service ecosystem.
"
1853,Quality Classification of Defective Parts from Injection Moulding,"  This report examines machine learning algorithms for detecting short forming
and weaving in plastic parts produced by injection moulding. Transfer learning
was implemented by using pretrained models and finetuning them on our dataset
of 494 samples of 150 by 150 pixels images. The models tested were Xception,
InceptionV3 and Resnet-50. Xception showed the highest overall accuracy
(86.66%), followed by InceptionV3 (82.47%) and Resnet-50 (80.41%). Short
forming was the easiest fault to identify, with the highest F1 score for each
model.
"
1854,"City-Scale Agent-Based Simulators for the Study of Non-Pharmaceutical
  Interventions in the Context of the COVID-19 Epidemic","  We highlight the usefulness of city-scale agent-based simulators in studying
various non-pharmaceutical interventions to manage an evolving pandemic. We
ground our studies in the context of the COVID-19 pandemic and demonstrate the
power of the simulator via several exploratory case studies in two
metropolises, Bengaluru and Mumbai. Such tools become common-place in any city
administration's tool kit in our march towards digital health.
"
1855,"Analysis of Fleet Management and Network Design for On-Demand Urban Air
  Mobility Operations","  A significant challenge in estimating operational feasibility of Urban Air
Mobility (UAM) missions lies in understanding how choices in design impact the
performance of a complex system-of-systems. This work examines the ability of
the UAM ecosystem and the operations within it to meet a variety of demand
profiles that may emerge in the coming years. We perform a set of simulation
driven feasibility and scalability analyses based on UAM operational models
with the goal of estimating capacity and throughput for a given set of
parameters that represent an operational UAM ecosystem. UAM ecosystem design
guidelines, vehicle constraints, and effective operational policies can be
drawn from our analysis. Results show that, while critical for enabling UAM,
the performance of the UAM ecosystem is robust to variations in ground
infrastructure and fleet design decisions, while being sensitive to decisions
for fleet and traffic management policies. We show that so long as the
ecosystem design parameters for ground infrastructure and fleet design fall
within a sensible range, the performance of the UAM ecosystem is affected by
the policies used to manage the UAM traffic.
"
1856,"Computational Framework for Behind-The-Meter DER Techno-Economic
  Modeling and Optimization -- REopt Lite","  The global energy system is undergoing a major transformation. Renewable
energy generation is growing and is projected to accelerate further with the
global emphasis on decarbonization. Furthermore, distributed generation is
projected to play a significant role in the new energy system, and energy
models are playing a key role in understanding how distributed generation can
be integrated reliably and economically. The deployment of massive amounts of
distributed generation requires understanding the interface of technology,
economics, and policy in the energy modeling process. In this work, we present
an end-to-end computational framework for distributed energy resource (DER)
modeling, REopt Lite which addresses this need effectively. We describe the
problem space, the building blocks of the model, the scaling capabilities of
the design, the optimization formulation, and the accessibility of the model.
We present a framework for accelerating the techno-economic analysis of
behind-the-meter distributed energy resources to enable rapid planning and
decision-making, thereby significantly boosting the rate the renewable energy
deployment. Lastly, but equally importantly, this computation framework is
open-sourced to facilitate transparency, flexibility, and wider collaboration
opportunities within the worldwide energy modeling community.
"
1857,Probabilistic Cellular Automata for Granular Media in Video Games,"  Granular materials are very common in the everyday world. Media such as sand,
soil, gravel, food stuffs, pharmaceuticals, etc. all have similar irregular
flow since they are composed of numerous small solid particles. In video games,
simulating these materials increases immersion and can be used for various game
mechanics. Computationally, full scale simulation is not typically feasible
except on the most powerful hardware and tends to be reduced in priority to
favor other, more integral, gameplay features. Here we study the computational
and qualitative aspects of side profile flow of sand-like particles using
cellular automata (CA). Our CA uses a standard square lattice that updates via
a custom, modified Margolus neighborhood. Each update occurs using a set of
probabilistic transitions that can be tuned to simulate friction between
particles. We focus on the look of the sandpile structure created from an
hourglass shape over time using different transition probabilities and the
computational impact of such a simulation.
"
1858,"Herramientas tecnol\'ogicas en Android para la formaci\'on de mapeadores
  y promotores de Mapa Verde","  When you talk about technologies and the environment, you usually imagine a
lot of equipment, techniques, technologies and tools polluting the natural
environment. The good and bad consequences of our development have been
projected on the planet for years, and part of that development is reflected in
the new technologies, among which is the mobile phone. In the municipality of
Consolaci\'on del Sur and from the Municipal University Center, the project
Implementation of the Green Map Methodology in the management of environmental
education in console communities for the formation of an environmental culture
for sustainable development is created, creating awareness of care and
protection of the environment. The present work is given to solve the following
problem: how to contribute in the construction of a package of computer tools
for the implementation of the Green Map methodology in environmental management
in console communities and the training of mappers and promoters of Green Map
for the development of green maps of the communities of the municipality
Consolaci\'on del Sur? For this purpose, two Android applications for mobile
devices based on the Green Map methodology were developed, thus responding to
the following objective: Develop a package of computer applications for the
implementation of the Green Map methodology in the management of environmental
education in console communities and the formation of mappers and promoters of
the Green Map that allows the development of the green maps of the communities
of the Consolaci\'on del Sur municipality.
"
1859,IoT Applications in Urban Sustainability,"  Internet of Things is one of the driving technologies behind the concept of
Smart Cities and is capable of playing a significant role in facilitating urban
sustainable development. This chapter explores the relationship between three
core concepts namely Smart Cities, Internet of Things and Sustainability;
thereby identifying the challenges and opportunities that exist in the
synergistic use of Internet of Things for sustainability, in the Smart Cities
context. Moreover, this chapter also presents some of the existing use cases
that apply Internet of Things for urban sustainable development, also
presenting the vision for these applications as they continue to evolve in and
adapt to the real world scenario. It is because of the interdisciplinary nature
of these applications that a clear comprehension of the associated challenges
becomes quintessential. Study of challenges and opportunities in this area
shall facilitate collaboration between different sectors of urban planning and
optimize the utilization of Internet of Things for sustainability.
"
1860,Statistically Significant Pattern Mining with Ordinal Utility,"  Statistically significant patterns mining (SSPM) is an essential and
challenging data mining task in the field of knowledge discovery in databases
(KDD), in which each pattern is evaluated via a hypothesis test. Our study aims
to introduce a preference relation into patterns and to discover the most
preferred patterns under the constraint of statistical significance, which has
never been considered in existing SSPM problems. We propose an iterative
multiple testing procedure that can alternately reject a hypothesis and safely
ignore the hypotheses that are less useful than the rejected hypothesis. One
advantage of filtering out patterns with low utility is that it avoids
consumption of the significance budget by rejection of useless (that is,
uninteresting) patterns. This allows the significance budget to be focused on
useful patterns, leading to more useful discoveries.
  We show that the proposed method can control the familywise error rate (FWER)
under certain assumptions, that can be satisfied by a realistic problem class
in SSPM.\@We also show that the proposed method always discovers a set of
patterns that is at least equally or more useful than those discovered using
the standard Tarone-Bonferroni method SSPM.\@Finally, we conducted several
experiments with both synthetic and real-world data to evaluate the performance
of our method. As a result, in the experiments with real-world datasets, the
proposed method discovered a larger number of more useful patterns than the
existing method for all five conducted tasks.
"
1861,"Investigating the Performance Gap between Testing on Real and Denoised
  Aggregates in Non-Intrusive Load Monitoring","  Prudent and meaningful performance evaluation of algorithms is essential for
the progression of any research field. In the field of Non-Intrusive Load
Monitoring (NILM), performance evaluation can be conducted on real-world
aggregate signals, provided by smart energy meters or artificial superpositions
of individual load signals (i.e., denoised aggregates). It has long been
suspected that testing on these denoised aggregates provides better evaluation
results mainly due to the the fact that the signal is less complex. Complexity
in real-world aggregate signals increases with the number of unknown/untracked
load. Although this is a known performance reporting problem, an investigation
in the actual performance gap between real and denoised testing is still
pending. In this paper, we examine the performance gap between testing on
real-world and denoised aggregates with the aim of bringing clarity into this
matter. Starting with an assessment of noise levels in datasets, we find
significant differences in test cases. We give broad insights into our
evaluation setup comprising three load disaggregation algorithms, two of them
relying on neural network architectures. The results presented in this paper,
based on studies covering three scenarios with ascending noise levels, show a
strong tendency towards load disaggregation algorithms providing significantly
better performance on denoised aggregate signals. A closer look into the
outcome of our studies reveals that all appliance types could be subject to
this phenomenon. We conclude the paper by discussing aspects that could be
causing these considerable gaps between real and denoised testing in NILM.
"
1862,"LED wristbands for Cell-based Crowd Evacuation: an Adaptive Exit-choice
  Guidance System Architecture","  Cell-based crowd evacuation systems provide adaptive or static exit-choice
indications that favor a coordinated group dynamic, improving evacuation time
and safety. While a great effort has been made to modeling its control logic by
assuming an ideal communication and positioning infrastructure, the
architectural dimension and the influence of pedestrian positioning uncertainty
have been largely overlooked. In our previous research, a Cell-based crowd
evacuation system (CellEVAC) was proposed that dynamically allocates exit gates
to pedestrians in a cell-based pedestrian positioning infrastructure. This
system provides optimal exit-choice indications through color-based indications
and a control logic module built upon an optimized discrete-choice model. Here,
we investigate how location-aware technologies and wearable devices can be used
for a realistic deployment of CellEVAC. We consider a simulated real evacuation
scenario (Madrid Arena) and propose a system architecture for CellEVAC that
includes: a controller node, a radio-controlled LED wristband subsystem, and a
cell-node network equipped with active Radio Frequency Identification (RFID)
devices. These subsystems coordinate to provide control, display and
positioning capabilities. We quantitatively study the sensitivity of evacuation
time and safety to uncertainty in the positioning system. Results showed that
CellEVAC was operational within a limited range of positioning uncertainty.
Further analyses revealed that reprogramming the control logic module through a
simulation-optimization process, simulating the positioning system's expected
uncertainty level, improved the CellEVAC performance in scenarios with poor
positioning systems.
"
1863,"Two-stage optimization of urban rail transit formation and real-time
  station control at comprehensive transportation hub","  This paper tries to discuss two strategies of dealing with this complex
passenger demand from two aspects: transit train formation and real-time
holding control. The genetic algorithm (GA) is designed to solve the integrated
two-stage model of optimizing the number, timetable and real-time holding
control of the multi-marshalling operated trains. The numerical results show
that the combined two-stage model of multi-marshalling operation and holding
control at stations can better deal with the demand fluctuation of urban rail
transit connecting with the comprehensive transportation hub.
"
1864,How to Design While Loops,"  Beginning students find the syntactic construct known as a while loop
difficult to master. The difficulties revolve around guaranteeing loop
termination and around learning how to properly sequence mutations to solve a
problem. In fact, both of these are intertwined and students need to be taught
a model that helps them reason about how to design while loops. For students
that have been introduced to how to design programs using structural recursion,
generative recursion, accumulative recursion, and mutation, the task of
teaching them how to design while loops is made easier. These students are
familiar, for example, with state variables, termination arguments, and
accumulator invariants. All of these are fundamental in the design of while
loops. This articles presents a novel technique used at Seton Hall University
to introduce beginners to the design of while loops. It presents a design
recipe that students can follow step-by-step to establish such things as the
driver of the loop, the loop invariant, and the proper sequencing of mutations.
The article also presents an example of designing a while-loop based function
using the new design recipe.
"
1865,On the Battery Consumption of Mobile Browsers,"  Mobile web browsing has recently surpassed desktop browsing both in term of
popularity and traffic. Following its desktop counterpart, the mobile browsers
ecosystem has been growing from few browsers (Chrome, Firefox, and Safari) to a
plethora of browsers, each with unique characteristics (battery friendly,
privacy preserving, lightweight, etc.). In this paper, we introduce a browser
benchmarking pipeline for Android browsers encompassing automation, in-depth
experimentation, and result analysis. We tested 15 Android browsers, using
Cappuccino a novel testing suite we built for third party Android applications.
We perform a battery-centric analysis of such browsers and show that: 1)
popular browsers tend also to consume the most, 2) adblocking produces
significant battery savings (between 20 and 40% depending on the browser), and
3) dark mode offers an extra 10% battery savings on AMOLED screens. We exploit
this observation to build AttentionDim, a screen dimming mechanism driven by
browser events. Via integration with the Brave browser and 10 volunteers, we
show potential battery savings up to 30%, on both devices with AMOLED and LCD
screens.
"
1866,JXES: JSON Support for the XES Event Log Standard,"  Process mining assumes the existence of an event log where each event refers
to a case, an activity, and a point in time. XES is an XML based IEEE approved
standard format for event logs supported by most of the process mining tools.
JSON (JavaScript Object Notation) is a lightweight data interchange format. In
this paper, we present JXES, the JSON standard for the event logs and also
provide implementation in ProM for importing and exporting event logs in JSON
format using 4 different parsers. The evaluation results show notable
performance differences between the different parsers (Simple JSON, Jackson,
GSON, Jsoninter).
"
1867,"Gaining or Losing Perspective for Piecewise-Linear Under-Estimators of
  Convex Univariate Functions","  We study MINLO (mixed-integer nonlinear optimization) formulations of the
disjunction $x\in\{0\}\cup[\ell,u]$, where $z$ is a binary indicator of
$x\in[\ell,u]$ ($0 \leq \ell <u$), and $y$ ""captures"" $f(x)$, which is assumed
to be convex and positive on its domain $[\ell,u]$, but otherwise $y=0$ when
$x=0$. This model is very useful in nonlinear combinatorial optimization, where
there is a fixed cost of operating an activity at level $x$ in the operating
range $[\ell,u]$, and then there is a further (convex) variable cost $f(x)$. In
particular, we study relaxations related to the perspective transformation of a
natural piecewise-linear under-estimator of $f$, obtained by choosing
linearization points for $f$. Using 3-d volume (in $(x,y,z)$) as a measure of
the tightness of a convex relaxation, we investigate relaxation quality as a
function of $f$, $\ell$, $u$, and the linearization points chosen. We make a
detailed investigation for convex power functions $f(x):=x^p$, $p>1$.
"
1868,"Arc Flow Formulations Based on Dynamic Programming: Theoretical
  Foundations and Applications","  Network flow formulations are among the most successful tools to solve
optimization problems. One of such formulations is the arc flow, where
variables represent flows on individual arcs of the network. For
$\mathcal{NP}$-hard problems, polynomial-sized arc flow models typically
provide weak linear relaxations and may have too much symmetry to be efficient
in practice. Instead, arc flow models with a pseudo-polynomial size usually
provide strong relaxations and are efficient in practice. The interest in
pseudo-polynomial arc flow formulations has grown considerably in the last
twenty years, in which they have been used to solve many open instances of hard
problems. A remarkable advantage of arc flow models is the possibility to solve
practical-sized instances directly by a Mixed Integer Linear Programming
solver, avoiding the implementation of complex methods based on column
generation.
  In this survey, we present theoretical foundations of pseudo-polynomial arc
flow formulations, by showing a relation between their network and Dynamic
Programming (DP). This relation allows a better understanding of the strength
of these formulations, through a link with models obtained by Dantzig-Wolfe
decomposition. The relation with DP also allows a new perspective to relate
state-space relaxation methods for DP with arc flow models. We also present a
dual point of view to contrast the linear relaxation of arc flow models with
that of models based on paths and cycles. To conclude, we review the main
solution methods and applications of arc flow models based on DP in several
domains such as cutting, packing, scheduling, and routing.
"
1869,"To Lane or Not to Lane? Comparing On-Road Experiences in Developing and
  Developed Countries using a New Simulator ""RoadBird""","  Even though the traffic systems in developed countries have been analyzed
with rigor and operated efficiently, the same does not generally hold for
developing countries due to inadequate planning, design, and operations of
their transportation systems. Because of inherent differences between internal
infrastructures, the systems deployed in developed countries may not be
amenable to developing ones. Besides, the traffic systems of developing
countries are not well-studied in the literature to the best of our knowledge.
For example, it is yet to explore how a developed country's lane-based traffic
flow would perform in the context of a developing country, which generally
experiences non-lane-based traffic. As such, by using our newly developed
traffic simulator 'RoadBird', we investigate outcomes of both lane-based and
non-lane-based traffic from the contexts of both developing and developed
countries. To do so, we run simulations over real road topologies (extracted
from the GIS maps of major cities such as Dhaka, Miami, and Riyadh) considering
different scenarios such as lane-based or non-lane-based flows, homogeneous or
heterogeneous traffic, with or without pedestrians, etc. We also incorporate
different car-following and lane-changing models to mimic traffic behaviors and
investigate their performances. While the lane changing dilemma remains an open
research question, our experimental evidences indicate: (i) lane-based
approaches will not necessarily perform better in the case of currently-adopted
non-lane-based scenarios; and (ii) non-lane-based strategies may benefit system
performance in lane-based scenarios while having heavy mixed traffic.
Nonetheless, we reveal several new insights for on-road experiences both in
developing and developed countries.
"
1870,On licenses for [Open] Hardware,"  This document explains the basic concepts related to software and hardware
licenses, and it summarizes the most popular licenses that are currently used
for hardware projects. Two case studies of hardware projects at different
levels of abstraction are also presented, together with a discussion of license
applicability, commercial issues, code protection, and related concerns. This
paper intends to help the reader understand how to release open hardware with
the most appropriate license, and to answer questions that are of current
interest. We have been mainly motivated by the growing influence of the open
RISC-V ISA, but trying to address a wider hardware point of view.
"
1871,"A microsimulation approach for the impact assessment of a
  Vehicle-to-Infrastructure based Road Hazard Warning system","  Cooperative Intelligent Transportation Systems (C-ITS) constitute
technologies which enable vehicles to communicate with each other and with road
infrastructure. Verification or testing is required for C-ITS applications, in
order to assess their impact on traffic operation. In this work, a microscopic
traffic simulation approach is used, to evaluate the impact of
Vehicle-to-Infrastructure (V2I) technologies in the context of a road traffic
accident. Specifically, the methodology is implemented to explicitly models
vehicles collisions, Road Hazard Warning (RHW), Emergency Electronic Brake
Light (EEBL) warnings and the resulting driver behavior. Moreover, a new gap
control mechanism is adopted, to improve safety by advising vehicles in hazard
lane to increase their headways with respect to their preceding vehicle, so
that they can avoid a collision. Perfect communication links to all vehicles
are assumed. The study findings indicate that the proposed V2I hazard warning
strategy has a positive impact on traffic flow safety and efficiency.
"
1872,"Investigating Cultural Aspects in the Fundamental Diagram using
  Convolutional Neural Networks and Simulation","  This paper presents a study regarding group behavior in a controlled
experiment focused on differences in an important attribute that vary across
cultures -- the personal spaces -- in two Countries: Brazil and Germany. In
order to coherently compare Germany and Brazil evolutions with same population
applying same task, we performed the pedestrian Fundamental Diagram experiment
in Brazil, as performed in Germany. We use CNNs to detect and track people in
video sequences. With this data, we use Voronoi Diagrams to find out the
neighbor relation among people and then compute the walking distances to find
out the personal spaces. Based on personal spaces analyses, we found out that
people behavior is more similar, in terms of their behaviours, in high dense
populations and vary more in low and medium densities. So, we focused our study
on cultural differences between the two Countries in low and medium densities.
Results indicate that personal space analyses can be a relevant feature in
order to understand cultural aspects in video sequences. In addition to the
cultural differences, we also investigate the personality model in crowds,
using OCEAN. We also proposed a way to simulate the FD experiment from other
countries using the OCEAN psychological traits model as input. The simulated
countries were consistent with the literature.
"
1873,"Comparison Analysis of Tree Based and Ensembled Regression Algorithms
  for Traffic Accident Severity Prediction","  Rapid increase of traffic volume on urban roads over time has changed the
traffic scenario globally. It has also increased the ratio of road accidents
that can be severe and fatal in the worst case. To improve traffic safety and
its management on urban roads, there is a need for prediction of severity level
of accidents. Various machine learning models are being used for accident
prediction. In this study, tree based ensemble models (Random Forest, AdaBoost,
Extra Tree, and Gradient Boosting) and ensemble of two statistical models
(Logistic Regression Stochastic Gradient Descent) as voting classifiers are
compared for prediction of road accident severity. Significant features that
are strongly correlated with the accident severity are identified by Random
Forest. Analysis proved Random Forest as the best performing model with highest
classification results with 0.974 accuracy, 0.954 precision, 0.930 recall and
0.942 F-score using 20 most significant features as compared to other
techniques classification of road accidents severity.
"
1874,"AI Chiller: An Open IoT Cloud Based Machine Learning Framework for the
  Energy Saving of Building HVAC System via Big Data Analytics on the Fusion of
  BMS and Environmental Data","  Energy saving and carbon emission reduction in buildings is one of the key
measures in combating climate change. Heating, Ventilation, and Air
Conditioning (HVAC) system account for the majority of the energy consumption
in the built environment, and among which, the chiller plant constitutes the
top portion. The optimization of chiller system power consumption had been
extensively studied in the mechanical engineering and building service domains.
Many works employ physical models from the domain knowledge. With the advance
of big data and AI, the adoption of machine learning into the optimization
problems becomes popular. Although many research works and projects turn to
this direction for energy saving, the application into the optimization problem
remains a challenging task. This work is targeted to outline a framework for
such problems on how the energy saving should be benchmarked, if holistic or
individually modeling should be used, how the optimization is to be conducted,
why data pattern augmentation at the initial deployment is a must, why the
gradually increasing changes strategy must be used. Results of analysis on
historical data and empirical experiment on live data are presented.
"
1875,C-ITS bundling for integrated traffic management,"  Cooperative Intelligent Transportation Systems (C-ITS) enable vehicles
communication with each other (Vehicle-to-Vehicle, V2V) and with roadside
infrastructure (Vehicle-to-Infrastructure, V2I). In the context of traffic
efficiency, C-ITS technologies could assist in road network status
visualization and monitoring, through data exchange, improving this way traffic
control organization and traffic management implementation. Bundling is the
provision of several C-ITS services as one combined service. The purpose of
bundling is to harvest the usability of C-ITS services by developing a strategy
for the operation and exploitation of services in real-time and within varying
geographical areas. Two different dimensions of bundling have been recognized
covering: 1) end-users, and 2) operators-managers. The objective of the
operators-managers dimension is the integration of C-ITS services in
operational traffic management. This work spotlights the operators-managers
bundling dimension, presenting a framework based on a step-by-step approach for
integrating C-ITS services in traditional traffic management.
"
1876,"Data Streams from the Low Frequency Instrument On-Board the Planck
  Satellite: Statistical Analysis and Compression Efficiency","  The expected data rate produced by the Low Frequency Instrument (LFI) planned
to fly on the ESA Planck mission in 2007, is over a factor 8 larger than the
bandwidth allowed by the spacecraft transmission system to download the LFI
data. We discuss the application of lossless compression to Planck/LFI data
streams in order to reduce the overall data flow. We perform both theoretical
analysis and experimental tests using realistically simulated data streams in
order to fix the statistical properties of the signal and the maximal
compression rate allowed by several lossless compression algorithms. We studied
the influence of signal composition and of acquisition parameters on the
compression rate Cr and develop a semiempirical formalism to account for it.
The best performing compressor tested up to now is the arithmetic compression
of order 1, designed for optimizing the compression of white noise like
signals, which allows an overall compression rate <Cr> = 2.65 +/- 0.02. We find
that such result is not improved by other lossless compressors, being the
signal almost white noise dominated. Lossless compression algorithms alone will
not solve the bandwidth problem but needs to be combined with other techniques.
"
1877,"Roughening of the (1+1) interfaces in two-component surface growth with
  an admixture of random deposition","  We simulate competitive two-component growth on a one dimensional substrate
of $L$ sites. One component is a Poisson-type deposition that generates
Kardar-Parisi-Zhang (KPZ) correlations. The other is random deposition (RD). We
derive the universal scaling function of the interface width for this model and
show that the RD admixture acts as a dilatation mechanism to the fundamental
time and height scales, but leaves the KPZ correlations intact. This
observation is generalized to other growth models. It is shown that the
flat-substrate initial condition is responsible for the existence of an early
non-scaling phase in the interface evolution. The length of this initial phase
is a non-universal parameter, but its presence is universal. In application to
parallel and distributed computations, the important consequence of the derived
scaling is the existence of the upper bound for the desynchronization in a
conservative update algorithm for parallel discrete-event simulations. It is
shown that such algorithms are generally scalable in a ring communication
topology.
"
1878,Von Neumann Quantum Logic vs. Classical von Neumann Architecture?,"  The name of John von Neumann is common both in quantum mechanics and computer
science. Are they really two absolutely unconnected areas? Many works devoted
to quantum computations and communications are serious argument to suggest
about existence of such a relation, but it is impossible to touch the new and
active theme in a short review. In the paper are described the structures and
models of linear algebra and just due to their generality it is possible to use
universal description of very different areas as quantum mechanics and theory
of Bayesian image analysis, associative memory, neural networks, fuzzy logic.
"
1879,Efficient generation of rotating workforce schedules,"  Generating high-quality schedules for a rotating workforce is a critical task
in all settings where a certain staffing level must be guaranteed beyond the
capacity of single employees, such as for instance in industrial plants,
hospitals, or airline companies. Results from ergonomics \cite{BEST91} indicate
that rotating workforce schedules have a profound impact on the health and
social life of employees as well as on their performance at work. Moreover,
rotating workforce schedules must satisfy legal requirements and should also
meet the objectives of the employing organization. We describe our solution to
this problem. A basic design decision was to aim at quickly obtaining
high-quality schedules for realistically sized problems while maintaining human
control. The interaction between the decision maker and the algorithm therefore
consists in four steps: (1) choosing a set of lengths of work blocks (a work
block is a sequence of consecutive days of work shifts), (2) choosing a
particular sequence of work and days-off blocks among those that have optimal
weekend characteristics, (3) enumerating possible shift sequences for the
chosen work blocks subject to shift change constraints and bounds on sequences
of shifts, and (4) assignment of shift sequences to work blocks while
fulfilling the staffing requirements. The combination of constraint
satisfaction and problem-oriented intelligent backtracking algorithms in each
of the four steps allows to find good solutions for real-world problems in
acceptable time. Computational results from real-world problems and from
benchmark examples found in the literature confirm the viability of our
approach. The algorithms are now part of a commercial shift scheduling software
package.
"
1880,On the theory of system administration,"  This paper describes necessary elements for constructing theoretical models
of network and system administration. Armed with a theoretical model it becomes
possible to determine best practices and optimal strategies in a way which
objectively relates policies and assumptions to results obtained. It is
concluded that a mixture of automation and human, or other intelligent
incursion is required to fully implement system policy with current technology.
Some aspects of the author's immunity model for automated system administration
are explained, as an example. A theoretical framework makes the prediction that
the optimal balance between resource availability and garbage collection
strategies is encompassed by the immunity model.
"
1881,"The Competitiveness of On-Line vis-a-vis Conventional Retailing: A
  Preliminary Study","  Previous research has directly studied whether on-line retailing is more
competitive than conventional retail markets. The evidence from books and music
CDs is mixed. Here, I use an indirect approach to compare the competitiveness
of on-line with conventional markets. Focusing on the retail market for books,
I identify a peculiarity in the pricing of bestsellers relative to other
titles. Supposing that competitive barriers are lower in on-line retailing, I
analyze how the lower barriers would affect the relative pricing of
bestsellers. The empirical data indicates that on-line retailing is more
competitive than conventional retailing.
"
1882,A Virtual Java Simulation Lab for Computer Science Students,"  The VJ-Lab is a project oriented to improve the students learning process of
Computer Science degree at the National University of La Plata. The VJ-Lab is a
Web application with Java based simulations. Java can be used to provide
simulation environments with simple pictorial interfaces that can help students
to understand the subject. There are many fields in which it is difficult to
give students a feel for the subject that they are learning. Computer based
simulations offer a fun and effective way to enable students to learn by doing.
Both, practicing skills and applying knowledge are both allowed in simulated
worlds. We will focus on the VJ-Lab project overview, the work in progress and
some Java based simulations running. They imitate the behavior of data network
protocol and data structure algorithms. These applets are produced by the
students of the 'Software Development Laboratory' course.
"
1883,A polynomial axles-detection algorithm for a four-contacts treadle,"  This submission was removed because it contained proprietary information that
was distributed without permission.
"
1884,"How to Commission, Operate and Maintain a Large Future Accelerator
  Complex from Far Remote","  A study on future large accelerators [1] has considered a facility, which is
designed, built and operated by a worldwide collaboration of equal partner
institutions, and which is remote from most of these institutions. The full
range of operation was considered including commi-ssioning, machine
development, maintenance, trouble shooting and repair. Experience from existing
accele-rators confirms that most of these activities are already performed
'remotely'. The large high-energy physics ex-periments and astronomy projects,
already involve inter-national collaborations of distant institutions. Based on
this experience, the prospects for a machine operated remotely from far sites
are encouraging. Experts from each laboratory would remain at their home
institution but continue to participate in the operation of the machine after
construction. Experts are required to be on site only during initial
commissioning and for par-ticularly difficult problems. Repairs require an
on-site non-expert maintenance crew. Most of the interventions can be made
without an expert and many of the rest resolved with remote assistance. There
appears to be no technical obstacle to controlling an accelerator from a
distance. The major challenge is to solve the complex management and
communication problems.
"
1885,"The Expresso Microarray Experiment Management System: The Functional
  Genomics of Stress Responses in Loblolly Pine","  Conception, design, and implementation of cDNA microarray experiments present
a variety of bioinformatics challenges for biologists and computational
scientists. The multiple stages of data acquisition and analysis have motivated
the design of Expresso, a system for microarray experiment management. Salient
aspects of Expresso include support for clone replication and randomized
placement; automatic gridding, extraction of expression data from each spot,
and quality monitoring; flexible methods of combining data from individual
spots into information about clones and functional categories; and the use of
inductive logic programming for higher-level data analysis and mining. The
development of Expresso is occurring in parallel with several generations of
microarray experiments aimed at elucidating genomic responses to drought stress
in loblolly pine seedlings. The current experimental design incorporates 384
pine cDNAs replicated and randomly placed in two specific microarray layouts.
We describe the design of Expresso as well as results of analysis with Expresso
that suggest the importance of molecular chaperones and membrane transport
proteins in mechanisms conferring successful adaptation to long-term drought
stress.
"
1886,"Overview of the Experimental Physics and Industrial Control System
  (EPICS) Channel Archiver","  The Channel Archiver has been operational for more than two years at Los
Alamos National Laboratory and other sites. This paper introduces the available
components (data sampling engine, viewers, scripting interface, HTTP/CGI
integration and data management), presents updated performance measurements and
reviews operational experience with the Channel Archiver.
"
1887,Integrating LabVIEW into a Distributed Computing Environment,"  Being easy to learn and well suited for a self-contained desktop laboratory
setup, many casual programmers prefer to use the National Instruments LabVIEW
environment to develop their logic. An ActiveX interface is presented that
allows integration into a plant-wide distributed environment based on the
Experimental Physics and Industrial Control System (EPICS). This paper
discusses the design decisions and provides performance information, especially
considering requirements for the Spallation Neutron Source (SNS) diagnostics
system.
"
1888,"L-Fuzzy Valued Inclusion Measure, L-Fuzzy Similarity and L-Fuzzy
  Distance","  The starting point of this paper is the introduction of a new measure of
inclusion of fuzzy set A in fuzzy set B. Previously used inclusion measures
take values in the interval [0,1]; the inclusion measure proposed here takes
values in a Boolean lattice. In other words, inclusion is viewed as an L-fuzzy
valued relation between fuzzy sets. This relation is re exive, antisymmetric
and transitive, i.e. it is a fuzzy order relation; in addition it possesess a
number of properties which various authors have postulated as axiomatically
appropriate for an inclusion measure. We also define an L-fuzzy valued measure
of similarity between fuzzy sets and and an L-fuzzy valued distance function
between fuzzy sets; these possess properties analogous to the ones of
real-valued similarity and distance functions.
  Keywords: Fuzzy Relations, inclusion measure, subsethood, L-fuzzy sets,
similarity, distance, transitivity.
"
1889,Automated Real-Time Testing (ARTT) for Embedded Control Systems (ECS),"  Developing real-time automated test systems for embedded control systems has
been a real problem. Some engineers and scientists have used customized
software and hardware as a solution, which can be very expensive and time
consuming to develop. We have discovered how to integrate a suite of
commercially available off-the-shelf software tools and hardware to develop a
scalable test platform that is capable of performing complete black-box testing
for a dual-channel real-time Embedded-PLC-based control system
(www.aps.anl.gov). We will discuss how the Vali/Test Pro testing methodology
was implemented to structure testing for a personnel safety system with large
quantities of requirements and test cases.
  This work was supported by the U.S. Department of Energy, Basic Energy
Sciences, under Contract No. W-31-109-Eng-38.
"
1890,"First Experiences Integrating PC Distributed I/O Into Argonne's ATLAS
  Control System","  First Experiences Integrating PC Distributed I/O Into Argonne's ATLAS Control
System The roots of ATLAS (Argonne Tandem-Linac Accelerator System) date back
to the early 1960s. Located at the Argonne National Laboratory, the accelerator
has been designated a National User Facility, which focuses primarily on
heavy-ion nuclear physics. Like the accelerator it services, the control system
has been in a constant state of evolution. The present real-time portion of the
control system is based on the commercial product Vsystem [1]. While Vsystem
has always been capable of distributed I/O processing, the latest offering of
this product provides for the use of relatively inexpensive PC hardware and
software. This paper reviews the status of the ATLAS control system, and
describes first experiences with PC distributed I/O.
"
1891,Gemini MCAO Control System,"  The Gemini Observatory is planning to implement a Multi Conjugate Adaptive
Optics (MCAO) System as a facility instrument for the Gemini-South telescope.
The system will include 5 Laser Guide Stars, 3 Natural Guide Stars, and 3
Deformable mirrors optically conjugated at different altitudes to achieve
near-uniform atmospheric compensation over a 1 arc minute square field of view.
The control of such a system will be split into 3 main functions: the control
of the opto-mechanical assemblies of the whole system (including the Laser, the
Beam Transfer Optics and the Adaptive Optics bench), the control of the
Adaptive Optics System itself at a rate of 800FPS and the control of the safety
system. The control of the Adaptive Optics System is the most critical in terms
of real time performances. The control system will be an EPICS based system. In
this paper, we will describe the requirements for the whole MCAO control
system, preliminary designs for the control of the opto-mechanical devices and
architecture options for the control of the Adaptive Optics system and the
safety system.
"
1892,SNS Standard Power Supply Interface,"  The SNS has developed a standard power supply interface for the approximately
350 magnet power supplies in the SNS accumulator ring, Linac and transport
lines. Power supply manufacturers are providing supplies compatible with the
standard interface. The SNS standard consists of a VME based power supply
controller module (PSC) and a power supply interface unit (PSI) that mounts on
the power supply. Communication between the two is via a pair of multimode
fibers. This PSI/PSC system supports one 16-bit analog reference, four 16-bit
analog readbacks, fifteen digital commands and sixteen digital status bits in a
single fiber-isolated module. The system can send commands to the supplies and
read data from them synchronized to an external signal at up to a 10KHz rate.
The PSC time stamps and stores this data in a circular buffer so historical
data leading up to a fault event can be analyzed. The PSC contains a serial
port so that local testing of hardware can be accomplished with a laptop. This
paper concentrates on the software being provided to control the power supply.
It includes the EPICS driver; software to test hardware and power supplies via
the serial port and VME interface.
"
1893,Overview of the NSTX Control System,"  The National Spherical Torus Experiment (NSTX) is an innovative magnetic
fusion device that was constructed by the Princeton Plasma Physics Laboratory
(PPPL) in collaboration with the Oak Ridge National Laboratory, Columbia
University, and the University of Washington at Seattle. Since achieving first
plasma in 1999, the device has been used for fusion research through an
international collaboration of over twenty institutions. The NSTX is operated
through a collection of control systems that encompass a wide range of
technology, from hardwired relay controls to real-time control systems with
giga-FLOPS of capability. This paper presents a broad introduction to the
control systems used on NSTX, with an emphasis on the computing controls, data
acquisition, and synchronization systems.
"
1894,"The Lattice of Fuzzy Intervals and Sufficient Conditions for its
  Distributivity","  Given a reference lattice, we define fuzzy intervals to be the fuzzy sets
such that their p-cuts are crisp closed intervals. We show that: given a
complete reference lattice, the collection of its fuzzy intervals is a complete
lattice. Furthermore we show that: if the reference lattice is completely
distributive then the lattice of its fuzzy intervals is distributive.
"
1895,"Evolutionary Circuit Design: Information Theory Perspective on Signal
  Propagation","  This paper presents case-study results on the application of information
theoretic approach to gate-level evolutionary circuit design. We introduce
information measures to provide better estimates of synthesis criteria of
digital circuits. For example, the analysis of signal propagation during
evolving gate-level synthesis can be improved by using information theoretic
measures that will make it possible to find the most effective geometry and
therefore predict the cost of the final design solution. The problem is
considered from the information engine point of view. That is, the process of
evolutionary gate-level circuit design is presented via such measures as
entropy, logical work and information vitality. Some examples of geometry
driven synthesis are provided to prove the above idea.
"
1896,Information Measures in Detecting and Recognizing Symmetries,"  This paper presents a method to detect and recognize symmetries in Boolean
functions. The idea is to use information theoretic measures of Boolean
functions to detect sub-space of possible symmetric variables. Coupled with the
new techniques of efficient estimations of information measures on Binary
Decision Diagrams (BDDs) we obtain promised results in symmetries detection for
large-scale functions.
"
1897,"Towards Efficient Calculation of Information Measures for Reordering of
  Binary Decision Diagrams","  This paper introduces new technique for efficient calculation of different
Shannon information measures which operates Binary Decision Diagrams (BDDs). We
offer an algorithm of BDD reordering which demonstrates the improvement of the
obtaining outcomes over the existing reordering approaches. The technique and
the reordering algorithm have been implemented, and the results on circuits'
benchmarks are analyzed. We point out that the results are quite promising, the
algorithm is very fast, and it is easy to implement. Finally, we show that our
approach to BDD reordering can yield to reduction in the power dissipation for
the circuits derived from BDDs.
"
1898,A universal alphabet and rewrite system,"  We present two ways in which an infinite universal alphabet may be generated
using a novel rewrite system that conserves zero (a special character of the
alphabet and the symbol for that character) at every step. The recursive method
delivers the entire alphabet in one step when invoked with the zero character
as the initial subset alphabet. The iterative method with the same start
delivers characters that act as ciphers for properties that the developing
subset alphabet contains. These properties emerge in an arbitrary sequence and
there are an infinite number of ways they may be selected. The subset alphabets
in addition to having mathematical interpretation as algebra can also be
constrained to emerge in a minimal way which then has application as a
foundational physical system. Each subset alphabet may itself be the basis of a
rewrite system where rules that operate on symbols (representing characters) or
collections of symbols manipulate the specific properties in a dynamic way.
"
1899,Symmetric and anti-symmetric quantum functions,"  This paper introduces and analyzes symmetric and anti-symmetric quantum
binary functions. Generally, such functions uniquely convert a given
computational basis state into a different basis state, but with either a plus
or a minus sign. Such functions may serve along with a constant function (in a
Deutsch-Jozsa type of algorithm) to provide 2**n deterministic qubit
combinations (for n qubits) instead of just one.
"
1900,On probabilistic analog automata,"  We consider probabilistic automata on a general state space and study their
computational power. The model is based on the concept of language recognition
by probabilistic automata due to Rabin and models of analog computation in a
noisy environment suggested by Maass and Orponen, and Maass and Sontag. Our
main result is a generalization of Rabin's reduction theorem that implies that
under very mild conditions, the computational power of the automaton is limited
to regular languages.
"
1901,"Modeling of aerodynamic Space-to-Surface flight with optimal trajectory
  for targeting","  Modeling has been created for a Space-to-Surface system defined for an
optimal trajectory for targeting in terminal phase. The modeling includes
models for simulation atmosphere, speed of sound, aerodynamic flight and
navigation by an infrared system. The modeling simulation includes statistical
analysis of the modeling results.
"
1902,"Semiclassical Quantum Computation Solutions to the Count to Infinity
  Problem: A Brief Discussion","  In this paper we briefly define distance vector routing algorithms, their
advantages and possible drawbacks. On these possible drawbacks, currently
widely used methods split horizon and poisoned reverse are defined and
compared. The count to infinity problem is specified and it is classified to be
a halting problem and a proposition stating that entangled states used in
quantum computation can be used to handle this problem is examined. Several
solutions to this problem by using entangled states are proposed and a very
brief introduction to entangled states is presented.
"
1903,CASTOR status and evolution,"  In January 1999, CERN began to develop CASTOR (""CERN Advanced STORage
manager""). This Hierarchical Storage Manager targetted at HEP applications has
been in full production at CERN since May 2001. It now contains more than two
Petabyte of data in roughly 9 million files. In 2002, 350 Terabytes of data
were stored for COMPASS at 45 MB/s and a Data Challenge was run for ALICE in
preparation for the LHC startup in 2007 and sustained a data transfer to tape
of 300 MB/s for one week (180 TB). The major functionality improvements were
the support for files larger than 2 GB (in collaboration with IN2P3) and the
development of Grid interfaces to CASTOR: GridFTP and SRM (""Storage Resource
Manager""). An ongoing effort is taking place to copy the existing data from
obsolete media like 9940 A to better cost effective offerings. CASTOR has also
been deployed at several HEP sites with little effort. In 2003, we plan to
continue working on Grid interfaces and to improve performance not only for
Central Data Recording but also for Data Analysis applications where thousands
of processes possibly access the same hot data. This could imply the selection
of another filesystem or the use of replication (hardware or software).
"
1904,"SCRAM: Software configuration and management for the LHC Computing Grid
  project","  Recently SCRAM (Software Configuration And Management) has been adopted by
the applications area of the LHC computing grid project as baseline
configuration management and build support infrastructure tool.
  SCRAM is a software engineering tool, that supports the configuration
management and management processes for software development. It resolves the
issues of configuration definition, assembly break-down, build, project
organization, run-time environment, installation, distribution, deployment, and
source code distribution. It was designed with a focus on supporting a
distributed, multi-project development work-model.
  We will describe the underlying technology, and the solutions SCRAM offers to
the above software engineering processes, while taking a users view of the
system under configuration management.
"
1905,Monitoring Systems and Services,"  The DESY Computer Center is the home of O(1000) computers supplying a wide
range of different services Monitoring such a large installation is a
challenge. After a long time running a SNMP based commercial Network Management
System, the evaluation of a new System was started. There are a lot of
different commercial and freeware products on the market, but none of them
fully satisfied all our requirements. After re-valuating our original
requirements we selected NAGIOS as our monitoring and alarming tool. After a
successful test we are in production since autumn 2002 and are extending the
service to fully support a distributed monitoring and alarming.
"
1906,Multi-valued Connectives for Fuzzy Sets,"  We present a procedure for the construction of multi-valued t-norms and
t-conorms. Our procedure makes use of a pair of single-valued t-norms and the
respective dual t-conorms and produces interval-valued t-norms and t-conorms.
In this manner we combine desirable characteristics of different t-norms and
t-conorms; if we use the t-norm min and t-conorm max, then the resulting
structure is a superlattice, i.e. the multivalued analog of a lattice.
"
1907,Supporting Dynamic Ad hoc Collaboration Capabilities,"  Modern HENP experiments such as CMS and Atlas involve as many as 2000
collaborators around the world. Collaborations this large will be unable to
meet often enough to support working closely together. Many of the tools
currently available for collaboration focus on heavy-weight applications such
as videoconferencing tools. While these are important, there is a more basic
need for tools that support connecting physicists to work together on an ad hoc
or continuous basis. Tools that support the day-to-day connectivity and
underlying needs of a group of collaborators are important for providing
light-weight, non-intrusive, and flexible ways to work collaboratively. Some
example tools include messaging, file-sharing, and shared plot viewers. An
important component of the environment is a scalable underlying communication
framework. In this paper we will describe our current progress on building a
dynamic and ad hoc collaboration environment and our vision for its evolution
into a HENP collaboration environment.
"
1908,Verification of Process Rewrite Systems in normal form,"  We consider the problem of model--checking for Process Rewrite Systems (PRSs)
in normal form. In a PRS in normal form every rewrite rule either only deals
with procedure calls and procedure termination, possibly with value return,
(this kind of rules allows to capture Pushdown Processes), or only deals with
dynamic activation of processes and synchronization (this kind of rules allows
to capture Petri Nets). The model-checking problem for PRSs and action-based
linear temporal logic (ALTL) is undecidable. However, decidability of
model--checking for PRSs and some interesting fragment of ALTL remains an open
question. In this paper we state decidability results concerning generalized
acceptance properties about infinite derivations (infinite term rewritings) in
PRSs in normal form. As a consequence, we obtain decidability of the
model-checking (restricted to infinite runs) for PRSs in normal form and a
meaningful fragment of ALTL.
"
1909,Using biased coins as oracles,"  While it is well known that a Turing machine equipped with the ability to
flip a fair coin cannot compute more that a standard Turing machine, we show
that this is not true for a biased coin. Indeed, any oracle set $X$ may be
coded as a probability $p_{X}$ such that if a Turing machine is given a coin
which lands heads with probability $p_{X}$ it can compute any function
recursive in $X$ with arbitrarily high probability. We also show how the
assumption of a non-recursive bias can be weakened by using a sequence of
increasingly accurate recursive biases or by choosing the bias at random from a
distribution with a non-recursive mean. We conclude by briefly mentioning some
implications regarding the physical realisability of such methods.
"
1910,"New Visualization of Surfaces in Parallel Coordinates - Eliminating
  Ambiguity and Some ""Over-Plotting""","  $\cal{A}$ point $P \in \Real^n$ is represented in Parallel Coordinates by a
polygonal line $\bar{P}$ (see \cite{Insel99a} for a recent survey). Earlier
\cite{inselberg85plane}, a surface $\sigma$ was represented as the {\em
envelope} of the polygonal lines representing it's points. This is ambiguous in
the sense that {\em different} surfaces can provide the {\em same} envelopes.
Here the ambiguity is eliminated by considering the surface $\sigma$ as the
envelope of it's {\em tangent planes} and in turn, representing each of these
planes by $n$-1 points \cite{Insel99a}. This, with some future extension, can
yield a new and unambiguous representation, $\bar{\sigma}$, of the surface
consisting of $n$-1 planar regions whose properties correspond lead to the {\em
recognition} of the surfaces' properties i.e. developable, ruled etc.
\cite{hung92smooth}) and {\em classification} criteria.
  It is further shown that the image (i.e. representation) of an algebraic
surface of degree 2 in $\Real^n$ is a region whose boundary is also an
algebraic curve of degree 2. This includes some {\em non-convex} surfaces which
with the previous ambiguous representation could not be treated. An efficient
construction algorithm for the representation of the quadratic surfaces (given
either by {\em explicit} or {\em implicit} equation) is provided. The results
obtained are suitable for applications, to be presented in a future paper, and
in particular for the approximation of complex surfaces based on their {\em
planar} images. An additional benefit is the elimination of the
``over-plotting'' problem i.e. the ``bunching'' of polygonal lines which often
obscure part of the parallel-coordinate display.
"
1911,"Algebraic Curves in Parallel Coordinates - Avoiding the ""Over-Plotting""
  Problem","  ${\cal U}$ntil now the representation (i.e. plotting) of curve in Parallel
Coordinates is constructed from the point $\leftrightarrow$ line duality. The
result is a ``line-curve'' which is seen as the envelope of it's tangents.
Usually this gives an unclear image and is at the heart of the
``over-plotting'' problem; a barrier in the effective use of Parallel
Coordinates. This problem is overcome by a transformation which provides
directly the ``point-curve'' representation of a curve. Earlier this was
applied to conics and their generalizations. Here the representation, also
called dual, is extended to all planar algebraic curves. Specifically, it is
shown that the dual of an algebraic curve of degree $n$ is an algebraic of
degree at most $n(n - 1)$ in the absence of singular points. The result that
conics map into conics follows as an easy special case. An algorithm, based on
algebraic geometry using resultants and homogeneous polynomials, is obtained
which constructs the dual image of the curve. This approach has potential
generalizations to multi-dimensional algebraic surfaces and their
approximation. The ``trade-off'' price then for obtaining {\em planar}
representation of multidimensional algebraic curves and hyper-surfaces is the
higher degree of the image's boundary which is also an algebraic curve in
$\|$-coords.
"
1912,"What we should teach, but don't: Proposal for a cross pollinated HCI-SE
  curriculum","  Software engineering (SE) and usability engineering (UE), as disciplines,
have reached substantial levels of maturity. Each of these two disciplines is
now well represented with respect to most computer science (CS) curricula. But,
the two disciplines are practiced almost independently - missing oppurtunities
to collaborate, coordinate and communicate about the overall design - and
thereby contributing to system failures. Today, a confluence of several
ingredients contribute to these failures: the increasing importance of the user
interface (UI) component in the overall system, the independent maturation of
the human computer interaction area, and the lack of a cohesive process model
to integrate the UI experts' UE development efforts with that of SE. This in
turn, we believe, is a result of a void in computing curricula: a lack of
education and training regarding the importance of communication, collaboration
and coordination between the SE and UE processes. In this paper we describe the
current approach to teaching SE and UE and its shortcomings. We identify and
analyze the barriers and issues involved in developing systems having
substantial interactive components. We then propose four major themes of
learning for a comprehensive computing curriculum integrating SE, UE, and
system architectures in a project environment.
"
1913,An approach to membrane computing under inexactitude,"  In this paper we introduce a fuzzy version of symport/antiport membrane
systems. Our fuzzy membrane systems handle possibly inexact copies of reactives
and their rules are endowed with threshold functions that determine whether a
rule can be applied or not to a given set of objects, depending of the degree
of accuracy of these objects to the reactives specified in the rule. We prove
that these fuzzy membrane systems generate exactly the recursively enumerable
finite-valued fuzzy sets of natural numbers.
"
1914,"Model checking for Process Rewrite Systems and a class of action--based
  regular properties","  We consider the model checking problem for Process Rewrite Systems (PRSs), an
infinite-state formalism (non Turing-powerful) which subsumes many common
models such as Pushdown Processes and Petri Nets. PRSs can be adopted as formal
models for programs with dynamic creation and synchronization of concurrent
processes, and with recursive procedures. The model-checking problem for PRSs
and action-based linear temporal logic (ALTL) is undecidable. However,
decidability for some interesting fragment of ALTL remains an open question. In
this paper we state decidability results concerning generalized acceptance
properties about infinite derivations (infinite term rewriting) in PRSs. As a
consequence, we obtain decidability of the model-checking (restricted to
infinite runs) for PRSs and a meaningful fragment of ALTL.
"
1915,Effects of wireless computing technology,"  Wireless technology can provide many benefits to computing including faster
response to queries, reduced time spent on paperwork, increased online time for
users, just-in-time and real time control, tighter communications between
clients and hosts. Wireless Computing is governed by two general forces:
Technology, which provides a set of basic building blocks and User
Applications, which determine a set of operations that must be carried out
efficiently on demand. This paper summarizes technological changes that are
underway and describes their impact on wireless computing development and
implementation. It also describes the applications that influence the
development and implementation of wireless computing and shows what current
systems offer.
"
1916,Scheduling with Fuzzy Methods,"  Nowadays, manufacturing industries -- driven by fierce competition and rising
customer requirements -- are forced to produce a broader range of individual
products of rising quality at the same (or preferably lower) cost. Meeting
these demands implies an even more complex production process and thus also an
appropriately increasing request to its scheduling. Aggravatingly, vagueness of
scheduling parameters -- such as times and conditions -- are often inherent in
the production process. In addition, the search for an optimal schedule
normally leads to very difficult problems (NP-hard problems in the complexity
theoretical sense), which cannot be solved effciently. With the intent to
minimize these problems, the introduced heuristic method combines standard
scheduling methods with fuzzy methods to get a nearly optimal schedule within
an appropriate time considering vagueness adequately.
"
1917,"Business Processes: The Theoretical Impact of Process Thinking on
  Information Systems Development","  This paper investigates two aspects of process thinking that affect the
success rate of IT projects. These two aspects are the changes in the structure
of organizations and the epistemology of Information Systems Development.
Firstly, the conception of business processes within the management of
organizations increases the structural complexity of Information Systems,
because existing systems have to be integrated into a coherent cross-functional
architecture. Secondly, process thinking leads to a particular view of
organizations that ultimately has a negative effect on the support of
Information Systems. As an illustration of process thinking, the Business
Process Reengineering movement adheres to a technocratic management perspective
of organizations. Particularly this conception of organization views people as
mechanisms to realize certain organizational goals. As a result of this view
stakeholders are confronted with the implemented systems, rather than consulted
about the scope and functionality of those systems. Therefore, both aspects of
process thinking have a negative impact on the success of IT projects. The
problem of structural complexity is an area that is addressed by Enterprise
Application Integration, and mainly requires technical solutions. However, the
problems associated with the conception of organization require a different,
markedly non-technical, perspective. Several directions are discussed to
overcome some limitations of process thinking, but these directions are merely
small pointers. If truly effective and useful Information Systems are to be
acquired, IT practitioners and scientists require a completely different
mindset.
"
1918,Demo or Practice: Critical Analysis of the Language/Action Perspective,"  Despite offering several promising concepts, the Language/Action Perspective
(LAP) is still not in the mainstream of Information Systems Development (ISD).
Since at present there is only a limited understanding of LAP theory and
practice, it remains unclear whether the lack of LAP's impact is due to
shortcomings in LAP theory itself. One classic problem within ISD is the
dichotomy between social perspectives and technical perspectives. LAP claims it
offers a solution to this problem. This paper investigates this claim as a
means to review LAP theory. To provide a structure to a critical analysis of
DEMO - an example methodology that belongs to the LAP research community - this
paper utilizes a paradigmatic framework. This framework is augmented by the
opinion of several DEMO practitioners by means of an expert discussion. With
use of a comparative evaluation of LAP theory and DEMO theory, the implication
of DEMO's reflection upon LAP is determined. The paper concludes by outlining
an agenda for further research if LAP is to improve its footprint in the field.
"
1919,Toward a Human-Centered Uml for Risk Analysis,"  Safety is now a major concern in many complex systems such as medical robots.
A way to control the complexity of such systems is to manage risk. The first
and important step of this activity is risk analysis. During risk analysis, two
main studies concerning human factors must be integrated: task analysis and
human error analysis. This multidisciplinary analysis often leads to a work
sharing between several stakeholders who use their own languages and
techniques. This often produces consistency errors and understanding
difficulties between them. Hence, this paper proposes to treat the risk
analysis on the common expression language UML (Unified Modeling Language) and
to handle human factors concepts for task analysis and human error analysis
based on the features of this language. The approach is applied to the
development of a medical robot for teleechography.
"
1920,Successful E-Business Systems - Paypal,"  PayPal is an account-based system that allows anyone with an email address to
send and receive online payment s. This service is easy to use for customers.
Members can instantaneously send money to anyone. Recipients are informed by
email that they have received a payment. PayPal is also available to people in
38 countries. This paper starts with introduction to the company and its
services. The information about the history and the current company situation
are covered. Later some interesting and different technical issues are
discussed. The Paper ends with analysis of the company and several future
recommendations.
"
1921,"Collective Intelligence Quanitifed for Computer-Mediated Group Problem
  Solving","  Collective Intelligence (CI) is the ability of a group to exhibit greater
intelligence than its individual members. Expressed by the common saying that
""two minds are better than one,"" CI has been a topic of interest for social
psychology and the information sciences. Computer mediation adds a new element
in the form of distributed networks and group support systems. These facilitate
highly organized group activities that were all but impossible before computer
mediation. This paper presents experimental findings on group problem solving
where a distributed software system automatically integrates input from many
humans. In order to quantify Collective Intelligence, we compare the
performance of groups to individuals when solving a mathematically formalized
problem. This study shows that groups can outperform individuals on difficult
but not easy problems, though groups are slower to produce solutions. The
subjects are 57 university students. The task is the 8-Puzzle sliding tile
game.
"
1922,"High efficiency and low absorption Fresnel compound zone plates for hard
  X-ray focusing","  Circular and linear zone plates have been fabricated on the surface of
silicon crystals for the energy of 8 keV by electron beam lithography and deep
ion plasma etching methods. Various variants of compound zone plates with
first, second, third diffraction orders have been made. The zone relief height
is about 10 mkm, the outermost zone width of the zone plate is 0.4 mkm. The
experimental testing of the zone plates has been conducted on SPring-8 and ESRF
synchrotron radiation sources. A focused spot size and diffraction efficiency
measured by knife-edge scanning are accordingly 0.5 mkm and 39% for the first
order circular zone plate.
"
1923,A Fast Combined Decimal Adder/Subtractor,"  This paper has been withdrawn.
"
1924,Improved direct sum theorem in classical communication complexity,"  Withdrawn due to critical error.
"
1925,Asynchronous pseudo-systems,"  The paper introduces the concept of asynchronous pseudo-system. Its purpose
is to correct/generalize/continue the study of the asynchronous systems (the
models of the asynchronous circuits) that has been started in [1], [2].
"
1926,f2mma: FORTRAN to Mathematica translator,"  f2mma program can be used to translate programs written in some subset of the
FORTRAN language into {\sl Mathematica} system's programming language. This
subset have been enough to translate GAPP (Global Analysis of Particle
Properties) programm into {\sl Mathematica} language automatically. Observables
table calculated with GAPP({\sl Mathematica}) is presented.
"
1927,Mapping DEVS Models onto UML Models,"  Discrete event simulation specification (DEVS) is a formalism designed to
describe both discrete state and continuous state systems. It is a powerful
abstract mathematical notation. However, until recently it lacked proper
graphical representation, which made computer simulation of DEVS models a
challenging issue. Unified modeling language (UML) is a multipurpose graphical
modeling language, a de-facto industrial modeling standard. There exist several
commercial and open-source UML editors and code generators. Most of them can
save UML models in XML-based XMI files ready for further automated processing.
In this paper, we propose a mapping of DEVS models onto UML state and component
diagrams. This mapping may lead to an eventual unification of the two modeling
formalisms, combining the abstractness of DEVS and expressive power and
``computer friendliness'' of the UML.
"
1928,Fat Tailed Distributions in Catastrophe Prediction,"  This paper discusses the use of fat-tailed distributions in catastrophe
prediction as opposed to the more common use of the Normal Distribution.
"
1929,The Fibonacci Sequence Mod m,"  This paper proposes a computational method for obtaining the length of the
cycle that arises from the Fibonacci series taken mod m (some number) and mod p
(some prime number).
"
1930,Efficient Teamwork,"  Our goal is to solve both problems of adverse selection and moral hazard for
multi-agent projects. In our model, each selected agent can work according to
his private ""capability tree"". This means a process involving hidden actions,
hidden chance events and hidden costs in a dynamic manner, and providing
contractible consequences which are affecting each other's working process and
the outcome of the project. We will construct a mechanism that induces truthful
revelation of the agents' capability trees and chance events and to follow the
instructions about their hidden decisions. This enables the planner to select
the optimal subset of agents and obtain the efficient joint execution. We will
construct another mechanism that is collusion-resistant but implements an only
approximately efficient outcome. The latter mechanism is widely applicable, and
the major application details will be elaborated.
"
1931,Quasi-Linear Soft Tissue Models Revisited,"  Incompressibility, nonlinear deformation under stress and viscoelasticity are
the fingerprint of soft tissue mechanical behavior. In order to model soft
tissues appropriately, we must pursue to complete these requirements. In this
work we revisited different soft tissue quasi-linear model possibilities in
trying to achieve for this commitment.
"
1932,Natural Economics,"  A few considerations on the nature of Economics and its relationship to human
communities through the prism of Self-Organizing-Systems.
"
1933,"A unifying framework for seed sensitivity and its application to subset
  seeds (Extended abstract)","  We propose a general approach to compute the seed sensitivity, that can be
applied to different definitions of seeds. It treats separately three
components of the seed sensitivity problem - a set of target alignments, an
associated probability distribution, and a seed model - that are specified by
distinct finite automata. The approach is then applied to a new concept of
subset seeds for which we propose an efficient automaton construction.
Experimental results confirm that sensitive subset seeds can be efficiently
designed using our approach, and can then be used in similarity search
producing better results than ordinary spaced seeds.
"
1934,Estimating seed sensitivity on homogeneous alignments,"  We address the problem of estimating the sensitivity of seed-based similarity
search algorithms. In contrast to approaches based on Markov models [18, 6, 3,
4, 10], we study the estimation based on homogeneous alignments. We describe an
algorithm for counting and random generation of those alignments and an
algorithm for exact computation of the sensitivity for a broad class of seed
strategies. We provide experimental results demonstrating a bias introduced by
ignoring the homogeneousness condition.
"
1935,"Mathematical Modeling of Aerodynamic Space -to - Surface Flight with
  Trajectory for Avoid Intercepting Process","  Modeling has been created for a Space-to-Surface system defined for an
optimal trajectory for targeting in terminal phase with avoids an intercepting
process. The modeling includes models for simulation atmosphere, speed of
sound, aerodynamic flight and navigation by an infrared system. The modeling
and simulation includes statistical analysis of the modeling results.
"
1936,"Computational Modeling in Applied Problems: collected papers on
  econometrics, operations research, game theory and simulation","  Computational models pervade all branches of the exact sciences and have in
recent times also started to prove to be of immense utility in some of the
traditionally 'soft' sciences like ecology, sociology and politics. This volume
is a collection of a few cutting-edge research papers on the application of
variety of computational models and tools in the analysis, interpretation and
solution of vexing real-world problems and issues in economics, management,
ecology and global politics by some prolific researchers in the field.
"
1937,"An Electronic Payment System to Ensure Cost Effectiveness with Easy
  Security Incorporation for the Developing Countries","  With the rapid growth of Information and Communication Technology, Electronic
commerce is now acting as a new means of carrying out business transactions
through electronic means such as Internet environment. To avoid the
complexities associated with the digital cash and electronic cash, consumers
and vendors are looking for credit card payments on the Internet as one
possible time-tested alternative. This gave rise of the on-line payment
processing using a third-party verification; which is not suitable for the
developing countries in most of the cases because of the excessive costs
associated with it for maintenance and establishment of an online third-party
processor. As a remedy of this problem, in this paper, we have proposed a
framework for easy security incorporation in credit card based electronic
payment system without the use of an on-line third- party processor; which
tends to be low cost and effective for the developing countries.
"
1938,Authorised Translations of Electronic Documents,"  A concept is proposed to extend authorised translations of documents to
electronically signed, digital documents. Central element of the solution is an
electronic seal, embodied as an XML data structure, which attests to the
correctness of the translation and the authorisation of the translator. The
seal contains a digital signature binding together original and translated
document, thus enabling forensic inspection and therefore legal security in the
appropriation of the translation. Organisational aspects of possible
implementation variants of electronic authorised translations are discussed and
a realisation as a stand-alone web-service is presented.
"
1939,Models simulation and interoperability using MDA and HLA,"  In the manufacturing context, there have been numerous efforts to use
modeling and simulation tools and techniques to improve manufacturing
efficiency over the last four decades. While an increasing number of
manufacturing system decisions are being made based on the use of models, their
use is still sporadic in many manufacturing environments. Our paper advocates
for an approach combining MDA (model driven architecture) and HLA (High Level
Architecture), the IEEE standard for modeling and simulation, in order to
overcome the deficiencies of current simulation methods at the level of
interoperability and reuse.
"
1940,"Syst\`{e}me de repr\'{e}sentation d'aide au besoin dans le domaine
  architectural","  The image is a very important mean of communication in the field of
architectural who intervenes in the various phases of the design of a project.
It can be regarded as a tool of decision-making aid. The study of our research
aims at to see the contribution of the Economic Intelligence in the resolution
of a decisional problem of the various partners (Architect, Contractor,
Customer) in the architectural field, in order to make strategic decisions
within the framework of the realization or design of an architectural work. The
economic Intelligence allows the taking into account of the real needs for the
user-decision makers, so that their waiting are considered at the first stage
of a search for information and not in the final stage of the development of
the tool in the evaluation of this last.
"
1941,Levels of Product Differentiation in the Global Mobile Phones Market,"  The sixth product level called compliant product is a connecting element
between the physical product characteristics and the strategy of the producer
company. The article discusses the differentiation among the product offers of
companies working in the global markets, as well as the strategies which they
use and could use in that respect.
"
1942,"Exploring Computer Science Concepts with a Ready-made Computer Game
  Framework","  Leveraging the prevailing interest in computer games among college students,
both for entertainment and as a possible career path, is a major reason for the
increasing prevalence of computer game design courses in computer science
curricula. Because implementing a computer game requires strong programming
skills, game design courses are most often restricted to more advanced computer
science students. This paper reports on a ready-made game design and
experimentation framework, implemented in Java, that makes game programming
more widely accessible. This framework, called Labyrinth, enables students at
all programming skill levels to participate in computer game design. We
describe the architecture of the framework, and discuss programming projects
suitable for a wide variety of computer science courses, from capstone to
non-major.
"
1943,A multipurpose Hopf deformation of the Algebra of Feynman-like Diagrams,"  We construct a three parameter deformation of the Hopf algebra
$\mathbf{LDIAG}$. This new algebra is a true Hopf deformation which reduces to
$\mathbf{LDIAG}$ on one hand and to $\mathbf{MQSym}$ on the other, relating
$\mathbf{LDIAG}$ to other Hopf algebras of interest in contemporary physics.
Further, its product law reproduces that of the algebra of polyzeta functions.
"
1944,Domain Wall Displacement Detection Technology Research Report,"  This article introduce a new data storage method called DWDD(Domain Wall
Displacement Detection) and tell you why it succeed.
"
1945,"Developing strategies to produce better scientific papers: a Recipe for
  non-native users of English","  In this paper we introduce the AMADEUS strategy, which has been used to
produce scientific writing tools for non-native users of English for 15 years,
and emphasize a learn-by-doing approach through which students and novice
writers can improve their scientific writing. More specifically, we provide a
9-step recipe for the students to compile writing material according to a
procedure that has proven efficient in scientific writing courses.
"
1946,Relatively inertial delays,"  The paper studies the relatively inertial delays that represent one of the
most important concepts in the modeling of the asynchronous circuits.
"
1947,"Safety Evaluation of Critical Applications Distributed on TDMA-Based
  Networks","  Critical embedded systems have to provide a high level of dependability. In
automotive domain, for example, TDMA protocols are largely recommended because
of their deterministic behavior. Nevertheless, under the transient
environmental perturbations, the loss of communication cycles may occur with a
certain probability and, consequently, the system may fail. This paper analyzes
the impact of the transient perturbations (especially due to Electromagnetic
Interferences) on the dependability of systems distributed on TDMA-based
networks. The dependability of such system is modeled as that of
""consecutive-k-out-of-n:F"" systems and we provide a efficient way for its
evaluation.
"
1948,Hard Disk Drive as a Magnetomechanical Logic Device,"  We consider the conditions how two binary numbers can be superimposed on the
same track with the use of different recording magnetic fields. As a result the
average magnetization of longitudinal medium along the track can have three
states: -M, 0 and +M. Possibility to perform logic operations with these states
is considered. We demonstrate OR, AND, XOR and NOT operations and discuss a
modification of a recording device.
"
1949,Analysing viewpoints in design through the argumentation process,"  We present an empirical study aimed at analysing the use of viewpoints in an
industrial Concurrent Engineering context. Our focus is on the viewpoints
expressed in the argumentative process taking place in evaluation meetings. Our
results show that arguments enabling a viewpoint or proposal to be defended are
often characterized by the use of constraints. Firstly, we show that, even if
some constraints are apparently identically used by the different specialists
involved in meetings, various meanings and weightings are associated with these
constraints by these different specialists. Secondly, we show that the implicit
or explicit nature of constraints depends on several interlocutive factors.
Thirdly, we show that an argument often covers not only one constraint but a
network of constraints. The type of combination reflects viewpoints which have
specific status in the meeting. Then, we will propose a first model of the
dynamics of viewpoints confrontation/integration.
"
1950,Verification Across Intellectual Property Boundaries,"  In many industries, the importance of software components provided by
third-party suppliers is steadily increasing. As the suppliers seek to secure
their intellectual property (IP) rights, the customer usually has no direct
access to the suppliers' source code, and is able to enforce the use of
verification tools only by legal requirements. In turn, the supplier has no
means to convince the customer about successful verification without revealing
the source code. This paper presents an approach to resolve the conflict
between the IP interests of the supplier and the quality interests of the
customer. We introduce a protocol in which a dedicated server (called the
""amanat"") is controlled by both parties: the customer controls the verification
task performed by the amanat, while the supplier controls the communication
channels of the amanat to ensure that the amanat does not leak information
about the source code. We argue that the protocol is both practically useful
and mathematically sound. As the protocol is based on well-known (and
relatively lightweight) cryptographic primitives, it allows a straightforward
implementation on top of existing verification tool chains. To substantiate our
security claims, we establish the correctness of the protocol by cryptographic
reduction proofs.
"
1951,A Dynamic I/O Model for TRACON Traffic Management,"  This work investigates the TRACON flow management around a major airport.
Aircraft flows are analyzed through a study of TRACON trajectories records.
Rerouting and queuing processes are highlighted and airport characteristics are
shown as function of the number of planes in the TRACON. Then, a simple
input-output TRACON queuing and landing model is proposed. This model is
calibrated and validated using available TRACON data. It reproduces the same
phenomenon as the real system. This model is used to show the impact of
limiting the number of aircrafts in the TRACON. A limited number of aircraft
does not increase delays but reduces the controller's workload and increases
safety.
"
1952,"Probability Bracket Notation, Probability Vectors, Markov Chains and
  Stochastic Processes","  Dirac notation has been widely used for vectors in Hilbert spaces of Quantum
Theories and now also in Information Retrieval. In this paper, we propose to
use Probability Bracket Notation (PBN) for probability modeling. The new
symbols are defined similarly (but not identically) as in Dirac notation. By
using PBN to represent fundamental definitions and theorems for discrete and
continuous random variables, we show that PBN could play a similar role in
probability sample space as Dirac notation in Hilbert space. We also find a
close relation between our system state P-kets and probability vectors in
Markov chains. In the end, we apply PBN to some important stochastic processes,
present the master equation of time-continuous Markov chains in both
Schrodinger and Heisenberg pictures. We identify our system state P-bra with
Doi's state function and Peliti's standard bra. We summarize the similarities
and differences between PBN and Dirac Notation in the two tables of Appendix A.
"
1953,Authentication via wireless networks,"  Personal authentication is an important process we encounter almost every
day; when we are logging on a computer, entering a company where we work, or a
restricted area, when we are using our plastic credit cards to pay for a
service or to complete some other financial transaction, etc. In each of these
processes of personal authentication some kind of magnetic or optical token is
required. But by using novel technologies like mobile computing and wireless
networking, it is possible to avoid carrying multitude of ID cards or
remembering a number of PIN codes. Article shows how to efficiently
authenticate users via Personal Area Networks (PAN) like Bluetooth or IrDA
using commonplace AES (Rijndel) or MD5 encryption. This method can be
implemented on many types of mobile devices like Pocket PC PDA with Windows CE
(Windows Mobile 2003) real-time operating system, or any other customized OS,
so we will explain all components and key features of such basic system.
"
1954,A Sum-Product Model as a Physical Basis for Shadow Fading,"  Shadow fading (slow fading) effects play a central role in mobile
communication system design and analysis. Experimental evidence indicates that
shadow fading exhibits log-normal power distribution almost universally, and
yet it is still not well understood what causes this. In this paper, we propose
a versatile sum-product signal model as a physical basis for shadow fading.
Simulation results imply that the proposed model results in log-normally
distributed local mean power regardless of the distributions of the
interactions in the radio channel, and hence it is capable of explaining the
log-normality in a wide variety of propagation scenarios. The sum-product model
also includes as its special cases the conventional product model as well as
the recently proposed sum model, and improves upon these by: a) being
applicable in both global and local distance scales; b) being more plausible
from physical point of view; c) providing better goodness-of-fit to log-normal
distribution than either of these models.
"
1955,"Induced Hilbert Space, Markov Chain, Diffusion Map and Fock Space in
  Thermophysics","  In this article, we continue to explore Probability Bracket Notation (PBN),
proposed in our previous article. Using both Dirac vector bracket notation
(VBN) and PBN, we define induced Hilbert space and induced sample space, and
propose that there exists an equivalence relation between a Hilbert space and a
sample space constructed from the same base observable(s). Then we investigate
Markov transition matrices and their eigenvectors to make diffusion maps with
two examples: a simple graph theory example, to serve as a prototype of
bidirectional transition operator; a famous text document example in IR
literature, to serve as a tutorial of diffusion map in text document space. We
show that the sample space of the Markov chain and the Hilbert space spanned by
the eigenvectors of the transition matrix are not equivalent. At the end, we
apply our PBN and equivalence proposal to Thermophysics by associating sample
(phase) space with the Hilbert space of a single particle and the Fock space of
many-particle systems.
"
1956,Comparing Architectures of Mobile Applications,"  This article describes various advantages and disadvantages of SMS, WAP, J2ME
and Windows CE technologies in designing mobile applications. In defining the
architecture of any software application it is important to get the best
trade-off between platform's possibilities and design requirements. Achieving
optimum software design is even more important with mobile applications where
all computer resources are limited. Therefore, it is important to have a
comparative analysis of all relevant contemporary approaches in designing
mobile applications. As always, the choice between these technologies is
determined by application requirements and system capabilities.
"
1957,Automatic Annotation of XHTML Pages with Audio Components,"  In this paper we present Deiush, a multimodal system for browsing hypertext
Web documents. The Deiush system is based on our novel approach to
automatically annotate hypertext Web documents (i.e. XHTML pages) with
browsable audio components. It combines two key technologies: (1) middleware
automatic separation of Web documents through structural and semantic analysis
which is annotated with audio components, transforming them into XHTML+VoiceXML
format to represent multimodal dialog; and (2) Opera Browser, an already
standardized browser which we adopt as an interface of the XHTML+VoiceXML
output of annotating. This paper describes the annotation technology of Deiush
and presents an initial system evaluation.
"
1958,Domain Directed Dialogs for Decision Processes,"  The search for a standardized optimum way to communicate using natural
language dialog has involved a lot of research. However, due to the diversity
of communication domains, we think that this is extremely difficult to achieve
and different dialogue management techniques should be applied for different
situations. Our work presents the basis of a communication mechanism that
supports decision processes, is based on decision trees, and minimizes the
number of steps (turn-takes) in the dialogue. The initial dialog workflow is
automatically generated and the user's interaction with the system can also
change the decision tree and create new dialog paths with optimized cost. The
decision tree represents the chronological ordering of the actions (via the
parent-child relationship) and uses an object frame to represent the
information state (capturing the notion of context). This paper presents our
framework, the formalism for interaction and dialogue, and an evaluation of the
system compared to relevant dialog planning frameworks (i.e. finite state
diagrams, frame-based, information state and planning-based dialogue systems).
"
1959,Asynchronous Event-Driven Particle Algorithms,"  We present, in a unifying way, the main components of three asynchronous
event-driven algorithms for simulating physical systems of interacting
particles. The first example, hard-particle molecular dynamics, is well-known.
We also present a recently-developed diffusion kinetic Monte Carlo algorithm,
as well as a novel stochastic molecular-dynamics algorithm that builds on the
Direct Simulation Monte Carlo. We explain how to effectively combine
asynchronous event-driven with classical time-driven or with synchronous
event-driven handling. Finally, we discuss some promises and challenges for
event-driven simulation of realistic physical systems.
"
1960,Numeration systems on a regular language,"  Generalizations of linear numeration systems in which the set of natural
numbers is recognizable by finite automata are obtained by describing an
arbitrary infinite regular language following the lexicographic ordering. For
these systems of numeration, we show that ultimately periodic sets are
recognizable. We also study the translation and the multiplication by constants
as well as the order dependence of the recognizability.
"
1961,The Sources of Certainty in Computation and Formal Systems,"  In his Discourse on the Method of Rightly Conducting the Reason, and Seeking
Truth in the Sciences, Rene Descartes sought ``clear and certain knowledge of
all that is useful in life.'' Almost three centuries later, in ``The
foundations of mathematics,'' David Hilbert tried to ``recast mathematical
definitions and inferences in such a way that they are unshakable.'' Hilbert's
program relied explicitly on formal systems (equivalently, computational
systems) to provide certainty in mathematics. The concepts of computation and
formal system were not defined in his time, but Descartes' method may be
understood as seeking certainty in essentially the same way.
  In this article, I explain formal systems as concrete artifacts, and
investigate the way in which they provide a high level of certainty---arguably
the highest level achievable by rational discourse. The rich understanding of
formal systems achieved by mathematical logic and computer science in this
century illuminates the nature of programs, such as Descartes' and Hilbert's,
that seek certainty through rigorous analysis.
"
1962,About the globular homology of higher dimensional automata,"  We introduce a new simplicial nerve of higher dimensional automata whose
homology groups yield a new definition of the globular homology. With this new
definition, the drawbacks noticed with the construction of math.CT/9902151
disappear. Moreover the important morphisms which associate to every globe its
corresponding branching area and merging area of execution paths become
morphisms of simplicial sets.
"
1963,The branching nerve of HDA and the Kan condition,"  One can associate to any strict globular $\omega$-category three augmented
simplicial nerves called the globular nerve, the branching and the merging
semi-cubical nerves. If this strict globular $\omega$-category is freely
generated by a precubical set, then the corresponding homology theories contain
different informations about the geometry of the higher dimensional automaton
modeled by the precubical set. Adding inverses in this $\omega$-category to any
morphism of dimension greater than 2 and with respect to any composition laws
of dimension greater than 1 does not change these homology theories. In such a
framework, the globular nerve always satisfies the Kan condition. On the other
hand, both branching and merging nerves never satisfy it, except in some very
particular and uninteresting situations. In this paper, we introduce two new
nerves (the branching and merging semi-globular nerves) satisfying the Kan
condition and having conjecturally the same simplicial homology as the
branching and merging semi-cubical nerves respectively in such framework. The
latter conjecture is related to the thin elements conjecture already introduced
in our previous papers.
"
1964,Hypercomputation: computing more than the Turing machine,"  Due to common misconceptions about the Church-Turing thesis, it has been
widely assumed that the Turing machine provides an upper bound on what is
computable. This is not so. The new field of hypercomputation studies models of
computation that can compute more than the Turing machine and addresses their
implications. In this report, I survey much of the work that has been done on
hypercomputation, explaining how such non-classical models fit into the
classical theory of computation and comparing their relative powers. I also
examine the physical requirements for such machines to be constructible and the
kinds of hypercomputation that may be possible within the universe. Finally, I
show how the possibility of hypercomputation weakens the impact of Godel's
Incompleteness Theorem and Chaitin's discovery of 'randomness' within
arithmetic.
"
1965,"Gaussian limits for multidimensional random sequential packing at
  saturation (extended version)","  Consider the random sequential packing model with infinite input and in any
dimension. When the input consists of non-zero volume convex solids we show
that the total number of solids accepted over cubes of volume $\lambda$ is
asymptotically normal as $\lambda \to \infty$. We provide a rate of
approximation to the normal and show that the finite dimensional distributions
of the packing measures converge to those of a mean zero generalized Gaussian
field. The method of proof involves showing that the collection of accepted
solids satisfies the weak spatial dependence condition known as stabilization.
"
1966,"Homotopy invariants of higher dimensional categories and concurrency in
  computer science","  The strict globular $\omega$-categories formalize the execution paths of a
parallel automaton and the homotopies between them. One associates to such (and
any) $\omega$-category $\C$ three homology theories. The first one is called
the globular homology. It contains the oriented loops of $\C$. The two other
ones are called the negative (resp. positive) corner homology. They contain in
a certain manner the branching areas of execution paths or negative corners
(resp. the merging areas of execution paths or positive corners) of $\C$. Two
natural linear maps called the negative (resp. the positive) Hurewicz morphism
from the globular homology to the negative (resp. positive) corner homology are
constructed. We explain the reason why these constructions allow to
reinterprete some geometric problems coming from computer science.
"
1967,Combinatorics of branchings in higher dimensional automata,"  We explore the combinatorial properties of the branching areas of execution
paths in higher dimensional automata. Mathematically, this means that we
investigate the combinatorics of the negative corner (or branching) homology of
a globular $\omega$-category and the combinatorics of a new homology theory
called the reduced branching homology. The latter is the homology of the
quotient of the branching complex by the sub-complex generated by its thin
elements. Conjecturally it coincides with the non reduced theory for higher
dimensional automata, that is $\omega$-categories freely generated by
precubical sets. As application, we calculate the branching homology of some
$\omega$-categories and we give some invariance results for the reduced
branching homology. We only treat the branching side. The merging side, that is
the case of merging areas of execution paths is similar and can be easily
deduced from the branching side.
"
1968,"Two-body problem on a sphere. Reduction, stochasticity, periodic orbits","  We consider the problem of two interacting particles on a sphere. The
potential of the interaction depends on the distance between the particles. The
case of Newtonian-type potentials is studied in most detail. We reduce this
system to a system with two degrees of freedom and give a number of remarkable
periodic orbits. We also discuss integrability and stochastization of the
motion.
"
1969,Galton Board,"  In this paper, we present results of simulations of a model of the Galton
board for various degrees of elasticity of the ball-to-nail collision.
"
1970,"Modeling Endogenous Social Networks: the Example of Emergence and
  Stability of Cooperation without Refusal","  Aggregated phenomena in social sciences and economics are highly dependent on
the way individuals interact. To help understanding the interplay between
socio-economic activities and underlying social networks, this paper studies a
sequential prisoner's dilemma with binary choice. It proposes an analytical and
computational insight about the role of endogenous networks in emergence and
sustainability of cooperation and exhibits an alternative to the choice and
refusal mechanism that is often proposed to explain cooperation. The study
focuses on heterogeneous equilibriums and emergence of cooperation from an
all-defector state that are the two stylized facts that this model successfully
reconstructs.
"
1971,"Preferential attachment in the growth of social networks: the case of
  Wikipedia","  We present an analysis of the statistical properties and growth of the free
on-line encyclopedia Wikipedia. By describing topics by vertices and hyperlinks
between them as edges, we can represent this encyclopedia as a directed graph.
The topological properties of this graph are in close analogy with that of the
World Wide Web, despite the very different growth mechanism. In particular we
measure a scale--invariant distribution of the in-- and out-- degree and we are
able to reproduce these features by means of a simple statistical model. As a
major consequence, Wikipedia growth can be described by local rules such as the
preferential attachment mechanism, though users can act globally on the
network.
"
1972,Stochastic Model for Power Grid Dynamics,"  We introduce a stochastic model that describes the quasi-static dynamics of
an electric transmission network under perturbations introduced by random load
fluctuations, random removing of system components from service, random repair
times for the failed components, and random response times to implement optimal
system corrections for removing line overloads in a damaged or stressed
transmission network. We use a linear approximation to the network flow
equations and apply linear programming techniques that optimize the dispatching
of generators and loads in order to eliminate the network overloads associated
with a damaged system. We also provide a simple model for the operator's
response to various contingency events that is not always optimal due to either
failure of the state estimation system or due to the incorrect subjective
assessment of the severity associated with these events. This further allows us
to use a game theoretic framework for casting the optimization of the
operator's response into the choice of the optimal strategy which minimizes the
operating cost. We use a simple strategy space which is the degree of tolerance
to line overloads and which is an automatic control (optimization) parameter
that can be adjusted to trade off automatic load shed without propagating
cascades versus reduced load shed and an increased risk of propagating
cascades. The tolerance parameter is chosen to describes a smooth transition
from a risk averse to a risk taken strategy...
"
1973,On Quantum Cellular Automata,"  In recent work [quant-ph/0405174] by Schumacher and Werner was discussed an
abstract algebraic approach to a model of reversible quantum cellular automata
(CA) on a lattice. It was used special model of CA based on partitioning scheme
and so there is a question about quantum CA derived from more general, standard
model of classical CA. In present work is considered an approach to definition
of a scheme with ""history"", valid for quantization both irreversible and
reversible classical CA directly using local transition rules. It is used
language of vectors in Hilbert spaces instead of C*-algebras, but results may
be compared in some cases. Finally, the quantum lattice gases, quantum walk and
""bots"" are also discussed briefly.
"
1974,Programmable Quantum Networks with Pure States,"  Modern classical computing devices, except of simplest calculators, have von
Neumann architecture, i.e., a part of the memory is used for the program and a
part for the data. It is likely, that analogues of such architecture are also
desirable for the future applications in quantum computing, communications and
control. It is also interesting for the modern theoretical research in the
quantum information science and raises challenging questions about an
experimental assessment of such a programmable models. Together with some
progress in the given direction, such ideas encounter specific problems arising
from the very essence of quantum laws. Currently are known two different ways
to overcome such problems, sometime denoted as a stochastic and deterministic
approach. The presented paper is devoted to the second one, that is also may be
called the programmable quantum networks with pure states.
  In the paper are discussed basic principles and theoretical models that can
be used for the design of such nano-devices, e.g., the conditional quantum
dynamics, the Nielsen-Chuang ""no-programming theorem, the idea of deterministic
and stochastic quantum gates arrays. Both programmable quantum networks with
finite registers and hybrid models with continuous quantum variables are
considered. As a basic model for the universal programmable quantum network
with pure states and finite program register is chosen a ""Control-Shift""
quantum processor architecture with three buses introduced in earlier works. It
is shown also, that quantum cellular automata approach to the construction of
an universal programmable quantum computer often may be considered as the
particular case of such design.
"
1975,Classical simulators of quantum computers and no-go theorems,"  It is discussed, why classical simulators of quantum computers escape from
some no-go claims like Kochen-Specker, Bell, or recent Conway-Kochen ""Free
Will"" theorems.
"
1976,Self-Testing of Universal and Fault-Tolerant Sets of Quantum Gates,"  We consider the design of self-testers for quantum gates. A self-tester for
the gates F_1,...,F_m is a classical procedure that, given any gates
G_1,...,G_m, decides with high probability if each G_i is close to F_i. This
decision has to rely only on measuring in the computational basis the effect of
iterating the gates on the classical states. It turns out that instead of
individual gates, we can only design procedures for families of gates. To
achieve our goal we borrow some elegant ideas of the theory of program testing:
we characterize the gate families by specific properties, we develop a theory
of robustness for them, and show that they lead to self-testers. In particular
we prove that the universal and fault-tolerant set of gates consisting of a
Hadamard gate, a c-NOT gate, and a phase rotation gate of angle pi/4 is
self-testable.
"
